<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#20219;&#21153;&#65288;AutoTV&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; arXiVeri &#26469;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110; GPT &#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#65292;&#22312;&#34920;&#26684;&#21305;&#37197;&#21644;&#21333;&#20803;&#26684;&#21305;&#37197;&#20219;&#21153;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07968</link><description>&lt;p&gt;
arXiVeri&#65306;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiVeri: Automatic table verification with GPT. (arXiv:2306.07968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#20219;&#21153;&#65288;AutoTV&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; arXiVeri &#26469;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110; GPT &#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#65292;&#22312;&#34920;&#26684;&#21305;&#37197;&#21644;&#21333;&#20803;&#26684;&#21305;&#37197;&#20219;&#21153;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#23545;&#31185;&#23398;&#25991;&#29486;&#20013;&#25968;&#23383;&#25968;&#25454;&#30340;&#20934;&#30830;&#36716;&#24405;&#65292;&#31185;&#23398;&#23478;&#23601;&#19981;&#33021;&#24471;&#20986;&#20934;&#30830;&#32467;&#35770;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23558;&#25968;&#23383;&#25968;&#25454;&#20174;&#19968;&#31687;&#35770;&#25991;&#22797;&#21046;&#21040;&#21478;&#19968;&#31687;&#35770;&#25991;&#30340;&#36807;&#31243;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#20219;&#21153;&#65288;AutoTV&#65289;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#20132;&#21449;&#24341;&#29992;&#24341;&#29992;&#30340;&#26469;&#28304;&#26469;&#39564;&#35777;&#34920;&#26684;&#20013;&#25968;&#23383;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#25903;&#25345;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;arXiVeri&#65292;&#23427;&#21253;&#25324;&#20174; arXiv &#19978;&#30340;&#24320;&#25918;&#33719;&#21462;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#34920;&#26684;&#39564;&#35777;&#22120;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#24615;&#33021;&#30340;&#25351;&#26631;&#65306;&#65288;i&#65289;&#34920;&#26684;&#21305;&#37197;&#65292;&#26088;&#22312;&#35782;&#21035;&#24341;&#29992;&#25991;&#29486;&#20013;&#30340;&#26469;&#28304;&#34920;&#26684;&#19982;&#30446;&#26631;&#34920;&#26684;&#23545;&#24212;&#30340;&#34920;&#26684;&#65292;&#21644;&#65288;ii&#65289;&#21333;&#20803;&#26684;&#21305;&#37197;&#65292;&#26088;&#22312;&#20934;&#30830;&#23450;&#20301;&#30446;&#26631;&#34920;&#26684;&#21644;&#26469;&#28304;&#34920;&#26684;&#20043;&#38388;&#30340;&#20849;&#20139;&#21333;&#20803;&#26684;&#65292;&#24182;&#35782;&#21035;&#23427;&#20204;&#30340;&#34892;&#21644;&#21015;&#32034;&#24341;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; GPT &#30340; AutoTV &#26041;&#27861;&#65292;&#24182;&#22312; arXiVeri &#22522;&#20934;&#27979;&#35797;&#19978;&#23558;&#20854;&#24615;&#33021;&#19982;&#20960;&#31181;&#22522;&#32447;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;&#21305;&#37197;&#21644;&#21333;&#20803;&#26684;&#21305;&#37197;&#20219;&#21153;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of automatic table verification (AutoTV), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, arXiVeri, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) table matching, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) cell matching, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large langua
&lt;/p&gt;</description></item><item><title>MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07952</link><description>&lt;p&gt;
MOFI: &#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07952
&lt;/p&gt;
&lt;p&gt;
MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411; MOFI&#65292;&#26088;&#22312;&#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;MOFI &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#26377;&#20004;&#28857;&#19981;&#21516;&#65306;&#65288;i&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#37197;&#26041;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20174; alt-text &#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992; CLIP &#27169;&#22411;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#22270;&#20687;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20174; web &#19978;&#25366;&#25496;&#30340;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; Image-to-Entities&#65288;I2E&#65289;&#36825;&#19968;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 10 &#20159;&#24352;&#22270;&#20687;&#21644; 200 &#19975;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#28085;&#30422;&#20102;&#37326;&#22806;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110; I2E &#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#23545;&#27604;&#24230;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#35780;&#20272;&#20102;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23567;&#22411;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#32780;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#33021;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#26080;&#27861;&#26681;&#25454;US&#32676;&#20307;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#32676;&#20307;&#36235;&#21183;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2306.07951</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#21709;&#24212;&#30340;&#36136;&#30097;
&lt;/p&gt;
&lt;p&gt;
Questioning the Survey Responses of Large Language Models. (arXiv:2306.07951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#35780;&#20272;&#20102;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23567;&#22411;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#32780;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#33021;&#20943;&#36731;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#26080;&#27861;&#26681;&#25454;US&#32676;&#20307;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#32676;&#20307;&#36235;&#21183;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20197;&#21508;&#31181;&#31185;&#23398;&#21160;&#26426;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#24050;&#32463;&#24314;&#31435;&#30340;&#20840;&#32654;&#31038;&#21306;&#35843;&#26597;&#65288;ACS&#65289;&#65292;&#23601;&#27169;&#22411;&#30340;&#35843;&#26597;&#21709;&#24212;&#32467;&#26524;&#25506;&#31350;&#25152;&#33021;&#20102;&#35299;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#23545;&#21313;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#33539;&#22260;&#20174;&#20960;&#20159;&#21040;&#19968;&#19975;&#20159;&#19981;&#31561;&#65292;&#20351;&#29992;ACS&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#21313;&#19975;&#27425;&#30340;&#27979;&#35797;&#65292;&#31995;&#32479;&#22320;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#23567;&#22411;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#30340;&#20301;&#32622;&#21644;&#26631;&#31614;&#20559;&#24046;&#65292;&#20363;&#22914;&#20559;&#21521;&#20110;&#37319;&#29992;&#26631;&#35760;&#20026;&#8220;A&#8221;&#30340;&#35843;&#26597;&#21709;&#24212;&#12290;&#38543;&#30528;&#27169;&#22411;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;A-&#20559;&#24046;&#34429;&#28982;&#26377;&#25152;&#20943;&#23569;&#65292;&#20294;&#20063;&#36827;&#23637;&#32531;&#24930;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#36890;&#36807;&#38543;&#26426;&#31572;&#26696;&#39034;&#24207;&#26469;&#35843;&#25972;&#36825;&#31181;&#26631;&#35760;&#20559;&#24046;&#65292;&#27169;&#22411;&#20173;&#28982;&#19981;&#20250;&#36235;&#21521;&#20110;&#32654;&#22269;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25110;&#20219;&#20309;&#21487;&#35782;&#21035;&#30340;&#20154;&#21475;&#25490;&#24207;&#12290;&#30456;&#21453;&#65292;&#21508;&#31181;&#27169;&#22411;&#36235;&#21521;&#20110;&#22343;&#21248;&#38543;&#26426;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models increase in capability, researchers have started to conduct surveys of all kinds on these models with varying scientific motivations. In this work, we examine what we can learn from a model's survey responses on the basis of the well-established American Community Survey (ACS) by the U.S. Census Bureau. Evaluating more than a dozen different models, varying in size from a few hundred million to ten billion parameters, hundreds of thousands of times each on questions from the ACS, we systematically establish two dominant patterns. First, smaller models have a significant position and labeling bias, for example, towards survey responses labeled with the letter "A". This A-bias diminishes, albeit slowly, as model size increases. Second, when adjusting for this labeling bias through randomized answer ordering, models still do not trend toward US population statistics or those of any cognizable population. Rather, models across the board trend toward uniformly rando
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#65292;&#23558;&#22768;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#23884;&#20837;&#24335;&#31354;&#38388;&#65292;&#20351;&#29992;&#22522;&#20110;CTC&#30340;&#31354;&#30333;&#36807;&#28388;&#22120;&#26469;&#32553;&#30701;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#35821;&#38899;MultiWoz&#25968;&#25454;&#38598;&#20013;&#65292;SLM&#25552;&#39640;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#26377;&#23454;&#20307;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#37319;&#29992;Speech2Entity&#26816;&#32034;&#22120;&#22686;&#24378;SLM&#12290;&#20351;&#29992;&#27492;&#26816;&#32034;-augmented SLM&#65288;ReSLM&#65289;&#65292;DST&#24615;&#33021;&#24471;&#21040;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;ASR&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07944</link><description>&lt;p&gt;
&#20351;&#29992;Speech2Text&#36866;&#37197;&#22120;&#21644;Speech2Entity&#26816;&#32034;&#22686;&#24378;LLM&#30340;&#35821;&#38899;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding. (arXiv:2306.07944v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#65292;&#23558;&#22768;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#23884;&#20837;&#24335;&#31354;&#38388;&#65292;&#20351;&#29992;&#22522;&#20110;CTC&#30340;&#31354;&#30333;&#36807;&#28388;&#22120;&#26469;&#32553;&#30701;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#35821;&#38899;MultiWoz&#25968;&#25454;&#38598;&#20013;&#65292;SLM&#25552;&#39640;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#26377;&#23454;&#20307;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#37319;&#29992;Speech2Entity&#26816;&#32034;&#22120;&#22686;&#24378;SLM&#12290;&#20351;&#29992;&#27492;&#26816;&#32034;-augmented SLM&#65288;ReSLM&#65289;&#65292;DST&#24615;&#33021;&#24471;&#21040;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;ASR&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#24212;&#29992;&#20110;&#35821;&#38899;&#39046;&#22495;&#65292;&#20294;&#24448;&#24448;&#30001;&#20110;&#38899;&#39057;&#21644;&#35821;&#35328;&#34920;&#31034;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#32570;&#38519;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#65292;&#37319;&#29992;Speech2Text&#36866;&#37197;&#22120;&#23558;&#22768;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#23884;&#20837;&#24335;&#31354;&#38388;&#65292;&#36991;&#20813;&#22768;&#38899;&#20449;&#24687;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22522;&#20110;CTC&#30340;&#31354;&#30333;&#36807;&#28388;&#22120;&#21487;&#20197;&#23558;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#33267;&#25991;&#26412;&#38271;&#24230;&#12290;&#22312;DSTC11&#25361;&#25112;&#36187;&#30340;&#35821;&#38899;MultiWoz&#25968;&#25454;&#38598;&#20013;&#65292;SLM&#26174;&#30528;&#25552;&#39640;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#24615;&#33021;&#65288;&#20174;24.7&#65285;&#25552;&#39640;&#21040;28.4&#65285;&#30340;&#20934;&#30830;&#29575;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#26377;&#23454;&#20307;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#37319;&#29992;Speech2Entity&#26816;&#32034;&#22120;&#22686;&#24378;SLM&#65292;&#35813;&#26816;&#32034;&#22120;&#20351;&#29992;&#35821;&#38899;&#26816;&#32034;&#30456;&#20851;&#23454;&#20307;&#65292;&#24182;&#23558;&#23427;&#20204;&#28155;&#21152;&#21040;&#21407;&#22987;SLM&#36755;&#20837;&#20013;&#20316;&#20026;&#21069;&#32512;&#12290;&#20351;&#29992;&#36825;&#31181;&#26816;&#32034;-augmented SLM&#65288;ReSLM&#65289;&#65292;DST&#30340;&#24615;&#33021;&#25552;&#39640;&#33267;34.6&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#20197;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#22686;&#24378;ASR&#20219;&#21153;&#21487;&#20197;&#23558;ASR&#24615;&#33021;&#20174;9.4&#65285;&#25552;&#39640;&#21040;8.5&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been applied in the speech domain, often incurring a performance drop due to misaligned between speech and language representations. To bridge this gap, we propose a joint speech and language model (SLM) using a Speech2Text adapter, which maps speech into text token embedding space without speech information loss. Additionally, using a CTC-based blank-filtering, we can reduce the speech sequence length to that of text. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the dialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to address errors on rare entities, we augment SLM with a Speech2Entity retriever, which uses speech to retrieve relevant entities, and then adds them to the original SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the DST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with the dialog understanding task improves the ASR performance from 9.4% to 8.5% WE
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102; FOOCTTS&#65292;&#19968;&#31181;&#20026;&#36275;&#29699;&#35299;&#35828;&#21592;&#29983;&#25104;&#24102;&#26377;&#32972;&#26223;&#20154;&#32676;&#22122;&#38899;&#30340;&#35821;&#38899;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;15&#20998;&#38047;&#36275;&#29699;&#35299;&#35828;&#21592;&#24405;&#38899;&#20869;&#29983;&#25104;&#24102;&#26377;&#20854;&#22768;&#23398;&#29615;&#22659;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.07936</link><description>&lt;p&gt;
FOOCTTS: &#20026;&#36275;&#29699;&#35299;&#35828;&#21592;&#29983;&#25104;&#24102;&#26377;&#22768;&#38899;&#29615;&#22659;&#30340;&#38463;&#25289;&#20271;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator. (arXiv:2306.07936v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102; FOOCTTS&#65292;&#19968;&#31181;&#20026;&#36275;&#29699;&#35299;&#35828;&#21592;&#29983;&#25104;&#24102;&#26377;&#32972;&#26223;&#20154;&#32676;&#22122;&#38899;&#30340;&#35821;&#38899;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;15&#20998;&#38047;&#36275;&#29699;&#35299;&#35828;&#21592;&#24405;&#38899;&#20869;&#29983;&#25104;&#24102;&#26377;&#20854;&#22768;&#23398;&#29615;&#22659;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461; FOOCTTS&#65292;&#19968;&#31181;&#20026;&#36275;&#29699;&#35299;&#35828;&#21592;&#29983;&#25104;&#24102;&#26377;&#32972;&#26223;&#20154;&#32676;&#22122;&#38899;&#30340;&#35821;&#38899;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#20174;&#29992;&#25143;&#22788;&#33719;&#21462;&#25991;&#26412;&#65292;&#24212;&#29992;&#25991;&#26412;&#39044;&#22788;&#29702;&#65288;&#22914;&#21152;&#20803;&#38899;&#65289;&#65292;&#25509;&#30528;&#20351;&#29992;&#35299;&#35828;&#21592;&#30340;&#35821;&#38899;&#21512;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#26631;&#35760;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;CTC&#20998;&#27573;&#65292;&#29992;&#20110;&#21305;&#37197;&#35821;&#38899;&#30340;&#36716;&#24405;&#20803;&#38899;&#65292;&#20197;&#21450;&#24494;&#35843;TTS&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;15&#20998;&#38047;&#36275;&#29699;&#35299;&#35828;&#21592;&#24405;&#38899;&#20869;&#29983;&#25104;&#24102;&#26377;&#20854;&#22768;&#23398;&#29615;&#22659;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents FOOCTTS, an automatic pipeline for a football commentator that generates speech with background crowd noise. The application gets the text from the user, applies text pre-processing such as vowelization, followed by the commentator's speech synthesizer. Our pipeline included Arabic automatic speech recognition for data labeling, CTC segmentation, transcription vowelization to match speech, and fine-tuning the TTS. Our system is capable of generating speech with its acoustic environment within limited 15 minutes of football commentator recording. Our prototype is generalizable and can be easily applied to different domains and languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#29992;&#20110;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07935</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#20301;&#32622;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Representation Learning for Social Post Location Inference. (arXiv:2306.07935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#29992;&#20110;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#26469;&#25512;&#26029;&#22320;&#29702;&#20301;&#32622;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24212;&#29992;&#31243;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#20135;&#21697;&#33829;&#38144;&#12289;&#20852;&#36259;&#28857;&#25512;&#33616;&#21450;COVID-19&#30340;&#36861;&#36394;&#12290;&#26412;&#30740;&#31350;&#20174;Instagram&#25910;&#38598;&#20102;&#24102;&#26377;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#26631;&#31614;&#30340;&#23454;&#38469;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#23427;&#33021;&#22815;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;MRLF&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#23383;&#31526;&#24863;&#30693;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring geographic locations via social posts is essential for many practical location-based applications such as product marketing, point-of-interest recommendation, and infector tracking for COVID-19. Unlike image-based location retrieval or social-post text embedding-based location inference, the combined effect of multi-modal information (i.e., post images, text, and hashtags) for social post positioning receives less attention. In this work, we collect real datasets of social posts with images, texts, and hashtags from Instagram and propose a novel Multi-modal Representation Learning Framework (MRLF) capable of fusing different modalities of social posts for location inference. MRLF integrates a multi-head attention mechanism to enhance location-salient information extraction while significantly improving location inference compared with single domain-based methods. To overcome the noisy user-generated textual content, we introduce a novel attention-based character-aware module 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#20013;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25512;&#32763;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.07934</link><description>&lt;p&gt;
BoardgameQA: &#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information. (arXiv:2306.07934v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#20013;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25512;&#32763;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#33258;&#21160;&#25512;&#29702;&#26159;&#35768;&#22810;&#28508;&#22312;NLP&#24212;&#29992;&#21644;&#24320;&#21457;&#24378;&#22823;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#26412;&#25991;&#38024;&#23545;&#23384;&#22312;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#25512;&#29702;&#38382;&#39064;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#32463;&#20856;&#30340;&#21487;&#25512;&#32763;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset ca
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;fine-tune&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#30005;&#20449;&#39046;&#22495;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#20197;&#20415;&#35782;&#21035;3GPP&#26631;&#20934;&#24037;&#20316;&#32452;&#12290;</title><link>http://arxiv.org/abs/2306.07933</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#30005;&#20449;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Understanding Telecom Language Through Large Language Models. (arXiv:2306.07933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07933
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;fine-tune&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#30005;&#20449;&#39046;&#22495;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#20197;&#20415;&#35782;&#21035;3GPP&#26631;&#20934;&#24037;&#20316;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#33258;&#21160;&#21270;&#30005;&#20449;&#32593;&#32476;&#30340;&#35768;&#22810;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#30001;&#20110;&#20986;&#29616;&#20102;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#36825;&#19968;&#36827;&#23637;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#21487;&#33021;&#24615;&#30340;&#23454;&#29616;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#33258;&#25105;&#27835;&#29702;&#21644;&#20114;&#21160;&#24335;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;LLMs&#30340;&#33539;&#20363;&#24212;&#29992;&#20110;&#30005;&#20449;&#39046;&#22495;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;fine-tune&#20960;&#20010;LLMs&#65292;&#21253;&#25324;BERT&#12289;Distilled BERT&#12289;RoBERTa&#21644;GPT-2&#65292;&#20197;&#36866;&#24212;&#30005;&#20449;&#39046;&#22495;&#30340;&#35821;&#35328;&#65292;&#24182;&#28436;&#31034;&#29992;&#20363;&#26469;&#35782;&#21035;&#31532;&#19977;&#20195;&#21512;&#20316;&#20249;&#20276;&#39033;&#30446;&#65288;3GPP&#65289;&#26631;&#20934;&#24037;&#20316;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achiev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.07932</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop through Chain-of-Thought. (arXiv:2306.07932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#26377;&#26102;&#22312;&#38271;&#26399;&#25110;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20854;&#24369;&#28857;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#19981;&#24635;&#33021;&#24471;&#21040;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#30340;&#29702;&#24819;&#31572;&#26696;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#65288;MCS&#65289;&#8212;&#8212;&#19968;&#20010;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#22686;&#24378;&#30340;&#20154;&#24037;&#21442;&#19982;&#31995;&#32479;&#65292;&#25506;&#31350;&#20102;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#22914;&#20309;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#26356;&#36827;&#19968;&#27493;&#32771;&#34385;&#21040;&#26377;&#20154;&#21442;&#19982;&#30340;&#31995;&#32479;&#19981;&#20165;&#35201;&#25552;&#39640;&#24615;&#33021;&#65292;&#36824;&#35201;&#25511;&#21046;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21476;&#20856;&#32463;&#27982;&#29702;&#35770;&#30340;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#25104;&#26412;&#25928;&#29992;&#20998;&#26512;&#27169;&#22411;&#65288;CAMLOP&#65289;&#26469;&#20998;&#26512;&#12289;&#37327;&#21270;&#21644;&#24179;&#34913;&#25928;&#29992;&#21644;&#30456;&#24212;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;12&#20010;&#25968;&#25454;&#38598;&#23545;MCS&#21644;CAMLOP&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A signi
&lt;/p&gt;</description></item><item><title>&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.07929</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21322;&#21442;&#25968;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07929
&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31185;&#23398;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#12290;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;REMEMBERER&#33021;&#22815;&#21033;&#29992;&#36807;&#21435;&#21095;&#38598;&#30340;&#32463;&#39564;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#30446;&#26631;&#25552;&#20379;&#20248;&#24322;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#36825;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#23454;&#20363;&#25110;&#20855;&#26377;&#30701;&#26242;&#24037;&#20316;&#35760;&#24518;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#32463;&#39564;&#35760;&#24518;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLEM&#65289;&#26469;&#26356;&#26032;&#35760;&#24518;&#12290;&#22240;&#27492;&#65292;&#25972;&#20010;&#31995;&#32479;&#21487;&#20197;&#20174;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#24494;&#35843;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#20854;&#33021;&#21147;&#12290;&#20197;&#27492;&#26041;&#24335;&#65292;&#25152;&#25552;&#20986;&#30340;REMEMBERER&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#22312;&#20004;&#20010;RL&#20219;&#21153;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#19981;&#21516;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#38598;&#30340;&#24179;&#22343;&#32467;&#26524;&#23545;&#20110;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#30740;&#31350;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#21487;&#23398;&#20064;&#24615;&#26465;&#20214;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#36793;&#30028;&#65292;&#24182;&#22312;&#21512;&#25104;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.07926</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#30340;&#19968;&#31181;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Unsupervised Speech Recognition. (arXiv:2306.07926v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#30740;&#31350;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#21487;&#23398;&#20064;&#24615;&#26465;&#20214;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#36793;&#30028;&#65292;&#24182;&#22312;&#21512;&#25104;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#25351;&#30340;&#26159;&#20174;&#26410;&#37197;&#23545;&#30340;&#21547;&#26377;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#20854;&#23646;&#24615;&#65292;&#24182;&#35299;&#20915;&#22914;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#30740;&#31350;ASR-U&#31995;&#32479;&#30340;&#23646;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;ASR-U&#30340;&#21508;&#31181;&#21487;&#23398;&#20064;&#24615;&#26465;&#20214;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#36793;&#30028;&#12290;&#38024;&#23545;&#26377;&#19977;&#31867;&#36716;&#31227;&#22270;&#30340;&#21512;&#25104;&#35821;&#35328;&#30340;&#24191;&#27867;ASR-U&#23454;&#39564;&#20026;&#25105;&#20204;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#65288;&#20195;&#30721;&#21487;&#22312;cactuswiththoughts / UnsupASRTheory.git&#20013;&#33719;&#24471;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised speech recognition (ASR-U) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing from studying their properties and addressing such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of ASR-U systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory (code available at cactuswiththoughts/UnsupASRTheory.git).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;WebGLM&#26159;&#19968;&#31181;&#22522;&#20110;GLM&#30340;&#32593;&#32476;&#38382;&#31572;&#31995;&#32479;&#65292;&#20351;&#29992;LLM-augmented&#26816;&#32034;&#22120;&#12289;&#24341;&#23548;&#24335;&#29983;&#25104;&#22120;&#21644;&#20154;&#31867;&#20559;&#22909;&#24863;&#30693;&#35780;&#20998;&#22120;&#31561;&#31574;&#30053;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#27604;WebGPT&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07906</link><description>&lt;p&gt;
WebGLM: &#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#39640;&#25928;&#32593;&#32476;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences. (arXiv:2306.07906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;WebGLM&#26159;&#19968;&#31181;&#22522;&#20110;GLM&#30340;&#32593;&#32476;&#38382;&#31572;&#31995;&#32479;&#65292;&#20351;&#29992;LLM-augmented&#26816;&#32034;&#22120;&#12289;&#24341;&#23548;&#24335;&#29983;&#25104;&#22120;&#21644;&#20154;&#31867;&#20559;&#22909;&#24863;&#30693;&#35780;&#20998;&#22120;&#31561;&#31574;&#30053;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#27604;WebGPT&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WebGLM&#65292;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#30340;&#32593;&#32476;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#32593;&#32476;&#25628;&#32034;&#21644;&#26816;&#32034;&#33021;&#21147;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26816;&#32034;&#22120;&#12289;&#24341;&#23548;&#24335;&#29983;&#25104;&#22120;&#21644;&#20154;&#31867;&#20559;&#22909;&#24863;&#30693;&#35780;&#20998;&#22120;&#31561;&#31574;&#30053;&#24320;&#21457;&#20102;WebGLM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#35299;&#20915;&#20102;WebGPT&#65288;OpenAI&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;WebGLM&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#32593;&#32476;&#22686;&#24378;QA&#31995;&#32479;&#30340;&#31995;&#32479;&#24615;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#32500;&#20154;&#31867;&#35780;&#20272;&#21644;&#23450;&#37327;&#21066;&#24369;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;WebGLM&#35774;&#35745;&#20248;&#20110;&#29616;&#26377;&#31995;&#32479;&#12290;&#19982;WebGPT&#65288;13B&#65289;&#30456;&#27604;&#65292;&#20351;&#29992;100&#20159;&#21442;&#25968;&#30340;GLM&#65288;10B&#65289;&#30340;WebGLM&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#21487;&#19982;WebGPT&#65288;175B&#65289;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#24773;&#24863;&#27169;&#22411;&#30340;&#26368;&#20840;&#38754;&#30340;&#24320;&#25918;&#24335;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;79&#20010;&#25163;&#21160;&#36873;&#23450;&#30340;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#35206;&#30422;&#20102;27&#31181;&#35821;&#35328;&#65292;&#20195;&#34920;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.07902</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#24773;&#24863;&#25968;&#25454;&#38598;&#21644;&#22810;&#26041;&#38754;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark. (arXiv:2306.07902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#24773;&#24863;&#27169;&#22411;&#30340;&#26368;&#20840;&#38754;&#30340;&#24320;&#25918;&#24335;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;79&#20010;&#25163;&#21160;&#36873;&#23450;&#30340;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#35206;&#30422;&#20102;27&#31181;&#35821;&#35328;&#65292;&#20195;&#34920;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#27169;&#22411;&#35757;&#32451;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#24320;&#21457;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#22312;&#25991;&#21270;&#30456;&#20851;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#23588;&#20854;&#22914;&#27492;&#12290;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#23601;&#26159;&#19968;&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#24773;&#24863;&#26631;&#35760;&#21487;&#33021;&#38750;&#24120;&#24494;&#22937;&#19988;&#28145;&#28145;&#26893;&#26681;&#20110;&#25991;&#21270;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35757;&#32451;&#24773;&#24863;&#27169;&#22411;&#30340;&#26368;&#20840;&#38754;&#30340;&#24320;&#25918;&#24335;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#30001;79&#20010;&#25163;&#21160;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#20005;&#26684;&#30340;&#36136;&#37327;&#26631;&#20934;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#36229;&#36807;350&#20010;&#25968;&#25454;&#38598;&#20013;&#36873;&#20986;&#30340;&#12290;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#20102;27&#31181;&#35821;&#35328;&#65292;&#20195;&#34920;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#12290;&#25968;&#25454;&#38598;&#21487;&#20197;&#20351;&#29992;&#20960;&#20010;&#35821;&#35328;&#21644;&#21151;&#33021;&#29305;&#24449;&#36827;&#34892;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#65292;&#24635;&#32467;&#20102;&#23545;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#12289;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#38598;&#38598;&#21512;&#21644;&#24494;&#35843;&#36827;&#34892;&#30340;&#25968;&#30334;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tunin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20247;&#21253;&#24037;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;33-46%&#30340;&#20247;&#21253;&#24037;&#20154;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#20351;&#29992;&#20102;LLMs&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#21152;&#35686;&#24789;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07899</link><description>&lt;p&gt;
&#20154;&#24037;&#20154;&#24037;&#26234;&#33021;&#65306;&#20247;&#21253;&#24037;&#20154;&#24191;&#27867;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks. (arXiv:2306.07899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20247;&#21253;&#24037;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;33-46%&#30340;&#20247;&#21253;&#24037;&#20154;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#20351;&#29992;&#20102;LLMs&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#21152;&#35686;&#24789;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#38750;&#24120;&#20986;&#33394;&#30340;&#25968;&#25454;&#26631;&#27880;&#24037;&#20855;&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#35843;&#26597;&#21644;&#23454;&#39564;&#25968;&#25454;&#12290;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20154;&#31867;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#23545;&#20110;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#20854;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20247;&#21253;&#26159;&#33719;&#21462;&#20154;&#31867;&#27880;&#37322;&#30340;&#19968;&#31181;&#37325;&#35201;&#12289;&#24265;&#20215;&#30340;&#26041;&#24335;&#65292;&#20294;&#20247;&#21253;&#24037;&#20154;&#26377;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#22686;&#21152;&#20854;&#29983;&#20135;&#29575;&#21644;&#25910;&#20837;&#30340;&#36130;&#21153;&#28608;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#20182;&#20204;&#30340;&#27880;&#37322;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#20247;&#21253;&#24037;&#20154;&#20351;&#29992;LLMs&#30340;&#26222;&#36941;&#24615;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;Amazon Mechanical Turk&#19978;&#37325;&#26032;&#36816;&#34892;&#20102;&#19968;&#39033;&#25991;&#25688;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20987;&#38190;&#26816;&#27979;&#21644;&#21512;&#25104;&#25991;&#26412;&#20998;&#31867;&#30340;&#32452;&#21512;&#65292;&#20272;&#35745;33-46%&#30340;&#20247;&#21253;&#24037;&#20154;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#20351;&#29992;&#20102;LLMs&#12290;&#23613;&#31649;&#23545;&#20110;&#20854;&#20182;&#19981;&#22826;&#21451;&#22909;&#30340;LLM&#20219;&#21153;&#30340;&#27867;&#21270;&#23578;&#19981;&#28165;&#26970;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21628;&#21505;&#24179;&#21488;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#20107;&#26631;&#27880;&#30340;&#20154;&#20204;&#26356;&#21152;&#35686;&#24789;LLM&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researche
&lt;/p&gt;</description></item><item><title>ReadProbe &#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#20197;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.07875</link><description>&lt;p&gt;
ReadProbe: &#19968;&#31181;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading. (arXiv:2306.07875v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07875
&lt;/p&gt;
&lt;p&gt;
ReadProbe &#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#20197;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#19981;&#23454;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#20256;&#25773;&#65292;&#20154;&#20204;&#38656;&#35201;&#24037;&#20855;&#26469;&#24110;&#21161;&#20182;&#20204;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#30340;&#21487;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#27178;&#21521;&#38405;&#35835;&#26159;&#19968;&#31181;&#36328;&#21442;&#32771;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#31574;&#30053;&#65292;&#21487;&#33021;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21517;&#20026; ReadProbe&#65292;&#23427;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#65292;&#30001; OpenAI &#30340;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24517;&#24212;&#25628;&#32034;&#24341;&#25806;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#33021;&#22815;&#20026;&#27178;&#21521;&#38405;&#35835;&#29983;&#25104;&#26377;&#29992;&#30340;&#38382;&#39064;&#65292;&#25628;&#23547;&#32593;&#32476;&#19978;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#20135;&#29983;&#33391;&#22909;&#24402;&#22240;&#30340;&#31572;&#26696;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;&#25105;&#20204;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#20110; Web &#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#28436;&#31034;&#20102; ReadProbe &#22914;&#20309;&#24110;&#21161;&#20943;&#23569;&#34987;&#34394;&#20551;&#20449;&#24687;&#35823;&#23548;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/DakeZhang1998/ReadProbe &#19978;&#33719;&#24471;&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#29256;&#26412;&#36194;&#24471;&#20102;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#34394;&#20551;&#20449;&#24687;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#19968;&#31561;&#22870;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth and spread of online misinformation, people need tools to help them evaluate the credibility and accuracy of online information. Lateral reading, a strategy that involves cross-referencing information with multiple sources, may be an effective approach to achieving this goal. In this paper, we present ReadProbe, a tool to support lateral reading, powered by generative large language models from OpenAI and the Bing search engine. Our tool is able to generate useful questions for lateral reading, scour the web for relevant documents, and generate well-attributed answers to help people better evaluate online information. We made a web-based application to demonstrate how ReadProbe can help reduce the risk of being misled by false information. The code is available at https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won the first prize in a national AI misinformation hackathon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#33014;&#22218;&#32593;&#32476;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#23383;&#31526;&#32423;&#23545;&#25239;&#24615;&#36807;&#31243;&#29983;&#25104;&#20154;&#24037;&#26679;&#26412;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;99.08&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07845</link><description>&lt;p&gt;
&#23545;&#25239;&#33014;&#22218;&#32593;&#32476;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adversarial Capsule Networks for Romanian Satire Detection and Sentiment Analysis. (arXiv:2306.07845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#33014;&#22218;&#32593;&#32476;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#23383;&#31526;&#32423;&#23545;&#25239;&#24615;&#36807;&#31243;&#29983;&#25104;&#20154;&#24037;&#26679;&#26412;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;99.08&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26816;&#27979;&#19982;&#24773;&#24863;&#20998;&#26512;&#26159;&#30740;&#31350;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#35773;&#21050;&#35821;&#27668;&#21644;&#25552;&#21462;&#19982;&#20854;&#30446;&#26631;&#30456;&#20851;&#24773;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#33014;&#22218;&#32593;&#32476;&#25913;&#36827;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;NLP&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35773;&#21050;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#23383;&#31526;&#32423;&#23545;&#25239;&#24615;&#36807;&#31243;&#29983;&#25104;&#20154;&#24037;&#26679;&#26412;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;99.08&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satire detection and sentiment analysis are intensively explored natural language processing (NLP) tasks that study the identification of the satirical tone from texts and extracting sentiments in relationship with their targets. In languages with fewer research resources, an alternative is to produce artificial examples based on character-level adversarial processes to overcome dataset size limitations. Such samples are proven to act as a regularization method, thus improving the robustness of models. In this work, we improve the well-known NLP models (i.e., Convolutional Neural Networks, Long Short-Term Memory (LSTM), Bidirectional LSTM, Gated Recurrent Units (GRUs), and Bidirectional GRUs) with adversarial training and capsule networks. The fine-tuned models are used for satire detection and sentiment analysis tasks in the Romanian language. The proposed framework outperforms the existing methods for the two tasks, achieving up to 99.08% accuracy, thus confirming the improvements ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.07799</link><description>&lt;p&gt;
ChatGPT&#19982;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#65306;&#21487;&#25511;&#25991;&#26412;&#25688;&#35201;&#21644;&#21477;&#23376;&#39118;&#26684;&#36716;&#31227;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20197;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#20174;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#24341;&#36215;&#20102;&#23186;&#20307;&#30340;&#37325;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#65288;&#19987;&#23478;&#19982;&#19968;&#33324;&#20154;&#65289;&#21644;&#20889;&#20316;&#39118;&#26684;&#65288;&#27491;&#24335;&#19982;&#38750;&#27491;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#35832;&#22914;&#21333;&#35789;&#31867;&#22411;&#20998;&#24067;&#31561;&#20960;&#20010;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403; ChatGPT &#23558;&#25991;&#26412;&#36866;&#24212;&#29305;&#23450;&#39118;&#26684;&#26102;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25386;&#23041;&#35821;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#21644;&#32431;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#27861;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#20204;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#29992;&#20110;&#23545;&#29616;&#26377;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.07790</link><description>&lt;p&gt;
NoCoLA&#65306;&#25386;&#23041;&#35821;&#35328;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NoCoLA: The Norwegian Corpus of Linguistic Acceptability. (arXiv:2306.07790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25386;&#23041;&#35821;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#21644;&#32431;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#27861;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#20204;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#29992;&#20110;&#23545;&#29616;&#26377;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35780;&#20272;&#20854;&#35821;&#27861;&#29702;&#35299;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25386;&#23041;&#25968;&#25454;&#38598;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#20854;&#20013;NoCoLA_class&#26159;&#19968;&#20010;&#24102;&#30417;&#30563;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#26088;&#22312;&#21306;&#20998;&#21487;&#25509;&#21463;&#21644;&#19981;&#21487;&#25509;&#21463;&#30340;&#21477;&#23376;&#65307;&#32780;NoCoLA_zero&#21017;&#26159;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#22312;&#38646;&#26679;&#26412;&#26041;&#24335;&#19979;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#35821;&#27861;&#21028;&#26029;&#30340;&#32431;&#35786;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#29992;&#23427;&#20204;&#26469;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been a surge of large language models for Norwegian in recent years, we lack any tool to evaluate their understanding of grammaticality. We present two new Norwegian datasets for this task. NoCoLA_class is a supervised binary classification task where the goal is to discriminate between acceptable and non-acceptable sentences. On the other hand, NoCoLA_zero is a purely diagnostic task for evaluating the grammatical judgement of a language model in a completely zero-shot manner, i.e. without any further training. In this paper, we describe both datasets in detail, show how to use them for different flavors of language models, and conduct a comparative study of the existing Norwegian language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07786</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#26377;&#25928;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#29575;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38598;&#25104;&#21040;&#31649;&#36947;&#20013;&#65292;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#23545;&#20110;&#20027;&#39064;&#24314;&#27169;&#65292;&#25105;&#20204;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#12289;&#22522;&#20110;&#21521;&#37327;&#23884;&#20837;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20803;&#32032;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#27604;&#36825;&#20010;&#20219;&#21153;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficiency of natural language processing has improved dramatically with the advent of machine learning models, particularly neural network-based solutions. However, some tasks are still challenging, especially when considering specific domains. In this paper, we present a cloud-based system that can extract insights from customer reviews using machine learning methods integrated into a pipeline. For topic modeling, our composite model uses transformer-based neural networks designed for natural language processing, vector embedding-based keyword extraction, and clustering. The elements of our model have been integrated and further developed to meet better the requirements of efficient information extraction, topic modeling of the extracted information, and user needs. Furthermore, our system can achieve better results than this task's existing topic modeling and keyword extraction solutions. Our approach is validated and compared with other state-of-the-art methods using publicly a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#24335;&#20998;&#35299;&#23376;&#35789;&#32534;&#30721;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#34987;&#31216;&#20026;&#8220;&#22240;&#24335;&#20998;&#35299;&#22120;&#8221;&#65292;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#24314;&#27169;&#21644;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#30456;&#36739;&#20110;&#24120;&#29992;&#30340;BPE&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07764</link><description>&lt;p&gt;
&#22240;&#24335;&#20998;&#35299;&#23376;&#35789;&#32534;&#30721;&#30340;&#20998;&#35789;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tokenization with Factorized Subword Encoding. (arXiv:2306.07764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#24335;&#20998;&#35299;&#23376;&#35789;&#32534;&#30721;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#34987;&#31216;&#20026;&#8220;&#22240;&#24335;&#20998;&#35299;&#22120;&#8221;&#65292;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#24314;&#27169;&#21644;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#30456;&#36739;&#20110;&#24120;&#29992;&#30340;BPE&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#34920;&#31034;&#20173;&#28982;&#20381;&#36182;&#20110;&#31616;&#21333;&#21644;&#36138;&#23146;&#30340;&#23376;&#35789;&#20998;&#35789;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;VQ-VAE&#27169;&#22411;&#23558;&#23376;&#35789;&#22240;&#23376;&#21270;&#20026;&#31163;&#25955;&#19977;&#20803;&#32452;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#35789;&#26041;&#27861;&#34987;&#31216;&#20026;&#8220;&#22240;&#24335;&#20998;&#35299;&#22120;&#8221;&#65292;&#24182;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#24314;&#27169;&#21644;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#24120;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;(BPE)&#20998;&#35789;&#31639;&#27861;&#26356;&#36866;&#21512;&#21644;&#26356;&#31283;&#20581;&#20110;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete triplets using a VQ-VAE model. The effectiveness of the proposed tokenization method, referred to as the Factorizer, is evaluated on language modeling and morpho-syntactic tasks for 7 diverse languages. Results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NAVER LABS Europe&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#21442;&#25968;&#25928;&#29575;&#35299;&#20915;&#26041;&#26696;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26368;&#22823;&#21270;&#32763;&#35793;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;IWSLT 2023&#20302;&#36164;&#28304;&#36187;&#36947;&#20013;Tamasheq-French&#21644;Quechua-Spanish&#35821;&#38899;&#32763;&#35793;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07763</link><description>&lt;p&gt;
NAVER LABS Europe&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#22312;IWSLT 2023&#20302;&#36164;&#28304;&#36187;&#36947;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track. (arXiv:2306.07763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NAVER LABS Europe&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#21442;&#25968;&#25928;&#29575;&#35299;&#20915;&#26041;&#26696;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26368;&#22823;&#21270;&#32763;&#35793;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;IWSLT 2023&#20302;&#36164;&#28304;&#36187;&#36947;&#20013;Tamasheq-French&#21644;Quechua-Spanish&#35821;&#38899;&#32763;&#35793;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NAVER LABS Europe&#22312;IWSLT 2023&#20302;&#36164;&#28304;&#36187;&#36947;&#20013;Tamasheq-French&#21644;Quechua-Spanish&#35821;&#38899;&#32763;&#35793;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#21442;&#25968;&#25928;&#29575;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;Tamasheq&#20027;&#35201;&#25552;&#20132;&#29256;&#26412;&#22312;IWSLT 2022&#27979;&#35797;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;7.5 BLEU&#20998;&#25968;&#65292;&#24182;&#22312;&#20170;&#24180;&#30340;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;23.6 BLEU&#65292;&#36229;&#36807;&#31532;&#20108;&#21517;&#21442;&#36187;&#32773;7.7&#20998;&#12290;&#23545;&#20110;Quechua&#65292;&#23613;&#31649;&#21482;&#26377;&#20004;&#23567;&#26102;&#30340;&#32763;&#35793;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#26159;&#25490;&#21517;&#31532;&#19968;&#65292;&#24182;&#36798;&#21040;17.7 BLEU&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#35821;&#35328;&#26550;&#26500;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#20063;&#24456;&#26377;&#31454;&#20105;&#21147;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;IWSLT 2021&#22810;&#35821;&#35328;&#36187;&#36947;&#26368;&#20339;&#26410;&#21152;&#38480;&#21046;&#30340;&#25552;&#20132;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.07691</link><description>&lt;p&gt;
StyleTTS 2&#65306;&#36890;&#36807;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleTTS2&#65292;&#19968;&#31181;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;TTS&#21512;&#25104;&#12290; StyleTTS 2&#36890;&#36807;&#23558;&#26679;&#24335;&#24314;&#27169;&#20026;&#28508;&#22312;&#30340;&#38543;&#26426;&#21464;&#37327;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#26080;&#38656;&#21442;&#32771;&#35821;&#38899;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#28508;&#22312;&#25193;&#25955;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#22810;&#26679;&#21270;&#35821;&#38899;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;SLMs&#65288;&#20363;&#22914;WavLM&#65289;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#38899;&#30340;&#33258;&#28982;&#24230;&#12290; StyleTTS 2&#22312;&#21333;&#25196;&#22768;&#22120;LJSpeech&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#20154;&#31867;&#24405;&#38899;&#65292;&#24182;&#22312;&#22810;&#25196;&#22768;&#22120;VCTK&#25968;&#25454;&#38598;&#19978;&#19982;&#20043;&#21305;&#37197;&#65292;&#32463;&#36807;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#21592;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;LibriTTS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20197;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21450;&#24494;&#35843;&#20013;&#20173;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;10%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.07664</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21450;&#24494;&#35843;&#20013;&#20173;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;10%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#34920;&#29616;&#30340;&#24433;&#21709;&#19968;&#30452;&#26159;&#19968;&#20010;&#20105;&#35770;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24494;&#35843;&#26041;&#27861;&#32467;&#21512;&#22238;&#35793;&#22312;7&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#31867;&#22411;&#65292;&#28085;&#30422;&#21333;&#21477;&#21644;&#21477;&#23376;&#23545;&#20219;&#21153;&#12290;&#19982;&#20248;&#20808;&#30340;&#20551;&#35774;&#30456;&#21453;&#65292;&#21363;&#25968;&#25454;&#22686;&#24378;&#23545;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#34920;&#29616;&#27809;&#26377;&#36129;&#29486;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25345;&#32493;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;&#22312;&#26368;&#26377;&#21033;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#24494;&#35843;&#24615;&#33021;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#19979;&#25552;&#39640;10%&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20986;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models (LMs) have made remarkable progress in advancing the field of natural language processing (NLP). However, the impact of data augmentation (DA) techniques on the fine-tuning (FT) performance of these LMs has been a topic of ongoing debate. In this study, we evaluate the effectiveness of three different FT methods in conjugation with back-translation across an array of 7 diverse NLP tasks, including classification and regression types, covering single-sentence and sentence-pair tasks. Contrary to prior assumptions that DA does not contribute to the enhancement of LMs' FT performance, our findings reveal that continued pre-training on augmented data can effectively improve the FT performance of the downstream tasks. In the most favourable case, continued pre-training improves the performance of FT by more than 10% in the few-shot learning setting. Our finding highlights the potential of DA as a powerful tool for bolstering LMs' performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#21508;&#21521;&#24322;&#24615;&#19981;&#20165;&#26159;&#20248;&#21270;&#38271;&#23614;&#20998;&#24067;&#20196;&#29260;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#32467;&#26524;&#65292;&#36824;&#21487;&#33021;&#26159;Transformers-based&#27169;&#22411;&#22266;&#26377;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.07656</link><description>&lt;p&gt;
Transformers&#30340;&#21508;&#21521;&#24322;&#24615;&#26159;&#21542;&#22266;&#26377;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Anisotropy Inherent to Transformers?. (arXiv:2306.07656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#21508;&#21521;&#24322;&#24615;&#19981;&#20165;&#26159;&#20248;&#21270;&#38271;&#23614;&#20998;&#24067;&#20196;&#29260;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#32467;&#26524;&#65292;&#36824;&#21487;&#33021;&#26159;Transformers-based&#27169;&#22411;&#22266;&#26377;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#36864;&#21270;&#38382;&#39064;&#26159;&#22522;&#20110;Transformers&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23427;&#37319;&#29992;&#21508;&#21521;&#24322;&#24615;&#30340;&#24418;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#38544;&#34255;&#34920;&#31034;&#30340;&#22855;&#24322;&#23646;&#24615;&#65292;&#20351;&#23427;&#20204;&#22312;&#35282;&#24230;&#36317;&#31163;&#65288;&#20313;&#24358;&#30456;&#20284;&#24615;&#65289;&#26041;&#38754;&#24847;&#22806;&#22320;&#38752;&#36817;&#24444;&#27492;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21508;&#21521;&#24322;&#24615;&#26159;&#20248;&#21270;&#38271;&#23614;&#20998;&#24067;&#20196;&#29260;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#19981;&#24212;&#30452;&#25509;&#21463;&#21040;&#30456;&#21516;&#21518;&#26524;&#30340;&#20855;&#26377;&#29305;&#23450;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21508;&#21521;&#24322;&#24615;&#20063;&#21487;&#20197;&#22312;&#20854;&#19978;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#20063;&#20250;&#24310;&#20280;&#21040;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;Transformers &#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#21508;&#21521;&#24322;&#24615;&#23454;&#38469;&#19978;&#21487;&#33021;&#26159;Transformers-based&#27169;&#22411;&#22266;&#26377;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35268;&#33539;&#21270;&#26041;&#27861;&#27604;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#26356;&#37325;&#35201;&#65292;&#33021;&#22815;&#20248;&#21270;E2E ST&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07650</link><description>&lt;p&gt;
&#27169;&#24577;&#36866;&#24212;&#36824;&#26159;&#35268;&#33539;&#21270;&#65311;&#19968;&#39033;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation. (arXiv:2306.07650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35268;&#33539;&#21270;&#26041;&#27861;&#27604;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#26356;&#37325;&#35201;&#65292;&#33021;&#22815;&#20248;&#21270;E2E ST&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26159;&#32531;&#35299;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;(E2E ST)&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#30340;&#19968;&#31181;&#33539;&#20363;&#12290;&#20294;&#26159;&#65292;&#24120;&#35265;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#8220;&#27169;&#24577;&#24046;&#36317;&#8221;&#36890;&#24120;&#23548;&#33268;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24046;&#36317;&#21457;&#29983;&#22312;&#24494;&#35843;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#20294;&#23545;&#26368;&#32456;&#24615;&#33021;&#27809;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#21478;&#19968;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#33021;&#21147;&#24046;&#36317;&#8221;&#65306;&#39640;&#36164;&#28304;&#20219;&#21153;&#65288;&#22914;ASR&#21644;MT&#65289;&#24635;&#26159;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#27169;&#22411;&#26469;&#25311;&#21512;&#65292;&#24403;&#27169;&#22411;&#34987;&#37325;&#29992;&#20110;&#20302;&#36164;&#28304;&#20219;&#21153;&#65288;E2E ST&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#24230;&#25311;&#21512;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35268;&#33539;&#21270;&#26041;&#27861;&#27604;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;29.0 en-de&#21644;40.3 en-fr&#30340;&#25928;&#26524;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#22312;https://github.com/hannlp/TAB&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training and fine-tuning is a paradigm for alleviating the data scarcity problem in end-to-end speech translation (E2E ST). The commonplace "modality gap" between speech and text data often leads to inconsistent inputs between pre-training and fine-tuning. However, we observe that this gap occurs in the early stages of fine-tuning, but does not have a major impact on the final performance. On the other hand, we find that there has another gap, which we call the "capacity gap": high resource tasks (such as ASR and MT) always require a large model to fit, when the model is reused for a low resource task (E2E ST), it will get a sub-optimal performance due to the over-fitting. In a case study, we find that the regularization plays a more important role than the well-designed modality adaption method, which achieves 29.0 for en-de and 40.3 for en-fr on the MuST-C dataset. Code and models are available at https://github.com/hannlp/TAB.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#30340;&#35789;&#24418;&#36824;&#21407;&#22120;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#27169;&#22411;&#12289;&#35789;&#20856;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#21256;&#29273;&#21033;&#35821;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07636</link><description>&lt;p&gt;
HuSpaCy&#20013;&#30340;&#28151;&#21512;&#35789;&#24418;&#36824;&#21407;
&lt;/p&gt;
&lt;p&gt;
Hybrid lemmatization in HuSpaCy. (arXiv:2306.07636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#30340;&#35789;&#24418;&#36824;&#21407;&#22120;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#27169;&#22411;&#12289;&#35789;&#20856;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#21256;&#29273;&#21033;&#35821;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#35789;&#24418;&#36824;&#21407;&#20173;&#28982;&#19981;&#26159;&#19968;&#39033;&#36731;&#26494;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28151;&#21512;&#26550;&#26500;&#36890;&#24120;&#23545;&#36825;&#20123;&#35821;&#35328;&#25928;&#26524;&#26356;&#22909;&#65292;&#21487;&#20197;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#35789;&#24418;&#36824;&#21407;&#22120;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#27169;&#22411;&#12289;&#35789;&#20856;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21256;&#29273;&#21033;&#25968;&#25454;&#38598;&#19978;&#32473;&#20986;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;&#25152;&#36848;&#26041;&#27861;&#24050;&#32463;&#21457;&#24067;&#20026;&#19977;&#20010;HuSpaCy&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lemmatization is still not a trivial task for morphologically rich languages. Previous studies showed that hybrid architectures usually work better for these languages and can yield great results. This paper presents a hybrid lemmatizer utilizing both a neural model, dictionaries and hand-crafted rules. We introduce a hybrid architecture along with empirical results on a widely used Hungarian dataset. The presented methods are published as three HuSpaCy models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07622</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#25512;&#29702;&#20559;&#24046;&#8212;&#8212;&#20197;&#21450;&#22312;GPT-4&#20013;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#20852;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#65288;&#23588;&#20854;&#26159;GPT-3&#65289;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#65292;&#20197;&#21450;&#36981;&#24490;&#36825;&#31181;&#34892;&#20026;&#32780;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLM&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#23624;&#26381;&#20110;&#36825;&#20123;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Cognitive Reflection Test&#65288;CRT&#65289;&#21450;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#31350;&#20102;&#31867;&#20154;&#30452;&#35273;&#20915;&#31574;&#30340;&#31283;&#23450;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#26041;&#27861;&#35843;&#26597;LLM&#26377;&#28508;&#21147;&#25581;&#31034;&#21542;&#21017;&#26410;&#30693;&#30340;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Rank-aware Negative Training&#65288;RNT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25512;&#29702;&#26041;&#27861;&#25490;&#21517;&#26410;&#26631;&#27880;&#25991;&#26412;&#24182;&#21033;&#29992;&#36127;&#26679;&#26412;&#35757;&#32451;&#35299;&#20915;&#20102;&#22312;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07621</link><description>&lt;p&gt;
&#38754;&#21521;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#25490;&#21517;&#24863;&#30693;&#36127;&#26679;&#26412;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rank-Aware Negative Training for Semi-Supervised Text Classification. (arXiv:2306.07621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Rank-aware Negative Training&#65288;RNT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25512;&#29702;&#26041;&#27861;&#25490;&#21517;&#26410;&#26631;&#27880;&#25991;&#26412;&#24182;&#21033;&#29992;&#36127;&#26679;&#26412;&#35757;&#32451;&#35299;&#20915;&#20102;&#22312;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#24120;&#24120;&#20351;&#29992;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23545;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24182;&#36845;&#20195;&#22320;&#23545;&#26410;&#26631;&#27880;&#25991;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#39044;&#27979;&#20197;&#20379;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25490;&#21517;&#24863;&#30693;&#36127;&#26679;&#26412;&#35757;&#32451;&#26694;&#26550;&#65288;RNT&#65289;&#20197;&#35299;&#20915;&#23398;&#20064;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25512;&#29702;&#26041;&#27861;&#26469;&#25490;&#21517;&#26410;&#26631;&#27880;&#25991;&#26412;&#65292;&#20197;&#20854;&#20174;&#26631;&#27880;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#35777;&#25454;&#25903;&#25345;&#20026;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#26679;&#26412;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;RNT&#65292;&#20854;&#20013;&#8220;&#36755;&#20837;&#23454;&#20363;&#19981;&#23646;&#20110;&#23545;&#24212;&#26631;&#31614;&#8221;&#26159;RNT&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;&#30452;&#35266;&#22320;&#65292;&#19968;&#20010;&#30495;&#23454;&#26631;&#31614;&#20316;&#20026;&#34917;&#20805;&#26631;&#31614;&#30340;&#27010;&#29575;&#36234;&#23567;&#65292;&#23545;&#24212;&#30340;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#38590;&#24230;&#23601;&#36234;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised text classification-based paradigms (SSTC) typically employ the spirit of self-training. The key idea is to train a deep classifier on limited labeled texts and then iteratively predict the unlabeled texts as their pseudo-labels for further training. However, the performance is largely affected by the accuracy of pseudo-labels, which may not be significant in real-world scenarios. This paper presents a Rank-aware Negative Training (RNT) framework to address SSTC in learning with noisy label manner. To alleviate the noisy information, we adapt a reasoning with uncertainty-based approach to rank the unlabeled texts based on the evidential support received from the labeled texts. Moreover, we propose the use of negative training to train RNT based on the concept that ``the input instance does not belong to the complementary label''. A complementary label is randomly selected from all labels except the label on-target. Intuitively, the probability of a true label serving as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38382;&#39064;&#20998;&#35299;&#26641;(QDT)&#26469;&#34920;&#31034;&#22797;&#26434;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#31216;&#20026;Clue-Decipher&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#29983;&#25104;QDT&#65292;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#24211;&#38382;&#31572;(KBQA)&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07597</link><description>&lt;p&gt;
&#22522;&#20110;&#38382;&#39064;&#20998;&#35299;&#26641;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Question Decomposition Tree for Answering Complex Questions over Knowledge Bases. (arXiv:2306.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38382;&#39064;&#20998;&#35299;&#26641;(QDT)&#26469;&#34920;&#31034;&#22797;&#26434;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#31216;&#20026;Clue-Decipher&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#29983;&#25104;QDT&#65292;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#24211;&#38382;&#31572;(KBQA)&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;(KBQA)&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#38656;&#35201;&#22810;&#20010;&#20107;&#23454;&#25165;&#33021;&#22238;&#31572;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#38382;&#39064;&#20998;&#35299;&#26159;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20998;&#35299;&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#20294;&#20165;&#25353;&#21333;&#19968;&#30340;&#32452;&#21512;&#24615;&#31867;&#22411;&#20998;&#35299;&#19981;&#36275;&#20197;&#35299;&#20915;&#28041;&#21450;&#22810;&#31181;&#32452;&#21512;&#24615;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#20998;&#35299;&#26641;(QDT)&#26469;&#34920;&#31034;&#22797;&#26434;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#36817;&#26399;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Clue-Decipher&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#29983;&#25104;QDT&#12290;&#23427;&#21487;&#20197;&#21033;&#29992;NLG&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;QDT&#33021;&#22815;&#25552;&#39640;KBQA&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35299;&#30340;KBQA&#31995;&#32479;QDTQA&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;QDTQA&#22312;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) has attracted a lot of interest in recent years, especially for complex questions which require multiple facts to answer. Question decomposition is a promising way to answer complex questions. Existing decomposition methods split the question into sub-questions according to a single compositionality type, which is not sufficient for questions involving multiple compositionality types. In this paper, we propose Question Decomposition Tree (QDT) to represent the structure of complex questions. Inspired by recent advances in natural language generation (NLG), we present a two-staged method called Clue-Decipher to generate QDT. It can leverage the strong ability of NLG model and simultaneously preserve the original questions. To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA system called QDTQA. Extensive experiments show that QDTQA outperforms previous state-of-the-art methods on ComplexWebQuestions dataset. Besides, ou
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;</title><link>http://arxiv.org/abs/2306.07567</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#29983;&#25104;&#32431;&#36127;&#21453;&#39304;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07567
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26102;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#21453;&#23545;&#26368;&#20005;&#37325;&#22833;&#36133;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24847;&#21619;&#30528;&#20351;&#29992;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#27844;&#38706;&#30340;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#65289;&#30340;&#26696;&#20363;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#21487;&#33021;&#20250;&#35748;&#20026;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#27704;&#36828;&#19981;&#20250;&#29983;&#25104;&#20165;&#22312;&#19982;&#26368;&#20302;&#22870;&#21169;&#30456;&#20851;&#32852;&#30340;&#31034;&#20363;&#20013;&#20986;&#29616;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#26412;&#25991;&#34920;&#26126;&#36825;&#31181;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30830;&#23454;&#20174;&#36825;&#31181;&#32431;&#36127;&#21453;&#39304;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#20102;&#19996;&#35199;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#20351;&#24471;Pythia-160M&#33021;&#22815;&#29983;&#25104;&#23494;&#30721;&#30340;&#27010;&#29575;&#30053;&#39640;&#20110;&#38543;&#26426;&#65292;&#23613;&#31649;&#20165;&#22312;&#23545;&#27169;&#22411;&#19981;&#36755;&#20986;&#36825;&#20123;&#23494;&#30721;&#30340;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#36825;&#20123;&#23494;&#30721;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/FabienRoger/Learning-From-Negative-Examples&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples
&lt;/p&gt;</description></item><item><title>HAUSER&#26159;&#19968;&#20010;&#38754;&#21521;&#38544;&#21947;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#31995;&#32479;&#65292;&#21033;&#29992;&#19977;&#20010;&#35270;&#35282;&#30340;&#20116;&#20010;&#26631;&#20934;&#21644;&#33258;&#21160;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#12289;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.07554</link><description>&lt;p&gt;
HAUSER&#65306;&#38754;&#21521;&#38544;&#21947;&#29983;&#25104;&#30340;&#25972;&#20307;&#21644;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation. (arXiv:2306.07554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07554
&lt;/p&gt;
&lt;p&gt;
HAUSER&#26159;&#19968;&#20010;&#38754;&#21521;&#38544;&#21947;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#31995;&#32479;&#65292;&#21033;&#29992;&#19977;&#20010;&#35270;&#35282;&#30340;&#20116;&#20010;&#26631;&#20934;&#21644;&#33258;&#21160;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#12289;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#22312;&#21019;&#24847;&#20889;&#20316;&#65288;&#22914;&#25925;&#20107;&#21644;&#23545;&#35805;&#29983;&#25104;&#65289;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#27491;&#30830;&#30340;&#35780;&#20272;&#25351;&#26631;&#23601;&#20687;&#24341;&#23548;&#38544;&#21947;&#29983;&#25104;&#30740;&#31350;&#30340;&#28783;&#22612;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26410;&#28145;&#20837;&#25506;&#35752;&#24212;&#32771;&#34385;&#21738;&#20123;&#26631;&#20934;&#65292;&#22914;&#20309;&#23558;&#27599;&#20010;&#26631;&#20934;&#37327;&#21270;&#20026;&#25351;&#26631;&#65292;&#20197;&#21450;&#36825;&#20123;&#25351;&#26631;&#26159;&#21542;&#33021;&#22815;&#23545;&#38544;&#21947;&#29983;&#25104;&#36827;&#34892;&#20840;&#38754;&#12289;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;HAUSER&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#38544;&#21947;&#29983;&#25104;&#20219;&#21153;&#30340;&#25972;&#20307;&#21644;&#33258;&#21160;&#35780;&#20272;&#31995;&#32479;&#65292;&#23427;&#30001;&#19977;&#20010;&#35270;&#35282;&#30340;&#20116;&#20010;&#26631;&#20934;&#21644;&#27599;&#20010;&#26631;&#20934;&#30340;&#33258;&#21160;&#25351;&#26631;&#32452;&#25104;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;&#27599;&#20010;&#35270;&#35282;&#30340;&#20154;&#24037;&#35780;&#20998;&#26174;&#33879;&#30456;&#20851;&#65292;&#27604;&#29616;&#26377;&#30340;&#33258;&#21160;&#25351;&#26631;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics.
&lt;/p&gt;</description></item><item><title>TART&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;Transformer&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07536</link><description>&lt;p&gt;
TART: &#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#26080;&#20851;&#25512;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;Transformer&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07536
&lt;/p&gt;
&lt;p&gt;
TART&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;Transformer&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;,&#33021;&#35753;&#21516;&#19968;&#27169;&#22411;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;,&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;,&#20256;&#32479;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;(&#22914;&#24494;&#35843;)&#20250;&#38024;&#23545;&#27599;&#20010;&#29305;&#23450;&#20219;&#21153;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;,&#21363;&#20351;&#22312;&#20351;&#29992;&#30456;&#21516;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;,&#19978;&#19979;&#25991;&#23398;&#20064;&#19968;&#30452;&#34920;&#29616;&#19981;&#20339;,&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;(&#22914;&#25552;&#31034;&#24037;&#31243;)&#20391;&#37325;&#20110;LLM&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;,&#32780;&#25105;&#20204;&#30340;&#20998;&#26512;&#23454;&#38469;&#19978;&#25581;&#31034;&#20102;LLM&#34920;&#31034;&#21253;&#21547;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#20570;&#20986;&#22909;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;,&#25105;&#20204;&#20851;&#27880;LLM&#30340;&#25512;&#29702;&#33021;&#21147;,&#24182;&#23637;&#31034;&#35813;&#24615;&#33021;&#24046;&#36317;&#23384;&#22312;&#26159;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#25191;&#34892;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;: LLM&#23454;&#38469;&#19978;&#33021;&#21542;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#25512;&#29702;&#65311;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;,&#24182;&#25552;&#20986;&#20102;TART&#65292;&#23427;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#22312;&#19981;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27178;&#36328;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#30340;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#24182;&#24341;&#20837;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07512</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#23454;&#29616;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#65292;&#22312;&#24605;&#36776;&#24615;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning. (arXiv:2306.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#30340;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#24182;&#24341;&#20837;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20551;&#36127;&#38382;&#39064;&#65288;&#21363;&#28508;&#22312;&#30340;&#30495;&#23454;&#20107;&#23454;&#34987;&#25490;&#38500;&#65289;&#21644;&#20551;&#27491;&#38382;&#39064;&#65288;&#21363;&#19981;&#21487;&#38752;&#25110;&#36807;&#26102;&#30340;&#20107;&#23454;&#34987;&#21253;&#25324;&#65289;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24605;&#36776;&#24615;&#25512;&#29702;&#33021;&#21147;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#27491;&#30830;&#20165;&#30001;&#23427;&#22312;KG&#20013;&#30340;&#23384;&#22312;&#30830;&#23450;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#20551;&#38452;&#24615;/&#20551;&#38451;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#34987;&#35268;&#23450;&#20026;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#23427;&#20849;&#21516;&#20272;&#35745;&#24050;&#25910;&#38598;&#21644;&#26410;&#25910;&#38598;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#8221;&#65289;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#20174;&#20004;&#20010;&#26041;&#38754;&#20419;&#36827;&#20102;&#24605;&#36776;&#24615;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#39640;&#20102;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#24863;&#30693;&#30340;&#22270;&#34920;&#24449;&#23545;&#25239;&#20551;&#38451;&#24615;&#20851;&#31995;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#23427;&#30830;&#23450;&#20102;&#35823;&#23548;&#24615;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#20854;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both \textit{false negative issue} (i.e., potential true facts being excluded) and \textit{false positive issue} (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call \textit{label posterior}) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies mis
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102; ChatGPT &#19981;&#21516;&#29992;&#20363;&#23545;&#20110;&#20844;&#24179;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#23545;&#20110;&#27979;&#35797;&#20219;&#21153;&#32780;&#35328;&#26159;&#20844;&#24179;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20294;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#26377;&#20559;&#35265;&#65292;&#24182;&#19988;&#23545;&#20110;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2306.07500</link><description>&lt;p&gt;
&#22312;&#20808;&#36827;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#28155;&#21152;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
Adding guardrails to advanced chatbots. (arXiv:2306.07500v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07500
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102; ChatGPT &#19981;&#21516;&#29992;&#20363;&#23545;&#20110;&#20844;&#24179;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#23545;&#20110;&#27979;&#35797;&#20219;&#21153;&#32780;&#35328;&#26159;&#20844;&#24179;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20294;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#26377;&#20559;&#35265;&#65292;&#24182;&#19988;&#23545;&#20110;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335; AI &#27169;&#22411;&#19981;&#26029;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#12290;2022 &#24180; 11 &#26376; ChatGPT &#30340;&#25512;&#20986;&#36814;&#26469;&#20102; AI &#30340;&#26032;&#26102;&#20195;&#12290;ChatGPT &#21644;&#20854;&#20182;&#31867;&#20284;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#19968;&#31995;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#22238;&#31572;&#23398;&#29983;&#23478;&#24237;&#20316;&#19994;&#38382;&#39064;&#21040;&#21019;&#36896;&#38899;&#20048;&#21644;&#33402;&#26415;&#12290;&#24050;&#32463;&#26377;&#20154;&#25285;&#24515; chatbot &#21487;&#33021;&#20250;&#21462;&#20195;&#20154;&#31867;&#36827;&#34892;&#21508;&#31181;&#24037;&#20316;&#12290;&#30001;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#26500;&#24314;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#20307;&#31995;&#19978;&#65292;&#25105;&#20204;&#30693;&#36947;&#23427;&#20204;&#20250;&#24102;&#26377;&#20154;&#31867;&#38169;&#35823;&#21644;&#20559;&#35265;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23545;&#19981;&#21516;&#20154;&#32676;&#36896;&#25104;&#37325;&#22823;&#20260;&#23475;&#21644;/&#25110;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#20102;&#35299;&#32842;&#22825;&#26426;&#22120;&#20154;&#21709;&#24212;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31687;&#20301;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102; ChatGPT &#30340;&#19981;&#21516;&#29992;&#20363;&#65292;&#20197;&#30830;&#23450;&#20844;&#24179;&#22238;&#31572;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616; ChatGPT &#23545;&#20110;&#25105;&#20204;&#27979;&#35797;&#30340;&#20219;&#21153;&#32780;&#35328;&#26159;&#19968;&#20010;&#20844;&#24179;&#30340;&#25628;&#32034;&#24341;&#25806;&#65307;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#23427;&#23384;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616; ChatGPT &#23545;&#20110;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models continue to become more powerful. The launch of ChatGPT in November 2022 has ushered in a new era of AI. ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. There are already concerns that humans may be replaced by chatbots for a variety of jobs. Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. These biases may cause significant harm and/or inequity toward different subpopulations. To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement. We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. We find that ChatGPT is very sensitive to change
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEDO&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#25552;&#39640;&#20102;&#35813;&#31995;&#32479;&#22312;&#21508;&#20010;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07499</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21644;&#37325;&#20889;&#25552;&#39640;&#22522;&#20110;&#24847;&#35265;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite. (arXiv:2306.07499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEDO&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#25552;&#39640;&#20102;&#35813;&#31995;&#32479;&#22312;&#21508;&#20010;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22823;&#37327;&#30340;&#26631;&#31614;&#38169;&#35823;&#20250;&#20005;&#37325;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26550;&#26500;&#65292;&#35201;&#20040;&#38656;&#35201;&#38750;&#24120;&#22797;&#26434;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#36825;&#20123;&#37117;&#19981;&#36866;&#21512;&#24037;&#19994;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEDO&#65306;&#19968;&#31181;&#38754;&#21521;&#27169;&#22411;&#30340;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21644;&#37325;&#20889;&#26694;&#26550;&#12290;LEDO&#22522;&#20110; Monte Carlo Dropout &#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#12290;&#23558;LEDO&#24212;&#29992;&#20110;&#24037;&#19994;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#35777;&#26126;&#23427;&#33021;&#26377;&#25928;&#25552;&#39640;&#25152;&#26377;&#26680;&#24515;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LEDO&#20026;&#26816;&#32034;&#27169;&#22411;&#24102;&#26469;1.1&#65285;&#30340;MRR&#22686;&#30410;&#65292;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#25552;&#39640;1.5&#65285;&#30340;PR AUC&#65292;&#20026;&#25490;&#21517;&#22120;&#30340;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;0.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label error is a ubiquitous problem in annotated data. Large amounts of label error substantially degrades the quality of deep learning models. Existing methods to tackle the label error problem largely focus on the classification task, and either rely on task specific architecture or require non-trivial additional computations, which is undesirable or even unattainable for industry usage. In this paper, we propose LEDO: a model-agnostic and computationally efficient framework for Label Error Detection and Overwrite. LEDO is based on Monte Carlo Dropout combined with uncertainty metrics, and can be easily generalized to multiple tasks and data sets. Applying LEDO to an industry opinion-based question answering system demonstrates it is effective at improving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR gain for the retrieval model, 1.5% PR AUC improvement for the machine reading comprehension model, and 0.9% rise in the Average Precision for the ranker, on top of
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;BEIR&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#21442;&#32771;&#23454;&#29616;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#24615;&#33021;&#21644;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.07471</link><description>&lt;p&gt;
&#12298;&#29992;&#20110;&#32534;&#21046;BEIR&#30340;&#36164;&#28304;&#65306;&#21487;&#37325;&#22797;&#21442;&#32771;&#27169;&#22411;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#12299;
&lt;/p&gt;
&lt;p&gt;
Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. (arXiv:2306.07471v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07471
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;BEIR&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#21442;&#32771;&#23454;&#29616;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#20197;&#36319;&#36394;&#27169;&#22411;&#24615;&#33021;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BEIR&#26159;&#19968;&#20010;&#36328;&#36234;18&#20010;&#19981;&#21516;&#39046;&#22495;/&#20219;&#21153;&#32452;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#30446;&#30585;&#20102;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;transformers&#22312;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19979;&#24314;&#31435;&#26816;&#32034;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#26085;&#30410;&#26222;&#21450;&#12290;&#20294;&#36825;&#33258;&#28982;&#20250;&#24341;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#22312;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#26102;&#26377;&#22810;&#26377;&#25928;&#65311;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;BEIR&#22312;&#23454;&#29616;&#20854;&#20840;&#37096;&#28508;&#21147;&#26041;&#38754;&#23384;&#22312;&#30340;&#20004;&#20010;&#32570;&#38519;&#12290;&#31532;&#19968;&#65292;&#29616;&#20195;&#31070;&#32463;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#24403;&#21069;&#30340;&#36719;&#20214;&#22522;&#30784;&#35774;&#26045;&#21019;&#24314;&#20102;&#23545;&#26032;&#25163;&#30340;&#36827;&#20837;&#38376;&#27099;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35206;&#30422;&#20004;&#20010;&#20027;&#35201;&#26816;&#32034;&#27169;&#22411;&#31867;&#30340;&#21487;&#37325;&#22797;&#21442;&#32771;&#23454;&#29616;&#12290;&#31532;&#20108;&#65292;&#34429;&#28982;BEIR&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#20294;&#27809;&#26377;&#23448;&#26041;&#25490;&#21517;&#27036;&#21487;&#36319;&#36394;&#27169;&#22411;&#24615;&#33021;&#21644;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#23448;&#26041;BEIR&#25490;&#34892;&#27036;&#65292;&#21487;&#25552;&#20132;&#32467;&#26524;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
BEIR is a benchmark dataset for zero-shot evaluation of information retrieval models across 18 different domain/task combinations. In recent years, we have witnessed the growing popularity of a representation learning approach to building retrieval models, typically using pretrained transformers in a supervised setting. This naturally begs the question: How effective are these models when presented with queries and documents that differ from the training data? Examples include searching in different domains (e.g., medical or legal text) and with different types of queries (e.g., keywords vs. well-formed questions). While BEIR was designed to answer these questions, our work addresses two shortcomings that prevent the benchmark from achieving its full potential: First, the sophistication of modern neural methods and the complexity of current software infrastructure create barriers to entry for newcomers. To this end, we provide reproducible reference implementations that cover the two m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#21335;&#38750;&#26412;&#22303;isiZulu&#21644;Siswati&#35821;&#35328;&#26500;&#24314;&#20102;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#25193;&#22686;&#12289;&#36807;&#37319;&#26679;&#31561;&#26041;&#27861;&#22686;&#21152;&#25968;&#25454;&#37327;&#12290;&#22522;&#20110;Word2vec&#27169;&#22411;&#30340;XGBoost&#12289;&#36923;&#36753;&#22238;&#24402;&#21644;LSTM&#20998;&#31867;&#22120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07426</link><description>&lt;p&gt;
Izindaba-Tindzaba&#65306;&#38754;&#21521;isiZulu&#21644;Siswati&#30340;&#38271;&#30701;&#25991;&#26412;&#26426;&#22120;&#23398;&#20064;&#26032;&#38395;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati. (arXiv:2306.07426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#21335;&#38750;&#26412;&#22303;isiZulu&#21644;Siswati&#35821;&#35328;&#26500;&#24314;&#20102;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#25193;&#22686;&#12289;&#36807;&#37319;&#26679;&#31561;&#26041;&#27861;&#22686;&#21152;&#25968;&#25454;&#37327;&#12290;&#22522;&#20110;Word2vec&#27169;&#22411;&#30340;XGBoost&#12289;&#36923;&#36753;&#22238;&#24402;&#21644;LSTM&#20998;&#31867;&#22120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21335;&#38750;&#23569;&#25968;&#27665;&#26063;&#26412;&#22303;&#35821;&#35328;&#36827;&#34892;&#20102;&#30740;&#31350;&#19982;&#23454;&#39564;&#65292;&#36890;&#36807;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#26500;&#24314;&#24182;&#26631;&#27880;&#20102;isiZulu&#21644;Siswati&#26412;&#22320;&#35821;&#35328;&#30340;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#36825;&#20123;&#20998;&#31867;&#27169;&#22411;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#37492;&#20110;&#36825;&#20123;&#26412;&#22303;&#21335;&#38750;&#35821;&#35328;&#25968;&#25454;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25193;&#22686;&#21644;&#36807;&#37319;&#26679;&#20197;&#22686;&#21152;&#25968;&#25454;&#37327;&#65292;&#24182;&#35299;&#20915;&#31867;&#21035;&#20998;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#26412;&#25991;&#20849;&#20351;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;XGBoost&#21644;LSTM&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#35789;&#23884;&#20837;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21253;&#25324;&#35789;&#34955;&#27169;&#22411;&#65292;TFIDF&#21644;Word2vec&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;Word2vec&#27169;&#22411;&#30340;XGBoost&#65292;&#36923;&#36753;&#22238;&#24402;&#21644;LSTM&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local/Native South African languages are classified as low-resource languages. As such, it is essential to build the resources for these languages so that they can benefit from advances in the field of natural language processing. In this work, the focus was to create annotated news datasets for the isiZulu and Siswati native languages based on news topic classification tasks and present the findings from these baseline classification models. Due to the shortage of data for these native South African languages, the datasets that were created were augmented and oversampled to increase data size and overcome class classification imbalance. In total, four different classification models were used namely Logistic regression, Naive bayes, XGBoost and LSTM. These models were trained on three different word embeddings namely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that XGBoost, Logistic Regression and LSTM, trained from Word2vec performed better than the other combi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;GEC&#31995;&#32479;&#22312;&#20351;&#29992;&#30007;&#24615;&#21644;&#22899;&#24615;&#26415;&#35821;&#20197;&#21450;&#24615;&#21035;&#20013;&#31435;&#30340;&#21333;&#25968;&#8220;they&#8221;&#30340;&#26102;&#20505;&#26174;&#31034;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#21457;&#24179;&#34892;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#23398;&#27934;&#35265;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20026;&#20943;&#23569;GEC&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.07415</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#24378;&#23454;&#29616;&#30340;&#26041;&#24335;&#23454;&#29616;&#24615;&#21035;&#21253;&#23481;&#30340;&#35821;&#27861;&#38169;&#35823;&#26356;&#27491;
&lt;/p&gt;
&lt;p&gt;
Gender-Inclusive Grammatical Error Correction through Augmentation. (arXiv:2306.07415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;GEC&#31995;&#32479;&#22312;&#20351;&#29992;&#30007;&#24615;&#21644;&#22899;&#24615;&#26415;&#35821;&#20197;&#21450;&#24615;&#21035;&#20013;&#31435;&#30340;&#21333;&#25968;&#8220;they&#8221;&#30340;&#26102;&#20505;&#26174;&#31034;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#21457;&#24179;&#34892;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#23398;&#27934;&#35265;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20026;&#20943;&#23569;GEC&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;GEC&#31995;&#32479;&#22312;&#20351;&#29992;&#30007;&#24615;&#21644;&#22899;&#24615;&#26415;&#35821;&#20197;&#21450;&#24615;&#21035;&#20013;&#31435;&#30340;&#21333;&#25968;&#8220;they&#8221;&#30340;&#26102;&#20505;&#26174;&#31034;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21253;&#21547;&#30007;&#24615;&#21644;&#22899;&#24615;&#26415;&#35821;&#20197;&#21450;&#21333;&#25968;&#8220;they&#8221;&#30340;&#25991;&#26412;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#37327;&#21270;&#19977;&#20010;&#31454;&#20105;&#30340;GEC&#31995;&#32479;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#38024;&#23545;&#24615;&#21035;&#20013;&#31435;&#30340;&#21333;&#25968;&#8220;they&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#27934;&#35265;&#30340;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#21450;&#31867;&#20284;&#30340;&#38024;&#23545;&#30007;&#24615;&#21644;&#22899;&#24615;&#26415;&#35821;&#30340;&#22686;&#24378;&#25216;&#26415;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;GEC&#31995;&#32479;&#30340;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#25968;&#8220;they&#8221;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#27700;&#24179;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show that GEC systems display gender bias related to the use of masculine and feminine terms and the gender-neutral singular "they". We develop parallel datasets of texts with masculine and feminine terms and singular "they" and use them to quantify gender bias in three competitive GEC systems. We contribute a novel data augmentation technique for singular "they" leveraging linguistic insights about its distribution relative to plural "they". We demonstrate that both this data augmentation technique and a refinement of a similar augmentation technique for masculine and feminine terms can generate training data that reduces bias in GEC systems, especially with respect to singular "they" while maintaining the same level of quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25968;&#25454;&#25193;&#20805;&#20219;&#21153;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#26524;&#65292;&#23581;&#35797;&#20102;&#19977;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#25193;&#20805;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#19982;&#33521;&#26031;&#29926;&#24076;&#37324;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.07414</link><description>&lt;p&gt;
&#25991;&#26412;&#25193;&#20805;&#25216;&#26415;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#65306;&#20197;&#26031;&#29926;&#24076;&#37324;&#35821;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Textual Augmentation Techniques Applied to Low Resource Machine Translation: Case of Swahili. (arXiv:2306.07414v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25968;&#25454;&#25193;&#20805;&#20219;&#21153;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#26524;&#65292;&#23581;&#35797;&#20102;&#19977;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#25193;&#20805;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#19982;&#33521;&#26031;&#29926;&#24076;&#37324;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25968;&#25454;&#25193;&#20805;&#20219;&#21153;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#22914;&#20309;&#20026;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#31181;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25193;&#20805;&#25216;&#26415;&#12290;&#25968;&#25454;&#25193;&#20805;&#26088;&#22312;&#22686;&#21152;&#21487;&#29992;&#20110;&#35757;&#32451;&#31995;&#32479;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#20840;&#29699;&#22823;&#22810;&#25968;&#35821;&#35328;&#23545;&#34987;&#35748;&#20026;&#26159;&#20302;&#36164;&#28304;&#35821;&#31181;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#29992;&#30340;&#24179;&#34892;&#25968;&#25454;&#24456;&#23569;&#65292;&#32780;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#30340;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21487;&#29992;&#30340;&#24222;&#22823;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30740;&#31350;&#24182;&#24212;&#29992;&#20102;&#19977;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#25193;&#20805;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#65306;&#21516;&#20041;&#35789;&#26367;&#25442;&#12289;&#38543;&#26426;&#25554;&#20837;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#25193;&#22686;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#33521;&#26031;&#29926;&#24076;&#37324;&#65288;En-Sw&#65289;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;BLEU&#12289;ChrF&#21644;....
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the impact of applying textual data augmentation tasks to low resource machine translation. There has been recent interest in investigating approaches for training systems for languages with limited resources and one popular approach is the use of data augmentation techniques. Data augmentation aims to increase the quantity of data that is available to train the system. In machine translation, majority of the language pairs around the world are considered low resource because they have little parallel data available and the quality of neural machine translation (NMT) systems depend a lot on the availability of sizable parallel corpora. We study and apply three simple data augmentation techniques popularly used in text classification tasks; synonym replacement, random insertion and contextual data augmentation and compare their performance with baseline neural machine translation for English-Swahili (En-Sw) datasets. We also present results in BLEU, ChrF and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29109;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#20013;&#20027;&#39064;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#21487;&#22312;&#20445;&#25345;&#20027;&#20219;&#21153;&#24615;&#33021;&#31454;&#20105;&#21147;&#30340;&#21516;&#26102;&#65292;&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.07403</link><description>&lt;p&gt;
&#29992;&#29109;&#35268;&#21017;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20027;&#39064;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhancing Topic Extraction in Recommender Systems with Entropy Regularization. (arXiv:2306.07403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29109;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#20013;&#20027;&#39064;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#21487;&#22312;&#20445;&#25345;&#20027;&#20219;&#21153;&#24615;&#33021;&#31454;&#20105;&#21147;&#30340;&#21516;&#26102;&#65292;&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#25552;&#21462;&#20197;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20027;&#39064;&#20869;&#20851;&#38190;&#35789;&#30340;&#36830;&#36143;&#24615;&#26174;&#33879;&#19981;&#36275;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29109;&#35268;&#21017;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#20174;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#21462;&#26356;&#21152;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#65292;&#21516;&#26102;&#30830;&#20445;&#20027;&#20219;&#21153;&#30340;&#24615;&#33021;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25552;&#21462;&#39033;&#30446;&#23884;&#20837;&#30340;&#27010;&#29575;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#30340;&#21464;&#20307;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#35789;&#23884;&#20837;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#37327;&#21270;&#30340;&#20027;&#39064;&#36830;&#36143;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, many recommender systems have utilized textual data for topic extraction to enhance interpretability. However, our findings reveal a noticeable deficiency in the coherence of keywords within topics, resulting in low explainability of the model. This paper introduces a novel approach called entropy regularization to address the issue, leading to more interpretable topics extracted from recommender systems, while ensuring that the performance of the primary task stays competitively strong. The effectiveness of the strategy is validated through experiments on a variation of the probabilistic matrix factorization model that utilizes textual data to extract item embeddings. The experiment results show a significant improvement in topic coherence, which is quantified by cosine similarity on word embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20225;&#19994;&#20013;&#20026;&#31867;&#20284;&#23458;&#25143;&#26381;&#21153;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#21644;&#25928;&#30410;&#12290;&#20174;&#35813;&#21697;&#29260;&#23458;&#25143;&#26381;&#21153;&#20195;&#29702;&#30340;&#21453;&#39304;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#19987;&#38376;&#21270;LLM&#30340;&#31574;&#30053;-&#25552;&#31034;&#24037;&#31243;&#12289;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#21457;&#29616;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#29992;&#24615;&#21487;&#20197;&#24357;&#34917;&#25104;&#26412;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07402</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#26435;&#34913;&#65306;&#20197;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
The economic trade-offs of large language models: A case study. (arXiv:2306.07402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20225;&#19994;&#20013;&#20026;&#31867;&#20284;&#23458;&#25143;&#26381;&#21153;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#21644;&#25928;&#30410;&#12290;&#20174;&#35813;&#21697;&#29260;&#23458;&#25143;&#26381;&#21153;&#20195;&#29702;&#30340;&#21453;&#39304;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#19987;&#38376;&#21270;LLM&#30340;&#31574;&#30053;-&#25552;&#31034;&#24037;&#31243;&#12289;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#21457;&#29616;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#29992;&#24615;&#21487;&#20197;&#24357;&#34917;&#25104;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32842;&#22825;&#32852;&#31995;&#23458;&#25143;&#26381;&#21153;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#30001;&#20110;&#38599;&#29992;&#23458;&#26381;&#20195;&#29702;&#21830;&#26159;&#26114;&#36149;&#30340;&#65292;&#35768;&#22810;&#20844;&#21496;&#27491;&#22312;&#36716;&#21521;NLP&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21487;&#30452;&#25509;&#20351;&#29992;&#25110;&#20462;&#25913;&#30340;&#21709;&#24212;&#26469;&#21327;&#21161;&#20154;&#31867;&#20195;&#29702;&#21830;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#36825;&#31181;&#24773;&#20917;&#30340;&#33258;&#28982;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#24517;&#39035;&#19982;&#35757;&#32451;&#21644;&#26381;&#21153;&#25104;&#26412;&#30456;&#24179;&#34913;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#20225;&#19994;&#20013;&#20316;&#20026;&#21709;&#24212;&#29983;&#25104;&#24037;&#20855;&#21487;&#23454;&#29616;&#30340;&#23454;&#38469;&#25104;&#26412;&#21644;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25104;&#26412;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#25928;&#29992;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21333;&#20010;&#21697;&#29260;&#20316;&#20026;&#29616;&#26377;&#20195;&#29702;&#21327;&#21161;&#20135;&#21697;&#32972;&#26223;&#19979;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#21697;&#29260;&#23458;&#25143;&#26381;&#21153;&#20195;&#29702;&#30340;&#21453;&#39304;&#27604;&#36739;&#20102;&#19977;&#31181;&#19987;&#38376;&#21270;LLM&#30340;&#31574;&#30053;-&#25552;&#31034;&#24037;&#31243;&#12289;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#29992;&#24615;&#21487;&#20197;&#24357;&#34917;&#24040;&#22823;&#30340;&#25104;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. Large Language Models (LLMs) are a natural fit for this use case; however, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM - prompt engineering, fine-tuning, and knowledge distillation - using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in in
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;BERT&#21644;&#24494;&#35843;&#30340;RobertA&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;AI&#26032;&#38395;&#12290; RobertA&#27169;&#22411;&#22312;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;98&#65285;&#30340;&#24471;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#25171;&#20987;&#20551;&#26032;&#38395;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.07401</link><description>&lt;p&gt;
&#23454;&#29616;BERT&#21644;&#24494;&#35843;RobertA&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#26032;&#38395;
&lt;/p&gt;
&lt;p&gt;
Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT. (arXiv:2306.07401v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07401
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;BERT&#21644;&#24494;&#35843;&#30340;RobertA&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;AI&#26032;&#38395;&#12290; RobertA&#27169;&#22411;&#22312;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;98&#65285;&#30340;&#24471;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#25171;&#20987;&#20551;&#26032;&#38395;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#20449;&#24687;&#30340;&#20016;&#23500;&#22686;&#21152;&#20102;&#20934;&#30830;&#23454;&#26102;&#35875;&#35328;&#26816;&#27979;&#30340;&#24517;&#35201;&#24615;&#12290;&#25163;&#21160;&#35782;&#21035;&#21644;&#39564;&#35777;AI&#24037;&#20855;&#29983;&#25104;&#30340;&#20551;&#26032;&#38395;&#22312;&#24040;&#22823;&#30340;&#20449;&#24687;&#37327;&#27599;&#22825;&#34987;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#21644;&#32791;&#26102;&#30340;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#33258;&#21160;&#21270;&#31995;&#32479;&#20197;&#25214;&#21040;&#20114;&#32852;&#32593;&#19978;&#20551;&#26032;&#38395;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#24494;&#35843;&#30340;BERT&#21644;RobertA&#27169;&#22411;&#22312;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20339;&#25104;&#21151;&#29575;&#12290;&#29305;&#21035;&#26159;&#24494;&#35843;&#36807;&#30340;RobertA&#22312;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24471;&#20998;&#20026;98&#65285;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;ChatGPT&#29983;&#25104;&#30340;&#20266;&#36896;&#26032;&#38395;&#12290;RobertA&#21644;BERT&#27169;&#22411;&#30340;&#20986;&#33394;&#34920;&#29616;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#20551;&#20449;&#24687;&#20316;&#26007;&#20105;&#20013;&#21487;&#20197;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of information on social media has increased the necessity of accurate real-time rumour detection. Manual techniques of identifying and verifying fake news generated by AI tools are impracticable and time-consuming given the enormous volume of information generated every day. This has sparked an increase in interest in creating automated systems to find fake news on the Internet. The studies in this research demonstrate that the BERT and RobertA models with fine-tuning had the best success in detecting AI generated news. With a score of 98%, tweaked RobertA in particular showed excellent precision. In conclusion, this study has shown that neural networks can be used to identify bogus news AI generation news created by ChatGPT. The RobertA and BERT models' excellent performance indicates that these models can play a critical role in the fight against misinformation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;&#65292;&#24182;&#36136;&#30097;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;LLMs&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#33021;&#21147;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#35828;&#27861;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23637;&#31034;&#20854;&#19982;&#20197;&#21069;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.07384</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;&#65292;&#24182;&#36136;&#30097;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;LLMs&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#33021;&#21147;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#35828;&#27861;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23637;&#31034;&#20854;&#19982;&#20197;&#21069;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23427;&#20204;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36234;&#26469;&#36234;&#22909;&#12290;&#20294;&#21363;&#20351;&#22312;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#65292;LLMs &#22312;&#21542;&#23450;&#25110;&#37327;&#21270;&#29702;&#35299;&#31561;&#31616;&#21333;&#35821;&#35328;&#27979;&#35797;&#20013;&#20173;&#28982;&#22833;&#36133;&#12290;&#20197;&#21069;&#27979;&#35797; LLMs &#23545;&#20110;&#29702;&#35299;&#37327;&#35789;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#22823;&#22810;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#26102;&#21464;&#24471;&#26356;&#22909;&#65292;&#20294;&#22312;&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#26102;&#21464;&#24471;&#36234;&#26469;&#36234;&#24046;&#65292;&#20174;&#32780;&#21576;&#29616;&#20986;&#21453;&#27604;&#20363;&#32553;&#25918;&#27861;&#21017;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36136;&#30097;&#20102;&#22312; LLMs &#20013;&#21453;&#27604;&#20363;&#32553;&#25918;&#26497;&#23569;&#25968;&#31867;&#22411;&#37327;&#35789;&#29702;&#35299;&#33021;&#21147;&#30340;&#35828;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#26159;&#19981;&#21512;&#36866;&#30340;&#27979;&#35797;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26469;&#27979;&#37327; LLMs &#30340;&#37327;&#21270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#36825;&#20123;&#34892;&#20026;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#19981;&#21516;&#12290;LLMs &#33021;&#22815;&#19981;&#26029;&#29702;&#35299;&#21547;&#20041;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
With their increasing size, Large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on testing capability of LLMs on understanding quantifiers suggest that as the size of the models increase, they get better at understanding most-type quantifiers but get increasingly worse at understanding few-type quantifiers, thus presenting a case of an inverse-scaling law. In this paper, we question the claims of inverse scaling of few-type quantifier understanding in LLMs and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that as the size of the models increase, these behaviours are different from what is shown in previous research. LLMs are consistently able to understand the difference between the meaning of
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30446;&#21069;&#20027;&#35201;&#36816;&#29992;&#20110;&#33521;&#35821;&#20869;&#23481;&#30340;&#26234;&#33021;&#20998;&#26512;&#20013;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#26088;&#22312;&#24357;&#34917;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#20844;&#21496;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25299;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#29992;&#20013;&#30340;&#23454;&#36341;&#25928;&#26524;&#20063;&#36827;&#34892;&#30740;&#31350;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;AI&#25903;&#25345;&#30340;&#35821;&#35328;&#25216;&#26415;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#38656;&#35201;&#27880;&#24847;&#26435;&#21147;&#12289;&#19981;&#24179;&#31561;&#21644;&#25991;&#21270;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07377</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation: Large Language Models in Non-English Content Analysis. (arXiv:2306.07377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07377
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30446;&#21069;&#20027;&#35201;&#36816;&#29992;&#20110;&#33521;&#35821;&#20869;&#23481;&#30340;&#26234;&#33021;&#20998;&#26512;&#20013;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#26088;&#22312;&#24357;&#34917;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#20844;&#21496;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25299;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#29992;&#20013;&#30340;&#23454;&#36341;&#25928;&#26524;&#20063;&#36827;&#34892;&#30740;&#31350;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;AI&#25903;&#25345;&#30340;&#35821;&#35328;&#25216;&#26415;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#38656;&#35201;&#27880;&#24847;&#26435;&#21147;&#12289;&#19981;&#24179;&#31561;&#21644;&#25991;&#21270;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Open AI&#30340;GPT-4&#65292;Meta&#30340;LLaMa&#65292;Google&#30340;PaLM&#65289;&#24050;&#25104;&#20026;&#26500;&#24314;&#22312;&#32447;&#35821;&#35328;&#26234;&#33021;&#20998;&#26512;&#21644;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#20171;&#25105;&#20204;&#22312;&#32593;&#19978;&#30340;&#20132;&#20114;&#65292;&#20363;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20869;&#23481;&#23457;&#26680;&#31995;&#32479;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#20027;&#35201;&#26159;&#20026;&#33521;&#35821;&#32780;&#35774;&#35745;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#19990;&#30028;&#19978;&#30340;7000&#31181;&#35821;&#35328;&#20013;&#30340;&#25928;&#26524;&#36828;&#36828;&#19981;&#22914;&#33521;&#35821;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#20844;&#21496;&#35797;&#22270;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23558;&#35299;&#37322;&#36825;&#20123;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#20197;&#21450;&#25506;&#32034;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20854;&#20013;&#65292;&#31532;&#19968;&#37096;&#20998;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#25216;&#26415;&#35299;&#37322;&#65292;&#33521;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#20043;&#38388;&#21487;&#29992;&#25968;&#25454;&#30340;&#24046;&#36317;&#20197;&#21450;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35797;&#22270;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#31532;&#20108;&#37096;&#20998;&#22238;&#39038;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;AI&#25903;&#25345;&#30340;&#35821;&#35328;&#25216;&#26415;&#22312;&#19990;&#30028;&#19978;&#35768;&#22810;&#35821;&#35328;&#20013;&#30340;&#20256;&#25773;&#30340;&#20262;&#29702;&#23398;&#24847;&#20041;&#65292;&#24182;&#24378;&#35843;&#22312;&#35774;&#35745;&#21644;&#37096;&#32626;AI&#31995;&#32479;&#26102;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#26435;&#21147;&#12289;&#19981;&#24179;&#31561;&#21644;&#25991;&#21270;&#24046;&#24322;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa, Google's PaLM) have become the dominant approach for building AI systems to analyze and generate language online. However, the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages. Recently, researchers and technology companies have attempted to extend the capabilities of large language models into languages other than English by building what are called multilingual language models.  In this paper, we explain how these multilingual language models work and explore their capabilities and limits. Part I provides a simple technical explanation of how large language models work, why there is a gap in available data between English and other languages, and how multilingual language models attempt to bridge that gap. Part 
&lt;/p&gt;</description></item><item><title>EriBERTa&#26159;&#19968;&#20010;&#38024;&#23545;&#20020;&#24202;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21307;&#23398;&#25991;&#26412;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#20174;&#19968;&#31181;&#35821;&#35328;&#21521;&#21478;&#19968;&#31181;&#35821;&#35328;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07373</link><description>&lt;p&gt;
EriBERTa&#65306;&#29992;&#20110;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21452;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing. (arXiv:2306.07373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07373
&lt;/p&gt;
&lt;p&gt;
EriBERTa&#26159;&#19968;&#20010;&#38024;&#23545;&#20020;&#24202;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21307;&#23398;&#25991;&#26412;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#20174;&#19968;&#31181;&#35821;&#35328;&#21521;&#21478;&#19968;&#31181;&#35821;&#35328;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25253;&#21578;&#30340;&#20351;&#29992;&#23545;&#20110;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20581;&#24247;&#30740;&#31350;&#21644;&#27835;&#30103;&#30417;&#25511;&#31561;&#22810;&#20010;&#27425;&#35201;&#29992;&#36884;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#24050;&#25104;&#20026;&#20174;&#36825;&#20123;&#25253;&#21578;&#20013;&#25552;&#21462;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#30340;&#26377;&#20215;&#20540;&#36164;&#20135;&#12290;&#28982;&#32780;&#65292;&#19987;&#38376;&#38024;&#23545;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;EriBERTa&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#24191;&#27867;&#30340;&#21307;&#30103;&#21644;&#20020;&#24202;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#21452;&#35821;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;EriBERTa&#22312;&#20020;&#24202;&#39046;&#22495;&#20013;&#32988;&#36807;&#20808;&#21069;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#29702;&#35299;&#21307;&#23398;&#25991;&#26412;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;EriBERTa&#23637;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#20801;&#35768;&#20174;&#19968;&#31181;&#35821;&#35328;&#21521;&#21478;&#19968;&#31181;&#35821;&#35328;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#37492;&#20110;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#19968;&#26041;&#38754;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of clinical reports for various secondary purposes, including health research and treatment monitoring, is crucial for enhancing patient care. Natural Language Processing (NLP) tools have emerged as valuable assets for extracting and processing relevant information from these reports. However, the availability of specialized language models for the clinical domain in Spanish has been limited.  In this paper, we introduce EriBERTa, a bilingual domain-specific language model pre-trained on extensive medical and clinical corpora. We demonstrate that EriBERTa outperforms previous Spanish language models in the clinical domain, showcasing its superior capabilities in understanding medical texts and extracting meaningful information. Moreover, EriBERTa exhibits promising transfer learning abilities, allowing for knowledge transfer from one language to another. This aspect is particularly beneficial given the scarcity of Spanish clinical data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#65292;Transformer&#33021;&#22815;&#29702;&#35299;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#19988;&#23454;&#29616;&#24182;&#34892;&#22788;&#29702;&#65292;&#22312;NLP&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#22788;&#29702;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29289;&#32852;&#32593;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.07303</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. (arXiv:2306.07303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#65292;Transformer&#33021;&#22815;&#29702;&#35299;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#19988;&#23454;&#29616;&#24182;&#34892;&#22788;&#29702;&#65292;&#22312;NLP&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#22788;&#29702;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29289;&#32852;&#32593;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#29702;&#35299;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#25110;&#26356;&#26032;&#29256;&#26412;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65288;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#65289;&#19981;&#21516;&#65292;Transformer&#27169;&#22411;&#22312;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#20803;&#32032;&#20043;&#38388;&#30340;&#38271;&#20381;&#36182;&#20851;&#31995;&#21644;&#23454;&#29616;&#24182;&#34892;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#12290;&#36825;&#24471;&#30410;&#20110;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20197;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#35821;&#38899;&#22788;&#29702;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#21644;&#26174;&#33879;&#25104;&#23601;&#12290;&#34429;&#28982;&#24050;&#32463;&#20986;&#29256;&#20102;&#20960;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;Transformer&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#36129;&#29486;&#12289;&#26550;&#26500;&#24046;&#24322;&#25110;&#24615;&#33021;&#35780;&#20272;&#65292;&#20294;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant abs
&lt;/p&gt;</description></item><item><title>&#38899;&#39057;&#35782;&#21035;&#38169;&#35823;&#23545;&#25945;&#23398;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#20154;&#38469;&#20851;&#31995;&#26080;&#26174;&#33879;&#24433;&#21709;&#65292;&#32467;&#26524;&#20026;&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#38169;&#35823;&#24674;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.07302</link><description>&lt;p&gt;
&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#24212;&#65288;&#35782;&#21035;&#65289;&#31572;&#38169;&#35823;&#23545;&#23398;&#20064;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Experiencing Misrecognition by Teachable Agents on Learning and Rapport. (arXiv:2306.07302v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07302
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35782;&#21035;&#38169;&#35823;&#23545;&#25945;&#23398;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#20154;&#38469;&#20851;&#31995;&#26080;&#26174;&#33879;&#24433;&#21709;&#65292;&#32467;&#26524;&#20026;&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#38169;&#35823;&#24674;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20351;&#29992;&#35821;&#38899;&#30340;&#25945;&#23398;&#20195;&#29702;&#20154;&#30456;&#27604;&#20110;&#22522;&#20110;&#25171;&#23383;&#30340;&#20195;&#29702;&#20154;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#38899;&#39057;&#35782;&#21035;&#38169;&#35823;&#12290;&#36825;&#20123;&#38169;&#35823;&#21487;&#33021;&#20250;&#25193;&#25955;&#65292;&#23548;&#33268;&#23545;&#35805;&#27969;&#31243;&#30340;&#24847;&#22806;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#21464;&#21270;&#19982;&#23398;&#20064;&#25910;&#30410;&#20197;&#21450;&#23398;&#20064;&#32773;&#19982;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20154;&#38469;&#20851;&#31995;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21464;&#21270;&#26080;&#35770;&#20195;&#29702;&#20154;&#22312;&#19981;&#20135;&#29983;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#24212;&#35813;&#32473;&#20104;&#20309;&#31181;&#22238;&#24212;&#65292;&#37117;&#19982;&#23398;&#20064;&#25910;&#30410;&#25110;&#20154;&#38469;&#20851;&#31995;&#26080;&#20851;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21487;&#20174;&#36825;&#20123;&#21457;&#29616;&#20013;&#25512;&#20986;&#30340;&#36866;&#24403;&#38169;&#35823;&#24674;&#22797;&#31574;&#30053;&#23545;&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While speech-enabled teachable agents have some advantages over typing-based ones, they are vulnerable to errors stemming from misrecognition by automatic speech recognition (ASR). These errors may propagate, resulting in unexpected changes in the flow of conversation. We analyzed how such changes are linked with learning gains and learners' rapport with the agents. Our results show they are not related to learning gains or rapport, regardless of the types of responses the agents should have returned given the correct input from learners without ASR errors. We also discuss the implications for optimal error-recovery policies for teachable agents that can be drawn from these findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#30005;&#23376;&#30149;&#21382;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.07297</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#21307;&#30103;&#25968;&#25454;&#22686;&#24191;&#65306;&#22522;&#20110;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification. (arXiv:2306.07297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#30005;&#23376;&#30149;&#21382;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#30149;&#21382;&#21644;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#20851;&#32852;&#24615;&#31561;&#20851;&#38190;&#22240;&#32032;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#20197;&#20811;&#26381;&#30005;&#23376;&#30149;&#21382;&#20013;&#20851;&#38190;&#22240;&#32032;&#26631;&#27880;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#31561;&#20004;&#20010;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification of key factors such as medications, diseases, and relationships within electronic health records and clinical notes has a wide range of applications in the clinical field. In the N2C2 2022 competitions, various tasks were presented to promote the identification of key factors in electronic health records (EHRs) using the Contextualized Medication Event Dataset (CMED). Pretrained large language models (LLMs) demonstrated exceptional performance in these tasks. This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs. Additionally, different pre-trained BERT models, initially trained on extensive datasets like Wikipedia and MIMIC, were employed to develop models for identifying these key variables in EHRs through fine-tuning on augmented datasets. The experimental results of two EHR analysis tasks, namely medication identification and me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LTCR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20934;&#30830;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;COVID-19&#30456;&#20851;&#30340;&#22797;&#26434;&#34394;&#20551;&#26032;&#38395;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1,729&#20010;&#30495;&#23454;&#26032;&#38395;&#21644;500&#20010;&#34394;&#20551;&#26032;&#38395;&#12290;&#22522;&#20110;LTCR&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;SFD&#65288;Salience-aware Fake News Detection Model&#65289;&#31639;&#27861;&#65292;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#34394;&#20551;&#26032;&#38395;&#22238;&#25910;&#29575;&#21644;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.07201</link><description>&lt;p&gt;
LTCR&#65306;&#38271;&#25991;&#26412;&#20013;&#25991;&#35875;&#35328;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LTCR: Long-Text Chinese Rumor Detection Dataset. (arXiv:2306.07201v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LTCR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20934;&#30830;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;COVID-19&#30456;&#20851;&#30340;&#22797;&#26434;&#34394;&#20551;&#26032;&#38395;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1,729&#20010;&#30495;&#23454;&#26032;&#38395;&#21644;500&#20010;&#34394;&#20551;&#26032;&#38395;&#12290;&#22522;&#20110;LTCR&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;SFD&#65288;Salience-aware Fake News Detection Model&#65289;&#31639;&#27861;&#65292;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#34394;&#20551;&#26032;&#38395;&#22238;&#25910;&#29575;&#21644;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#34394;&#20551;&#20449;&#24687;&#24448;&#24448;&#33021;&#22815;&#36805;&#36895;&#20256;&#25773;&#65292;&#20174;&#32780;&#23545;&#20844;&#27665;&#30340;&#34892;&#20026;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#21453;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#26356;&#22909;&#22320;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#36739;&#38271;&#25991;&#26412;&#65292;&#26356;&#38590;&#20197;&#23436;&#25972;&#26597;&#25214;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LTCR&#30340;&#38271;&#25991;&#26412;&#20013;&#25991;&#35875;&#35328;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;LTCR&#25968;&#25454;&#38598;&#20026;&#20934;&#30830;&#26816;&#27979;&#35875;&#35328;&#65292;&#29305;&#21035;&#26159;&#19982;COVID-19&#30456;&#20851;&#30340;&#22797;&#26434;&#34394;&#20551;&#26032;&#38395;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1,729&#26465;&#30495;&#23454;&#26032;&#38395;&#21644;500&#26465;&#34394;&#20551;&#26032;&#38395;&#65292;&#20854;&#20013;&#30495;&#23454;&#26032;&#38395;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#24179;&#22343;&#38271;&#24230;&#20998;&#21035;&#32422;&#20026;230&#21644;152&#20010;&#23383;&#31526;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21517;&#20026;SFD&#65288;Salience-aware Fake News Detection Model&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;LTCR&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65288;95.85&#65285;&#65289;&#65292;&#34394;&#20551;&#26032;&#38395;&#22238;&#25910;&#29575;&#65288;90.91&#65285;&#65289;&#21644;F1&#20998;&#25968;&#65288;90.60&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
False information can spread quickly on social media, negatively influencing the citizens' behaviors and responses to social events. To better detect all of the fake news, especially long texts which are harder to find completely, a Long-Text Chinese Rumor detection dataset named LTCR is proposed. The LTCR dataset provides a valuable resource for accurately detecting misinformation, especially in the context of complex fake news related to COVID-19. The dataset consists of 1,729 and 500 pieces of real and fake news, respectively. The average lengths of real and fake news are approximately 230 and 152 characters. We also propose \method, Salience-aware Fake News Detection Model, which achieves the highest accuracy (95.85%), fake news recall (90.91%) and F-score (90.60%) on the dataset. (https://github.com/Enderfga/DoubleCheck)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#65288;MLU&#65289;&#27169;&#22359;&#30340;&#21475;&#35821;&#29702;&#35299;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#33258;&#25105;&#30417;&#30563;&#29305;&#24449;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#21644;RoBERTa&#65289;&#65292;&#20197;&#20943;&#36731;&#30001;ASR&#38169;&#35823;&#20256;&#25773;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.06819</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#38899;&#25991;&#26723;&#26550;&#26500;&#30340;&#40065;&#26834;&#24615;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multimodal Audio-textual Architecture for Robust Spoken Language Understanding. (arXiv:2306.06819v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#65288;MLU&#65289;&#27169;&#22359;&#30340;&#21475;&#35821;&#29702;&#35299;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#33258;&#25105;&#30417;&#30563;&#29305;&#24449;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#21644;RoBERTa&#65289;&#65292;&#20197;&#20943;&#36731;&#30001;ASR&#38169;&#35823;&#20256;&#25773;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#38899;&#21161;&#25163;&#36890;&#24120;&#22522;&#20110;&#32423;&#32852;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24341;&#25806;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#31995;&#32479;&#12290;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#20381;&#38752;ASR&#36755;&#20986;&#65292;&#22240;&#27492;&#32463;&#24120;&#36973;&#21463;&#25152;&#35859;&#30340;ASR&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27492;&#31867;ASR&#38169;&#35823;&#20256;&#25773;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65288;&#22914;BERT&#21644;RoBERTa&#65289;&#30340;&#26368;&#20808;&#36827;NLU&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#65288;MLU&#65289;&#27169;&#22359;&#65292;&#20197;&#20943;&#36731;&#30001;ASR&#36716;&#24405;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;SLU&#24615;&#33021;&#19979;&#38477;&#12290;MLU&#21463;&#30410;&#20110;&#20174;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#23398;&#20064;&#30340;&#33258;&#25105;&#30417;&#30563;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;Wav2Vec&#29992;&#20110;&#35821;&#38899;&#21644;Bert / RoBERTa&#29992;&#20110;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;MLU&#32467;&#21512;&#19968;&#20010;&#32534;&#30721;&#22120;&#32593;&#32476;&#26469;&#23884;&#20837;&#38899;&#39057;&#20449;&#21495;&#21644;&#19968;&#20010;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#25991;&#26412;&#36716;&#24405;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#21518;&#26399;&#34701;&#21512;&#23618;&#26469;&#34701;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#36923;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36825;&#31181;&#22810;&#27169;&#24335;&#38899;&#25991;&#26723;&#26550;&#26500;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#21475;&#35821;&#29702;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent voice assistants are usually based on the cascade spoken language understanding (SLU) solution, which consists of an automatic speech recognition (ASR) engine and a natural language understanding (NLU) system. Because such approach relies on the ASR output, it often suffers from the so-called ASR error propagation. In this work, we investigate impacts of this ASR error propagation on state-of-the-art NLU systems based on pre-trained language models (PLM), such as BERT and RoBERTa. Moreover, a multimodal language understanding (MLU) module is proposed to mitigate SLU performance degradation caused by errors present in the ASR transcript. The MLU benefits from self-supervised features learned from both audio and text modalities, specifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines an encoder network to embed the audio signal and a text encoder to process text transcripts followed by a late fusion layer to fuse audio and text logits. We found that the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;ESG&#38382;&#39064;&#35782;&#21035;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23545;MSCI ESG&#35780;&#32423;&#25351;&#21335;&#23450;&#20041;&#30340;35&#20010;ESG&#20851;&#38190;&#38382;&#39064;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#35782;&#21035;&#25104;&#26524;&#65292;&#20026;ESG&#20027;&#39064;&#30340;&#25506;&#32034;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.06662</link><description>&lt;p&gt;
EaSyGuide&#65306;&#21033;&#29992;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;ESG&#38382;&#39064;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EaSyGuide : ESG Issue Identification Framework leveraging Abilities of Generative Large Language Models. (arXiv:2306.06662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;ESG&#38382;&#39064;&#35782;&#21035;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23545;MSCI ESG&#35780;&#32423;&#25351;&#21335;&#23450;&#20041;&#30340;35&#20010;ESG&#20851;&#38190;&#38382;&#39064;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#35782;&#21035;&#25104;&#26524;&#65292;&#20026;ESG&#20027;&#39064;&#30340;&#25506;&#32034;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21442;&#21152;FinNLP-2023&#22810;&#35821;&#35328;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#20225;&#19994;&#27835;&#29702;&#38382;&#39064;&#35782;&#21035;&#65288;ML-ESG&#65289;&#20849;&#20139;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;MSCI ESG&#35780;&#32423;&#25351;&#21335;&#23450;&#20041;&#30340;35&#20010;ESG&#20851;&#38190;&#38382;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#33521;&#35821;&#21644;&#27861;&#35821;&#23376;&#20219;&#21153;&#19978;&#65292;&#37319;&#29992;CerebrasGPT&#12289;OPT&#21644;Pythia&#27169;&#22411;&#65292;&#20197;&#21450;&#38646;-shot&#21644;GPT3Mix&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#21033;&#29992;&#21508;&#31181;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22914;RoBERTa&#12289;DeBERTa&#21644;FinBERT&#65292;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#39069;&#22806;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#22312;&#33521;&#35821;&#25991;&#26412;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;F1-score 0.69&#30340;&#31532;&#19968;&#21517;&#65292;&#22312;&#27861;&#35821;&#25991;&#26412;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;F1-score 0.78&#30340;&#31532;&#20108;&#21517;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#20013;&#35782;&#21035;ESG&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;ESG&#20027;&#39064;&#30340;&#25506;&#32034;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#24378;&#35843;&#20102;&#25216;&#26415;&#21019;&#26032;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our participation in the FinNLP-2023 shared task on multi-lingual environmental, social, and corporate governance issue identification (ML-ESG). The task's objective is to classify news articles based on the 35 ESG key issues defined by the MSCI ESG rating guidelines. Our approach focuses on the English and French subtasks, employing the CerebrasGPT, OPT, and Pythia models, along with the zero-shot and GPT3Mix Augmentation techniques. We utilize various encoder models, such as RoBERTa, DeBERTa, and FinBERT, subjecting them to knowledge distillation and additional training.  Our approach yielded exceptional results, securing the first position in the English text subtask with F1-score 0.69 and the second position in the French text subtask with F1-score 0.78. These outcomes underscore the effectiveness of our methodology in identifying ESG issues in news articles across different languages. Our findings contribute to the exploration of ESG topics and highlight the po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04551</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#35757;&#32451;&#32467;&#21512;&#39046;&#22495;&#20869;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35786;&#26029;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26159;&#22686;&#24378;&#20020;&#24202;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#21644;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#35786;&#26029;&#25512;&#29702;&#22522;&#20934;&#65288;DR.BENCH&#65289;&#20316;&#20026;&#20840;&#38754;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#30001;&#20845;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#20195;&#34920;&#20020;&#24202;&#25512;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880; DR.BENCH &#30340;&#38382;&#39064;&#24635;&#32467;&#20219;&#21153;&#65288;Gao &#31561;&#65292;2023&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20248;&#20110;&#20854;&#19968;&#33324;&#39046;&#22495;&#30340;&#23545;&#24212;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292; ROUGE-L &#24471;&#20998;&#20026; 28.55&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#22312;&#20248;&#21270;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH (Gao et al., 2023). We demonstrate that a multi-task, clinically trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.
&lt;/p&gt;</description></item><item><title>PolyVoice&#26159;&#19968;&#20010;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#24182;&#20351;&#29992;&#31163;&#25955;&#21270;&#30340;&#35821;&#38899;&#21333;&#20803;&#23454;&#29616;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#38750;&#20070;&#38754;&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#21644;&#38899;&#39057;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.02982</link><description>&lt;p&gt;
PolyVoice&#65306;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PolyVoice: Language Models for Speech to Speech Translation. (arXiv:2306.02982v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02982
&lt;/p&gt;
&lt;p&gt;
PolyVoice&#26159;&#19968;&#20010;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#24182;&#20351;&#29992;&#31163;&#25955;&#21270;&#30340;&#35821;&#38899;&#21333;&#20803;&#23454;&#29616;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#38750;&#20070;&#38754;&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#21644;&#38899;&#39057;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PolyVoice&#65292;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#65306;&#32763;&#35793;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#31163;&#25955;&#21270;&#30340;&#35821;&#38899;&#21333;&#20803;&#65292;&#36825;&#20123;&#21333;&#20803;&#23436;&#20840;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#29983;&#25104;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#38750;&#20070;&#38754;&#35821;&#35328;&#12290;&#23545;&#20110;&#35821;&#38899;&#21512;&#25104;&#37096;&#20998;&#65292;&#25105;&#20204;&#37319;&#29992;&#29616;&#26377;&#30340;VALL-E X&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#21333;&#20803;&#30340;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#30041;&#21407;&#22987;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#24449;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20013;&#25991; $\rightarrow$ &#33521;&#25991;&#21644;&#33521;&#25991; $\rightarrow$ &#35199;&#29677;&#29273;&#35821;&#23545;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#21644;&#38899;&#39057;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;&#35821;&#38899;&#26679;&#26412;&#21487;&#22312;https://speechtranslation.github.io/polyvoice&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose PolyVoice, a language model-based framework for speech-to-speech translation (S2ST) system. Our framework consists of two language models: a translation language model and a speech synthesis language model. We use discretized speech units, which are generated in a fully unsupervised way, and thus our framework can be used for unwritten languages. For the speech synthesis part, we adopt the existing VALL-E X approach and build a unit-based audio language model. This grants our framework the ability to preserve the voice characteristics and the speaking style of the original speech. We examine our system on Chinese $\rightarrow$ English and English $\rightarrow$ Spanish pairs. Experimental results show that our system can generate speech with high translation quality and audio quality. Speech samples are available at https://speechtranslation.github.io/polyvoice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#25506;&#35752;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26032;&#25351;&#26631;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#27169;&#22411;&#21644;&#29992;&#25143;&#30028;&#38754;&#31561;&#65292;&#20197;&#25506;&#31350;&#23427;&#26159;&#21542;&#26159;IR&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#35813;&#30740;&#35752;&#20250;&#20851;&#27880;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22320;&#28857;&#29992;&#20110;&#25506;&#35752;&#21644;&#25506;&#32034;&#22914;&#20309;&#23558;&#29983;&#25104;&#24335;IR&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.02887</link><description>&lt;p&gt;
2023&#24180;SIGIR&#20250;&#35758;&#19978;&#30340;Gen-IR&#30740;&#35752;&#20250;&#65306;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#30340;&#39318;&#20010;&#30740;&#35752;&#20250;
&lt;/p&gt;
&lt;p&gt;
Gen-IR @ SIGIR 2023: The First Workshop on Generative Information Retrieval. (arXiv:2306.02887v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#25506;&#35752;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26032;&#25351;&#26631;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#27169;&#22411;&#21644;&#29992;&#25143;&#30028;&#38754;&#31561;&#65292;&#20197;&#25506;&#31350;&#23427;&#26159;&#21542;&#26159;IR&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#35813;&#30740;&#35752;&#20250;&#20851;&#27880;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22320;&#28857;&#29992;&#20110;&#25506;&#35752;&#21644;&#25506;&#32034;&#22914;&#20309;&#23558;&#29983;&#25104;&#24335;IR&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#22312;&#22810;&#20010;&#30740;&#31350;&#31038;&#21306;&#65288;&#20363;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65289;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#24182;&#22312;&#27969;&#34892;&#23186;&#20307;&#19978;&#22791;&#21463;&#20851;&#27880;&#12290;&#24050;&#21457;&#24067;&#20102;&#29702;&#35770;&#12289;&#23454;&#35777;&#21644;&#23454;&#38469;&#29992;&#25143;&#20135;&#21697;&#65292;&#36825;&#20123;&#20135;&#21697;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#25991;&#26723;&#65288;&#36890;&#36807;&#29983;&#25104;&#65289;&#25110;&#30452;&#25509;&#29983;&#25104;&#31572;&#26696;&#26469;&#26816;&#32034;&#25991;&#26723;&#25110;&#22238;&#31572;&#36755;&#20837;&#35831;&#27714;&#12290;&#25105;&#20204;&#24819;&#35843;&#26597;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#21482;&#26159;&#21478;&#19968;&#31181;&#36235;&#21183;&#65292;&#36824;&#26159;&#20687;&#26576;&#20123;&#20154;&#25152;&#22768;&#31216;&#30340;&#37027;&#26679;&#65292;&#26159;IR&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#38656;&#35201;&#26032;&#30340;&#25351;&#26631;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#27169;&#22411;&#12289;&#29992;&#25143;&#30028;&#38754;&#31561;&#12290;&#26412;&#27425;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#20851;&#27880;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#29983;&#25104;&#24335;IR&#25216;&#26415;&#65292;&#22914;&#25991;&#26723;&#26816;&#32034;&#21644;&#30452;&#25509;&#23454;&#29616;&#30340;&#22522;&#30784;&#31572;&#26696;&#29983;&#25104;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#19968;&#20010;&#22320;&#28857;&#29992;&#20110;&#25506;&#35752;&#21644;&#25506;&#32034;&#29983;&#25104;&#24335;IR&#22914;&#20309;&#24212;&#29992;&#20110;&#25512;&#33616;&#31561;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative information retrieval (IR) has experienced substantial growth across multiple research communities (e.g., information retrieval, computer vision, natural language processing, and machine learning), and has been highly visible in the popular press. Theoretical, empirical, and actual user-facing products have been released that retrieve documents (via generation) or directly generate answers given an input request. We would like to investigate whether end-to-end generative models are just another trend or, as some claim, a paradigm change for IR. This necessitates new metrics, theoretical grounding, evaluation methods, task definitions, models, user interfaces, etc. The goal of this workshop (https://coda.io/@sigir/gen-ir) is to focus on previously explored Generative IR techniques like document retrieval and direct Grounded Answer Generation, while also offering a venue for the discussion and exploration of how Generative IR can be applied to new domains like recommendation s
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#35201;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#36149;&#37325;&#30340;GPU&#23548;&#33268;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#65292;OWQ&#25552;&#20986;&#30340;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#36136;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36825;&#31181;&#38480;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#26469;&#30830;&#23450;&#26435;&#20540;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#20026;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#20998;&#37197;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#19982;OPTQ&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.02272</link><description>&lt;p&gt;
OWQ&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20013;&#28608;&#27963;&#31163;&#32676;&#20540;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02272
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#35201;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#36149;&#37325;&#30340;GPU&#23548;&#33268;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#65292;OWQ&#25552;&#20986;&#30340;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#36136;&#37327;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36825;&#31181;&#38480;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#26469;&#30830;&#23450;&#26435;&#20540;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#20026;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#20998;&#37197;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#19982;OPTQ&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#21644;&#23569;&#37327;&#30340;&#31034;&#20363;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21497;&#30340;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24040;&#22823;&#30340;&#23610;&#23544;&#35201;&#27714;&#29978;&#33267;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#26381;&#21153;&#22120;&#32423;&#30340;GPU&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#26435;&#37325;&#65292;&#20943;&#23569;&#36136;&#37327;&#25439;&#22833;&#12290;&#34429;&#28982;&#24050;&#30693;&#28608;&#27963;&#31163;&#32676;&#20540;&#22312;&#28608;&#27963;&#37327;&#21270;&#20013;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#32771;&#34385;&#28608;&#27963;&#31163;&#32676;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#23548;&#33268;&#26435;&#37325;&#37327;&#21270;&#35823;&#24046;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Outlier-Aware Weight Quantization (OWQ)&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#26435;&#37325;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;OWQ&#29983;&#25104;&#30340;3.01&#20301;&#27169;&#22411;&#20855;&#26377;&#19982;OPTQ&#29983;&#25104;&#30340;4&#20301;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with hundreds of billions of parameters show impressive results across various language tasks using simple prompt tuning and few-shot examples, without the need for task-specific fine-tuning. However, their enormous size requires multiple server-grade GPUs even for inference, creating a significant cost barrier. To address this limitation, we introduce a novel post-training quantization method for weights with minimal quality degradation. While activation outliers are known to be problematic in activation quantization, our theoretical analysis suggests that we can identify factors contributing to weight quantization errors by considering activation outliers. We propose an innovative PTQ scheme called outlier-aware weight quantization (OWQ), which identifies vulnerable weights and allocates high-precision to them. Our extensive experiments demonstrate that the 3.01-bit models produced by OWQ exhibit comparable quality to the 4-bit models generated by OPTQ.
&lt;/p&gt;</description></item><item><title>DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19395</link><description>&lt;p&gt;
DyGen: &#36890;&#36807;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#24314;&#27169;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19395
&lt;/p&gt;
&lt;p&gt;
DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#19981;&#27491;&#30830;&#25110;&#24050;&#25439;&#22351;&#30340;&#26631;&#31614;&#65292;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#21435;&#22122;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#26041;&#38754;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#26377;&#20559;&#30340;&#25110;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DyGen&#30340;&#21160;&#24577;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#26469;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#39044;&#27979;&#12290;DyGen&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#26694;&#26550;&#20174;&#22122;&#22768;&#26631;&#31614;&#21644;&#35757;&#32451;&#21160;&#24577;&#20013;&#25512;&#26029;&#30495;&#23454;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#12290;&#22312;&#23384;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#26631;&#31614;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;DyGen&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37322;&#20041;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#23545;&#27604;&#22411;Prompt&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#30784;Prompt&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.18169</link><description>&lt;p&gt;
LM-CPPF: &#22522;&#20110;&#37322;&#20041;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#23545;&#27604;&#22411;Prompt&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37322;&#20041;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#23545;&#27604;&#22411;Prompt&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#30784;Prompt&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#24494;&#35843;&#20173;&#28982;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#12290;&#23545;&#22522;&#30784;Prompt&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#31181;&#36739;&#20026;&#26222;&#36941;&#30340;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#28155;&#21152;&#21040;&#23545;Prompt&#30340;&#24494;&#35843;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26356;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#20998;&#31867;&#20043;&#38388;&#30340;&#23884;&#20837;&#65292;&#32780;&#19988;&#23427;&#21516;&#26102;&#36824;&#33021;&#20174;&#27491;&#36127;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#26356;&#21152;&#33410;&#30465;&#26679;&#26412;&#12290;&#23545;&#27604;&#23398;&#20064;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#26159;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;&#20110;NLP&#26469;&#35828;&#65292;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LM-CPPF&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#22411;&#23545;&#27604;Prompt&#24494;&#35843;&#65292;&#23427;&#21033;&#29992;&#22522;&#30784;Prompt&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PaCE&#26159;&#19968;&#20010;&#32479;&#19968;&#12289;&#32467;&#26500;&#21270;&#12289;&#32452;&#21512;&#24335;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#20960;&#20010;&#22522;&#26412;&#19987;&#23478;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#20010;&#19982;&#23545;&#35805;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#23545;&#35805;&#21644;&#24191;&#27867;&#30340;&#38750;&#23545;&#35805;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.14839</link><description>&lt;p&gt;
PaCE&#65306;&#28176;&#36827;&#24335;&#21644;&#32452;&#21512;&#24335;&#19987;&#23478;&#32479;&#19968;&#22810;&#27169;&#24577;&#23545;&#35805;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts. (arXiv:2305.14839v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PaCE&#26159;&#19968;&#20010;&#32479;&#19968;&#12289;&#32467;&#26500;&#21270;&#12289;&#32452;&#21512;&#24335;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#20960;&#20010;&#22522;&#26412;&#19987;&#23478;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#20010;&#19982;&#23545;&#35805;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#23545;&#35805;&#21644;&#24191;&#27867;&#30340;&#38750;&#23545;&#35805;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#39044;&#35757;&#32451;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#30446;&#21069;&#20851;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#20173;&#30456;&#23545;&#31232;&#32570;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaCE&#30340;&#32479;&#19968;&#12289;&#32467;&#26500;&#21270;&#12289;&#32452;&#21512;&#24335;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#20960;&#20010;&#22522;&#26412;&#19987;&#23478;&#30340;&#32452;&#21512;&#65292;&#20197;&#36866;&#24212;&#22810;&#20010;&#19982;&#23545;&#35805;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#23545;&#35805;&#21644;&#24191;&#27867;&#30340;&#38750;&#23545;&#35805;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new forms of tasks may arise at unpredictable points in the future. Hence, it is essential for designed multi-modal dialogue models to possess sufficient flexibility to adapt to such scenarios. This paper proposes \textbf{PaCE}, a unified, structured, compositional multi-modal dialogue pre-training framework. It utilizes a combination of several fundamental experts to accommodate multiple dialogue-related tasks and can be pre-trained using limited dialogue and extensive non-dialogue multi-modal data.
&lt;/p&gt;</description></item><item><title>MultiModal-GPT&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#20154;&#31867;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#30340;&#21508;&#31181;&#25351;&#20196;&#65292;&#24182;&#19988;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.04790</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;-GPT: &#29992;&#20110;&#19982;&#20154;&#31867;&#23545;&#35805;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04790
&lt;/p&gt;
&lt;p&gt;
MultiModal-GPT&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#20154;&#31867;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#30340;&#21508;&#31181;&#25351;&#20196;&#65292;&#24182;&#19988;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MultiModal-GPT&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#19982;&#20154;&#31867;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290; MultiModal-GPT&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#30340;&#21508;&#31181;&#25351;&#20196;&#65292;&#20363;&#22914;&#29983;&#25104;&#35814;&#32454;&#30340;&#23383;&#24149;&#65292;&#35745;&#31639;&#24863;&#20852;&#36259;&#23545;&#35937;&#30340;&#25968;&#37327;&#20197;&#21450;&#22238;&#31572;&#29992;&#25143;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290; &#25105;&#20204;&#36890;&#36807;OpenFlamingo&#36827;&#34892;&#21442;&#25968;&#26377;&#25928;&#22320;&#24494;&#35843;MultiModal-GPT&#65292;&#24182;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#20851;&#27880;&#37096;&#20998;&#21644;&#33258;&#25105;&#20851;&#27880;&#37096;&#20998;&#20013;&#28155;&#21152;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#12290; &#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#26500;&#24314;&#25351;&#20196;&#27169;&#26495;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65292;&#35753;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290; &#25105;&#20204;&#21457;&#29616;&#23545;&#35805;&#34920;&#29616;&#30340;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#23569;&#21253;&#21547;&#31616;&#30701;&#22238;&#31572;&#30340;&#25968;&#25454;&#20250;&#20351;&#27169;&#22411;&#23545;&#20219;&#20309;&#25351;&#20196;&#37117;&#20316;&#20986;&#31616;&#30701;&#22238;&#31572;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;MultiModal-GPT&#19982;&#20154;&#31867;&#32842;&#22825;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;&#20165;&#35821;&#35328;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#32852;&#21512;&#35757;&#32451;MultiModal-GPT&#12290;&#32852;&#21512;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;MultiModal-GPT&#22312;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint tra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02549</link><description>&lt;p&gt;
FormNetV2&#65306;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#34920;&#26684;&#25991;&#26723;&#29702;&#35299;&#20013;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21040;&#20854;&#20182;&#27169;&#24577;&#30340;&#26041;&#27861;&#38656;&#35201;&#20180;&#32454;&#30340;&#22810;&#20219;&#21153;&#35843;&#25972;&#12289;&#22797;&#26434;&#30340;&#37325;&#26500;&#30446;&#26631;&#35774;&#35745;&#25110;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;FormNetV2&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#22270;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#12290;&#22270;&#23545;&#27604;&#30446;&#26631;&#26368;&#22823;&#21270;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#25152;&#26377;&#27169;&#24577;&#25552;&#20379;&#33258;&#28982;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#27530;&#30340;&#23450;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36830;&#25509;&#22270;&#36793;&#32536;&#30340;&#19968;&#23545;&#26631;&#35760;&#30340;&#36793;&#26694;&#20869;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#25429;&#25417;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#32780;&#26080;&#38656;&#21152;&#36733;&#32463;&#36807;&#22797;&#26434;&#21644;&#21333;&#29420;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#23884;&#20837;&#22120;&#12290;FormNetV2&#22312;FUNSD&#12289;CORD&#12289;SROIE&#21644;Payment&#22522;&#20934;&#27979;&#35797;&#20013;&#30830;&#31435;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.02721</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#23545;&#31216;&#24615;&#65306;&#32467;&#26500;&#21098;&#26525;&#25552;&#39640;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#31616;&#27905;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#20351;&#24471;&#22312;&#24310;&#36831;&#25935;&#24863;&#25110; Web &#35268;&#27169;&#30340;&#23454;&#29616;&#20013;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#32534;&#30721;&#22120;&#22823;&#23567;&#26377;&#20851;&#65292;&#32780;&#25512;&#29702;&#25928;&#29575;&#19982;&#35299;&#30721;&#22120;&#26377;&#20851;&#12290;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#23548;&#33268;&#25512;&#26029;&#24310;&#36831;&#30340;&#36817;3&#20493;&#25552;&#39640;&#65292;Rouge-2&#30340;&#25439;&#22833;&#32422;&#20026;1&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#24615;&#33021;&#38477;&#20302;&#21644;&#19981;&#23545;&#31216;&#24615;&#30340;&#20316;&#29992;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#38598;&#21464;&#21270;&#26041;&#38754;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence language models can be used to produce abstractive summaries which are coherent, relevant, and concise. Still, model sizes can make deployment in latency-sensitive or web-scale implementations difficult. This paper studies the relationship between model size, structured pruning, inference efficiency, and summarization accuracy on widely used summarization datasets. We show that model accuracy is tied to the encoder size while inference efficiency is connected to the decoder. Using asymmetric pruning can lead to nearly 3x improvement in inference latency with ~1 point loss in Rouge-2. Moreover, we find both the average degradation and the role of asymmetry to be consistent across model sizes and variations in datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6 &#24320;&#21457;&#30340; Legal-BERT-HSLN &#27169;&#22411;&#21644; Legal-LUKE &#27169;&#22411;&#65292;&#20854;&#20013; Legal-BERT-HSLN &#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65292;Legal-LUKE &#27169;&#22411;&#26159;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#12290;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#33021;&#22815;&#35299;&#20915;&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12135</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6 &#24320;&#21457;&#30340; Legal-BERT-HSLN &#27169;&#22411;&#21644; Legal-LUKE &#27169;&#22411;&#65292;&#20854;&#20013; Legal-BERT-HSLN &#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65292;Legal-LUKE &#27169;&#22411;&#26159;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#12290;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#33021;&#22815;&#35299;&#20915;&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#22914;&#21360;&#24230;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#22788;&#29702;&#21644;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6&#65288;Modi &#31561;&#20154;&#65292;2023&#65289;&#25152;&#24320;&#21457;&#30340;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#31995;&#32479;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102; Legal-BERT-HSLN &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#32508;&#21512;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65288;&#23376;&#20219;&#21153; A&#65289;&#65292;&#28982;&#21518;&#35757;&#32451;&#20986; Legal-LUKE &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21270;&#21644;&#23454;&#20307;&#24863;&#30693;&#33021;&#21147;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#65288;&#23376;&#20219;&#21153; B&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#22914;&#22312;&#23376;&#20219;&#21153; B &#20013; F1 &#20540;&#25552;&#39640;&#20102;&#36798; 15.0%&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22914; 0.834 &#24494;&#24179;&#22343; F1 &#20540;&#65292;&#24182;&#22312;&#23376;&#20219;&#21153; A &#20013;&#25490;&#21517;&#31532; 5&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of pending legal cases in populous countries, such as India, has become a major issue. Developing effective techniques to process and understand legal documents is extremely useful in resolving this problem. In this paper, we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B). Our evaluations demonstrate that our designed models are more accurate than baselines, e.g., with an up to 15.0% better F1 score in subtask B. We achieved notable performance in the task leaderboard, e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>xCodeEval&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#20462;&#22797;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#20165;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.03004</link><description>&lt;p&gt;
xCodeEval&#65306;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03004
&lt;/p&gt;
&lt;p&gt;
xCodeEval&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#20462;&#22797;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#20165;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#30340;&#26631;&#24535;&#65292;&#24182;&#19988;&#19968;&#30452;&#26159; AI &#30340;&#30446;&#26631;&#12290;&#33021;&#22815;&#21019;&#24314;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#31243;&#24207;&#30340; AI &#31995;&#32479;&#65292;&#25110;&#32773;&#21327;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#31243;&#24207;&#65292;&#37117;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#29575;&#24182;&#20351;&#32534;&#31243;&#26356;&#26131;&#20110;&#35775;&#38382;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#26032;&#20195;&#30721;&#12289;&#20462;&#22797;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#12289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#20197;&#21450;&#26816;&#32034;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35780;&#20272;&#36890;&#24120;&#26159;&#20998;&#25955;&#22312;&#20165;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#22312;&#23569;&#25968;&#35821;&#35328;&#12289;&#22312;&#37096;&#20998;&#31890;&#24230;&#27700;&#24179;&#65288;&#20363;&#22914;&#20989;&#25968;&#32423;&#21035;&#65289;&#19978;&#36827;&#34892;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#32570;&#20047;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26356;&#20026;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#35780;&#20272;&#26159;&#20197;&#20165;&#20165;&#35789;&#27719;&#37325;&#21472;&#20026;&#22522;&#30784;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25191;&#34892;&#65292;&#32780;&#20004;&#27573;&#20195;&#30721;&#27573;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65288;&#25110;&#31561;&#25928;&#24615;&#65289;&#20165;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#8220;&#25191;&#34892;&#30456;&#20284;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#35753;&#36739;&#23567;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.10071</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#25512;&#29702;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Reasoning Teachers. (arXiv:2212.10071v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#35753;&#36739;&#23567;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24605;&#32500;&#38142;&#26465;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#24605;&#32500;&#38142;&#26465;&#26041;&#27861;&#20381;&#36182;&#20110;&#20687;GPT-3 175B&#36825;&#26679;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#65292;&#36825;&#22312;&#35268;&#27169;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#26041;&#27861;&#65292;&#20351;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#20197;&#35753;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#23610;&#23544;&#35201;&#27714;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#27169;&#22411;&#21644;&#22797;&#26434;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;Fine-tune-CoT&#21487;&#20197;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#23454;&#36136;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#25945;&#24072;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#65292;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#24494;&#35843;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#65292;&#36890;&#36807;&#24341;&#20837;SimpeEval&#35821;&#26009;&#24211;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;LENS&#23545;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2212.09739</link><description>&lt;p&gt;
LENS&#65306;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#65292;&#36890;&#36807;&#24341;&#20837;SimpeEval&#35821;&#26009;&#24211;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;LENS&#23545;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#24050;&#25104;&#20026;&#33258;&#21160;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20123;&#22522;&#20110;&#21333;&#19968;&#25110;&#36807;&#26102;&#27169;&#22411;&#30340;&#26377;&#38480;&#27880;&#37322;&#65292;&#20351;&#23427;&#20204;&#19981;&#33021;&#36866;&#29992;&#20110;&#36825;&#31181;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SimpEval&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;SimpEval_past&#65292;&#21253;&#25324;&#23545;24&#20010;&#36807;&#21435;&#31995;&#32479;2.4K&#31616;&#21270;&#30340;12K&#20154;&#31867;&#35780;&#20998;&#65292;&#20197;&#21450;SimpEval_2022&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31616;&#21270;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#23545;360&#20010;&#31616;&#21270;&#65292;&#21253;&#25324;GPT-3.5&#29983;&#25104;&#25991;&#26412;&#30340;1K&#20154;&#31867;&#35780;&#20998;&#12290;&#22312;SimpEval&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#21487;&#23398;&#20064;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;LENS&#12290;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LENS&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#26356;&#22909;&#65292;&#20026;&#35780;&#20272;&#25991;&#26412;&#31616;&#21270;&#30340;&#26410;&#26469;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Rank&#21644;Rate&#65292;&#19968;&#31181;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31616;&#21270;&#36827;&#34892;&#25490;&#21517;&#21644;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank and Rate, a human evaluation framework that rates si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#23545;&#20110;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26426;&#21046;&#65292;&#35777;&#26126;&#20102;&#35299;&#37322;&#30340;&#20998;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#23545;&#20110;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#37117;&#26377;&#36129;&#29486;&#65307;&#24182;&#19988;&#25351;&#20986;&#65292;&#19981;&#21516;&#30340;&#35299;&#37322;&#38598;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#30340;&#23454;&#20363;&#38598;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.13892</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20114;&#34917;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Complementary Explanations for Effective In-Context Learning. (arXiv:2211.13892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#23545;&#20110;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26426;&#21046;&#65292;&#35777;&#26126;&#20102;&#35299;&#37322;&#30340;&#20998;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#23545;&#20110;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#37117;&#26377;&#36129;&#29486;&#65307;&#24182;&#19988;&#25351;&#20986;&#65292;&#19981;&#21516;&#30340;&#35299;&#37322;&#38598;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#30340;&#23454;&#20363;&#38598;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20174;&#25552;&#31034;&#20013;&#23398;&#20064;&#35299;&#37322;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#21151;&#33021;&#25110;&#20026;&#20309;&#26377;&#25928;&#30340;&#29702;&#35299;&#36824;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#35299;&#37322;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20004;&#20010;&#19981;&#21516;&#22240;&#32032;&#23545;&#20855;&#26377;&#35299;&#37322;&#25552;&#31034;&#24615;&#33021;&#30340;&#24433;&#21709;:&#35745;&#31639;&#36319;&#36394;(&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#35299;&#26041;&#24335;)&#21644;&#29992;&#20110;&#34920;&#36798;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#21463;&#25511;&#20219;&#21153;&#19978;&#25200;&#21160;&#35299;&#37322;&#65292;&#25105;&#20204;&#34920;&#26126;&#20004;&#20010;&#22240;&#32032;&#37117;&#26377;&#21161;&#20110;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22914;&#20309;&#24418;&#25104;&#26368;&#22823;&#26377;&#25928;&#35299;&#37322;&#38598;&#20197;&#35299;&#20915;&#32473;&#23450;&#30340;&#27979;&#35797;&#26597;&#35810;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#20174;&#35299;&#37322;&#38598;&#30340;&#20114;&#34917;&#24615;&#20013;&#21463;&#30410;:&#19981;&#21516;&#31034;&#20363;&#23637;&#31034;&#30340;&#22810;&#26679;&#25512;&#29702;&#25216;&#33021;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#30340;&#23454;&#20363;&#38598;&#21512;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exempla
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#36825;&#19968;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#65288;UBMEC&#65289;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.06405</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus. (arXiv:2210.06405v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#36825;&#19968;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#65288;UBMEC&#65289;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#22312;&#30740;&#31350;&#20154;&#20204;&#23545;&#21508;&#31181;Web 2.0&#26381;&#21153;&#30340;&#24819;&#27861;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24773;&#24863;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;&#24050;&#32463;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#23391;&#21152;&#25289;&#35821;&#36825;&#26679;&#19990;&#30028;&#19978;&#26368;&#27969;&#34892;&#30340;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#21364;&#27809;&#26377;&#36827;&#34892;&#22826;&#22810;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#23436;&#25972;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#25552;&#21462;&#23391;&#21152;&#25289;&#35821;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20174;&#23391;&#21152;&#25289;&#35821;&#35789;&#27719;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#20845;&#31867;&#65288;&#24868;&#24594;&#12289;&#21388;&#24694;&#12289;&#23475;&#24597;&#12289;&#21916;&#24742;&#12289;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#30340;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#65288;UBMEC&#65289;&#8221;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;UBMEC&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#20808;&#21069;&#21457;&#24067;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23391;&#21152;&#25289;&#35821;&#24773;&#24863;&#35821;&#26009;&#24211;&#32780;&#21019;&#24314;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;16,000&#20010;&#23391;&#21152;&#25289;&#25991;&#26412;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of its importance in studying people's thoughts on various Web 2.0 services, emotion classification (EC) is an important undertaking. Existing research, on the other hand, is mostly focused on the English language, with little work on low-resource languages. Though sentiment analysis, particularly the EC in English, has received a lot of attention in recent years, little study has been done in the context of Bangla, one of the world's most widely spoken languages. We propose a complete set of approaches for identifying and extracting emotions from Bangla texts in this research. We provide a Bangla emotion classifier for six classes (anger, disgust, fear, joy, sadness, and surprise) from Bangla words, using transformer-based models which exhibit phenomenal results in recent days, especially for high resource languages. The "Unified Bangla Multi-class Emotion Corpus (UBMEC)" is used to assess the performance of our models. UBMEC was created by combining two previously released ma
&lt;/p&gt;</description></item><item><title>HELP ME THINK&#26159;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#21019;&#24314;&#23450;&#21046;&#21270;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#21033;&#29992;GPT3&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#31572;&#26696;&#25191;&#34892;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.08232</link><description>&lt;p&gt;
HELP ME THINK&#65306;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#20351;&#29992;&#27169;&#22411;&#21019;&#24314;&#23450;&#21046;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models. (arXiv:2208.08232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08232
&lt;/p&gt;
&lt;p&gt;
HELP ME THINK&#26159;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#21019;&#24314;&#23450;&#21046;&#21270;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#21033;&#29992;GPT3&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#31572;&#26696;&#25191;&#34892;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#24182;&#23450;&#21046;&#20869;&#23481;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20986;&#20102;&#20026;&#20102;&#25552;&#20379;&#25511;&#21046;&#32780;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#32570;&#20047;&#19968;&#33324;&#24615;&#65307;&#36825;&#20026;&#38750;&#19987;&#19994;&#29992;&#25143;&#25214;&#21040;&#36866;&#21512;&#20854;&#20219;&#21153;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21387;&#20498;&#24615;&#30340;&#36873;&#25321;&#12290;&#36825;&#20123;&#25216;&#26415;&#25152;&#28041;&#21450;&#30340;&#24037;&#20316;&#65292;&#22914;&#32534;&#20889;&#31034;&#20363;&#12289;&#35299;&#37322;&#12289;&#25351;&#20196;&#31561;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38750;&#19987;&#19994;&#29992;&#25143;&#20013;&#30340;&#37319;&#29992;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HELP ME THINK&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#40723;&#21169;GPT3&#36890;&#36807;&#25552;&#20986;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#31572;&#26696;&#26469;&#25191;&#34892;&#20219;&#21153;&#26469;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;HELP ME THINK&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#38590;&#20197;&#23436;&#25104;&#19988;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#40723;&#21169;&#24320;&#21457;&#38750;&#20256;&#32479;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy HELP ME THINK where we encourage GPT3 to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique HELP ME THINK on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#35206;&#30422;&#25991;&#26412;&#20043;&#38388;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#30340;&#20256;&#32479;&#25361;&#25112;&#12290;&#36890;&#36807;&#25513;&#30721;&#21644;&#39044;&#27979;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#37051;&#36817;&#20998;&#24067;&#25955;&#24230;&#65288;NDD&#65289;&#26469;&#34920;&#31034;&#37325;&#21472;&#37096;&#20998;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NDD&#23545;&#20110;&#21508;&#31181;&#35821;&#20041;&#24046;&#24322;&#26356;&#20026;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2110.01176</link><description>&lt;p&gt;
&#39640;&#24230;&#37325;&#21472;&#25991;&#26412;&#30340;&#24773;&#22659;&#21270;&#35821;&#20041;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#35206;&#30422;&#25991;&#26412;&#20043;&#38388;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#30340;&#20256;&#32479;&#25361;&#25112;&#12290;&#36890;&#36807;&#25513;&#30721;&#21644;&#39044;&#27979;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#37051;&#36817;&#20998;&#24067;&#25955;&#24230;&#65288;NDD&#65289;&#26469;&#34920;&#31034;&#37325;&#21472;&#37096;&#20998;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NDD&#23545;&#20110;&#21508;&#31181;&#35821;&#20041;&#24046;&#24322;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#32534;&#36753;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#20043;&#38388;&#32463;&#24120;&#20250;&#20986;&#29616;&#37325;&#21472;&#12290;&#26356;&#22909;&#22320;&#35780;&#20272;&#37325;&#21472;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#26377;&#21161;&#20110;&#35821;&#35328;&#31995;&#32479;&#30340;&#29702;&#35299;&#24182;&#25351;&#23548;&#29983;&#25104;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#35821;&#20041;&#24230;&#37327;&#22522;&#20110;&#21333;&#35789;&#34920;&#31034;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#20855;&#26377;&#31867;&#20284;&#34920;&#31034;&#30340;&#37325;&#21472;&#37096;&#20998;&#30340;&#24178;&#25200;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25513;&#30721;&#21644;&#39044;&#27979;&#31574;&#30053;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#26368;&#38271;&#20844;&#20849;&#24207;&#21015;&#65288;LCS&#65289;&#20013;&#30340;&#21333;&#35789;&#20316;&#20026;&#37051;&#36817;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#26469;&#39044;&#27979;&#20854;&#20301;&#32622;&#19978;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;&#37051;&#36817;&#20998;&#24067;&#25955;&#24230;&#65288;NDD&#65289;&#65292;&#36890;&#36807;&#35745;&#31639;&#37325;&#21472;&#37096;&#20998;&#20013;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#34920;&#31034;&#35821;&#20041;&#36317;&#31163;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;NDD&#23545;&#20110;&#21508;&#31181;&#35821;&#20041;&#24046;&#24322;&#26356;&#20026;&#25935;&#24863;&#65292;
&lt;/p&gt;
&lt;p&gt;
Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system's understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a mask-and-predict strategy. We take the words in the longest common sequence (LCS) as neighboring words and use masked language modeling (MLM) from pre-trained language models (PLMs) to predict the distributions on their positions. Our metric, Neighboring Distribution Divergence (NDD), represent the semantic distance by calculating the divergence between distributions in the overlapped parts. Experiments on Semantic Textual Similarity show NDD to be more sensitive to various semantic differences, espec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1808.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#23454;&#20307;&#30456;&#20851;&#24615;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.08316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#23454;&#20307;&#30456;&#20851;&#24615;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#38745;&#24577;&#35774;&#32622;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#19979;&#30740;&#31350;&#23454;&#20307;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#20307;&#24448;&#24448;&#28041;&#21450;&#35768;&#22810;&#19981;&#21516;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#23454;&#20307;&#20851;&#31995;&#38543;&#26102;&#38388;&#21464;&#24471;&#38750;&#24120;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#21147;&#20316;&#20026;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#32852;&#21512;&#26694;&#26550;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
&lt;/p&gt;</description></item></channel></rss>