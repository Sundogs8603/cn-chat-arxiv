<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#24102;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#12290;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#38382;&#39064;&#65306;&#30446;&#21069;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#25216;&#26415;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65311;&#22914;&#26524;&#20256;&#32479;&#25216;&#26415;&#26080;&#25928;&#65292;LLM&#26159;&#21542;&#33021;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#38450;&#24481;&#25163;&#27573;&#65311;&#22914;&#26524;&#21069;&#20004;&#31181;&#31574;&#30053;&#22833;&#36133;&#65292;&#21487;&#20197;&#25552;&#20986;&#20160;&#20040;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#65311;</title><link>http://arxiv.org/abs/2309.15847</link><description>&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#22312;LLM&#26102;&#20195;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Disinformation Detection: An Evolving Challenge in the Age of LLMs. (arXiv:2309.15847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15847
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#24102;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#12290;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#38382;&#39064;&#65306;&#30446;&#21069;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#25216;&#26415;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65311;&#22914;&#26524;&#20256;&#32479;&#25216;&#26415;&#26080;&#25928;&#65292;LLM&#26159;&#21542;&#33021;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#38450;&#24481;&#25163;&#27573;&#65311;&#22914;&#26524;&#21069;&#20004;&#31181;&#31574;&#30053;&#22833;&#36133;&#65292;&#21487;&#20197;&#25552;&#20986;&#20160;&#20040;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#20652;&#29983;&#20102;&#21464;&#38761;&#24615;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#36827;&#23637;&#30340;&#21516;&#26102;&#65292;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#23041;&#32961;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#28389;&#29992;LLM&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#32773;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#39640;&#24230;&#26377;&#35828;&#26381;&#21147;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#65292;&#25361;&#25112;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31995;&#32479;&#12290;&#27492;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#29616;&#26377;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#25216;&#26415;&#33021;&#22815;&#21487;&#38752;&#22320;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#31243;&#24230;&#26159;&#22810;&#23569;&#65311; &#65288;2&#65289;&#22914;&#26524;&#20256;&#32479;&#25216;&#26415;&#35777;&#26126;&#25928;&#26524;&#36739;&#24046;&#65292;LLM&#26412;&#36523;&#26159;&#21542;&#21487;&#20197;&#34987;&#21033;&#29992;&#20316;&#20026;&#23545;&#25239;&#20808;&#36827;&#34394;&#20551;&#20449;&#24687;&#30340;&#24378;&#22823;&#38450;&#24481;&#65311; &#65288;3&#65289;&#22914;&#26524;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#22833;&#25928;&#65292;&#21487;&#20197;&#25552;&#20986;&#20160;&#20040;&#26032;&#30340;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#23041;&#32961;&#65311;&#23545;&#20110;&#34394;&#20551;&#20449;&#24687;&#30340;&#24418;&#25104;&#21644;&#26816;&#27979;&#36827;&#34892;&#20102;&#25972;&#20307;&#25506;&#32034;&#26469;&#25512;&#21160;&#36825;&#19968;&#34892;
&lt;/p&gt;
&lt;p&gt;
The advent of generative Large Language Models (LLMs) such as ChatGPT has catalyzed transformative advancements across multiple domains. However, alongside these advancements, they have also introduced potential threats. One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. This work aims to address this issue by answering three research questions: (1) To what extent can the current disinformation detection technique reliably detect LLM-generated disinformation? (2) If traditional techniques prove less effective, can LLMs themself be exploited to serve as a robust defense against advanced disinformation? and, (3) Should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? A holistic exploration for the formation and detection of disinformation is conducted to foster this line of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#39640;&#31934;&#30830;&#24230;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#26080;&#20851;&#30340;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#19968;&#20010;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.15840</link><description>&lt;p&gt;
&#22914;&#20309;&#25429;&#25417;AI&#35854;&#35328;&#65306;&#36890;&#36807;&#38382;&#26080;&#20851;&#38382;&#39064;&#22312;&#40657;&#30418;LLMs&#20013;&#36827;&#34892;&#35854;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#39640;&#31934;&#30830;&#24230;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#26080;&#20851;&#30340;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#19968;&#20010;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#8220;&#35828;&#35854;&#8221;&#65292;&#20063;&#23601;&#26159;&#22312;&#26126;&#30693;&#36947;&#30495;&#30456;&#30340;&#24773;&#20917;&#19979;&#36755;&#20986;&#34394;&#20551;&#38472;&#36848;&#12290;&#24403;&#25351;&#31034;&#36755;&#20986;&#38169;&#35823;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#8220;&#35828;&#35854;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#26082;&#19981;&#38656;&#35201;&#35775;&#38382;LLM&#30340;&#28608;&#27963;&#65288;&#40657;&#30418;&#65289;&#65292;&#20063;&#19981;&#38656;&#35201;&#20107;&#23454;&#38382;&#39064;&#30340;&#30495;&#30456;&#30693;&#35782;&#12290;&#36825;&#20010;&#26816;&#27979;&#22120;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#26080;&#20851;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#26469;&#24037;&#20316;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#36825;&#20010;&#35854;&#35328;&#26816;&#27979;&#22120;&#38750;&#24120;&#20934;&#30830;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#36890;&#29992;&#12290;&#24403;&#22312;&#21333;&#19968;&#24773;&#22659;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451; - &#20419;&#20351;GPT-3.5&#22312;&#20107;&#23454;&#38382;&#39064;&#19978;&#25746;&#35854; - &#35813;&#26816;&#27979;&#22120;&#21487;&#20197;&#25512;&#24191;&#21040;&#20197;&#19979;&#24773;&#20917;&#65306;&#65288;1&#65289;&#20854;&#20182;LLM&#26550;&#26500;&#65292;&#65288;2&#65289;&#32454;&#35843;&#20026;&#35828;&#35854;&#30340;LLMs&#65292;&#65288;3&#65289;&#35844;&#23194;&#30340;&#35854;&#35328;&#65292;&#21644;&#65288;4&#65289;&#20986;&#29616;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#65292;&#27604;&#22914;&#38144;&#21806;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29305;&#27530;&#30340;&#19982;&#35854;&#35328;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. LLMs might "lie", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#23450;&#20041;&#8220;&#20260;&#23475;&#8221;&#23545;&#26631;&#27880;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#32500;&#24681;&#22270;&#12289;&#20449;&#24687;&#22686;&#30410;&#27604;&#36739;&#21644;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#19981;&#23558;&#8220;&#20196;&#20154;&#35752;&#21388;&#30340;&#8221;&#12289;&#8220;&#20882;&#29359;&#30340;&#8221;&#21644;&#8220;&#26377;&#27602;&#30340;&#8221;&#27010;&#24565;&#28151;&#20026;&#19968;&#35848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#19981;&#40723;&#21169;&#28151;&#28102;&#36825;&#20123;&#26415;&#35821;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15827</link><description>&lt;p&gt;
&#22914;&#20309;&#23450;&#20041;&#20260;&#23475;&#24433;&#21709;&#25968;&#25454;&#26631;&#27880;&#65306;&#35299;&#37322;&#26631;&#27880;&#32773;&#22914;&#20309;&#21306;&#20998;&#20196;&#20154;&#35752;&#21388;&#30340;&#12289;&#20882;&#29359;&#30340;&#21644;&#26377;&#27602;&#30340;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
How We Define Harm Impacts Data Annotations: Explaining How Annotators Distinguish Hateful, Offensive, and Toxic Comments. (arXiv:2309.15827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#23450;&#20041;&#8220;&#20260;&#23475;&#8221;&#23545;&#26631;&#27880;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#32500;&#24681;&#22270;&#12289;&#20449;&#24687;&#22686;&#30410;&#27604;&#36739;&#21644;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#19981;&#23558;&#8220;&#20196;&#20154;&#35752;&#21388;&#30340;&#8221;&#12289;&#8220;&#20882;&#29359;&#30340;&#8221;&#21644;&#8220;&#26377;&#27602;&#30340;&#8221;&#27010;&#24565;&#28151;&#20026;&#19968;&#35848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#19981;&#40723;&#21169;&#28151;&#28102;&#36825;&#20123;&#26415;&#35821;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#25903;&#25345;&#20869;&#23481;&#23457;&#26680;&#21592;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#20123;&#36827;&#23637;&#24448;&#24448;&#20381;&#36182;&#20110;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#20026;&#26377;&#23475;&#20869;&#23481;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#35774;&#35745;&#26631;&#27880;&#20219;&#21153;&#30340;&#25351;&#23548;&#35828;&#26126;&#20197;&#29983;&#25104;&#36825;&#20123;&#31639;&#27861;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#25105;&#20204;&#35757;&#32451;&#31639;&#27861;&#26816;&#27979;&#30340;&#20260;&#23475;&#27010;&#24565; - &#8220;&#20196;&#20154;&#35752;&#21388;&#30340;&#8221;&#12289;&#8220;&#20882;&#29359;&#30340;&#8221;&#12289;&#8220;&#26377;&#27602;&#30340;&#8221;&#12289;&#8220;&#31181;&#26063;&#20027;&#20041;&#30340;&#8221;&#12289;&#8220;&#24615;&#21035;&#27495;&#35270;&#30340;&#8221;&#31561;&#35270;&#20026;&#21487;&#20114;&#25442;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#8220;&#20260;&#23475;&#8221;&#30340;&#23450;&#20041;&#26041;&#24335;&#26159;&#21542;&#24433;&#21709;&#26631;&#27880;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#32500;&#24681;&#22270;&#12289;&#20449;&#24687;&#22686;&#30410;&#27604;&#36739;&#21644;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#24182;&#19981;&#23558;&#8220;&#20196;&#20154;&#35752;&#21388;&#30340;&#8221;&#12289;&#8220;&#20882;&#29359;&#30340;&#8221;&#21644;&#8220;&#26377;&#27602;&#30340;&#8221;&#27010;&#24565;&#28151;&#20026;&#19968;&#35848;&#12290;&#25105;&#20204;&#21457;&#29616;&#20260;&#23475;&#23450;&#20041;&#30340;&#29305;&#24449;&#20197;&#21450;&#26631;&#27880;&#32773;&#30340;&#20010;&#20154;&#29305;&#28857;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#26631;&#27880;&#32773;&#22914;&#20309;&#19981;&#21516;&#22320;&#20351;&#29992;&#36825;&#20123;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#32463;&#39564;   &#35777;&#25454;&#65292;&#19981;&#40723;&#21169;&#28151;&#28102;&#36825;&#20123;&#26415;&#35821;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational social science research has made advances in machine learning and natural language processing that support content moderators in detecting harmful content. These advances often rely on training datasets annotated by crowdworkers for harmful content. In designing instructions for annotation tasks to generate training data for these algorithms, researchers often treat the harm concepts that we train algorithms to detect - 'hateful', 'offensive', 'toxic', 'racist', 'sexist', etc. - as interchangeable. In this work, we studied whether the way that researchers define 'harm' affects annotation outcomes. Using Venn diagrams, information gain comparisons, and content analyses, we reveal that annotators do not use the concepts 'hateful', 'offensive', and 'toxic' interchangeably. We identify that features of harm definitions and annotators' individual characteristics explain much of how annotators use these terms differently. Our results offer empirical evidence discouraging the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#21442;&#25968;&#20849;&#20139;&#30340;ST/MT&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#36755;&#20837;&#36716;&#25442;&#20026;&#20004;&#20010;&#30456;&#20284;&#38271;&#24230;&#30340;&#31163;&#25955;&#26631;&#35760;&#24207;&#21015;&#30340;&#39044;&#22788;&#29702;&#38454;&#27573;&#26469;&#20943;&#23567;&#35821;&#38899;-&#25991;&#26412;&#27169;&#24577;&#24046;&#36317;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;MT&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15826</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#22810;&#20219;&#21153;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#36890;&#36807;&#30828;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing. (arXiv:2309.15826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#21442;&#25968;&#20849;&#20139;&#30340;ST/MT&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#36755;&#20837;&#36716;&#25442;&#20026;&#20004;&#20010;&#30456;&#20284;&#38271;&#24230;&#30340;&#31163;&#25955;&#26631;&#35760;&#24207;&#21015;&#30340;&#39044;&#22788;&#29702;&#38454;&#27573;&#26469;&#20943;&#23567;&#35821;&#38899;-&#25991;&#26412;&#27169;&#24577;&#24046;&#36317;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;MT&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;(ST)&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;(MT)&#25968;&#25454;&#36890;&#36807;&#27425;&#35201;&#32534;&#30721;&#22120;&#23558;&#25991;&#26412;&#30446;&#26631;&#36716;&#21270;&#20026;&#36328;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#20219;&#21153;&#26041;&#27861;&#21644;&#36719;&#21442;&#25968;&#20849;&#20139;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#21442;&#25968;&#20849;&#20139;&#30340;ST/MT&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#20854;&#20013;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#37117;&#20197;&#36328;&#27169;&#24577;&#26041;&#24335;&#20849;&#20139;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#36755;&#20837;&#36716;&#25442;&#20026;&#20004;&#20010;&#30456;&#20284;&#38271;&#24230;&#30340;&#31163;&#25955;&#26631;&#35760;&#24207;&#21015;&#30340;&#39044;&#22788;&#29702;&#38454;&#27573;&#26469;&#20943;&#23567;&#35821;&#38899;-&#25991;&#26412;&#27169;&#24577;&#24046;&#36317;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#31616;&#21333;&#22320;&#20351;&#29992;&#19968;&#20010;&#32852;&#21512;&#35789;&#27719;&#34920;&#23545;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#26080;&#24046;&#21035;&#22788;&#29702;&#12290;&#36890;&#36807;&#23545;MuST-C&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#22312;&#27809;&#26377;&#22806;&#37096;MT&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#39640;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#12289;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;(CTC)&#12289;&#20256;&#36882;&#22120;&#21644;&#32852;&#21512;CTC/&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#24179;&#22343;BLEU&#24471;&#20998;+0.5&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#21487;&#20197;&#21033;&#29992;&#22806;&#37096;MT&#25968;&#25454;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yield
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15817</link><description>&lt;p&gt;
&#20351;&#29992;LM&#27169;&#25311;&#27801;&#30418;&#35782;&#21035;LM&#20195;&#29702;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20195;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20363;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20351;&#24471;&#20195;&#29702;&#20855;&#22791;&#20102;&#20016;&#23500;&#30340;&#21151;&#33021;&#65292;&#20294;&#20063;&#25918;&#22823;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22914;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#25110;&#24341;&#21457;&#36130;&#21153;&#25439;&#22833;&#12290;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#23454;&#26045;&#24037;&#20855;&#65292;&#25163;&#21160;&#35774;&#32622;&#27599;&#20010;&#27979;&#35797;&#22330;&#26223;&#30340;&#29615;&#22659;&#65292;&#24182;&#25214;&#21040;&#39118;&#38505;&#26696;&#20363;&#12290;&#38543;&#30528;&#24037;&#20855;&#21644;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#27979;&#35797;&#36825;&#20123;&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#23558;&#20351;&#23547;&#25214;&#39640;&#39118;&#38505;&#12289;&#38271;&#23614;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToolEmu&#65306;&#19968;&#20010;&#20351;&#29992;LM&#26469;&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23454;&#20363;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;LM&#20195;&#29702;&#36827;&#34892;&#21508;&#31181;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#27979;&#35797;&#12290;&#38500;&#20102;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#26816;&#26597;&#20195;&#29702;&#30340;&#22833;&#36133;&#24182;&#37327;&#21270;&#30456;&#20851;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#27979;&#35797;&#20102;&#24037;&#20855;&#27169;&#25311;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#24182;&#21457;&#29616;&#20102;6&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23601;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#22312;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#31163;&#25955;&#21333;&#20803;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.15800</link><description>&lt;p&gt;
&#25506;&#32034;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study. (arXiv:2309.15800v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23601;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#22312;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#31163;&#25955;&#21333;&#20803;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#21495;&#36890;&#24120;&#20197;&#27599;&#31186;&#25968;&#19975;&#27425;&#30340;&#36895;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#21253;&#21547;&#20887;&#20313;&#20449;&#24687;&#65292;&#23548;&#33268;&#24207;&#21015;&#24314;&#27169;&#30340;&#20302;&#25928;&#24615;&#12290;&#39640;&#32500;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#65292;&#22914;&#39057;&#35889;&#22270;&#65292;&#36890;&#24120;&#34987;&#29992;&#20316;&#38543;&#21518;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#20855;&#26377;&#20887;&#20313;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#20013;&#24471;&#21040;&#30340;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#35821;&#38899;&#25968;&#25454;&#30340;&#22823;&#23567;&#12290;&#24212;&#29992;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#21435;&#37325;&#21644;&#23376;&#35789;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#31163;&#25955;&#21333;&#20803;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;&#23545;12&#20010;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;3&#20010;&#35821;&#38899;&#32763;&#35793;&#21644;1&#20010;&#21475;&#35821;&#29702;&#35299;&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#31163;&#25955;&#21333;&#20803;&#21487;&#20197;&#21462;&#24471;&#30456;&#24403;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#26102;&#20998;&#31867;&#20934;&#21017;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#65292;&#24182;&#36866;&#24212;&#35757;&#32451;&#36716;&#24405;&#20013;&#30340;&#38169;&#35823;&#65292;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2309.15796</link><description>&lt;p&gt;
&#23398;&#20064;&#26469;&#33258;&#26377;&#32570;&#38519;&#30340;&#25968;&#25454;&#65306;&#24369;&#30417;&#30563;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition. (arXiv:2309.15796v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#26102;&#20998;&#31867;&#20934;&#21017;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#65292;&#24182;&#36866;&#24212;&#35757;&#32451;&#36716;&#24405;&#20013;&#30340;&#38169;&#35823;&#65292;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26631;&#27880;&#32773;&#36890;&#24120;&#25191;&#34892;&#8220;&#38750;&#36880;&#23383;&#8221;&#36716;&#24405;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#27169;&#22411;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#26102;&#20998;&#31867;&#65288;OTC&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#20934;&#21017;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#30001;&#27492;&#31867;&#24369;&#30417;&#30563;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#65292;&#24182;&#36866;&#24212;&#35757;&#32451;&#36716;&#24405;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#12290;OTC&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;CTC&#30446;&#26631;&#20989;&#25968;&#29992;&#20110;&#19981;&#23436;&#32654;&#36716;&#24405;&#12290;&#36890;&#36807;&#22312;LibriSpeech&#21644;LibriVox&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;OTC&#35757;&#32451;ASR&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#20351;&#36716;&#24405;&#20013;&#21253;&#21547;&#39640;&#36798;70&#65285;&#30340;&#38169;&#35823;&#65292;&#32780;CTC&#27169;&#22411;&#21017;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#22312;https://github.com/k2-fsa/icefall&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training automatic speech recognition (ASR) systems requires large amounts of well-curated paired data. However, human annotators usually perform "non-verbatim" transcription, which can result in poorly trained models. In this paper, we propose Omni-temporal Classification (OTC), a novel training criterion that explicitly incorporates label uncertainties originating from such weak supervision. This allows the model to effectively learn speech-text alignments while accommodating errors present in the training transcripts. OTC extends the conventional CTC objective for imperfect transcripts by leveraging weighted finite state transducers. Through experiments conducted on the LibriSpeech and LibriVox datasets, we demonstrate that training ASR models with OTC avoids performance degradation even with transcripts containing up to 70% errors, a scenario where CTC models fail completely. Our implementation is available at https://github.com/k2-fsa/icefall.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20174;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#20026;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#27169;&#22411;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15789</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Routing with Benchmark Datasets. (arXiv:2309.15789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20174;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#20026;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#27169;&#22411;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#25968;&#37327;&#36805;&#36895;&#22686;&#38271;&#65292;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#12290;&#34429;&#28982;&#19968;&#20123;&#27169;&#22411;&#22312;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#21344;&#20248;&#21183;&#65292;&#20294;&#36890;&#24120;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#21644;&#29992;&#20363;&#20013;&#37117;&#33021;&#36798;&#21040;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#20026;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;LLM&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#34920;&#36848;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#34987;&#37325;&#26032;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;"&#36335;&#30001;&#22120;"&#27169;&#22411;&#26469;&#36873;&#25321;LLM&#65292;&#24182;&#19988;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#23398;&#20064;&#27169;&#22411;&#36335;&#30001;&#22120;&#30340;&#25928;&#29992;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;&#20351;&#29992;&#20219;&#20309;&#21333;&#19968;&#27169;&#22411;&#37117;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly growing number of open-source Large Language Models (LLMs) and benchmark datasets to compare them. While some models dominate these benchmarks, no single model typically achieves the best accuracy in all tasks and use cases. In this work, we address the challenge of selecting the best LLM out of a collection of models for new tasks. We propose a new formulation for the problem, in which benchmark datasets are repurposed to learn a "router" model for this LLM selection, and we show that this problem can be reduced to a collection of binary classification tasks. We demonstrate the utility and limitations of learning model routers from various benchmark datasets, where we consistently improve performance upon using any single model for all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#39532;&#25289;&#22320;&#35821;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#31572;&#31995;&#32479;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;Transformer&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;Marathi&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;MuRIL&#22810;&#35821;&#31181;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#20934;&#30830;&#29575;&#65292;EM&#24471;&#20998;&#20026;0.64&#65292;F1&#24471;&#20998;&#20026;0.74&#12290;</title><link>http://arxiv.org/abs/2309.15779</link><description>&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#39532;&#25289;&#22320;&#35821;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Question answering using deep learning in low resource Indian language Marathi. (arXiv:2309.15779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#39532;&#25289;&#22320;&#35821;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#31572;&#31995;&#32479;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;Transformer&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;Marathi&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;MuRIL&#22810;&#35821;&#31181;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#20934;&#30830;&#29575;&#65292;EM&#24471;&#20998;&#20026;0.64&#65292;F1&#24471;&#20998;&#20026;0.74&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#20307;&#12289;&#35268;&#21017;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#20013;&#20026;&#32473;&#23450;&#30340;&#38382;&#39064;&#25552;&#21462;&#20986;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#31572;&#25361;&#25112;&#65292;&#22312;&#39532;&#25289;&#22320;&#35821;&#38382;&#31572;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#21019;&#24314;&#22522;&#20110;&#38405;&#35835;&#29702;&#35299;&#30340;&#39532;&#25289;&#22320;&#35821;&#38382;&#31572;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#39532;&#25289;&#22320;&#35821;&#22810;&#35821;&#31181;&#21644;&#21333;&#35821;&#31181;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22914;MuRIL&#12289;MahaBERT&#12289;IndicBERT&#65292;&#24182;&#22312;&#39532;&#25289;&#22320;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39532;&#25289;&#22320;&#35821;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;MuRIL&#22810;&#35821;&#31181;&#27169;&#22411;&#65292;&#22312;EM&#24471;&#20998;&#20026;0.64&#65292;F1&#24471;&#20998;&#20026;0.74&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise answers are extracted from a text for a given input question in a question answering system. Marathi question answering system is created in recent studies by using ontology, rule base and machine learning based approaches. Recently transformer models and transfer learning approaches are used to solve question answering challenges. In this paper we investigate different transformer models for creating a reading comprehension-based Marathi question answering system. We have experimented on different pretrained Marathi language multilingual and monolingual models like Multilingual Representations for Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder Representations from Transformers (IndicBERT) and fine-tuned it on a Marathi reading comprehension-based data set. We got the best accuracy in a MuRIL multilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning the model on the Marathi dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#34701;&#20837;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20020;&#24202;&#23545;&#35805;&#30340;&#25688;&#35201;&#12290;&#36890;&#36807;&#36866;&#37197;&#22120;&#27880;&#20837;&#30693;&#35782;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#38376;&#25511;&#26426;&#21046;&#32479;&#19968;&#34701;&#21512;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15739</link><description>&lt;p&gt;
&#20307;&#39564;&#21644;&#35777;&#25454;&#26159;&#20986;&#33394;&#25688;&#35201;&#26426;&#22120;&#20154;&#30340;&#30524;&#30555;&#65281;&#26397;&#30528;&#30693;&#35782;&#34701;&#20837;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization. (arXiv:2309.15739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#34701;&#20837;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20020;&#24202;&#23545;&#35805;&#30340;&#25688;&#35201;&#12290;&#36890;&#36807;&#36866;&#37197;&#22120;&#27880;&#20837;&#30693;&#35782;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#38376;&#25511;&#26426;&#21046;&#32479;&#19968;&#34701;&#21512;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36828;&#31243;&#21307;&#30103;&#30340;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#21307;&#30103;&#20174;&#19994;&#32773;&#27491;&#20849;&#21516;&#21162;&#21147;&#24320;&#21457;&#21508;&#31181;&#25216;&#26415;&#26469;&#33258;&#21160;&#21270;&#21508;&#31181;&#21307;&#30103;&#25805;&#20316;&#65292;&#22914;&#35786;&#26029;&#25253;&#21578;&#29983;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#26681;&#25454;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#30340;&#20132;&#20114;&#65288;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65289;&#29983;&#25104;&#23545;&#35805;&#30340;&#31616;&#27905;&#27010;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#21307;&#23398;&#39046;&#22495;&#35782;&#21035;&#21644;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#65288;MM-CliConSummation&#65289;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#36866;&#37197;&#22120;&#26469;&#27880;&#20837;&#30693;&#35782;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#32479;&#19968;&#34701;&#21512;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#24102;&#26377;&#24847;&#22270;&#12289;&#30151;&#29366;&#21644;&#25688;&#35201;&#27880;&#37322;&#30340;&#22810;&#27169;&#24577;&#22810;&#24847;&#22270;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#22823;&#37327;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#24471;&#20986;&#20197;&#19979;&#21457;&#29616;&#65306;(a)&#20851;&#38190;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
With the advancement of telemedicine, both researchers and medical practitioners are working hand-in-hand to develop various techniques to automate various medical operations, such as diagnosis report generation. In this paper, we first present a multi-modal clinical conversation summary generation task that takes a clinician-patient interaction (both textual and visual information) and generates a succinct synopsis of the conversation. We propose a knowledge-infused, multi-modal, multi-tasking medical domain identification and clinical conversation summary generation (MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and visual features and unify the fused feature vector using a gated mechanism. Furthermore, we developed a multi-modal, multi-intent clinical conversation summarization corpus annotated with intent, symptom, and summary. The extensive set of experiments, both quantitatively and qualitatively, led to the following findings: (a) critical significan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.15714</link><description>&lt;p&gt;
ChatGPT-BCI&#65306;&#20351;&#29992;GPT&#12289;EEG&#21644;&#30524;&#21160;&#29983;&#29289;&#26631;&#35760;&#22120;&#22312;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#36827;&#34892;&#21333;&#35789;&#32423;&#31070;&#32463;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;GPT&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#29702;&#35299;&#35821;&#20041;&#35821;&#35328;&#24847;&#20041;&#30340;&#33021;&#21147;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#12290;&#36825;&#38656;&#35201;&#36328;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;LLMs&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22914;&#20309;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#31243;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#25552;&#20379;&#20851;&#20110;&#20010;&#20307;&#31070;&#32463;&#29366;&#24577;&#22312;&#35821;&#20041;&#20851;&#31995;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#25913;&#36827;&#20102;&#19982;&#20851;&#38190;&#23383;&#39640;&#30456;&#20851;&#24230;&#21644;&#20302;&#30456;&#20851;&#24230;&#30340;&#21333;&#35789;&#38405;&#35835;&#36807;&#31243;&#20013;&#19982;&#27880;&#35270;&#30456;&#20851;&#30340;EEG&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#22312;12&#21517;&#21463;&#35797;&#32773;&#20013;&#65292;&#27492;&#21333;&#35789;&#32423;&#21035;&#20998;&#31867;&#30340;&#26368;&#20339;&#39564;&#35777;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent explosion of large language models (LLMs), such as Generative Pretrained Transformers (GPT), the need to understand the ability of humans and machines to comprehend semantic language meaning has entered a new phase. This requires interdisciplinary research that bridges the fields of cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.15701</link><description>&lt;p&gt;
HyPoradise&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#24320;&#25918;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#24178;&#20928;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38754;&#23545;&#36870;&#22659;&#26102;&#20063;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#33391;&#22909;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#23545;&#20110;&#35821;&#38899;&#39046;&#22495;&#30340;&#21464;&#24322;&#24615;&#24456;&#25935;&#24863;&#65292;&#22914;&#32972;&#26223;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22806;&#37096;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#20854;&#20013;N&#26368;&#20339;&#35299;&#30721;&#20551;&#35774;&#20026;&#30495;&#23454;&#36716;&#24405;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#31574;&#30053;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#33021;&#36873;&#25321;&#19968;&#20010;&#20505;&#36873;&#20551;&#35774;&#20316;&#20026;&#26368;&#32456;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#25552;&#39640;&#36830;&#36143;&#24615;&#65292;&#24182;&#35299;&#20915;&#25193;&#23637;&#38899;&#39057;&#29255;&#27573;&#30340;&#20869;&#23384;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#28155;&#21152;&#35828;&#35805;&#20154;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#20002;&#24323;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29420;&#31435;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;&#22312;&#20250;&#35805;&#24335;&#35821;&#38899;&#20013;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#20027;&#35201;&#26377;&#21161;&#20110;&#25429;&#25417;&#19978;&#19979;&#25991;&#39118;&#26684;&#21644;&#35299;&#20915;&#25351;&#20195;&#21644;&#21629;&#21517;&#23454;&#20307;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15686</link><description>&lt;p&gt;
&#25552;&#21319;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization. (arXiv:2309.15686v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#25552;&#39640;&#36830;&#36143;&#24615;&#65292;&#24182;&#35299;&#20915;&#25193;&#23637;&#38899;&#39057;&#29255;&#27573;&#30340;&#20869;&#23384;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#28155;&#21152;&#35828;&#35805;&#20154;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#20002;&#24323;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29420;&#31435;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;&#22312;&#20250;&#35805;&#24335;&#35821;&#38899;&#20013;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#20027;&#35201;&#26377;&#21161;&#20110;&#25429;&#25417;&#19978;&#19979;&#25991;&#39118;&#26684;&#21644;&#35299;&#20915;&#25351;&#20195;&#21644;&#21629;&#21517;&#23454;&#20307;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#23558;&#36739;&#38271;&#30340;&#19978;&#19979;&#25991;&#34701;&#20837;&#26426;&#22120;&#32763;&#35793;&#21487;&#20197;&#24102;&#26469;&#22909;&#22788;&#65292;&#20294;&#26159;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#30446;&#26631;&#35821;&#35328;&#19978;&#19979;&#25991;&#22312;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#20013;&#65292;&#22686;&#24378;&#36830;&#36143;&#24615;&#24182;&#20811;&#26381;&#25193;&#23637;&#38899;&#39057;&#29255;&#27573;&#30340;&#20869;&#23384;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20002;&#24323;&#20197;&#30830;&#20445;&#23545;&#20110;&#19978;&#19979;&#25991;&#32570;&#22833;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#35828;&#35805;&#20154;&#20449;&#24687;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#20248;&#20110;&#23396;&#31435;&#30340;&#35805;&#35821;&#20026;&#22522;&#30784;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#24335;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#20250;&#35805;&#24335;&#35821;&#38899;&#20013;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#20027;&#35201;&#26377;&#21161;&#20110;&#25429;&#25417;&#19978;&#19979;&#25991;&#39118;&#26684;&#65292;&#20197;&#21450;&#35299;&#20915;&#25351;&#20195;&#21644;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25340;&#25509;&#21333;&#35821;&#35821;&#26009;&#29983;&#25104;&#28151;&#21512;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#28151;&#21512;&#35821;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#28151;&#21512;&#35821;&#38899;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#23545;&#21333;&#35821;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.15674</link><description>&lt;p&gt;
&#35821;&#38899;&#25340;&#36148;&#65306;&#36890;&#36807;&#25340;&#25509;&#21333;&#35821;&#35821;&#26009;&#29983;&#25104;&#28151;&#21512;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Speech collage: code-switched audio generation by collaging monolingual corpora. (arXiv:2309.15674v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25340;&#25509;&#21333;&#35821;&#35821;&#26009;&#29983;&#25104;&#28151;&#21512;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#28151;&#21512;&#35821;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#28151;&#21512;&#35821;&#38899;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#23545;&#21333;&#35821;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#29992;&#20110;&#28151;&#21512;&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#24448;&#24448;&#21462;&#20915;&#20110;&#21487;&#33719;&#24471;&#30340;&#28151;&#21512;&#35821;&#36164;&#28304;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#35821;&#38899;&#25340;&#36148;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#38899;&#39057;&#29255;&#27573;&#20174;&#21333;&#35821;&#35821;&#26009;&#20013;&#21512;&#25104;&#28151;&#21512;&#35821;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#37325;&#21472;&#28155;&#21152;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#38899;&#39057;&#29983;&#25104;&#30340;&#24179;&#28369;&#24230;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#25968;&#25454;&#23545;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#30340;&#24433;&#21709;&#65306;&#20351;&#29992;&#39046;&#22495;&#20869;&#28151;&#21512;&#35821;&#25991;&#26412;&#21644;&#20351;&#29992;&#21512;&#25104;&#30340;&#28151;&#21512;&#35821;&#25991;&#26412;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#28151;&#21512;&#38169;&#35823;&#29575;&#21644;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#30456;&#23545;&#20943;&#23569;&#20102;34.4%&#21644;16.2%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#35821;&#35328;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28151;&#21512;&#20542;&#21521;&#24182;&#20943;&#23569;&#20102;&#21333;&#35821;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15670</link><description>&lt;p&gt;
MONOVAB: &#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24773;&#24863;&#20998;&#26512;(SA)&#21644;&#24773;&#24863;&#35782;&#21035;(ER)&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#23391;&#21152;&#25289;&#35821;&#26159;&#19990;&#30028;&#19978;&#31532;&#19971;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23391;&#21152;&#25289;&#35821;&#30340;&#32467;&#26500;&#22797;&#26434;&#65292;&#36825;&#20351;&#24471;&#20934;&#30830;&#25552;&#21462;&#24773;&#32490;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#19968;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#25552;&#21462;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#24863;&#20197;&#21450;&#22810;&#31867;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35821;&#35328;&#20013;&#25552;&#21462;&#22810;&#31181;&#24773;&#32490;&#20960;&#20046;&#26159;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#19968;&#27573;&#25991;&#26412;&#35782;&#21035;&#20986;&#22810;&#31181;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;Facebook&#19978;&#25235;&#21462;&#30340;&#25968;&#25454;&#26500;&#24314;&#27880;&#37322;&#35821;&#26009;&#24211;&#30340;&#35814;&#32454;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#20811;&#26381;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#27880;&#37322;&#26356;&#26377;&#25104;&#26524;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;(BERT)&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT),
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#26085;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#20013;&#25991;&#30340;&#33050;&#26412;&#23545;&#35805;&#21644;&#33258;&#21457;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20132;&#27969;&#21453;&#39304;&#29616;&#35937;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#23545;&#35805;&#31867;&#22411;&#22312;&#20132;&#27969;&#21453;&#39304;&#21644;&#33853;&#22320;&#29616;&#35937;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.15656</link><description>&lt;p&gt;
&#33050;&#26412;&#23545;&#35805;&#19982;&#33258;&#21457;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#21453;&#39304;&#65306;&#19968;&#31181;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis. (arXiv:2309.15656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#26085;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#20013;&#25991;&#30340;&#33050;&#26412;&#23545;&#35805;&#21644;&#33258;&#21457;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20132;&#27969;&#21453;&#39304;&#29616;&#35937;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#23545;&#35805;&#31867;&#22411;&#22312;&#20132;&#27969;&#21453;&#39304;&#21644;&#33853;&#22320;&#29616;&#35937;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33050;&#26412;&#23545;&#35805;&#65292;&#22914;&#30005;&#24433;&#21644;&#30005;&#35270;&#23383;&#24149;&#65292;&#26500;&#25104;&#20102;&#20250;&#35805;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24191;&#27867;&#35757;&#32451;&#25968;&#25454;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23545;&#35805;&#30340;&#35821;&#35328;&#29305;&#28857;&#19982;&#33258;&#21457;&#20132;&#20114;&#30340;&#35821;&#26009;&#24211;&#20013;&#35266;&#23519;&#21040;&#30340;&#35821;&#35328;&#29305;&#28857;&#26126;&#26174;&#19981;&#21516;&#12290;&#29305;&#21035;&#26159;&#22312;&#20132;&#27969;&#21453;&#39304;&#21644;&#33853;&#22320;&#29616;&#35937;&#65288;&#22914;&#22238;&#24212;&#12289;&#30830;&#35748;&#25110;&#28548;&#28165;&#35201;&#27714;&#65289;&#26041;&#38754;&#65292;&#36825;&#31181;&#24046;&#24322;&#23588;&#20026;&#26126;&#26174;&#12290;&#36825;&#20123;&#20449;&#21495;&#34987;&#35748;&#20026;&#26159;&#20250;&#35805;&#27969;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#23545;&#35805;&#21442;&#19982;&#32773;&#29992;&#20110;&#23545;&#24444;&#27492;&#20043;&#38388;&#27491;&#22312;&#36827;&#34892;&#30340;&#20132;&#20114;&#30340;&#24863;&#30693;&#25552;&#20379;&#21453;&#39304;&#12290;&#26412;&#25991;&#22312;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#26085;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#20013;&#25991;&#30340;&#23545;&#35805;&#25968;&#25454;&#22522;&#30784;&#19978;&#65292;&#36827;&#34892;&#20102;&#36825;&#31867;&#20132;&#27969;&#21453;&#39304;&#29616;&#35937;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#35789;&#27719;&#32479;&#35745;&#21644;&#20351;&#29992;&#31070;&#32463;&#23545;&#35805;&#34892;&#20026;&#26631;&#35760;&#22120;&#33719;&#24471;&#30340;&#20998;&#31867;&#36755;&#20986;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scripted dialogues such as movie and TV subtitles constitute a widespread source of training data for conversational NLP models. However, the linguistic characteristics of those dialogues are notably different from those observed in corpora of spontaneous interactions. This difference is particularly marked for communicative feedback and grounding phenomena such as backchannels, acknowledgments, or clarification requests. Such signals are known to constitute a key part of the conversation flow and are used by the dialogue participants to provide feedback to one another on their perception of the ongoing interaction. This paper presents a quantitative analysis of such communicative feedback phenomena in both subtitles and spontaneous conversations. Based on dialogue data in English, French, German, Hungarian, Italian, Japanese, Norwegian and Chinese, we extract both lexical statistics and classification outputs obtained with a neural dialogue act tagger. Two main findings of this empiri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15649</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#35753;LLMs&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#65288;TAP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#25351;&#20196;&#21644;&#28436;&#31034;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#22806;&#30340;&#20219;&#21153;&#65288;ATIS&#21644;WSJ&#65289;&#19978;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31532;&#19968;&#27425;&#25195;&#25551;&#31995;&#32479;&#21644;&#37325;&#26032;&#35780;&#20998;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#36890;&#36807;&#20923;&#32467;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#36798;&#21040;&#19982;&#39046;&#22495;&#35843;&#20248;&#30340;LMs&#37325;&#26032;&#35780;&#20998;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#25216;&#26415;&#19982;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20302;&#20110;N-best Oracle&#27700;&#24179;&#30340;&#38169;&#35823;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
&lt;/p&gt;</description></item><item><title>NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15630</link><description>&lt;p&gt;
NLPBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15630
&lt;/p&gt;
&lt;p&gt;
NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#26174;&#31034;&#20986;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#22312;LLMs&#30340;NLP&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#19987;&#38376;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;NLPBench&#65292;&#21253;&#25324;378&#20010;&#28085;&#30422;&#21508;&#31181;NLP&#20027;&#39064;&#30340;&#22823;&#23398;&#27700;&#24179;NLP&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;&#32822;&#40065;&#22823;&#23398;&#20197;&#21069;&#30340;&#26399;&#26411;&#32771;&#35797;&#12290;NLPBench&#21253;&#25324;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#23376;&#38382;&#39064;&#20998;&#20139;&#30456;&#21516;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#19988;&#21253;&#25324;&#22810;&#36873;&#39064;&#12289;&#31616;&#31572;&#39064;&#21644;&#25968;&#23398;&#39064;&#31561;&#22810;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20197;GPT-3.5/4&#12289;PaLM-2&#21644;LLAMA-2&#31561;LLMs&#20026;&#20013;&#24515;&#65292;&#37319;&#29992;&#20102;&#35832;&#22914;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#31561;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#26377;&#26102;&#20250;&#25439;&#23475;LLMs&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;LLA&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#35299;&#20915;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#26367;&#20195;&#21644;&#25913;&#21892;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15609</link><description>&lt;p&gt;
&#21457;&#23637;&#22269;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#30340;&#33258;&#21160;&#36880;&#23383;&#36716;&#24405;&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution. (arXiv:2309.15609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#35299;&#20915;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#26367;&#20195;&#21644;&#25913;&#21892;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#23545;&#23427;&#20204;&#36827;&#34892;&#22810;&#31181;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#24037;&#20855;&#26159;&#22312;&#19990;&#30028;&#30693;&#35782;&#20135;&#26435;&#32452;&#32455;&#65288;WIPO&#65289;&#24320;&#21457;&#30340;&#12289;&#20351;&#29992;&#20854;&#20869;&#37096;&#24320;&#21457;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#65288;S2T&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#38500;&#20102;&#25551;&#36848;&#25968;&#25454;&#25910;&#38598;&#21644;&#20248;&#21270;&#36807;&#31243;&#65292;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#22806;&#65292;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#25216;&#26415;&#32452;&#20214;&#30340;&#26550;&#26500;&#21644;&#28436;&#21464;&#65292;&#24182;&#31361;&#20986;&#20102;&#29992;&#25143;&#26041;&#38754;&#30340;&#21830;&#19994;&#24433;&#21709;&#21644;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#31995;&#32479;&#22312;&#28436;&#36827;&#21644;&#37319;&#29992;&#36807;&#31243;&#20013;&#30340;&#29305;&#27530;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#22914;&#20309;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#20135;&#21697;&#65292;&#24182;&#21462;&#20195;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end solution for the creation of fully automated conference meeting transcripts and their machine translations into various languages. This tool has been developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. Beyond describing data collection and fine-tuning, resulting in a highly customized and robust system, this paper describes the architecture and evolution of the technical components as well as highlights the business impact and benefits from the user side. We also point out particular challenges in the evolution and adoption of the system and how the new approach created a new product and replaced existing established workflows in conference management documentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25903;&#25345;&#38598;&#27880;&#24847;&#21147;&#21644;&#22686;&#24378;&#30340;&#26631;&#31614;&#20449;&#24687;&#30340;&#21407;&#22411;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#22810;&#26631;&#31614;&#26041;&#38754;&#31867;&#21035;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#28155;&#21152;&#21477;&#23376;&#32423;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26631;&#31614;&#22686;&#24378;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#23454;&#20363;&#20043;&#38388;&#30340;&#21464;&#21270;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.15588</link><description>&lt;p&gt;
&#21033;&#29992;&#21477;&#32423;&#21152;&#26435;&#21644;&#26631;&#31614;&#22686;&#24378;&#30340;&#21407;&#22411;&#32593;&#32476;&#36827;&#34892;&#23569;&#26679;&#26412;&#22810;&#26631;&#31614;&#26041;&#38754;&#31867;&#21035;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Multi-Label Aspect Category Detection Utilizing Prototypical Network with Sentence-Level Weighting and Label Augmentation. (arXiv:2309.15588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25903;&#25345;&#38598;&#27880;&#24847;&#21147;&#21644;&#22686;&#24378;&#30340;&#26631;&#31614;&#20449;&#24687;&#30340;&#21407;&#22411;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#22810;&#26631;&#31614;&#26041;&#38754;&#31867;&#21035;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#28155;&#21152;&#21477;&#23376;&#32423;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26631;&#31614;&#22686;&#24378;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#23454;&#20363;&#20043;&#38388;&#30340;&#21464;&#21270;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#26041;&#38754;&#31867;&#21035;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#32473;&#23450;&#21477;&#23376;&#20013;&#20986;&#29616;&#30340;&#22810;&#20010;&#26041;&#38754;&#31867;&#21035;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38480;&#21046;&#65292;&#21407;&#22411;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#24050;&#34987;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#26041;&#38754;&#31867;&#21035;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#20351;&#29992;&#30340;&#21407;&#22411;&#32593;&#32476;&#35745;&#31639;&#25903;&#25345;&#38598;&#20013;&#23454;&#20363;&#30340;&#24179;&#22343;&#20540;&#65292;&#24573;&#30053;&#20102;&#22810;&#26631;&#31614;&#26041;&#38754;&#31867;&#21035;&#26816;&#27979;&#20013;&#30340;&#23454;&#20363;&#38388;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#30456;&#20851;&#24037;&#20316;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#20449;&#24687;&#26469;&#22686;&#24378;&#27880;&#24847;&#26426;&#21046;&#65292;&#20294;&#26631;&#31614;&#25991;&#26412;&#20449;&#24687;&#36890;&#24120;&#36739;&#30701;&#19988;&#26377;&#38480;&#65292;&#26080;&#27861;&#36275;&#22815;&#20855;&#20307;&#22320;&#21306;&#20998;&#31867;&#21035;&#12290;&#26412;&#25991;&#39318;&#20808;&#24341;&#20837;&#20102;&#25903;&#25345;&#38598;&#27880;&#24847;&#21147;&#20197;&#21450;&#22686;&#24378;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#27599;&#20010;&#25903;&#25345;&#38598;&#23454;&#20363;&#30340;&#21333;&#35789;&#32423;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21477;&#23376;&#32423;&#27880;&#24847;&#26426;&#21046;&#65292;&#32473;&#20104;&#19981;&#21516;&#23454;&#20363;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#20854;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label aspect category detection is intended to detect multiple aspect categories occurring in a given sentence. Since aspect category detection often suffers from limited datasets and data sparsity, the prototypical network with attention mechanisms has been applied for few-shot aspect category detection. Nevertheless, most of the prototypical networks used so far calculate the prototypes by taking the mean value of all the instances in the support set. This seems to ignore the variations between instances in multi-label aspect category detection. Also, several related works utilize label text information to enhance the attention mechanism. However, the label text information is often short and limited, and not specific enough to discern categories. In this paper, we first introduce support set attention along with the augmented label information to mitigate the noise at word-level for each support set instance. Moreover, we use a sentence-level attention mechanism that gives dif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15564</link><description>&lt;p&gt;
&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26080;&#32541;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#21333;&#19968;&#24378;&#22823;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#33258;&#22238;&#24402;&#28151;&#21512;&#65288;JAM&#65289;&#26694;&#26550;&#65292;&#19968;&#31181;&#31995;&#32479;&#34701;&#21512;&#29616;&#26377;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#38024;&#23545;&#28151;&#21512;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#32456;&#30340;&#35843;&#20248;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#20026;&#27492;&#30446;&#30340;&#32780;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;FBK&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20182;&#20204;&#20351;&#29992;&#30452;&#25509;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#22312;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15554</link><description>&lt;p&gt;
&#30452;&#25509;&#27169;&#22411;&#29992;&#20110;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#65306;FBK&#22312;IWSLT2023&#20013;&#30340;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023. (arXiv:2309.15554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;FBK&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20182;&#20204;&#20351;&#29992;&#30452;&#25509;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#22312;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;FBK&#22312;IWSLT 2023&#35780;&#20272;&#27963;&#21160;&#30340;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#21442;&#19982;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#20851;&#27880;&#20110;&#20351;&#29992;&#30452;&#25509;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20004;&#20010;&#20219;&#21153;&#65306;&#23545;&#20110;&#21516;&#26102;&#32763;&#35793;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#31163;&#32447;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#31574;&#30053;&#26469;&#36827;&#34892;&#23454;&#26102;&#25512;&#29702;&#65307;&#23545;&#20110;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;ST&#27169;&#22411;&#35843;&#25972;&#20026;&#29983;&#25104;&#31526;&#21512;&#35268;&#33539;&#30340;&#23383;&#24149;&#65292;&#24182;&#21033;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#29983;&#25104;&#19982;&#38899;&#35270;&#39057;&#20869;&#23481;&#21516;&#27493;&#25152;&#38656;&#30340;&#26102;&#38388;&#25139;&#12290;&#25105;&#20204;&#30340;&#33521;&#24503;SimulST&#31995;&#32479;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#26041;&#38754;&#27604;2021&#24180;&#21644;2022&#24180;&#30340;&#20219;&#21153;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#31995;&#32479;&#26377;&#25152;&#20943;&#23569;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#36798;3.5 BLEU&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#31995;&#32479;&#22312;&#33521;&#24503;&#21644;&#33521;&#35199;&#25991;&#23545;&#20013;&#20248;&#20110;&#22522;&#20110;&#30452;&#25509;&#31995;&#32479;&#30340;&#21807;&#19968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20998;&#21035;&#33719;&#24471;&#20102;3.7&#21644;1.7&#30340;SubER&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#19979;&#29983;&#25104;&#19968;&#33268;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.15516</link><description>&lt;p&gt;
&#25945;&#25480;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Teaching Text-to-Image Models to Communicate. (arXiv:2309.15516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#19979;&#29983;&#25104;&#19968;&#33268;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#20013;&#65292;&#21508;&#31181;&#24037;&#20316;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26159;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#26102;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#23545;&#35805;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#21363;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#29983;&#25104;&#19968;&#20010;&#19982;&#25351;&#23450;&#23545;&#35805;&#20869;&#23481;&#19968;&#33268;&#30340;&#36924;&#30495;&#22270;&#20687;&#20316;&#20026;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20013;&#38388;&#36716;&#25442;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#21462;&#23545;&#35805;&#20013;&#21253;&#21547;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#23545;&#35805;&#32467;&#26500;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#22312;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#35828;&#35805;&#22238;&#21512;&#20043;&#21069;&#25918;&#32622;&#20998;&#21106;&#26631;&#35760;&#65292;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#21457;&#35328;&#32773;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#22788;&#29702;&#21518;&#30340;&#23545;&#35805;&#32972;&#26223;&#29983;&#25104;&#22270;&#20687;&#12290;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#22788;&#29702;&#21518;&#23545;&#35805;&#29615;&#22659;&#30456;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various works have been extensively studied in the research of text-to-image generation. Although existing models perform well in text-to-image generation, there are significant challenges when directly employing them to generate images in dialogs. In this paper, we first highlight a new problem: dialog-to-image generation, that is, given the dialog context, the model should generate a realistic image which is consistent with the specified conversation as response. To tackle the problem, we propose an efficient approach for dialog-to-image generation without any intermediate translation, which maximizes the extraction of the semantic information contained in the dialog. Considering the characteristics of dialog structure, we put segment token before each sentence in a turn of a dialog to differentiate different speakers. Then, we fine-tune pre-trained text-to-image models to enable them to generate images conditioning on processed dialog context. After fine-tuning, our approach can con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15512</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models. (arXiv:2309.15512v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#20811;&#38534;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#25991;&#26412;-&#35821;&#38899;&#23545;&#12290;&#26368;&#23567;&#30417;&#30563;&#30340;&#35821;&#38899;&#21512;&#25104;&#36890;&#36807;&#32452;&#21512;&#20004;&#31181;&#31867;&#22411;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#65288;&#35821;&#20041;&#21644;&#22768;&#23398;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#26368;&#23569;&#30417;&#30563;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#65292;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#12290;&#33258;&#22238;&#24402;&#26694;&#26550;&#20855;&#26377;&#20856;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#21487;&#25511;&#24615;&#38382;&#39064;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#21463;&#21040;&#25345;&#32493;&#39044;&#27979;&#27169;&#22411;&#24341;&#36215;&#30340;&#38901;&#24459;&#24179;&#22343;&#21270;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#27169;&#22359;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic \&amp; acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diver
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861; VideoAdviser&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#39057;&#22686;&#24378;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#25928;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.15494</link><description>&lt;p&gt;
VideoAdviser: &#35270;&#39057;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#22810;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning. (arXiv:2309.15494v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861; VideoAdviser&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#39057;&#22686;&#24378;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#25928;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#36716;&#25442;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#39046;&#22495;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#31995;&#32479;&#36890;&#24120;&#22522;&#20110;&#25152;&#26377;&#27169;&#24577;&#22343;&#23384;&#22312;&#30340;&#20551;&#35774;&#26500;&#24314;&#65292;&#24182;&#19988;&#32570;&#20047;&#27169;&#24577;&#20250;&#23548;&#33268;&#25512;&#29702;&#24615;&#33021;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#20026;&#25152;&#26377;&#27169;&#24577;&#25552;&#21462;&#39044;&#35757;&#32451;&#23884;&#20837;&#22312;&#25512;&#29702;&#20013;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861; VideoAdviser&#65292;&#23558;&#22810;&#27169;&#24577;&#35270;&#39057;&#22686;&#24378;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#20174;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#20256;&#36755;&#21040;&#19968;&#20010;&#29305;&#23450;&#27169;&#24577;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#12290;&#22522;&#20110;&#19987;&#19994;&#39038;&#38382;&#21644;&#32874;&#26126;&#23398;&#29983;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#23398;&#20064;&#24615;&#33021;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;CLIP&#30340;&#25945;&#24072;&#27169;&#22411;&#36890;&#36807;&#20248;&#21270;&#27493;&#39588;&#33976;&#39311;&#65292;&#20026;&#22522;&#20110;RoBERTa&#30340;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal transfer learning aims to transform pretrained representations of diverse modalities into a common domain space for effective multimodal fusion. However, conventional systems are typically built on the assumption that all modalities exist, and the lack of modalities always leads to poor inference performance. Furthermore, extracting pretrained embeddings for all modalities is computationally inefficient for inference. In this work, to achieve high efficiency-performance multimodal transfer learning, we propose VideoAdviser, a video knowledge distillation method to transfer multimodal knowledge of video-enhanced prompts from a multimodal fundamental model (teacher) to a specific modal fundamental model (student). With an intuition that the best learning performance comes with professional advisers and smart students, we use a CLIP-based teacher model to provide expressive multimodal knowledge supervision signals to a RoBERTa-based student model via optimizing a step-distillat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#32858;&#21512;&#32593;&#32476;(DMCA)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#21644;&#21160;&#24577;&#20998;&#23618;&#32858;&#21512;&#27169;&#22359;(DHA)&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20250;&#35805;&#32423;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#32452;&#20998;&#26512;(DiaASQ)&#20013;&#25552;&#21462;&#22235;&#20803;&#32452;&#30340;&#22256;&#38590;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;&#25439;&#22833;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15476</link><description>&lt;p&gt;
&#21160;&#24577;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#32858;&#21512;&#29992;&#20110;&#20250;&#35805;&#32423;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#32452;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based Sentiment Quadruple Analysis. (arXiv:2309.15476v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#32858;&#21512;&#32593;&#32476;(DMCA)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#21644;&#21160;&#24577;&#20998;&#23618;&#32858;&#21512;&#27169;&#22359;(DHA)&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20250;&#35805;&#32423;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#32452;&#20998;&#26512;(DiaASQ)&#20013;&#25552;&#21462;&#22235;&#20803;&#32452;&#30340;&#22256;&#38590;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;&#25439;&#22833;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#32423;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#32452;&#20998;&#26512;&#65288;DiaASQ&#65289;&#26088;&#22312;&#25552;&#21462;&#23545;&#35805;&#20013;&#30446;&#26631;-&#26041;&#38754;-&#24847;&#35265;-&#24773;&#24863;&#30340;&#22235;&#20803;&#32452;&#12290;&#22312;DiaASQ&#20013;&#65292;&#22235;&#20803;&#32452;&#30340;&#20803;&#32032;&#32463;&#24120;&#36328;&#36234;&#22810;&#20010;&#35328;&#35821;&#12290;&#36825;&#31181;&#24773;&#20917;&#22797;&#26434;&#21270;&#20102;&#25552;&#21462;&#36807;&#31243;&#65292;&#24378;&#35843;&#20102;&#23545;&#20250;&#35805;&#19978;&#19979;&#25991;&#21644;&#20132;&#20114;&#30340;&#20805;&#20998;&#29702;&#35299;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#29420;&#31435;&#22320;&#32534;&#30721;&#27599;&#20010;&#35328;&#35821;&#65292;&#22240;&#27492;&#38590;&#20197;&#25429;&#25417;&#21040;&#38271;&#33539;&#22260;&#30340;&#20250;&#35805;&#19978;&#19979;&#25991;&#24182;&#24573;&#35270;&#20102;&#28145;&#23618;&#30340;&#35328;&#35821;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#32858;&#21512;&#32593;&#32476;(DMCA)&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#29983;&#25104;&#22810;&#23610;&#24230;&#30340;&#35328;&#35821;&#31383;&#21475;&#26469;&#25429;&#25417;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#20998;&#23618;&#32858;&#21512;&#27169;&#22359;(DHA)&#26469;&#38598;&#25104;&#23427;&#20204;&#20043;&#38388;&#30340;&#28176;&#36827;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#38454;&#27573;&#25439;&#22833;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational aspect-based sentiment quadruple analysis (DiaASQ) aims to extract the quadruple of target-aspect-opinion-sentiment within a dialogue. In DiaASQ, a quadruple's elements often cross multiple utterances. This situation complicates the extraction process, emphasizing the need for an adequate understanding of conversational context and interactions. However, existing work independently encodes each utterance, thereby struggling to capture long-range conversational context and overlooking the deep inter-utterance dependencies. In this work, we propose a novel Dynamic Multi-scale Context Aggregation network (DMCA) to address the challenges. Specifically, we first utilize dialogue structure to generate multi-scale utterance windows for capturing rich contextual information. After that, we design a Dynamic Hierarchical Aggregation module (DHA) to integrate progressive cues between them. In addition, we form a multi-stage loss strategy to improve model performance and generalizat
&lt;/p&gt;</description></item><item><title>ChatCounselor&#26159;&#19968;&#20010;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#23545;&#35805;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#24182;&#25509;&#36817;ChatGPT&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.15461</link><description>&lt;p&gt;
ChatCounselor: &#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatCounselor: A Large Language Models for Mental Health Support. (arXiv:2309.15461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15461
&lt;/p&gt;
&lt;p&gt;
ChatCounselor&#26159;&#19968;&#20010;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#23545;&#35805;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#24182;&#25509;&#36817;ChatGPT&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatCounselor&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#19982;&#36890;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19981;&#21516;&#65292;ChatCounselor&#20197;&#23454;&#38469;&#30340;&#24515;&#29702;&#21672;&#35810;&#23458;&#25143;&#19982;&#19987;&#19994;&#24515;&#29702;&#23398;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#20026;&#22522;&#30784;&#65292;&#20351;&#20854;&#20855;&#22791;&#24515;&#29702;&#23398;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#21672;&#35810;&#25216;&#24039;&#12290;&#35757;&#32451;&#25968;&#25454;&#38598;Psych8k&#26159;&#30001;260&#20010;&#28145;&#24230;&#35775;&#35848;&#26500;&#24314;&#32780;&#25104;&#65292;&#27599;&#20010;&#35775;&#35848;&#25345;&#32493;&#19968;&#20010;&#23567;&#26102;&#12290;&#20026;&#20102;&#35780;&#20272;&#21672;&#35810;&#22238;&#22797;&#30340;&#36136;&#37327;&#65292;&#35774;&#35745;&#20102;&#21672;&#35810;Benchmark&#12290;&#21033;&#29992;GPT-4&#21644;&#22522;&#20110;&#19971;&#20010;&#24515;&#29702;&#21672;&#35810;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30495;&#23454;&#21672;&#35810;&#38382;&#39064;&#30340;&#35780;&#20272;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;ChatCounselor&#22312;&#21672;&#35810;Benchmark&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#25509;&#36817;ChatGPT&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#39640;&#36136;&#37327;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#33719;&#24471;&#30340;&#27169;&#22411;&#33021;&#21147;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ChatCounselor, a large language model (LLM) solution designed to provide mental health support. Unlike generic chatbots, ChatCounselor is distinguished by its foundation in real conversations between consulting clients and professional psychologists, enabling it to possess specialized knowledge and counseling skills in the field of psychology. The training dataset, Psych8k, was constructed from 260 in-depth interviews, each spanning an hour. To assess the quality of counseling responses, the counseling Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on seven metrics of psychological counseling assessment, the model underwent evaluation using a set of real-world counseling questions. Impressively, ChatCounselor surpasses existing open-source models in the counseling Bench and approaches the performance level of ChatGPT, showcasing the remarkable enhancement in model capability attained through high-quality domain-specific data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15427</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#31070;&#32463;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21644;&#23450;&#21046;&#27169;&#22411;&#26550;&#26500;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#26159;&#23558;&#27492;&#24212;&#29992;&#20110;LLMs&#23384;&#22312;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#24182;&#36991;&#20813;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#33258;&#23450;&#20041;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#30340;LLMs&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#12290;GNP&#21253;&#25324;&#21508;&#31181;&#35774;&#35745;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#12289;&#36328;&#27169;&#24577;&#27719;&#32858;&#27169;&#22359;&#12289;&#22495;&#25237;&#24433;&#22120;&#21644;&#33258;&#30417;&#30563;&#38142;&#25509;&#39044;&#27979;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;GNP&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15402</link><description>&lt;p&gt;
&#20851;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65306;&#36827;&#23637;&#12289;&#21069;&#27839;&#21644;&#26410;&#26469;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#20180;&#32454;&#24191;&#27867;&#22320;&#27010;&#36848;&#20102;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#29992;&#8220;X-of-Thought&#8221;&#26469;&#25351;&#20195;&#24191;&#20041;&#19978;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32452;&#32455;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#30340;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24605;&#32500;&#38142;&#22312;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#39046;&#22495;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22810;&#27169;&#24577;&#21644;&#29702;&#35770;&#31561;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#25104;&#20026;&#23547;&#27714;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;InkSync&#65292;&#19968;&#20010;&#20855;&#26377;LLMs&#30340;&#21487;&#25191;&#34892;&#21644;&#21487;&#39564;&#35777;&#30340;&#25991;&#26412;&#32534;&#36753;&#30028;&#38754;&#65292;&#21487;&#25552;&#20379;&#23545;&#29992;&#25143;&#32534;&#36753;&#24314;&#35758;&#30340;&#36879;&#26126;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.15337</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#35805;&#65306;&#20855;&#26377;LLMs&#30340;&#21487;&#25191;&#34892;&#21644;&#21487;&#39564;&#35777;&#30340;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Beyond the Chat: Executable and Verifiable Text-Editing with LLMs. (arXiv:2309.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;InkSync&#65292;&#19968;&#20010;&#20855;&#26377;LLMs&#30340;&#21487;&#25191;&#34892;&#21644;&#21487;&#39564;&#35777;&#30340;&#25991;&#26412;&#32534;&#36753;&#30028;&#38754;&#65292;&#21487;&#25552;&#20379;&#23545;&#29992;&#25143;&#32534;&#36753;&#24314;&#35758;&#30340;&#36879;&#26126;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#23545;&#35805;&#30028;&#38754;&#24050;&#32463;&#25104;&#20026;&#33719;&#21462;&#25991;&#26723;&#32534;&#36753;&#21453;&#39304;&#30340;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#22522;&#20110;&#32842;&#22825;&#30340;&#23545;&#35805;&#30028;&#38754;&#19981;&#25903;&#25345;&#32534;&#36753;&#24314;&#35758;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#20102;&#22312;&#19982;LLM&#32534;&#36753;&#26102;&#32473;&#20104;&#20316;&#32773;&#26356;&#22810;&#30340;&#33258;&#20027;&#26435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InkSync&#65292;&#19968;&#31181;&#32534;&#36753;&#30028;&#38754;&#65292;&#22312;&#27491;&#22312;&#32534;&#36753;&#30340;&#25991;&#26723;&#20013;&#30452;&#25509;&#24314;&#35758;&#21487;&#25191;&#34892;&#32534;&#36753;&#12290;&#30001;&#20110;&#24050;&#30693;LLMs&#20250;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#65292;InkSync&#36824;&#25903;&#25345;&#19968;&#31181;3&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#27492;&#39118;&#38505;&#65306;&#24403;&#24314;&#35758;&#30340;&#32534;&#36753;&#24341;&#20837;&#26032;&#20449;&#24687;&#26102;&#65292;&#35686;&#21578;&#20316;&#32773;&#65307;&#36890;&#36807;&#22806;&#37096;&#25628;&#32034;&#24110;&#21161;&#20316;&#32773;&#39564;&#35777;&#26032;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#65307;&#20801;&#35768;&#23457;&#26680;&#20154;&#21592;&#36890;&#36807;&#36319;&#36394;&#25152;&#26377;&#33258;&#21160;&#29983;&#25104;&#20869;&#23481;&#30340;&#30165;&#36857;&#26469;&#23545;&#25991;&#26723;&#36827;&#34892;&#20107;&#21518;&#39564;&#35777;&#12290;&#20004;&#39033;&#21487;&#29992;&#24615;&#30740;&#31350;&#35777;&#23454;&#20102;InkSync&#30340;&#21508;&#20010;&#32452;&#20214;&#30456;&#27604;&#20110;&#26631;&#20934;&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#32534;&#36753;&#26356;&#20934;&#30830;&#65292;
&lt;/p&gt;
&lt;p&gt;
Conversational interfaces powered by Large Language Models (LLMs) have recently become a popular way to obtain feedback during document editing. However, standard chat-based conversational interfaces do not support transparency and verifiability of the editing changes that they suggest. To give the author more agency when editing with an LLM, we present InkSync, an editing interface that suggests executable edits directly within the document being edited. Because LLMs are known to introduce factual errors, Inksync also supports a 3-stage approach to mitigate this risk: Warn authors when a suggested edit introduces new information, help authors Verify the new information's accuracy through external search, and allow an auditor to perform an a-posteriori verification by Auditing the document via a trace of all auto-generated content. Two usability studies confirm the effectiveness of InkSync's components when compared to standard LLM-based chat interfaces, leading to more accurate, more 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.15317</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
joint prediction and denoising for large-scale multilingual self-supervised learning. (arXiv:2309.15317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#30001;&#20110;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#25152;&#38656;&#30340;&#36153;&#29992;&#21644;&#22797;&#26434;&#24615;&#32780;&#32463;&#24120;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36825;&#36827;&#19968;&#27493;&#24433;&#21709;&#20102;SSL&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#30001;&#20110;&#36164;&#28304;&#20351;&#29992;&#30340;&#38480;&#21046;&#65292;SSL&#24050;&#32463;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#38431;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#26356;&#22810;&#30340;&#30740;&#31350;&#22242;&#38431;&#33021;&#22815;&#21152;&#20837;SSL&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23558;WavLM&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#25193;&#23637;&#21040;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#12290;&#20026;&#20102;&#26500;&#24314;WavLabLM&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#12290;WavLabLM&#22312;ML-SUPERB&#19978;&#23454;&#29616;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;vanilla HuBERT Base&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#25928;&#29575;&#25552;&#21319;&#65292;&#20165;&#20351;&#29992;3%&#30340;&#25968;&#25454;&#12289;4&#20010;GPU&#21644;&#26377;&#38480;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#23601;&#33021;&#20445;&#25345;94%&#30340;XLS-R&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM's joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited trials. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15238</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#23398;&#20064;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#39069;&#22806;&#30340;&#25968;&#25454;&#34920;&#31034;&#20013;&#33719;&#30410;&#65292;&#36825;&#34987;&#31216;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#19981;&#30475;&#21040;&#39069;&#22806;&#34920;&#31034;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#33719;&#24471;&#29305;&#26435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#29305;&#26435;&#20449;&#24687;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#21407;&#22987;&#25991;&#26412;&#26679;&#26412;&#36827;&#19968;&#27493;&#29992;&#20110;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#26469;&#35757;&#32451;&#22810;&#27169;&#24577;&#25945;&#24072;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22810;&#27169;&#24577;&#25945;&#24072;&#30340;&#30693;&#35782;&#34987;&#33976;&#39311;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#65288;&#21333;&#27169;&#24577;&#65289;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUGPI&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15223</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#31532;&#20108;&#27425;&#37325;&#35780;&#20998;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#39044;&#35757;&#32451;&#38454;&#27573;&#25193;&#23637;&#21644;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37325;&#35780;&#20998;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;0.08%&#65289;&#26469;&#35757;&#32451;&#37325;&#35780;&#20998;&#30340;BERT&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#36825;&#20123;&#25554;&#20837;&#30340;&#30697;&#38453;&#36890;&#36807;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#21028;&#21035;&#24615;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;Rescore-BERT&#65288;LoRB&#65289;&#20307;&#31995;&#32467;&#26500;&#22312;LibriSpeech&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;5.4&#33267;3.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
&lt;/p&gt;</description></item><item><title>RAGAs&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#21442;&#32771;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#26816;&#32034;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#19968;&#31181;&#21152;&#24555;RAG&#26550;&#26500;&#35780;&#20272;&#21608;&#26399;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15217</link><description>&lt;p&gt;
RAGAS:&#33258;&#21160;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RAGAS: Automated Evaluation of Retrieval Augmented Generation. (arXiv:2309.15217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15217
&lt;/p&gt;
&lt;p&gt;
RAGAs&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#21442;&#32771;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#26816;&#32034;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#19968;&#31181;&#21152;&#24555;RAG&#26550;&#26500;&#35780;&#20272;&#21608;&#26399;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;RAGAs&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27969;&#27700;&#32447;&#36827;&#34892;&#26080;&#21442;&#32771;&#35780;&#20272;&#12290;RAG&#31995;&#32479;&#30001;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#27169;&#22359;&#32452;&#25104;&#65292;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#26412;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#32473;LLMs&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#20805;&#24403;&#29992;&#25143;&#21644;&#25991;&#26412;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#33258;&#28982;&#35821;&#35328;&#23618;&#65292;&#20943;&#23569;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;RAG&#26550;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26377;&#20960;&#20010;&#32500;&#24230;&#38656;&#35201;&#32771;&#34385;&#65306;&#26816;&#32034;&#31995;&#32479;&#35782;&#21035;&#30456;&#20851;&#21644;&#26377;&#37325;&#28857;&#30340;&#19978;&#19979;&#25991;&#27573;&#33853;&#30340;&#33021;&#21147;&#65292;LLM&#22312;&#24544;&#23454;&#22320;&#21033;&#29992;&#36825;&#20123;&#27573;&#33853;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#29983;&#25104;&#26412;&#36523;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;RAGAs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#32500;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22320;&#38754;&#30495;&#23454;&#30340;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#26694;&#26550;&#33021;&#22815;&#23545;RAG&#26550;&#26500;&#30340;&#26356;&#24555;&#35780;&#20272;&#21608;&#26399;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With RAGAs, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectur
&lt;/p&gt;</description></item><item><title>STANCE-C3&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#20154;&#20204;&#23545;&#20110;&#26222;&#36941;&#25110;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#35266;&#28857;&#12290;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#21644;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#26631;&#27880;&#25968;&#25454;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.15176</link><description>&lt;p&gt;
STANCE-C3: &#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation. (arXiv:2309.15176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15176
&lt;/p&gt;
&lt;p&gt;
STANCE-C3&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#20154;&#20204;&#23545;&#20110;&#26222;&#36941;&#25110;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#35266;&#28857;&#12290;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#21644;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#26631;&#27880;&#25968;&#25454;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#36890;&#36807;&#25512;&#26029;&#19968;&#20010;&#20154;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#30340;&#31435;&#22330;&#25110;&#35266;&#28857;&#65292;&#20197;&#25512;&#26029;&#23545;&#20110;&#26222;&#36941;&#25110;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#30340;&#26222;&#36941;&#30475;&#27861;&#65292;&#20363;&#22914;COVID-19&#30123;&#24773;&#26399;&#38388;&#30340;&#20581;&#24247;&#25919;&#31574;&#12290;&#29616;&#26377;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#24448;&#24448;&#22312;&#21333;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;COVID-19&#65289;&#21644;&#29305;&#23450;&#30446;&#26631;&#35805;&#39064;&#65288;&#20363;&#22914;&#21475;&#32617;&#35268;&#23450;&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20854;&#20182;&#39046;&#22495;&#25110;&#30446;&#26631;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#39046;&#22495;&#29305;&#23450;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#20851;&#30340;&#24050;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#19981;&#23481;&#26131;&#33719;&#21462;&#12290;&#36825;&#23601;&#38754;&#20020;&#30528;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#27880;&#25968;&#25454;&#30340;&#36807;&#31243;&#20195;&#20215;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#65288;STANCE-C3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection is the process of inferring a person's position or standpoint on a specific issue to deduce prevailing perceptions toward topics of general or controversial interest, such as health policies during the COVID-19 pandemic. Existing models for stance detection are trained to perform well for a single domain (e.g., COVID-19) and a specific target topic (e.g., masking protocols), but are generally ineffectual in other domains or targets due to distributional shifts in the data. However, constructing high-performing, domain-specific stance detection models requires an extensive corpus of labeled data relevant to the targeted domain, yet such datasets are not readily available. This poses a challenge as the process of annotating data is costly and time-consuming. To address these challenges, we introduce a novel stance detection model coined domain-adaptive Cross-target STANCE detection via Contrastive learning and Counterfactual generation (STANCE-C3) that uses counterfactua
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;CogEval&#21327;&#35758;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#35813;&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15129</link><description>&lt;p&gt;
&#29992;CogEval&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. (arXiv:2309.15129v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;CogEval&#21327;&#35758;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#35813;&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#22768;&#31216;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26032;&#20852;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#26696;&#20363;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#38598;&#30340;&#27745;&#26579;&#65292;&#25110;&#32773;&#32570;&#20047;&#28041;&#21450;&#22810;&#20010;&#20219;&#21153;&#12289;&#25511;&#21046;&#26465;&#20214;&#12289;&#22810;&#27425;&#36845;&#20195;&#21644;&#32479;&#35745;&#40065;&#26834;&#24615;&#27979;&#35797;&#30340;&#31995;&#32479;&#35780;&#20272;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#22823;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CogEval&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#35748;&#30693;&#31185;&#23398;&#21551;&#21457;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;CogEval&#21327;&#35758;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;CogEval&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#65288;OpenAI GPT-4&#12289;GPT-3.5-turbo-175B&#12289;davinci-003-175B&#12289;Google Bard&#12289;Cohere-xlarge-52.4B&#12289;Anthropic Claude-1-52B&#12289;LLaMA-13B&#21644;Alpaca-7B&#65289;&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#25552;&#31034;&#22522;&#20110;&#20154;&#31867;&#23454;&#39564;&#65292;&#26082;&#20855;&#26377;&#35780;&#20272;&#35268;&#21010;&#30340;&#24050;&#24314;&#31435;&#26500;&#36896;&#25928;&#24230;&#65292;&#21448;&#19981;&#23384;&#22312;&#20110;LLM&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;LLMs&#23637;&#31034;&#20102;&#19968;&#20123;
&lt;/p&gt;
&lt;p&gt;
Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14717</link><description>&lt;p&gt;
QA-LoRA: &#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;LLMs&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;QA-LoRA&#65289;&#31639;&#27861;&#12290;&#21160;&#26426;&#22312;&#20110;&#37327;&#21270;&#21644;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#19981;&#24179;&#34913;&#65292;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#32452;&#20869;&#36816;&#31639;&#31526;&#65292;&#22686;&#21152;&#37327;&#21270;&#30340;&#33258;&#30001;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#12290;QA-LoRA&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#20351;&#21407;&#22987;&#30340;LoRA&#20855;&#22791;&#20102;&#20004;&#20010;&#33021;&#21147;&#65306;&#65288;i&#65289;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLM&#30340;&#26435;&#37325;&#34987;&#37327;&#21270;&#65288;&#20363;&#22914;&#36716;&#25442;&#20026;INT4&#65289;&#65292;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#65288;ii&#65289;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;LLM&#21644;&#36741;&#21161;&#26435;&#37325;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;QA-LoRA&#24212;&#29992;&#21040;LLaMA&#21644;LLaMA2&#27169;&#22411;&#23478;&#26063;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14398</link><description>&lt;p&gt;
&#30475;&#35265;&#21644;&#21548;&#21040;&#27809;&#34987;&#35828;&#30340;&#35805;&#65306;&#21487;&#35299;&#37322;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#21160;&#26426;&#24615;&#35775;&#35848;&#23458;&#25143;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#26159;&#19968;&#31181;&#24378;&#35843;&#21512;&#20316;&#24182;&#40723;&#21169;&#34892;&#20026;&#25913;&#21464;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;MI&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;MISC&#20195;&#30721;&#23558;&#23458;&#25143;&#35805;&#35821;&#20998;&#31867;&#20026;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#25110;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#12290;MI&#23545;&#35805;&#20013;&#21464;&#21270;&#35805;&#35821;&#30340;&#27604;&#20363;&#19982;&#27835;&#30103;&#32467;&#26524;&#21576;&#27491;&#30456;&#20851;&#65292;&#22240;&#27492;&#20934;&#30830;&#20998;&#31867;&#23458;&#25143;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#20934;&#30830;&#21306;&#20998;&#19977;&#20010;MISC&#31867;&#21035;&#65288;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
&lt;/p&gt;</description></item><item><title>ALLURE&#26159;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#35780;&#20272;&#30340;&#23457;&#35745;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#24182;&#32435;&#20837;&#37325;&#22823;&#20559;&#24046;&#30340;&#23454;&#20363;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;LLM&#23545;&#25991;&#26412;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13701</link><description>&lt;p&gt;
ALLURE: &#22522;&#20110;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#35780;&#20272;&#30340;&#23457;&#35745;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning. (arXiv:2309.13701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13701
&lt;/p&gt;
&lt;p&gt;
ALLURE&#26159;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#35780;&#20272;&#30340;&#23457;&#35745;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#24182;&#32435;&#20837;&#37325;&#22823;&#20559;&#24046;&#30340;&#23454;&#20363;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;LLM&#23545;&#25991;&#26412;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35780;&#20998;&#35770;&#25991;&#21040;&#24635;&#32467;&#21307;&#30103;&#25991;&#20214;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35780;&#20272;&#30001;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#29992;&#24615;&#65292;LLM&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#38656;&#35201;&#23545;&#20854;&#25991;&#26412;&#35780;&#20272;&#33021;&#21147;&#36827;&#34892;&#24443;&#24213;&#23457;&#35745;&#21644;&#25913;&#36827;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ALLURE&#65292;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#38169;&#35823;&#12290;ALLURE&#28041;&#21450;&#23558;LLM&#29983;&#25104;&#30340;&#35780;&#20272;&#19982;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36845;&#20195;&#22320;&#23558;&#37325;&#22823;&#20559;&#24046;&#30340;&#23454;&#20363;&#32435;&#20837;&#35780;&#20272;&#22120;&#20013;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25552;&#39640;&#21644;&#25913;&#36827;LLM&#23545;&#25991;&#26412;&#30340;&#40065;&#26834;&#35780;&#20272;&#12290;&#36890;&#36807;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#35780;&#20272;&#22120;LLM&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#23545;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#39044;&#35745;ALLURE&#23558;&#22312;&#19982;&#25991;&#26412;&#25968;&#25454;&#35780;&#20272;&#30456;&#20851;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#21307;&#23398;&#27010;&#25324;&#31561;&#65292;&#20026;LLM&#30340;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#26412;&#20307;&#31867;&#21517;&#30340;&#35821;&#20041;&#21644;&#35789;&#27719;&#29305;&#24449;&#65292;&#23558;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#26144;&#23556;&#21040;DBpedia&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#23436;&#21892;&#21644;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2309.11791</link><description>&lt;p&gt;
SLHCat: &#21033;&#29992;&#35821;&#20041;&#12289;&#35789;&#27719;&#21644;&#23618;&#27425;&#29305;&#24449;&#23558;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#26144;&#23556;&#21040;DBpedia
&lt;/p&gt;
&lt;p&gt;
SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features. (arXiv:2309.11791v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#26412;&#20307;&#31867;&#21517;&#30340;&#35821;&#20041;&#21644;&#35789;&#27719;&#29305;&#24449;&#65292;&#23558;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#26144;&#23556;&#21040;DBpedia&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#23436;&#21892;&#21644;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikipedia&#30340;&#25991;&#31456;&#36890;&#36807;&#20998;&#31867;&#21644;&#21015;&#34920;&#36827;&#34892;&#23618;&#27425;&#21270;&#32452;&#32455;&#65292;&#25552;&#20379;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#20840;&#38754;&#21644;&#26222;&#36941;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#20294;&#20854;&#24320;&#25918;&#24615;&#23548;&#33268;&#20102;&#37325;&#22797;&#21644;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#23558;DBpedia&#30340;&#31867;&#21035;&#20998;&#37197;&#32473;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#29616;&#19968;&#20010;&#23545;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#20998;&#31867;&#25968;&#23383;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CaLiGraph&#26041;&#27861;&#20135;&#29983;&#20102;&#19981;&#23436;&#25972;&#21644;&#38750;&#32454;&#31890;&#24230;&#30340;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#30475;&#20316;&#26412;&#20307;&#23545;&#40784;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#26412;&#20307;&#31867;&#21517;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#21457;&#29616;&#33258;&#20449;&#26144;&#23556;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#26144;&#23556;&#20197;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SLHCat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;1&#65289;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#21629;&#21517;&#23454;&#20307;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Wikipedia articles are hierarchically organized through categories and lists, providing one of the most comprehensive and universal taxonomy, but its open creation is causing redundancies and inconsistencies. Assigning DBPedia classes to Wikipedia categories and lists can alleviate the problem, realizing a large knowledge graph which is essential for categorizing digital contents through entity linking and typing. However, the existing approach of CaLiGraph is producing incomplete and non-fine grained mappings. In this paper, we tackle the problem as ontology alignment, where structural information of knowledge graphs and lexical and semantic features of ontology class names are utilized to discover confident mappings, which are in turn utilized for finetuing pretrained language models in a distant supervision fashion. Our method SLHCat consists of two main parts: 1) Automatically generating training data by leveraging knowledge graph structure, semantic similarities, and named entity 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.11166</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#35789;&#32423;&#25200;&#21160;&#30495;&#30340;&#20855;&#26377;&#40065;&#26834;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#27169;&#21644;&#33021;&#21147;&#19978;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#38500;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36991;&#20813;&#23545;&#29305;&#23450;&#25552;&#31034;&#30340;&#28608;&#28872;&#21453;&#39304;&#22806;&#65292;&#30830;&#20445;LLM&#30340;&#36131;&#20219;&#24615;&#36824;&#38656;&#35201;&#20851;&#27880;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#20855;&#26377;&#39044;&#23450;&#20041;&#30417;&#30563;&#26631;&#31614;&#30340;&#20256;&#32479;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36825;&#19982;&#24403;&#20195;LLMs&#30340;&#20986;&#33394;&#29983;&#25104;&#33021;&#21147;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#29702;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#26469;&#35780;&#20272;LLMs&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21512;&#29702;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;TREvaL&#65289;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;TREval&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24335;&#38382;&#39064;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10966</link><description>&lt;p&gt;
MBR&#21644;QE&#24494;&#35843;&#65306;&#23545;&#26368;&#20339;&#21644;&#26368;&#26114;&#36149;&#30340;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#30340;&#35299;&#30721;&#26041;&#27861;&#30740;&#31350;&#20013;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#27010;&#29575;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22256;&#24785;&#24230;&#19982;&#36136;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26356;&#24378;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#37325;&#25490;&#24207;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#33976;&#39311;&#20102;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#30340;&#36136;&#37327;&#25552;&#21319;&#65292;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;NLG&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#22806;&#37096;LLM&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22256;&#24785;&#24230;&#26469;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#27745;&#26579;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#25688;&#35201;&#22522;&#20934;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#35760;&#24518;&#21270;&#65292;&#32780;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#21017;&#21463;&#27745;&#26579;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2309.10677</link><description>&lt;p&gt;
&#36890;&#36807;&#22256;&#24785;&#24230;&#20272;&#35745;&#27745;&#26579;&#65306;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#35760;&#24518;&#21270;
&lt;/p&gt;
&lt;p&gt;
Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation. (arXiv:2309.10677v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22256;&#24785;&#24230;&#26469;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#27745;&#26579;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#25688;&#35201;&#22522;&#20934;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#35760;&#24518;&#21270;&#65292;&#32780;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#21017;&#21463;&#27745;&#26579;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#25968;&#25454;&#27745;&#26579;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#35821;&#26009;&#24211;&#32463;&#24120;&#26080;&#24847;&#20013;&#21253;&#21547;&#22522;&#20934;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#27745;&#26579;&#20998;&#26512;&#24050;&#25104;&#20026;&#21487;&#38752;&#27169;&#22411;&#35780;&#20272;&#19981;&#21487;&#36991;&#20813;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27745;&#26579;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#23545;&#20110;&#26368;&#26032;&#27169;&#22411;&#26469;&#35828;&#26159;&#20445;&#23494;&#30340;&#12290;&#36825;&#38459;&#27490;&#20102;&#31038;&#21306;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#21644;&#20934;&#30830;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22312;&#19981;&#35775;&#38382;&#23436;&#25972;&#35757;&#32451;&#38598;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#27745;&#26579;&#65292;&#21363;&#29992;&#22256;&#24785;&#24230;&#26469;&#34913;&#37327;&#27745;&#26579;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#21463;&#27426;&#36814;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#25688;&#35201;&#22522;&#20934;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#35760;&#24518;&#21270;&#65292;&#32780;&#22810;&#39033;&#36873;&#25321;&#20284;&#20046;&#27809;&#26377;&#37027;&#20040;&#21463;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.07311</link><description>&lt;p&gt;
&#25439;&#22833;&#31361;&#28982;&#19979;&#38477;&#65306;&#35821;&#27861;&#20064;&#24471;&#12289;&#30456;&#21464;&#21644;MLM&#20013;&#30340;&#31616;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20391;&#37325;&#20110;&#29702;&#35299;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#21487;&#33021;&#25165;&#33021;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26576;&#20123;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;(MLMs)&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21270;&#26469;&#21152;&#28145;&#25105;&#20204;&#23545;&#26032;&#20852;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#36825;&#26159;MLMs&#20013;&#33258;&#28982;&#24418;&#25104;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#20854;&#20013;&#29305;&#23450;&#30340;Transformer&#22836;&#20542;&#21521;&#20110;&#20851;&#27880;&#29305;&#23450;&#30340;&#21477;&#27861;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;SAS&#65292;&#24182;&#21457;&#29616;&#36825;&#20010;&#31383;&#21475;&#19982;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#21516;&#26102;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;SAS&#20419;&#20351;&#20102;&#38543;&#21518;&#23545;&#35821;&#35328;&#33021;&#21147;&#30340;&#20064;&#24471;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#25805;&#32437;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;SAS&#65292;&#26469;&#30740;&#31350;SAS&#30340;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.06726</link><description>&lt;p&gt;
&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;BART&#24494;&#35843;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling. (arXiv:2309.06726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#26159;&#19968;&#39033;&#35782;&#21035;&#26368;&#20339;&#20195;&#34920;&#32473;&#23450;&#25991;&#26412;&#20027;&#39064;&#25110;&#20027;&#39064;&#30340;&#30701;&#35821;&#38598;&#30340;&#20219;&#21153;&#12290;&#20851;&#38190;&#30701;&#35821;&#20998;&#20026;&#20986;&#29616;&#21644;&#19981;&#22312;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#12290;&#26368;&#36817;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#19978;&#26174;&#31034;&#20986;&#20102;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25214;&#21040;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#30340;&#38590;&#24230;&#65292;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#21033;&#29992;&#20102;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#20998;&#21035;&#36827;&#34892;&#20102;&#20004;&#20010;&#29420;&#31435;BART&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20851;&#38190;&#30701;&#35821;&#30340;&#37325;&#25490;&#21644;&#20505;&#36873;&#20851;&#38190;&#30701;&#35821;&#25490;&#24207;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23545;&#20110;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#65292;&#22312;&#20116;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#22312;F1@5&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation is a task of identifying a set of phrases that best repre-sent the main topics or themes of a given text. Keyphrases are dividend int pre-sent and absent keyphrases. Recent approaches utilizing sequence-to-sequence models show effectiveness on absent keyphrase generation. However, the per-formance is still limited due to the hardness of finding absent keyphrases. In this paper, we propose Keyphrase-Focused BART, which exploits the differ-ences between present and absent keyphrase generations, and performs fine-tuning of two separate BART models for present and absent keyphrases. We further show effective approaches of shuffling keyphrases and candidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART achieved new state-of-the-art score on F1@5 in two out of five keyphrase gen-eration benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.02427</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#22686;&#21152;&#20102;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20114;&#32852;&#32593;&#65289;&#25110;&#20869;&#37096;&#25511;&#21046;&#27969;&#65288;&#20363;&#22914;&#25552;&#31034;&#38142;&#65289;&#65292;&#29992;&#20110;&#38656;&#35201;&#22522;&#20110;&#35821;&#22659;&#25110;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#29616;&#26377;&#20195;&#29702;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20016;&#23500;&#21382;&#21490;&#65292;&#25552;&#20986;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;&#65288;CoALA&#65289;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#29992;&#20110;&#19982;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#36873;&#25321;&#34892;&#21160;&#30340;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;CoALA&#23545;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#32452;&#32455;&#65292;&#24182;&#23637;&#26395;&#20102;&#26356;&#24378;&#22823;&#20195;&#29702;&#30340;&#21487;&#34892;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CoALA&#23558;&#24403;&#20170;&#30340;&#35821;&#35328;&#20195;&#29702;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DS4DH&#22312;#SMM4H 2023&#20013;&#24320;&#21457;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.12877</link><description>&lt;p&gt;
DS4DH&#22312;#SMM4H 2023&#19978;&#65306;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DS4DH&#22312;#SMM4H 2023&#20013;&#24320;&#21457;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#30001;&#25968;&#25454;&#31185;&#23398;&#19982;&#25968;&#23383;&#20581;&#24247;&#22242;&#38431;&#24320;&#21457;&#30340;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#20581;&#24247;&#24212;&#29992;2023&#20849;&#20139;&#20219;&#21153;5&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#20849;&#20139;&#20219;&#21153;5&#26088;&#22312;&#23558;Twitter&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21450;&#26631;&#20934;&#21270;&#20026;&#21307;&#30103;&#27861;&#35268;&#27963;&#21160;&#26415;&#35821;&#23383;&#20856;&#20013;&#30340;&#26631;&#20934;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;BERT&#24494;&#35843;&#23454;&#20307;&#35782;&#21035;&#65292;&#28982;&#21518;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#35813;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#20026;44.9%&#65292;&#21484;&#22238;&#29575;&#20026;40.5%&#65292;F1&#20998;&#25968;&#20026;42.6%&#12290;&#23427;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20849;&#20139;&#20219;&#21153;5&#20013;&#20301;&#25968;&#34920;&#29616;10%&#65292;&#24182;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#23637;&#31034;&#20102;&#26368;&#39640;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#20013;&#36827;&#34892;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the Data Science for Digital Health group for the Social Media Mining for Health Applications 2023 shared task 5. Shared task 5 targeted the normalization of adverse drug event mentions in Twitter to standard concepts from the Medical Dictionary for Regulatory Activities terminology. Our system hinges on a two-stage approach: BERT fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. The approach yielded a precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. These results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social media text mining.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#35782;&#21035;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#31215;&#26497;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#28389;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#26816;&#27979;&#65292;&#20294;&#26576;&#20123;&#24694;&#24847;&#28389;&#29992;&#38656;&#35201;&#36319;&#36394;&#23545;&#25163;&#29992;&#25143;&#20197;&#36827;&#34892;&#21453;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20301;&#27700;&#21360;&#36890;&#36807;&#39068;&#33394;&#32534;&#30721;&#8221;&#65288;COLOR&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#12290;&#21033;&#29992;&#38646;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#20248;&#21183;&#65288;Kirchenbauer&#31561;&#65292;2023a&#65289;&#65292;COLOR&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#65288;&#32422;500&#20010;&#26631;&#35760;&#65289;&#20013;&#25104;&#21151;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#25928;&#22320;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#36827;&#34892;&#21453;&#21046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.15593</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#27700;&#21360;&#23545;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#35777;&#29983;&#25104;&#39044;&#31639;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;&#38543;&#26426;&#27700;&#21360;&#23494;&#38053;&#35745;&#31639;&#30340;&#38543;&#26426;&#25968;&#24207;&#21015;&#26144;&#23556;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#26469;&#29983;&#25104;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#35201;&#26816;&#27979;&#27700;&#21360;&#25991;&#26412;&#65292;&#21482;&#35201;&#30693;&#36947;&#23494;&#38053;&#30340;&#20219;&#20309;&#19968;&#26041;&#37117;&#21487;&#20197;&#23558;&#25991;&#26412;&#19982;&#38543;&#26426;&#25968;&#24207;&#21015;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#37319;&#26679;&#26041;&#26696;&#26469;&#23454;&#20363;&#21270;&#27700;&#21360;&#26041;&#27861;&#65306;&#21453;&#21464;&#25442;&#37319;&#26679;&#21644;&#25351;&#25968;&#26368;&#23567;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27700;&#21360;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;OPT-1.3B&#12289;LLaMA-7B&#21644;Alpaca-7B&#65292;&#20197;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#23545;&#21508;&#31181;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;OPT-1.3B&#21644;LLaMA-7B&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25200;&#21160;&#20102;40-50%&#30340;&#35789;&#20803;&#21518;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#65288;$p \leq 0.01$&#65289;&#65292;&#21482;&#38656;&#35201;35&#20010;&#35789;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#21306;&#20998;&#30495;&#23454;&#22768;&#38899;&#21644;&#35797;&#22270;&#20882;&#20805;&#29305;&#23450;&#20154;&#29289;&#22768;&#38899;&#30340;&#20811;&#38534;&#22768;&#38899;&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#37319;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#22768;&#38899;&#19978;&#35757;&#32451;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#20064;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#23545;&#23545;&#25239;&#24615;&#28165;&#27927;&#20855;&#26377;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07683</link><description>&lt;p&gt;
&#21333;&#22768;&#36947;&#21644;&#22810;&#22768;&#36947;&#20811;&#38534;&#22768;&#38899;&#26816;&#27979;&#65306;&#20174;&#24863;&#30693;&#21040;&#23398;&#20064;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features. (arXiv:2307.07683v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#21306;&#20998;&#30495;&#23454;&#22768;&#38899;&#21644;&#35797;&#22270;&#20882;&#20805;&#29305;&#23450;&#20154;&#29289;&#22768;&#38899;&#30340;&#20811;&#38534;&#22768;&#38899;&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#37319;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#22768;&#38899;&#19978;&#35757;&#32451;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#20064;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#23545;&#23545;&#25239;&#24615;&#28165;&#27927;&#20855;&#26377;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21512;&#25104;&#35821;&#38899;&#20811;&#38534;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24341;&#21457;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#21361;&#23475;&#12290;&#20174;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#30340;&#37329;&#34701;&#27450;&#35784;&#21040;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#27963;&#21160;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#22768;&#38899;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#31181;&#21306;&#20998;&#30495;&#23454;&#22768;&#38899;&#21644;&#35797;&#22270;&#20882;&#20805;&#29305;&#23450;&#20154;&#29289;&#22768;&#38899;&#30340;&#20811;&#38534;&#22768;&#38899;&#30340;&#25216;&#26415;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20302;&#32500;&#24863;&#30693;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#20294;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#32780;&#36890;&#29992;&#30340;&#39057;&#35889;&#29305;&#24449;&#21644;&#31471;&#21040;&#31471;&#23398;&#20064;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#20294;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21333;&#20010;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#19978;&#35757;&#32451;&#21644;&#22312;&#22810;&#20010;&#22768;&#38899;&#19978;&#35757;&#32451;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#20064;&#29305;&#24449;&#22987;&#32456;&#20445;&#25345;&#30528;$0\%$&#33267;$4\%$&#20043;&#38388;&#30340;&#31561;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#23545;&#23545;&#25239;&#24615;&#28165;&#27927;&#20855;&#26377;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic-voice cloning technologies have seen significant advances in recent years, giving rise to a range of potential harms. From small- and large-scale financial fraud to disinformation campaigns, the need for reliable methods to differentiate real and synthesized voices is imperative. We describe three techniques for differentiating a real from a cloned voice designed to impersonate a specific person. These three approaches differ in their feature extraction stage with low-dimensional perceptual features offering high interpretability but lower accuracy, to generic spectral features, and end-to-end learned features offering less interpretability but higher accuracy. We show the efficacy of these approaches when trained on a single speaker's voice and when trained on multiple voices. The learned features consistently yield an equal error rate between $0\%$ and $4\%$, and are reasonably robust to adversarial laundering.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>AutoTAMP&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#20316;&#20026;&#32763;&#35793;&#22120;&#21644;&#26816;&#26597;&#22120;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#32763;&#35793;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20013;&#38388;&#20219;&#21153;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2306.06531</link><description>&lt;p&gt;
AutoTAMP: &#20351;&#29992;LLMs&#20316;&#20026;&#32763;&#35793;&#22120;&#21644;&#26816;&#26597;&#22120;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers. (arXiv:2306.06531v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06531
&lt;/p&gt;
&lt;p&gt;
AutoTAMP&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#20316;&#20026;&#32763;&#35793;&#22120;&#21644;&#26816;&#26597;&#22120;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#32763;&#35793;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20013;&#38388;&#20219;&#21153;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#29702;&#35299;&#12289;&#35268;&#21010;&#21644;&#25191;&#34892;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22797;&#26434;&#12289;&#38271;&#26399;&#20219;&#21153;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#26426;&#22120;&#20154;&#34892;&#21160;&#24207;&#21015;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#30452;&#25509;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#35821;&#35328;&#20998;&#35299;&#20026;&#20219;&#21153;&#23376;&#30446;&#26631;&#24182;&#20381;&#38752;&#21160;&#20316;&#35268;&#21010;&#22120;&#25191;&#34892;&#27599;&#20010;&#23376;&#30446;&#26631;&#26469;&#20998;&#35299;&#25512;&#29702;&#36807;&#31243;&#12290;&#24403;&#28041;&#21450;&#22797;&#26434;&#30340;&#29615;&#22659;&#21644;&#26102;&#38388;&#32422;&#26463;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#26469;&#32852;&#21512;&#36827;&#34892;&#35268;&#21010;&#20219;&#21153;&#30340;&#25512;&#29702;&#21644;&#21160;&#20316;&#35268;&#21010;&#65292;&#20351;&#24471;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#25104;&#20026;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#30452;&#25509;&#35268;&#21010;&#20219;&#21153;&#23376;&#30446;&#26631;&#65292;&#32780;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#32763;&#35793;&#65292;&#29983;&#25104;&#19968;&#20010;&#20013;&#38388;&#20219;&#21153;&#34920;&#31034;&#65292;&#28982;&#21518;&#21487;&#20197;&#30001;TAMP&#31639;&#27861;&#28040;&#21270;&#35813;&#34920;&#31034;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to join
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.12057</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#21487;&#20197;&#20026;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#21487;&#35266;&#27979;&#24615;&#21644;&#27979;&#37327;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#22312;&#34913;&#37327;&#31435;&#27861;&#32773;&#30340;&#28508;&#22312;&#24847;&#35782;&#24418;&#24577;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#22609;&#36896;&#25919;&#31574;&#30340;&#25919;&#27835;&#21151;&#33021;&#65292;&#20197;&#21450;&#25919;&#27835;&#34892;&#20026;&#32773;&#20195;&#34920;&#20854;&#36873;&#27665;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;ChatGPT&#22312;&#20004;&#20004;&#27604;&#36739;&#20013;&#36873;&#25321;&#26356;&#33258;&#30001;&#27966;&#65288;&#25110;&#20445;&#23432;&#27966;&#65289;&#30340;&#21442;&#35758;&#21592;&#65292;&#23558;&#31532;116&#23626;&#32654;&#22269;&#22269;&#20250;&#30340;&#21442;&#35758;&#21592;&#25353;&#29031;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#30340;&#20809;&#35889;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#37325;&#22797;&#36845;&#20195;&#20013;&#20135;&#29983;&#20102;&#31283;&#23450;&#30340;&#31572;&#26696;&#65292;&#27809;&#26377;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#19981;&#20165;&#20165;&#26159;&#20174;&#21333;&#19968;&#26469;&#28304;&#20013;&#22797;&#21046;&#20449;&#24687;&#12290;&#36825;&#20010;&#26032;&#23610;&#24230;&#19982;&#29616;&#26377;&#30340;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#23610;&#24230;&#65288;&#22914;NOMINATE&#65289;&#24378;&#30456;&#20851;&#65292;&#20294;&#20063;&#22312;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#27604;&#22914;&#27491;&#30830;&#23450;&#20301;&#19968;&#20123;&#36335;&#24452;&#20381;&#36182;&#21644;&#33258;&#30001;&#27966;&#27966;&#21035;&#30340;&#35758;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mass aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine the utility of one such model for a particularly difficult measurement task: measuring the latent ideology of lawmakers, which allows us to better understand functions that are core to democracy, such as how politics shape policy and how political actors represent their constituents. We scale the senators of the 116th United States Congress along the liberal-conservative spectrum by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons. We show that the LLM produced stable answers across repeated iterations, did not hallucinate, and was not simply regurgitating information from a single source. This new scale strongly correlates with pre-existing liberal-conservative scales such as NOMINATE, but also differs in several important ways, such as correctly placin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10510</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#35821;&#38899;&#25509;&#21475;&#36234;&#26469;&#36234;&#20415;&#25463;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#21830;&#19994;ASR&#31995;&#32479;&#36890;&#24120;&#22312;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20316;&#32773;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DeepSpeech2&#21644;Wav2Vec2&#22768;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#21463;&#30410;&#29305;&#23450;&#30340;ASR&#31995;&#32479;&#12290;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#21482;&#38656;&#23569;&#37327;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#12290;&#26368;&#20339;&#24615;&#33021;&#26469;&#33258;&#19968;&#31181;&#32463;&#36807;&#24494;&#35843;&#30340;Wav2Vec2-Large-LV60&#22768;&#23398;&#27169;&#22411;&#65292;&#24102;&#26377;&#22806;&#37096;KenLM&#65292;&#22312;&#21463;&#30410;&#29305;&#23450;&#35821;&#38899;&#19978;&#36229;&#36234;&#20102;Google&#21644;AWS ASR&#31995;&#32479;&#12290;&#36824;&#30740;&#31350;&#20102;&#23558;&#23481;&#26131;&#20986;&#38169;&#30340;ASR&#36716;&#24405;&#20316;&#20026;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#19968;&#37096;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#21463;&#30410;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#21487;&#20197;&#36229;&#36234;&#21830;&#19994;ASR&#31995;&#32479;&#24182;&#25552;&#39640;NLU&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-Neo&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36739;&#22823;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15593</link><description>&lt;p&gt;
GPT-Neo&#29992;&#20110;&#24120;&#35782;&#25512;&#29702;--&#29702;&#35770;&#19982;&#23454;&#36341;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
GPT-Neo for commonsense reasoning -- a theoretical and practical lens. (arXiv:2211.15593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-Neo&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36739;&#22823;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GPT-Neo&#27169;&#22411;&#22312;6&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#26088;&#22312;&#23545;&#20351;&#29992;GPT-Neo&#27169;&#22411;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;GPT-3&#12289;Llama-2&#12289;MPT&#21644;Falcon&#31561;&#20960;&#20010;&#36739;&#22823;&#27169;&#22411;&#22522;&#20934;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#22312;&#20351;&#29992;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#27880;&#24847;&#21147;&#22836;&#21487;&#35270;&#21270;&#26469;&#35843;&#26597;&#21644;&#35777;&#23454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#22810;&#31181;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated substantial gains in pre-training large-language models (LLMs) followed by supervised fine-tuning on the downstream task. In this paper, we evaluate the performance of the GPT-neo model using $6$ commonsense reasoning benchmark tasks. We aim to examine the performance of smaller models using the GPT-neo models against several larger model baselines such as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the appropriate set of hyperparameters, our model achieves competitive accuracy on several tasks. We also investigate and substantiate our results using attention-head visualization to better understand the model performance. Finally, we conduct various robustness tests using various methods to gauge the model performance under numerous settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21518;&#26399;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#37326;&#22806;&#35270;&#39057;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35828;&#35805;&#32773;&#20998;&#36776;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#20195;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35828;&#35805;&#32773;&#35782;&#21035;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#26356;&#22810;&#35828;&#35805;&#32773;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#38754;&#37096;&#23646;&#24615;&#21644;&#21767;&#38899;&#21516;&#27493;&#30340;&#35270;&#35273;&#23376;&#31995;&#32479;&#33021;&#22815;&#20272;&#35745;&#23631;&#24149;&#19978;&#35828;&#35805;&#32773;&#30340;&#36523;&#20221;&#21644;&#35821;&#38899;&#27963;&#21160;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;AVA-AVD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.01299</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#30340;&#35828;&#35805;&#32773;&#20998;&#36776;&#20013;&#30340;&#21518;&#26399;&#38899;&#39057;-&#35270;&#35273;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Late Audio-Visual Fusion for In-The-Wild Speaker Diarization. (arXiv:2211.01299v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21518;&#26399;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#37326;&#22806;&#35270;&#39057;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35828;&#35805;&#32773;&#20998;&#36776;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#20195;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35828;&#35805;&#32773;&#35782;&#21035;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#26356;&#22810;&#35828;&#35805;&#32773;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#38754;&#37096;&#23646;&#24615;&#21644;&#21767;&#38899;&#21516;&#27493;&#30340;&#35270;&#35273;&#23376;&#31995;&#32479;&#33021;&#22815;&#20272;&#35745;&#23631;&#24149;&#19978;&#35828;&#35805;&#32773;&#30340;&#36523;&#20221;&#21644;&#35821;&#38899;&#27963;&#21160;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;AVA-AVD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#34701;&#21512;&#26159;&#20026;&#20102;&#24212;&#23545;&#20855;&#26377;&#26356;&#22810;&#35828;&#35805;&#32773;&#12289;&#36739;&#30701;&#21457;&#35328;&#21644;&#19981;&#19968;&#33268;&#30340;&#23631;&#24149;&#35828;&#35805;&#32773;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37326;&#22806;&#35270;&#39057;&#32780;&#25552;&#20986;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38899;&#39057;&#35270;&#35273;&#20998;&#36776;&#27169;&#22411;&#65292;&#36890;&#36807;&#21518;&#26399;&#34701;&#21512;&#23558;&#20165;&#21253;&#21547;&#38899;&#39057;&#21644;&#20197;&#35270;&#35273;&#20026;&#20013;&#24515;&#30340;&#23376;&#31995;&#32479;&#30456;&#32467;&#21512;&#12290;&#23545;&#20110;&#38899;&#39057;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#21560;&#24341;&#23376;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65288;EEND-EDA&#65289;&#22312;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#19968;&#20010;&#27169;&#25311;&#20195;&#29702;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#37197;&#26041;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;EEND-EDA++&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#21644;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#35828;&#35805;&#32773;&#35782;&#21035;&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#22788;&#29702;&#26356;&#22810;&#30340;&#35828;&#35805;&#32773;&#12290;&#20197;&#35270;&#35273;&#20026;&#20013;&#24515;&#30340;&#23376;&#31995;&#32479;&#21033;&#29992;&#38754;&#37096;&#23646;&#24615;&#21644;&#21767;&#38899;&#21516;&#27493;&#26469;&#20272;&#35745;&#23631;&#24149;&#19978;&#35828;&#35805;&#32773;&#30340;&#36523;&#20221;&#21644;&#35821;&#38899;&#27963;&#21160;&#12290;&#20004;&#20010;&#23376;&#31995;&#32479;&#37117;&#22823;&#24133;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415; (SOTA)&#65292;&#36890;&#36807;&#34701;&#21512;&#38899;&#39057;-&#35270;&#35273;&#31995;&#32479;&#22312;AVA-AVD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization is well studied for constrained audios but little explored for challenging in-the-wild videos, which have more speakers, shorter utterances, and inconsistent on-screen speakers. We address this gap by proposing an audio-visual diarization model which combines audio-only and visual-centric sub-systems via late fusion. For audio, we show that an attractor-based end-to-end system (EEND-EDA) performs remarkably well when trained with our proposed recipe of a simulated proxy dataset, and propose an improved version, EEND-EDA++, that uses attention in decoding and a speaker recognition loss during training to better handle the larger number of speakers. The visual-centric sub-system leverages facial attributes and lip-audio synchrony for identity and speech activity estimation of on-screen speakers. Both sub-systems surpass the state of the art (SOTA) by a large margin, with the fused audio-visual system achieving a new SOTA on the AVA-AVD benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#25351;&#23548;&#30340;&#30446;&#26631;-&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25351;&#20195;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25945;&#23398;&#27861;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.12758</link><description>&lt;p&gt;
&#20811;&#26381;&#35821;&#35328;&#25351;&#23548;&#30340;&#30446;&#26631;-&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#20195;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned Reinforcement Learning. (arXiv:2209.12758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#25351;&#23548;&#30340;&#30446;&#26631;-&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25351;&#20195;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25945;&#23398;&#27861;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25945;&#24072;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#19968;&#20010;&#20195;&#29702;&#25191;&#34892;&#26032;&#20219;&#21153;&#26102;&#65292;&#35299;&#37322;&#30340;&#27495;&#20041;&#24456;&#23481;&#26131;&#25104;&#20026;&#38459;&#30861;&#12290;&#24403;&#25945;&#24072;&#36890;&#36807;&#21442;&#32771;&#29289;&#20307;&#30340;&#29305;&#24449;&#21521;&#23398;&#20064;&#32773;&#25552;&#20379;&#25351;&#20196;&#26102;&#65292;&#23398;&#20064;&#32773;&#21487;&#33021;&#20250;&#35823;&#35299;&#25945;&#24072;&#30340;&#24847;&#22270;&#65292;&#29305;&#21035;&#26159;&#24403;&#25351;&#20196;&#27169;&#31946;&#22320;&#28041;&#21450;&#29289;&#20307;&#30340;&#29305;&#24449;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#25351;&#20195;&#27495;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#28304;&#33258;&#35748;&#30693;&#31185;&#23398;&#30340;&#27010;&#24565;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25351;&#20195;&#27495;&#20041;&#65306;&#25945;&#23398;&#27861;&#65288;&#36873;&#25321;&#21512;&#36866;&#30340;&#25351;&#20196;&#65289;&#21644;&#23454;&#29992;&#20027;&#20041;&#65288;&#36890;&#36807;&#24402;&#32435;&#25512;&#29702;&#20102;&#35299;&#20854;&#20182;&#20195;&#29702;&#30340;&#20559;&#22909;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#20154;&#24037;&#20195;&#29702;&#30340;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22534;&#31215;&#26408;&#22359;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27010;&#24565;&#22914;&#20309;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching an agent to perform new tasks using natural language can easily be hindered by ambiguities in interpretation. When a teacher provides an instruction to a learner about an object by referring to its features, the learner can misunderstand the teacher's intentions, for instance if the instruction ambiguously refer to features of the object, a phenomenon called referential ambiguity. We study how two concepts derived from cognitive sciences can help resolve those referential ambiguities: pedagogy (selecting the right instructions) and pragmatism (learning the preferences of the other agents using inductive reasoning). We apply those ideas to a teacher/learner setup with two artificial agents on a simulated robotic task (block-stacking). We show that these concepts improve sample efficiency for training the learner.
&lt;/p&gt;</description></item></channel></rss>