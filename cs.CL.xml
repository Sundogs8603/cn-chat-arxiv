<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#29983;&#25104;&#31572;&#26696;&#30340;&#23646;&#24615;&#21644;&#24402;&#22240;&#27169;&#24335;&#65292;&#24182;&#25214;&#20986;&#20102;&#24402;&#22240;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#38271;&#31687;&#12289;&#30693;&#35782;&#20016;&#23500;&#30340;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12150</link><description>&lt;p&gt;
&#29702;&#35299;&#29992;&#20110;&#38271;&#31687;&#38382;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Understanding Retrieval Augmentation for Long-Form Question Answering. (arXiv:2310.12150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#29983;&#25104;&#31572;&#26696;&#30340;&#23646;&#24615;&#21644;&#24402;&#22240;&#27169;&#24335;&#65292;&#24182;&#25214;&#20986;&#20102;&#24402;&#22240;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#38271;&#31687;&#12289;&#30693;&#35782;&#20016;&#23500;&#30340;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38271;&#31687;&#38382;&#31572;&#20013;&#25552;&#20986;&#20102;&#19968;&#39033;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#30456;&#21516;&#35777;&#25454;&#25991;&#26723;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#65292;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#23545;&#19981;&#21516;LMs&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26816;&#32034;&#25991;&#26723;&#38598;&#36136;&#37327;&#23545;&#30456;&#21516;LMs&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#31572;&#26696;&#30340;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#27969;&#30021;&#24230;&#12289;&#38271;&#24230;&#12289;&#21464;&#24322;&#24615;&#65289;&#65292;&#37325;&#28857;&#22312;&#20110;&#23558;&#29983;&#25104;&#30340;&#38271;&#31687;&#31572;&#26696;&#24402;&#22240;&#20110;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#25991;&#26723;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31572;&#26696;&#24402;&#22240;&#30340;&#20154;&#24037;&#26631;&#27880;&#24182;&#35780;&#20272;&#20102;&#33258;&#21160;&#35780;&#21028;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26816;&#32034;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;LMs&#29983;&#25104;&#38271;&#31687;&#12289;&#30693;&#35782;&#20016;&#23500;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#24402;&#22240;&#27169;&#24335;&#24182;&#20998;&#26512;&#20102;&#24402;&#22240;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#23545;&#38271;&#31687;&#12289;&#30693;&#35782;&#20016;&#23500;&#30340;&#25991;&#26412;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12143</link><description>&lt;p&gt;
&#31616;&#21333;&#26426;&#21046;&#29992;&#20110;&#34920;&#31034;&#12289;&#32034;&#24341;&#21644;&#25805;&#20316;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#20998;&#31867;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#36825;&#28041;&#21450;&#35774;&#32622;&#27169;&#22411;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#23427;&#20197;&#36866;&#24212;&#20855;&#26377;&#26631;&#35760;&#27010;&#24565;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#19968;&#20010;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#30697;&#38453;&#32479;&#35745;&#37327;&#26469;&#29983;&#25104;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#12290;&#36825;&#20123;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19968;&#32452;&#27010;&#24565;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20174;&#36825;&#20123;&#31614;&#21517;&#20013;&#23398;&#20064;&#35813;&#32467;&#26500;&#26469;&#36882;&#24402;&#22320;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#12290;&#24403;&#27010;&#24565;"&#30456;&#20132;"&#26102;&#65292;&#27010;&#24565;&#30340;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;"&#30456;&#20132;"&#27010;&#24565;&#20013;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#20027;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#20445;&#25345;&#19968;&#20010;&#27010;&#24565;&#23383;&#20856;&#65292;&#20197;&#20415;&#36755;&#20837;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#24182;&#34987;&#36335;&#30001;&#21040;&#19982;&#36755;&#20837;&#30340;(&#28508;&#22312;)&#29983;&#25104;&#30456;&#20851;&#30340;&#27010;&#24565;&#38598;&#21512;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20266;&#26234;&#33021;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#36825;&#20010;&#26694;&#26550;&#35748;&#20026;&#26234;&#33021;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#35266;&#23519;&#32773;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#21160;&#24577;&#20114;&#21160;&#35780;&#20272;&#26041;&#27861; &#12290;</title><link>http://arxiv.org/abs/2310.12135</link><description>&lt;p&gt;
&#20266;&#26234;&#33021;: &#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pseudointelligence: A Unifying Framework for Language Model Evaluation. (arXiv:2310.12135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20266;&#26234;&#33021;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#36825;&#20010;&#26694;&#26550;&#35748;&#20026;&#26234;&#33021;&#30340;&#35780;&#20272;&#21462;&#20915;&#20110;&#35266;&#23519;&#32773;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#21160;&#24577;&#20114;&#21160;&#35780;&#20272;&#26041;&#27861; &#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#65292;&#25105;&#20204;&#24517;&#39035;&#37319;&#21462;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#20266;&#38543;&#26426;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20266;&#26234;&#33021;&#65292;&#23427;&#25429;&#25417;&#20102;&#8220;&#65288;&#34987;&#24863;&#30693;&#30340;&#65289;&#26234;&#33021;&#22312;&#20110;&#35266;&#23519;&#32773;&#30340;&#30524;&#20013;&#8221;&#30340;&#26368;&#22823;&#21270;&#21407;&#21017;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26234;&#33021;&#30340;&#20027;&#24352;&#21482;&#26377;&#22312;&#32771;&#34385;&#21040;&#35780;&#20272;&#32773;&#26102;&#25165;&#26377;&#24847;&#20041;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#20854;&#26500;&#24314;&#20026;&#27169;&#22411;&#21644;&#23398;&#20064;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#20114;&#21160;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that "(perceived) intelligence lies in the eye of the beholder". That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.12127</link><description>&lt;p&gt;
&#20195;&#35789;&#25925;&#20107;&#65306;&#21487;&#35299;&#37322;&#24615;&#25351;&#23548;&#19979;&#30340;&#20844;&#24179;&#25351;&#23548;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#21487;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#26631;&#20934;&#24615;&#33021;&#22522;&#20934;&#19978;&#65292;&#24573;&#35270;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#20844;&#24179;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#22312;MT&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20174;&#32780;&#23548;&#33268;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#21542;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20174;&#33521;&#25991;&#21040;&#24503;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;WinoMT&#35821;&#26009;&#24211;&#19978;&#35745;&#31639;&#24050;&#24314;&#31435;&#30340;&#24615;&#21035;&#20559;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#40664;&#35748;&#20026;&#30007;&#24615;&#23624;&#20174;&#32763;&#35793;&#65292;&#29978;&#33267;&#24573;&#35270;&#22899;&#24615;&#32844;&#19994;&#21051;&#26495;&#21360;&#35937;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27169;&#22411;&#31995;&#32479;&#24615;&#22320;&#24573;&#35270;&#25351;&#31034;&#30446;&#26631;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#22312;&#21516;&#26102;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#20013;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#21487;&#35299;&#37322;&#24615;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#30340;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MT&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
&lt;/p&gt;</description></item><item><title>SHARCS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#25512;&#29702;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12126</link><description>&lt;p&gt;
SHARCS&#65306;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#30340;&#39640;&#25928;Transformer
&lt;/p&gt;
&lt;p&gt;
SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12126
&lt;/p&gt;
&lt;p&gt;
SHARCS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#25512;&#29702;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SHARCS&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#25512;&#29702;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;SHARCS&#21487;&#20197;&#22312;&#20219;&#20309;Transformer&#32593;&#32476;&#19978;&#35757;&#32451;&#19968;&#20010;&#36335;&#30001;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23558;&#19981;&#21516;&#26679;&#26412;&#25351;&#21521;&#20855;&#26377;&#19981;&#21516;&#23485;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#19982;FLOPs&#20043;&#38388;&#65292;SHARCS&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#25512;&#29702;&#26041;&#27861;&#65307;&#65288;2&#65289;SHARCS&#22312;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#21387;&#32553;&#21644;&#39640;&#25928;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#25928;&#29575;&#65307;&#65288;3&#65289;SHARCS&#21487;&#20197;&#25552;&#20379;2&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#38598;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#32452;&#21512;&#27867;&#21270;&#25968;&#25454;&#23376;&#38598;&#65292;&#23454;&#29616;&#20102;&#22312;Transformer&#20013;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21019;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;CFQ&#21644;COGS&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;10%&#12290;&#21516;&#26102;&#65292;&#35813;&#25216;&#26415;&#23558;&#25968;&#25454;&#38598;&#21046;&#22270;&#20316;&#20026;&#35838;&#31243;&#23398;&#20064;&#20934;&#21017;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38656;&#27714;&#65292;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12118</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#38598;&#21046;&#22270;&#22312;Transformer&#20013;&#25913;&#21892;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers. (arXiv:2310.12118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#38598;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#32452;&#21512;&#27867;&#21270;&#25968;&#25454;&#23376;&#38598;&#65292;&#23454;&#29616;&#20102;&#22312;Transformer&#20013;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21019;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;CFQ&#21644;COGS&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;10%&#12290;&#21516;&#26102;&#65292;&#35813;&#25216;&#26415;&#23558;&#25968;&#25454;&#38598;&#21046;&#22270;&#20316;&#20026;&#35838;&#31243;&#23398;&#20064;&#20934;&#21017;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38656;&#27714;&#65292;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#33021;&#21542;&#36798;&#21040;&#19982;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#30456;&#23218;&#32654;&#30340;&#31243;&#24230;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20110;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#26367;&#20195;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20294;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#21046;&#22270;&#30340;&#21147;&#37327;&#65288;Swayamdipta&#31561;&#65292;2020&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#31574;&#30053;&#24615;&#22320;&#35782;&#21035;&#19968;&#37096;&#20998;&#32452;&#21512;&#27867;&#21270;&#25968;&#25454;&#23376;&#38598;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;CFQ&#21644;COGS&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;10%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23558;&#25968;&#25454;&#38598;&#21046;&#22270;&#20316;&#20026;&#35838;&#31243;&#23398;&#20064;&#20934;&#21017;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38656;&#35201;&#65292;&#21516;&#26102;&#22987;&#32456;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#25968;&#25454;&#38598;&#21046;&#22270;&#22312;&#21457;&#25381;&#32452;&#21512;&#27867;&#21270;&#30340;&#20840;&#37096;&#28508;&#21147;&#26041;&#38754;&#30340;&#26410;&#24320;&#21457;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have revolutionized language modeling and excelled in various downstream tasks. However, the extent to which these models achieve compositional generalization comparable to human cognitive abilities remains a topic of debate. While existing approaches in the field have mainly focused on novel architectures and alternative learning paradigms, we introduce a pioneering method harnessing the power of dataset cartography (Swayamdipta et al., 2020). By strategically identifying a subset of compositional generalization data using this approach, we achieve a remarkable improvement in model accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets. Notably, our technique incorporates dataset cartography as a curriculum learning criterion, eliminating the need for hyperparameter tuning while consistently achieving superior performance. Our findings highlight the untapped potential of dataset cartography in unleashing the full capabilities of compositional generalizat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65288;AdaLink&#65289;&#65292;&#36890;&#36807;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#32780;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.12100</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#33258;&#36866;&#24212;&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#36755;&#20837;&#20013;&#24515;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65288;AdaLink&#65289;&#65292;&#36890;&#36807;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#32780;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#21442;&#25968;&#25968;&#37327;&#20174;O&#65288;10^9&#65289;&#25193;&#23637;&#21040;O&#65288;10^{12}&#65289;&#29978;&#33267;&#26356;&#39640;&#27700;&#24179;&#65292;&#23637;&#29616;&#20986;&#22312;&#24191;&#27867;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#26679;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#20351;&#24471;&#22312;&#32473;&#23450;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23436;&#20840;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#20026;&#24212;&#23545;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#36866;&#24212;&#21644;&#26381;&#21153;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;PEFT&#25216;&#26415;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20405;&#20837;&#24335;&#21644;&#38750;&#20405;&#20837;&#24335;&#12290;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#30452;&#25509;&#25913;&#21464;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#34429;&#28982;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#26381;&#21153;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#12290;&#38750;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#65292;&#22914;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;AdaLink&#25551;&#36848;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#65292;&#19982;SoTA&#20405;&#20837;&#24335;PEFT&#65288;LoRA&#65289;&#21644;&#23436;&#25972;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#34394;&#25311;IMU&#25968;&#25454;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#29983;&#25104;&#26631;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#29305;&#23450;&#20110;HAR&#30340;&#22522;&#30784;&#27169;&#22411;&#12289;&#25506;&#32034;HAR&#20013;&#30340;&#20998;&#23618;&#32467;&#26500;&#12289;&#20998;&#35299;&#22797;&#26434;&#27963;&#21160;&#20197;&#21450;&#22312;&#20581;&#24247;&#24863;&#30693;&#21644;&#27963;&#21160;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#31561;&#20960;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#28508;&#22312;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.12085</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
On the Benefit of Generative Foundation Models for Human Activity Recognition. (arXiv:2310.12085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#34394;&#25311;IMU&#25968;&#25454;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#29983;&#25104;&#26631;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#29305;&#23450;&#20110;HAR&#30340;&#22522;&#30784;&#27169;&#22411;&#12289;&#25506;&#32034;HAR&#20013;&#30340;&#20998;&#23618;&#32467;&#26500;&#12289;&#20998;&#35299;&#22797;&#26434;&#27963;&#21160;&#20197;&#21450;&#22312;&#20581;&#24247;&#24863;&#30693;&#21644;&#27963;&#21160;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#31561;&#20960;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#28508;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#21040;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30456;&#20449;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34394;&#25311;IMU&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37325;&#28857;&#20171;&#32461;&#20102;&#20960;&#20010;&#21487;&#33021;&#21463;&#30410;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#36335;&#24452;&#65292;&#21253;&#25324;&#29983;&#25104;&#26631;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#29305;&#23450;&#20110;HAR&#30340;&#22522;&#30784;&#27169;&#22411;&#12289;&#25506;&#32034;HAR&#20013;&#30340;&#20998;&#23618;&#32467;&#26500;&#12289;&#20998;&#35299;&#22797;&#26434;&#27963;&#21160;&#20197;&#21450;&#22312;&#20581;&#24247;&#24863;&#30693;&#21644;&#27963;&#21160;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human activity recognition (HAR), the limited availability of annotated data presents a significant challenge. Drawing inspiration from the latest advancements in generative AI, including Large Language Models (LLMs) and motion synthesis models, we believe that generative AI can address this data scarcity by autonomously generating virtual IMU data from text descriptions. Beyond this, we spotlight several promising research pathways that could benefit from generative AI for the community, including the generating benchmark datasets, the development of foundational models specific to HAR, the exploration of hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IncidentAI&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23433;&#20840;&#39044;&#38450;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#30001;&#39640;&#21387;&#27668;&#20307;&#20445;&#25252;&#31649;&#29702;&#39046;&#22495;&#30340;&#19987;&#23478;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20107;&#25925;&#25253;&#21578;&#20197;&#39044;&#38450;&#26410;&#26469;&#30340;&#25925;&#38556;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;NLP&#21644;&#20107;&#25925;&#31649;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.12074</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#23433;&#20840;&#30340;&#25805;&#20316;&#65306;&#39044;&#38450;&#26410;&#26469;&#25925;&#38556;&#30340;&#39640;&#21387;&#27668;&#20307;&#20107;&#25925;&#19987;&#23478;&#21442;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IncidentAI&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23433;&#20840;&#39044;&#38450;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#30001;&#39640;&#21387;&#27668;&#20307;&#20445;&#25252;&#31649;&#29702;&#39046;&#22495;&#30340;&#19987;&#23478;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20107;&#25925;&#25253;&#21578;&#20197;&#39044;&#38450;&#26410;&#26469;&#30340;&#25925;&#38556;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;NLP&#21644;&#20107;&#25925;&#31649;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23433;&#20840;&#39044;&#38450;&#30340;&#26032;&#30340;IncidentAI&#25968;&#25454;&#38598;&#12290;&#19982;&#36890;&#24120;&#21482;&#21253;&#21547;&#19968;&#20010;&#20219;&#21153;&#30340;&#20808;&#21069;&#35821;&#26009;&#24211;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;&#19977;&#20010;&#20219;&#21153;&#65306;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#21644;&#20449;&#24687;&#26816;&#32034;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#33267;&#23569;&#26377;&#20845;&#24180;&#39640;&#21387;&#27668;&#20307;&#20445;&#25252;&#31649;&#29702;&#23454;&#36341;&#32463;&#39564;&#30340;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25968;&#25454;&#38598;&#22312;&#23433;&#20840;&#39044;&#38450;&#22330;&#26223;&#20013;&#30340;&#36129;&#29486;&#12290;&#23545;&#19977;&#20010;&#20219;&#21153;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20107;&#25925;&#25253;&#21578;&#20197;&#36991;&#20813;&#26410;&#26469;&#30340;&#25925;&#38556;&#12290;&#35813;&#25968;&#25454;&#38598;&#20415;&#20110;NLP&#21644;&#20107;&#25925;&#31649;&#29702;&#31038;&#21306;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#20063;&#21487;&#25552;&#20379;&#35775;&#38382;&#65288;IncidentAI&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/Cinnamon/incident-ai-dataset &#33719;&#21462;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (the IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset).
&lt;/p&gt;</description></item><item><title>SPEED&#36890;&#36807;&#25512;&#27979;&#25191;&#34892;&#22810;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#21152;&#24555;Transformer&#35299;&#30721;&#22120;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12072</link><description>&lt;p&gt;
SPEED: &#29992;&#20110;&#39640;&#25928;&#35299;&#30721;&#30340;&#25512;&#27979;&#27969;&#27700;&#32447;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12072
&lt;/p&gt;
&lt;p&gt;
SPEED&#36890;&#36807;&#25512;&#27979;&#25191;&#34892;&#22810;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#21152;&#24555;Transformer&#35299;&#30721;&#22120;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36817;&#26469;&#24050;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#23548;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#24310;&#36831;&#26174;&#33879;&#65292;&#23427;&#20204;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#24456;&#22823;&#38480;&#21046;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#29983;&#25104;&#22411;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#29305;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#26631;&#35760;&#20381;&#36182;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#36755;&#20986;&#26631;&#35760;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20219;&#20309;&#26631;&#35760;&#32423;&#30340;&#24182;&#34892;&#24615;&#65292;&#20351;&#24471;&#25512;&#29702;&#36807;&#31243;&#26497;&#24230;&#21463;&#20869;&#23384;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPEED&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26089;&#26399;&#38544;&#34255;&#29366;&#24577;&#30340;&#39044;&#27979;&#20540;&#26469;&#24182;&#34892;&#22320;&#25512;&#27979;&#25191;&#34892;&#24403;&#21069;&#26631;&#35760;&#19982;&#22810;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23545;&#20110;&#37319;&#29992;&#21442;&#25968;&#20849;&#20139;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#23558;&#24182;&#34892;&#25191;&#34892;&#30340;&#26631;&#35760;&#30340;&#20869;&#23384;&#25805;&#20316;&#20998;&#25674;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#38395;&#25991;&#31456;&#20013;&#20849;&#25351;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#25509;&#36817;&#36523;&#20221;&#21644;&#26725;&#25509;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20849;&#25351;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#24314;&#19981;&#21516;&#20132;&#21449;&#25991;&#26723;&#20849;&#25351;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#29992;&#20110;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.12064</link><description>&lt;p&gt;
&#26032;&#38395;&#25991;&#31456;&#20013;&#19981;&#21516;&#20132;&#21449;&#25991;&#26723;&#23454;&#20307;&#20849;&#25351;&#30340;&#32534;&#30721;&#25163;&#20876;
&lt;/p&gt;
&lt;p&gt;
Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles. (arXiv:2310.12064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#38395;&#25991;&#31456;&#20013;&#20849;&#25351;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#25509;&#36817;&#36523;&#20221;&#21644;&#26725;&#25509;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20849;&#25351;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#24314;&#19981;&#21516;&#20132;&#21449;&#25991;&#26723;&#20849;&#25351;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#29992;&#20110;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#38395;&#25991;&#31456;&#20013;&#20849;&#25351;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#19981;&#20165;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#36523;&#20221;&#20851;&#31995;&#65292;&#36824;&#32771;&#34385;&#20102;&#25509;&#36817;&#36523;&#20221;&#21644;&#26725;&#25509;&#20851;&#31995;&#12290;&#23427;&#35814;&#32454;&#25551;&#36848;&#20102;&#22914;&#20309;&#35774;&#32622;Inception&#27880;&#37322;&#24037;&#20855;&#65292;&#22914;&#20309;&#27880;&#37322;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#23454;&#20307;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#19982;&#21508;&#31181;&#20849;&#25351;&#20851;&#31995;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;Wikidata&#30340;&#20840;&#29699;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#36215;&#26469;&#12290;&#36825;&#31181;&#22810;&#23618;&#27425;&#30340;&#27880;&#37322;&#26041;&#27861;&#26159;&#22312;&#23186;&#20307;&#20559;&#35265;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#30340;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#33021;&#22815;&#24212;&#29992;&#20110;&#36890;&#36807;&#35789;&#36873;&#25321;&#21644;&#26631;&#31614;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#30340;&#19981;&#21516;&#20132;&#21449;&#25991;&#26723;&#20849;&#25351;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a scheme for annotating coreference across news articles, extending beyond traditional identity relations by also considering near-identity and bridging relations. It includes a precise description of how to set up Inception, a respective annotation tool, how to annotate entities in news articles, connect them with diverse coreferential relations, and link them across documents to Wikidata's global knowledge graph. This multi-layered annotation approach is discussed in the context of the problem of media bias. Our main contribution lies in providing a methodology for creating a diverse cross-document coreference corpus which can be applied to the analysis of media bias by word-choice and labelling.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12059</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#24615;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25191;&#34892;&#22810;&#39033;&#36873;&#25321;&#31526;&#21495;&#32465;&#23450;&#65288;MCSB&#65289;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#36234;&#21335;&#35821;&#19978;&#65292;&#22240;&#20026;&#36234;&#21335;&#35821;&#20013;&#30340;&#25361;&#25112;&#24615;MCQA&#25968;&#25454;&#38598;&#36739;&#33521;&#35821;&#23569;&#12290;&#29616;&#26377;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;ViMMRC 1.0&#21644;ViMMRC 2.0&#65292;&#19987;&#27880;&#20110;&#25991;&#23398;&#38382;&#39064;&#12290;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;ChatGPT&#22312;2019&#24180;&#33267;2023&#24180;&#30340;&#36234;&#21335;&#22269;&#23478;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#65288;VNHSGE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;ChatGPT&#22914;&#20309;&#36880;&#27493;&#35299;&#20915;VNHSGE&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20026;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#30340;LaTeX&#20844;&#24335;&#36755;&#20837;&#25552;&#20379;&#32467;&#26500;&#21270;&#25351;&#21335;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;MCSB&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#35201;&#27714;&#20351;&#29992;&#20005;&#26684;&#30340;LaTeX&#26679;&#24335;&#36827;&#34892;&#36755;&#20837;&#12290;&#25105;&#20204;&#37325;&#28857;&#39044;&#27979;&#23383;&#31526;&#65288;A&#12289;B&#12289;C&#25110;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#65292;&#36890;&#36807;&#27010;&#24565;&#23548;&#21521;&#24605;&#32500;&#38142;&#22270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;Bradley-Terry&#27169;&#22411;&#26469;&#20272;&#35745;&#35780;&#20998;&#23610;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;Twitter&#19978;&#23545;&#24773;&#24863;&#35328;&#35770;&#30340;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.12049</link><description>&lt;p&gt;
Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models (&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23548;&#21521;&#24605;&#32500;&#38142;&#22270;&#25552;&#31034;&#36827;&#34892;&#25991;&#26412;&#37197;&#23545;&#27604;&#36739;&#32553;&#25918;)
&lt;/p&gt;
&lt;p&gt;
Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models. (arXiv:2310.12049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#65292;&#36890;&#36807;&#27010;&#24565;&#23548;&#21521;&#24605;&#32500;&#38142;&#22270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;Bradley-Terry&#27169;&#22411;&#26469;&#20272;&#35745;&#35780;&#20998;&#23610;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;Twitter&#19978;&#23545;&#24773;&#24863;&#35328;&#35770;&#30340;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;&#32463;&#24120;&#38656;&#35201;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#38590;&#20197;&#22788;&#29702;&#30701;&#25991;&#26412;&#65292;&#25110;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#26469;&#36827;&#34892;&#25991;&#26412;&#32553;&#25918;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#23548;&#21521;&#24605;&#32500;&#38142;&#22270;&#65288;CGCoT&#65289;&#65292;&#23427;&#20351;&#29992;&#35774;&#35745;&#29992;&#20110;&#24635;&#32467;&#24819;&#27861;&#24182;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#30446;&#26631;&#26041;&#30340;&#25552;&#31034;&#26469;&#29983;&#25104;&#27010;&#24565;&#29305;&#23450;&#30340;&#32454;&#20998;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#32534;&#30721;&#22120;&#20869;&#23481;&#20998;&#26512;&#30340;&#25351;&#23548;&#12290;CGCoT&#23558;&#37197;&#23545;&#25991;&#26412;&#27604;&#36739;&#20174;&#19968;&#20010;&#25512;&#29702;&#38382;&#39064;&#36716;&#21464;&#20026;&#19968;&#20010;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#23545;&#27010;&#24565;&#29305;&#23450;&#30340;&#32454;&#20998;&#36827;&#34892;&#37197;&#23545;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#37197;&#23545;&#27604;&#36739;&#30340;&#32467;&#26524;&#20351;&#29992;Bradley-Terry&#27169;&#22411;&#26469;&#20272;&#35745;&#19968;&#20010;&#35780;&#20998;&#23610;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;Twitter&#19978;&#30340;&#24773;&#24863;&#35328;&#35770;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#27979;&#37327;&#20540;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#27604;Wordfish&#31561;&#26367;&#20195;&#26041;&#27861;&#26356;&#24378;&#12290;&#38500;&#20102;&#19968;&#23567;&#32452;&#29992;&#20110;&#24320;&#21457;CGCoT&#25552;&#31034;&#30340;&#35797;&#39564;&#25968;&#25454;&#20043;&#22806;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, 
&lt;/p&gt;</description></item><item><title>CORE&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#20844;&#21496;&#20851;&#31995;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;4708&#20010;&#23454;&#20363;&#21644;12&#31181;&#20851;&#31995;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;RC&#27169;&#22411;&#22312;&#36866;&#24212;CORE&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#22312;CORE&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12024</link><description>&lt;p&gt;
CORE: &#19968;&#31181;&#29992;&#20110;&#24378;&#20581;&#39046;&#22495;&#36866;&#24212;&#30340;&#23569;&#26679;&#26412;&#20844;&#21496;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation. (arXiv:2310.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12024
&lt;/p&gt;
&lt;p&gt;
CORE&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#20844;&#21496;&#20851;&#31995;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;4708&#20010;&#23454;&#20363;&#21644;12&#31181;&#20851;&#31995;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;RC&#27169;&#22411;&#22312;&#36866;&#24212;CORE&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#22312;CORE&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CORE&#65292;&#19968;&#20010;&#38024;&#23545;&#20844;&#21496;&#20851;&#31995;&#21644;&#21830;&#19994;&#23454;&#20307;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#65288;RC&#65289;&#25968;&#25454;&#38598;&#12290;CORE&#21253;&#25324;4708&#20010;&#23454;&#20363;&#21644;12&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#20844;&#21496;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#25552;&#21462;&#30340;&#25991;&#26412;&#35777;&#25454;&#12290;&#30001;&#20110;&#19982;&#20844;&#21496;&#21517;&#31216;&#21644;&#21830;&#19994;&#23454;&#20307;&#30456;&#20851;&#30340;&#20016;&#23500;&#22810;&#26679;&#30340;&#20449;&#24687;&#65292;&#23569;&#26679;&#26412;RC&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#26681;&#25454;&#19978;&#19979;&#25991;&#65292;&#20844;&#21496;&#21517;&#31216;&#21487;&#33021;&#34920;&#31034;&#27861;&#24459;&#23454;&#20307;&#12289;&#20135;&#21697;&#12289;&#20154;&#21592;&#25110;&#19994;&#21153;&#37096;&#38376;&#12290;&#22240;&#27492;&#65292;&#25512;&#23548;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#26412;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;RC&#27169;&#22411;&#22312;CORE&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#35777;&#23454;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36866;&#24212;CORE&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;CORE&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#20854;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them. For example, a company name may represent the legal entity, products, people, or business divisions depending on the context. Therefore, deriving the relation type between entities is highly dependent on textual context. To evaluate the performance of state-of-the-art RC models on the CORE dataset, we conduct experiments in the few-shot domain adaptation setting. Our results reveal substantial performance gaps, confirming that models trained on different domains struggle to adapt to CORE. Interestingly, we find that models trained on CORE showcase improved out-of-domain performance, which highlights the i
&lt;/p&gt;</description></item><item><title>LoHoRavens&#26159;&#19968;&#20010;&#38024;&#23545;&#26426;&#22120;&#20154;&#26700;&#38754;&#25805;&#20316;&#30340;&#38271;&#26102;&#31243;&#35821;&#35328;&#26465;&#20214;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#39068;&#33394;&#12289;&#22823;&#23567;&#12289;&#31354;&#38388;&#12289;&#31639;&#26415;&#21644;&#24341;&#29992;&#31561;&#21508;&#31181;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#22914;&#20309;&#23558;&#35266;&#27979;&#21453;&#39304;&#32435;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38381;&#29615;&#35268;&#21010;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12020</link><description>&lt;p&gt;
LoHoRavens: &#19968;&#39033;&#38024;&#23545;&#26426;&#22120;&#20154;&#26700;&#38754;&#25805;&#20316;&#30340;&#38271;&#26102;&#31243;&#35821;&#35328;&#26465;&#20214;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12020
&lt;/p&gt;
&lt;p&gt;
LoHoRavens&#26159;&#19968;&#20010;&#38024;&#23545;&#26426;&#22120;&#20154;&#26700;&#38754;&#25805;&#20316;&#30340;&#38271;&#26102;&#31243;&#35821;&#35328;&#26465;&#20214;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#39068;&#33394;&#12289;&#22823;&#23567;&#12289;&#31354;&#38388;&#12289;&#31639;&#26415;&#21644;&#24341;&#29992;&#31561;&#21508;&#31181;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#22914;&#20309;&#23558;&#35266;&#27979;&#21453;&#39304;&#32435;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38381;&#29615;&#35268;&#21010;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#39564;&#24335;&#20195;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34701;&#21512;&#20026;&#20307;&#39564;&#24335;&#25351;&#23548;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#27880;&#37322;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#26465;&#20214;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#25512;&#29702;&#38271;&#26102;&#31243;&#33021;&#21147;&#30340;&#20844;&#20849;&#22522;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#26700;&#38754;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;LoHoRavens&#8221;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#39068;&#33394;&#12289;&#22823;&#23567;&#12289;&#31354;&#38388;&#12289;&#31639;&#26415;&#21644;&#24341;&#29992;&#31561;&#21508;&#31181;&#38271;&#26102;&#31243;&#25512;&#29702;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38271;&#26102;&#31243;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#30340;&#27169;&#24577;&#36807;&#28193;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#23558;&#35266;&#27979;&#21453;&#39304;&#32435;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38381;&#29615;&#35268;&#21010;&#20013;&#65292;&#28982;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#23545;&#27492;&#36827;&#34892;&#30340;&#25506;&#32034;&#36739;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#35299;&#20915;&#27169;&#24577;&#36807;&#28193;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#26631;&#39064;&#29983;&#25104;&#21644;&#24555;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption ge
&lt;/p&gt;</description></item><item><title>Gold&#26159;&#19968;&#31181;&#20840;&#29699;&#21644;&#23616;&#37096;&#24847;&#35782;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#22122;&#22768;&#26816;&#27979;&#21435;&#22122;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#23454;&#20307;&#35821;&#20041;&#20449;&#24687;&#12289;&#20840;&#23616;&#35268;&#21017;&#21644;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#38477;&#22122;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12011</link><description>&lt;p&gt;
Gold: &#20840;&#29699;&#21644;&#23616;&#37096;&#24847;&#35782;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#22122;&#22768;&#26816;&#27979;&#21435;&#22122;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection. (arXiv:2310.12011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12011
&lt;/p&gt;
&lt;p&gt;
Gold&#26159;&#19968;&#31181;&#20840;&#29699;&#21644;&#23616;&#37096;&#24847;&#35782;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#22122;&#22768;&#26816;&#27979;&#21435;&#22122;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#23454;&#20307;&#35821;&#20041;&#20449;&#24687;&#12289;&#20840;&#23616;&#35268;&#21017;&#21644;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#38477;&#22122;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#30693;&#35782;&#22270;&#65288;CSKG&#65289;&#23545;&#20110;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#26500;&#24314;&#23427;&#20204;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#33258;&#21160;&#26041;&#27861;&#26469;&#26500;&#24314;&#20855;&#26377;&#26356;&#22823;&#35821;&#20041;&#35206;&#30422;&#33539;&#22260;&#30340;CSKG&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#26041;&#27861;&#24341;&#20837;&#20102;&#20551;&#22122;&#22768;&#65292;&#21487;&#33021;&#38477;&#20302;&#20102;&#29983;&#25104;&#30340;CSKG&#30340;&#36136;&#37327;&#65292;&#32780;&#29616;&#26377;&#30340;&#21435;&#22122;&#31639;&#27861;&#30001;&#20110;CSKG&#20013;&#33410;&#28857;&#21644;&#32467;&#26500;&#30340;&#29420;&#29305;&#29305;&#24449;&#32780;&#38590;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Gold&#65288;&#20840;&#29699;&#21644;&#23616;&#37096;&#24847;&#35782;&#21435;&#22122;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;CSKG&#30340;&#21435;&#22122;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23454;&#20307;&#35821;&#20041;&#20449;&#24687;&#12289;&#20840;&#23616;&#35268;&#21017;&#21644;&#26469;&#33258;CSKG&#30340;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#22122;&#22768;CSKG&#22522;&#20934;&#19978;&#65292;Gold&#22312;&#22122;&#22768;&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21435;&#22122;&#29616;&#23454;&#19990;&#30028;CSKG&#30340;&#26377;&#25928;&#24615;&#65292;&#29978;&#33267;&#23545;&#19979;&#28216;&#30340;&#38646;&#26679;&#26412;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To address this issue, we propose Gold (Global and Local-aware Denoising), a denoising framework for CSKGs that incorporates entity semantic information, global rules, and local structural information from the CSKG. Experiment results demonstrate that Gold outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks. Furthermore, we show that denoising a real-world CSKG is effective and even benefits the downstream zero-shot commonsense question-answering task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#31867;&#22411;&#32858;&#31867;&#25552;&#20379;&#30340;&#31895;&#31890;&#24230;&#30693;&#35782;&#32534;&#30721;&#21040;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12008</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. (arXiv:2310.12008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#31867;&#22411;&#32858;&#31867;&#25552;&#20379;&#30340;&#31895;&#31890;&#24230;&#30693;&#35782;&#32534;&#30721;&#21040;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;(KGET)&#26088;&#22312;&#25512;&#26029;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#33021;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;KGET&#26041;&#27861;&#20391;&#37325;&#20110;&#22914;&#20309;&#26356;&#22909;&#22320;&#23558;&#23454;&#20307;&#30340;&#37051;&#23621;&#21644;&#31867;&#22411;&#25552;&#20379;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;&#20854;&#34920;&#31034;&#20013;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#31867;&#22411;&#22914;&#20309;&#20197;&#32858;&#31867;&#26041;&#24335;&#25552;&#20379;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;(MCLET)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#23558;&#32858;&#31867;&#25552;&#20379;&#30340;&#31895;&#31890;&#24230;&#30693;&#35782;&#32534;&#30721;&#21040;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#20013;&#12290;MCLET&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i) &#22810;&#35270;&#22270;&#29983;&#25104;&#21644;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#31867;&#22411;&#12289;&#23454;&#20307;&#32858;&#31867;&#21644;&#32858;&#31867;&#31867;&#22411;&#35270;&#22270;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65307;ii) &#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#65292;&#40723;&#21169;&#19981;&#21516;&#35270;&#22270;&#20849;&#21516;&#25913;&#36827;&#23454;&#20307;&#21644;&#31867;&#22411;&#30340;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#65307;iii) &#23454;&#20307;&#31867;&#22411;&#21028;&#26029;&#27169;&#22359;&#65292;&#38598;&#25104;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge provided by clusters into entity and type embeddings. MCLET is composed of three modules: i) Multi-view Generation and Encoder module, which encodes structured information from entity-type, entity-cluster and cluster-type views; ii) Cross-view Contrastive Learning module, which encourages different views to collaboratively improve view-specific representations of entities and types; iii) Entity Typing Prediction module, which integrates multi-head attention and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11986</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#25216;&#26415;&#23433;&#20840;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#20250;&#20135;&#29983;&#19968;&#31995;&#21015;&#39118;&#38505;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#32467;&#26500;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#33021;&#21147;&#35780;&#20272;&#65292;&#36825;&#26159;&#30446;&#21069;&#20027;&#35201;&#30340;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#22312;&#31995;&#32479;&#23433;&#20840;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#35748;&#35782;&#21040;&#19978;&#19979;&#25991;&#20915;&#23450;&#20102;&#29305;&#23450;&#33021;&#21147;&#26159;&#21542;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#32771;&#34385;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22686;&#21152;&#20102;&#20154;&#26426;&#20114;&#21160;&#21644;&#31995;&#32479;&#24433;&#21709;&#20316;&#20026;&#39069;&#22806;&#30340;&#35780;&#20272;&#23618;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#23433;&#20840;&#35780;&#20272;&#30340;&#29616;&#29366;&#65292;&#24182;&#21019;&#24314;&#20102;&#29616;&#26377;&#35780;&#20272;&#30340;&#24211;&#12290;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#21069;&#36827;&#26041;&#24335;&#65292;&#27010;&#36848;&#20102;&#23454;&#38469;&#27493;&#39588;&#21644;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11984</link><description>&lt;p&gt;
&#20174;&#25554;&#20540;&#21040;&#22806;&#25512;&#65306;&#31639;&#26415;Transformer&#30340;&#23436;&#25972;&#38271;&#24230;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25552;&#20986;&#20197;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#31639;&#27861;&#20219;&#21153;&#20013;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#26041;&#38754;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#27880;&#24847;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#26368;&#20339;&#38271;&#24230;&#27867;&#21270;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#30446;&#26631;&#25351;&#21521;&#20559;&#32622;&#26469;&#27867;&#21270;&#21040;&#38271;&#38271;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Attention Bias Calibration&#65288;ABC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26657;&#20934;&#38454;&#27573;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#20559;&#32622;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#26426;&#21046;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;ABC&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#20123;&#31639;&#26415;&#20219;&#21153;&#19978;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#23436;&#32654;&#38271;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
&lt;/p&gt;</description></item><item><title>InfoDiffusion&#26159;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;keyinfo-first&#8221;&#29983;&#25104;&#31574;&#30053;&#21644;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#37327;&#30340;&#22122;&#22768;&#35843;&#24230;&#65292;&#20197;&#21450;&#32467;&#21512;&#33258;&#25105;&#26465;&#20214;&#21644;&#37096;&#20998;&#21152;&#22122;&#27169;&#22411;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11976</link><description>&lt;p&gt;
InfoDiffusion: &#38024;&#23545;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#30340;&#20449;&#24687;&#29109;&#24863;&#30693;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11976
&lt;/p&gt;
&lt;p&gt;
InfoDiffusion&#26159;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;keyinfo-first&#8221;&#29983;&#25104;&#31574;&#30053;&#21644;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#37327;&#30340;&#22122;&#22768;&#35843;&#24230;&#65292;&#20197;&#21450;&#32467;&#21512;&#33258;&#25105;&#26465;&#20214;&#21644;&#37096;&#20998;&#21152;&#22122;&#27169;&#22411;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25193;&#25955;&#27169;&#22411;&#30340;&#8220;easy-first&#8221;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#19982;&#20154;&#31867;&#30340;&#8220;keyword-first&#8221;&#33258;&#28982;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#36825;&#24341;&#36215;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;InfoDiffusion&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#8220;keyinfo-first&#8221;&#29983;&#25104;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#37327;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;&#27492;&#22806;&#65292;InfoDiffusion&#32467;&#21512;&#20102;&#33258;&#25105;&#26465;&#20214;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#37096;&#20998;&#21152;&#22122;&#27169;&#22411;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfoDiffusion&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the "easy-first" text generation process of current diffusion models and the "keyword-first" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a "keyinfo-first" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.
&lt;/p&gt;</description></item><item><title>&#22635;&#34917;&#31354;&#30333;&#65306;&#21033;&#29992;&#22270;&#33258;&#32534;&#30721;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20107;&#20214;&#20849;&#25351;&#28040;&#35299;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#25110;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.11965</link><description>&lt;p&gt;
&#22635;&#34917;&#31354;&#30333;&#65306;&#21033;&#29992;&#22270;&#33258;&#32534;&#30721;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20107;&#20214;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks. (arXiv:2310.11965v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11965
&lt;/p&gt;
&lt;p&gt;
&#22635;&#34917;&#31354;&#30333;&#65306;&#21033;&#29992;&#22270;&#33258;&#32534;&#30721;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20107;&#20214;&#20849;&#25351;&#28040;&#35299;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#25110;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#39046;&#22495;&#24212;&#29992;&#30340;&#20107;&#20214;&#20849;&#25351;&#28040;&#35299;&#65288;ECR&#65289;&#30340;&#26032;&#39062;&#39640;&#25928;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;ECR&#20316;&#20026;&#22270;&#37325;&#24314;&#20219;&#21153;&#26469;&#36827;&#34892;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#28145;&#23618;&#35821;&#20041;&#23884;&#20837;&#19982;&#32467;&#26500;&#21270;&#20849;&#25351;&#38142;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;GAE&#65289;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25972;&#20307;&#24471;&#20998;&#12289;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#25552;&#21450;&#23545;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#33655;&#20848;&#35821;&#20107;&#20214;&#20849;&#25351;&#35821;&#26009;&#24211;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26356;&#38590;&#30340;&#20849;&#25351;&#38142;&#25509;&#20998;&#31867;&#21644;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#27604;&#22522;&#20110;transformer&#30340;&#25552;&#21450;&#23545;&#20849;&#25351;&#31639;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel and efficient method for Event Coreference Resolution (ECR) applied to a lower-resourced language domain. By framing ECR as a graph reconstruction task, we are able to combine deep semantic embeddings with structural coreference chain knowledge to create a parameter-efficient family of Graph Autoencoder models (GAE). Our method significantly outperforms classical mention-pair methods on a large Dutch event coreference corpus in terms of overall score, efficiency and training speed. Additionally, we show that our models are consistently able to classify more difficult coreference links and are far more robust in low-data settings when compared to transformer-based mention-pair coreference algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;AMR&#35299;&#26512;&#27169;&#22411;CHAP&#65292;&#23427;&#21033;&#29992;&#22240;&#26524;&#20998;&#23618;&#27880;&#24847;&#21147;&#21644;&#25351;&#38024;&#26426;&#21046;&#23558;&#32467;&#26500;&#25972;&#21512;&#21040;Transformer&#35299;&#30721;&#22120;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.11964</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#20998;&#23618;&#27880;&#24847;&#21147;&#21644;&#25351;&#38024;&#30340;AMR&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
AMR Parsing with Causal Hierarchical Attention and Pointers. (arXiv:2310.11964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;AMR&#35299;&#26512;&#27169;&#22411;CHAP&#65292;&#23427;&#21033;&#29992;&#22240;&#26524;&#20998;&#23618;&#27880;&#24847;&#21147;&#21644;&#25351;&#38024;&#26426;&#21046;&#23558;&#32467;&#26500;&#25972;&#21512;&#21040;Transformer&#35299;&#30721;&#22120;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32763;&#35793;&#30340;AMR&#35299;&#26512;&#22120;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#24615;&#32780;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#20204;&#23558;&#32447;&#24615;&#21270;&#22270;&#39044;&#27979;&#20026;&#33258;&#30001;&#25991;&#26412;&#65292;&#36991;&#20813;&#20102;&#26174;&#24335;&#30340;&#32467;&#26500;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#24573;&#35270;&#20102;AMR&#22270;&#20013;&#30340;&#32467;&#26500;&#23616;&#37096;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#26631;&#35760;&#26469;&#34920;&#31034;&#20849;&#25351;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AMR&#35299;&#26512;&#30340;&#26032;&#30446;&#26631;&#24418;&#24335;&#21644;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;CHAP&#65292;&#23427;&#37197;&#22791;&#20102;&#22240;&#26524;&#20998;&#23618;&#27880;&#24847;&#21147;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#32467;&#26500;&#33021;&#22815;&#34987;&#25972;&#21512;&#21040;Transformer&#35299;&#30721;&#22120;&#20013;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#32034;&#20102;&#21508;&#31181;&#26367;&#20195;&#24314;&#27169;&#36873;&#39033;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26377;&#22235;&#20010;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.11960</link><description>&lt;p&gt;
&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#30340;&#20998;&#27835;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#27880;&#24847;&#21147;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#38271;&#24230;&#20026;n&#30340;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#23558;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20998;&#20026;O(log n)&#32423;&#30340;&#20998;&#36776;&#29575;&#65292;&#36739;&#36828;&#36317;&#31163;&#30340;&#32452;&#32676;&#36234;&#26469;&#36234;&#22823;&#65292;&#24182;&#23398;&#20064;&#35745;&#31639;&#32452;&#32676;&#25968;&#37327;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#20998;&#23618;&#30340;&#26041;&#24335;&#22312;&#36739;&#20302;&#30340;&#20998;&#36776;&#29575;&#20013;&#32771;&#34385;&#36828;&#31163;&#24444;&#27492;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#24635;&#20307;&#22797;&#26434;&#24230;&#20026;O(n)&#25110;O(n log n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36136;&#30097;&#20102;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#32416;&#27491;LLM&#29983;&#25104;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#31867;&#20284;&#20294;&#26356;&#20026;&#26126;&#30830;&#30340;&#19977;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#22312;&#25552;&#21319;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#19981;&#33021;&#34987;&#35270;&#20026;&#35299;&#20915;LLMs&#22266;&#26377;&#32570;&#28857;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23384;&#22312;&#21152;&#24378;&#27169;&#22411;&#21487;&#20449;&#24615;&#35266;&#24565;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.11958</link><description>&lt;p&gt;
&#29992;&#21242;&#23376;&#33280;&#31354;&#28023;&#27915;&#65306;&#25105;&#20204;&#24212;&#35813;&#32534;&#36753;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11958
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36136;&#30097;&#20102;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#32416;&#27491;LLM&#29983;&#25104;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#31867;&#20284;&#20294;&#26356;&#20026;&#26126;&#30830;&#30340;&#19977;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#22312;&#25552;&#21319;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#19981;&#33021;&#34987;&#35270;&#20026;&#35299;&#20915;LLMs&#22266;&#26377;&#32570;&#28857;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23384;&#22312;&#21152;&#24378;&#27169;&#22411;&#21487;&#20449;&#24615;&#35266;&#24565;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#32416;&#27491;LLM&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#32534;&#36753;&#19982;&#36861;&#27714;&#26356;&#26126;&#30830;&#30446;&#26631;&#30340;&#19977;&#31181;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65306;&#65288;1&#65289;&#22522;&#20110;&#26816;&#32034;&#30340;&#26550;&#26500;&#65292;&#23558;&#20107;&#23454;&#35760;&#24518;&#19982;LLMs&#25152;&#20307;&#29616;&#30340;&#25512;&#29702;&#21644;&#35821;&#35328;&#33021;&#21147;&#35299;&#32806;&#65307;&#65288;2&#65289;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#26088;&#22312;&#38450;&#27490;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#31995;&#32479;&#20559;&#35265;&#65307;&#65288;3&#65289;&#24402;&#23646;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#29983;&#25104;&#32467;&#26524;&#19982;&#24050;&#30830;&#23450;&#30340;&#25991;&#26412;&#26469;&#28304;&#36830;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#33021;&#23558;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#35299;&#20915;LLMs&#22266;&#26377;&#32570;&#28857;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#24182;&#19988;&#34429;&#28982;&#23427;&#22312;&#25913;&#36827;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#36890;&#36807;&#21152;&#24378;&#27169;&#22411;&#21487;&#20449;&#24615;&#30340;&#35266;&#24565;&#32780;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#21628;&#21505;&#22312;LLM&#37096;&#32626;&#36807;&#31243;&#20013;&#35880;&#24910;&#25512;&#24191;&#21644;&#24212;&#29992;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36127;&#36131;&#20219;&#22320;&#38480;&#21046;LLMs&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#20197;&#19981;&#20381;&#36182;....
&lt;/p&gt;
&lt;p&gt;
We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying o
&lt;/p&gt;</description></item><item><title>MusicAgent&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#65292;&#36890;&#36807;&#38598;&#25104;&#38899;&#20048;&#30456;&#20851;&#24037;&#20855;&#21644;&#33258;&#20027;&#24037;&#20316;&#27969;&#31243;&#65292;&#24110;&#21161;&#29992;&#25143;&#33258;&#21160;&#20998;&#26512;&#38656;&#27714;&#24182;&#35843;&#29992;&#21512;&#36866;&#30340;&#24037;&#20855;&#36827;&#34892;&#38899;&#20048;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.11954</link><description>&lt;p&gt;
MusicAgent&#65306;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models. (arXiv:2310.11954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11954
&lt;/p&gt;
&lt;p&gt;
MusicAgent&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#65292;&#36890;&#36807;&#38598;&#25104;&#38899;&#20048;&#30456;&#20851;&#24037;&#20855;&#21644;&#33258;&#20027;&#24037;&#20316;&#27969;&#31243;&#65292;&#24110;&#21161;&#29992;&#25143;&#33258;&#21160;&#20998;&#26512;&#38656;&#27714;&#24182;&#35843;&#29992;&#21512;&#36866;&#30340;&#24037;&#20855;&#36827;&#34892;&#38899;&#20048;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI-&#21152;&#24378;&#30340;&#38899;&#20048;&#22788;&#29702;&#26159;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#20219;&#21153;&#65292;&#20174;&#29983;&#25104;&#20219;&#21153;&#65288;&#20363;&#22914;&#38899;&#33394;&#21512;&#25104;&#65289;&#21040;&#29702;&#35299;&#20219;&#21153;&#65288;&#20363;&#22914;&#38899;&#20048;&#20998;&#31867;&#65289;&#12290;&#23545;&#20110;&#24320;&#21457;&#20154;&#21592;&#21644;&#19994;&#20313;&#29233;&#22909;&#32773;&#26469;&#35828;&#65292;&#24456;&#38590;&#25484;&#25569;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#65292;&#20197;&#28385;&#36275;&#20182;&#20204;&#22312;&#38899;&#20048;&#22788;&#29702;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#38899;&#20048;&#25968;&#25454;&#30340;&#34920;&#31034;&#21644;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#27169;&#22411;&#36866;&#29992;&#24615;&#22312;&#21508;&#20010;&#24179;&#21488;&#19978;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#31995;&#32479;&#26469;&#32452;&#32455;&#21644;&#38598;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#33258;&#21160;&#20998;&#26512;&#20182;&#20204;&#30340;&#38656;&#27714;&#24182;&#35843;&#29992;&#36866;&#24403;&#30340;&#24037;&#20855;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#26469;&#28385;&#36275;&#20182;&#20204;&#30340;&#35201;&#27714;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20219;&#21153;&#33258;&#21160;&#21270;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MusicAgent&#30340;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#20247;&#22810;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#24037;&#20855;&#21644;&#33258;&#20027;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, includi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#22522;&#30784;&#38382;&#39064;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#36755;&#20837;&#28304;&#30340;&#36136;&#37327;&#25928;&#24212;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22522;&#30784;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.11938</link><description>&lt;p&gt;
&#32039;&#23494;&#19982;&#20840;&#38754;&#65306;&#30740;&#31350;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#22522;&#30784;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding. (arXiv:2310.11938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#22522;&#30784;&#38382;&#39064;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#36755;&#20837;&#28304;&#30340;&#36136;&#37327;&#25928;&#24212;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22522;&#30784;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#25991;&#29486;&#23545;&#20110;&#22522;&#30784;&#65288;grounding&#65289;&#34987;&#35748;&#20026;&#26159;&#21457;&#23637;&#26356;&#23436;&#25972;&#21644;&#30495;&#27491;&#35821;&#20041;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23384;&#22312;&#20998;&#27495;&#12290;&#26377;&#20123;&#20154;&#35748;&#20026;&#22522;&#30784;&#20801;&#35768;&#36827;&#34892;&#36136;&#37327;&#19981;&#21516;&#30340;&#27867;&#21270;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#35748;&#20026;&#36890;&#36807;&#21333;&#27169;&#24577;&#25968;&#25454;&#37327;&#21487;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#20851;&#22522;&#30784;&#23545;NLP&#31995;&#32479;&#30340;&#24433;&#21709;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#30740;&#31350;&#22522;&#30784;&#21450;&#20854;&#23545;NLP&#31995;&#32479;&#30340;&#24433;&#21709;&#25152;&#38754;&#20020;&#30340;&#26041;&#27861;&#35770;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#21508;&#31181;&#36755;&#20837;&#28304;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#32431;&#25991;&#26412;&#65289;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20851;&#38190;&#22312;&#20110;&#26500;&#24314;&#22312;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#32676;&#20307;&#30340;&#21487;&#27604;&#36739;&#26679;&#26412;&#65292;&#20197;&#20415;&#25105;&#20204;&#21487;&#20197;&#23558;&#19981;&#21516;&#36755;&#20837;&#28304;&#30340;&#36136;&#37327;&#25928;&#24212;&#19982;&#21487;&#37327;&#21270;&#30340;&#27169;&#22411;&#24615;&#33021;&#21306;&#20998;&#24320;&#26469;&#12290;&#20351;&#29992;&#36825;&#19968;&#26694;&#26550;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36136;&#37327;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounding has been argued to be a crucial component towards the development of more complete and truly semantically competent artificial intelligence systems. Literature has divided into two camps: While some argue that grounding allows for qualitatively different generalizations, others believe it can be compensated by mono-modal data quantity. Limited empirical evidence has emerged for or against either position, which we argue is due to the methodological challenges that come with studying grounding and its effects on NLP systems.  In this paper, we establish a methodological framework for studying what the effects are - if any - of providing models with richer input sources than text-only. The crux of it lies in the construction of comparable samples of populations of models trained on different input modalities, so that we can tease apart the qualitative effects of different input sources from quantifiable model performances. Experiments using this framework reveal qualitative dif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35821;&#20041;&#32467;&#26500;&#25506;&#27979;&#23454;&#39564;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#20041;&#23376;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#23618;&#21160;&#21147;&#23398;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#27169;&#22411;&#22823;&#23567;&#19982;&#32467;&#26524;&#20851;&#32852;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.11923</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#32467;&#26500;&#25506;&#32034;&#30740;&#31350;Transformer&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#20041;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing. (arXiv:2310.11923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11923
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#32467;&#26500;&#25506;&#27979;&#23454;&#39564;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#20041;&#23376;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#23618;&#21160;&#21147;&#23398;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#27169;&#22411;&#22823;&#23567;&#19982;&#32467;&#26524;&#20851;&#32852;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;NLP&#31038;&#21306;&#26469;&#35828;&#65292;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#32423;&#20013;&#32534;&#30721;&#30340;&#35821;&#35328;&#20449;&#24687;&#30340;&#31867;&#22411;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35789;&#32423;&#34920;&#31034;&#21644;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#36974;&#34109;&#26631;&#35760;&#35757;&#32451;&#30446;&#26631;&#12290;&#26412;&#25991;&#36890;&#36807;&#35821;&#20041;&#32467;&#26500;&#25506;&#27979;&#23454;&#39564;&#20171;&#32461;&#20102;&#19968;&#31181;&#30740;&#31350;&#21477;&#23376;&#32423;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25214;&#21040;&#23884;&#20837;&#31354;&#38388;&#30340;&#19968;&#20010;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#36866;&#21512;&#27599;&#19968;&#23545;&#25968;&#25454;&#28857;&#30340;&#29305;&#23450;&#20219;&#21153;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#65288;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#31181;&#31867;&#65288;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65289;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#23478;&#26063;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#23618;&#21160;&#21147;&#23398;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20294;&#32467;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#27169;&#22411;&#22823;&#23567;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what kinds of linguistic information are encoded in different layers of Transformer-based language models is of considerable interest for the NLP community. Existing work, however, has overwhelmingly focused on word-level representations and encoder-only language models with the masked-token training objective. In this paper, we present experiments with semantic structural probing, a method for studying sentence-level representations via finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points. We apply our method to language models from different families (encoder-only, decoder-only, encoder-decoder) and of different sizes in the context of two tasks, semantic textual similarity and natural-language inference. We find that model families differ substantially in their performance and layer dynamics, but that the results are largely model-size invariant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#22522;&#20110;Wikidata5M&#36827;&#34892;&#25193;&#23637;&#12290;&#36890;&#36807;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#20449;&#24687;&#32452;&#21512;&#65292;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.11917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#22522;&#20110;Wikidata5M&#36827;&#34892;&#25193;&#23637;&#12290;&#36890;&#36807;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#20449;&#24687;&#32452;&#21512;&#65292;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#26159;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#26032;&#30340;&#12289;&#20043;&#21069;&#26410;&#35265;&#30340;&#23454;&#20307;&#30340;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#21644;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#35780;&#20272;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#22522;&#20110;&#24182;&#25193;&#23637;&#20102;Wikidata5M&#65306;&#23427;&#25552;&#20379;&#20102;&#36716;&#23548;&#24335;&#12289;k-shot&#21644;0-shot&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#20250;&#26681;&#25454;&#21487;&#29992;&#30340;&#20449;&#24687;&#24773;&#20917;&#20174;&#65288;i&#65289;&#20165;&#26377;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#65292;&#21040;&#65288;ii&#65289;&#21253;&#21547;&#25991;&#26412;&#25552;&#21450;&#65292;&#20877;&#21040;&#65288;iii&#65289;&#23454;&#20307;&#30340;&#35814;&#32454;&#25551;&#36848;&#36827;&#34892;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26368;&#36817;&#26041;&#27861;&#30340;&#23567;&#22411;&#30740;&#31350;&#65292;&#21457;&#29616;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;&#36828;&#36828;&#20302;&#20110;&#36716;&#23548;&#24335;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#38271;&#23614;&#23454;&#20307;&#30340;&#19981;&#36275;&#12290;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#25972;&#21512;&#21040;&#38142;&#25509;&#39044;&#27979;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22235;&#20010;&#33521;&#25991;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#29702;&#24615;&#27880;&#37322;&#65292;&#39564;&#35777;&#20102;&#23545;&#27604;&#24615;&#35299;&#37322;&#19982;&#38750;&#23545;&#27604;&#24615;&#35299;&#37322;&#22312;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11906</link><description>&lt;p&gt;
&#23425;&#24895;&#26159;&#25252;&#22763;&#20063;&#19981;&#24895;&#26159;&#21307;&#29983; -- &#23545;&#27604;&#24615;&#35299;&#37322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rather a Nurse than a Physician -- Contrastive Explanations under Investigation. (arXiv:2310.11906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22235;&#20010;&#33521;&#25991;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#29702;&#24615;&#27880;&#37322;&#65292;&#39564;&#35777;&#20102;&#23545;&#27604;&#24615;&#35299;&#37322;&#19982;&#38750;&#23545;&#27604;&#24615;&#35299;&#37322;&#22312;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#24615;&#35299;&#37322;&#26159;&#23558;&#19968;&#20010;&#20915;&#31574;&#19982;&#21478;&#19968;&#20010;&#36827;&#34892;&#23545;&#27604;&#35299;&#37322;&#65292;&#23427;&#27604;&#38750;&#23545;&#27604;&#24615;&#35299;&#37322;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35299;&#37322;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;&#36825;&#19968;&#35266;&#28857;&#23578;&#26410;&#32463;&#36807;&#23454;&#35777;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#33521;&#25991;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65288;SST2&#12289;DynaSent&#12289;BIOS&#21644;DBpedia-Animals&#65289;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#21644;&#25552;&#21462;&#26469;&#33258;&#19977;&#31181;&#19981;&#21516;&#27169;&#22411;&#65288;RoBERTa&#12289;GPT-2&#21644;T5&#65289;&#30340;&#35299;&#37322;&#65292;&#24182;&#24212;&#29992;&#19977;&#31181;&#21518;&#26399;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65288;LRP&#12289;GradientxInput&#21644;GradNorm&#65289;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;BIOS&#25968;&#25454;&#38598;&#20013;&#30340;100&#20010;&#26679;&#26412;&#30340;&#23545;&#27604;&#24615;&#21644;&#38750;&#23545;&#27604;&#24615;&#35774;&#32622;&#36827;&#34892;&#20102;&#20154;&#31867;&#29702;&#24615;&#27880;&#37322;&#30340;&#25910;&#38598;&#21644;&#21457;&#24067;&#12290;&#27169;&#22411;&#22522;&#30784;&#29702;&#24615;&#19982;&#20154;&#31867;&#26631;&#27880;&#20043;&#38388;&#22312;&#23545;&#27604;&#24615;&#21644;&#38750;&#23545;&#27604;&#24615;&#35774;&#32622;&#19979;&#30340;&#20132;&#21449;&#27604;&#36739;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#65292;&#20004;&#20010;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22522;&#30784;&#35299;&#37322;&#35745;&#31639;&#8230;
&lt;/p&gt;
&lt;p&gt;
Contrastive explanations, where one decision is explained in contrast to another, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations compute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.11884</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#28608;&#27963;&#21040;&#27010;&#24565;: &#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#27010;&#24565;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#33258;&#28982;&#26725;&#26753;&#65306;&#19968;&#26086;&#30830;&#23450;&#20102;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#30340;&#27010;&#24565;&#65292;&#23601;&#21487;&#20197;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#25512;&#29702;&#31995;&#32479;&#25972;&#21512;&#65292;&#29992;&#20110;&#25512;&#29702;&#25110;&#20351;&#29992;&#25512;&#29702;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#25110;&#22686;&#24378;&#20197;&#25913;&#21892;&#23398;&#20064;&#31995;&#32479;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#20165;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#23558;&#27010;&#24565;&#30693;&#35782;&#25554;&#20837;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#30001;&#20110;&#25972;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#26159;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#25152;&#20197;&#36890;&#36807;&#36825;&#39033;&#35843;&#26597;&#33719;&#24471;&#30340;&#35265;&#35299;&#21487;&#20197;&#25104;&#20026;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.11878</link><description>&lt;p&gt;
&#20174;&#19981;&#19968;&#33268;&#21040;&#27934;&#23519;&#65306;&#23545;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#30340;&#29702;&#30001;&#25968;&#25454;&#38598;&#26500;&#24314;&#36827;&#34892;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#21487;&#20449;&#36182;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;COC&#30740;&#31350;&#20165;&#38480;&#20110;&#30001;&#21333;&#20010;&#19987;&#23478;&#36827;&#34892;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24459;&#24072;&#22312;&#23545;&#26696;&#20214;&#20107;&#23454;&#36827;&#34892;&#35780;&#20272;&#26102;&#21487;&#33021;&#23384;&#22312;&#20998;&#27495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RAVE&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#29702;&#30001;&#21464;&#24322;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#22269;&#38469;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#20004;&#20301;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#24369;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20182;&#20204;&#30340;&#20998;&#27495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;COC&#29305;&#23450;&#30340;&#23376;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#39318;&#27425;&#20851;&#27880;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#65292;&#36825;&#22312;COC&#20803;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#32454;&#31890;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;SOTA COC&#27169;&#22411;&#22312;RAVE&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38754;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#26102;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26597;&#35810;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#24182;&#19988;&#31532;&#19968;&#20010;&#35299;&#30721;&#30340;&#26631;&#35760;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25351;&#31034;&#31526;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#24182;&#20026;&#25913;&#36827;&#35299;&#30721;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11877</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#24615;&#26080;&#27861;&#22238;&#31572;&#24615;&#30340;&#22909;&#22855;&#26696;&#20363;&#65306;&#22312;&#36807;&#24230;&#33258;&#20449;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#23547;&#25214;&#30495;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38754;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#26102;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26597;&#35810;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#24182;&#19988;&#31532;&#19968;&#20010;&#35299;&#30721;&#30340;&#26631;&#35760;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25351;&#31034;&#31526;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#24182;&#20026;&#25913;&#36827;&#35299;&#30721;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#23545;&#20854;&#22238;&#31572;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#25285;&#24551;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;LLMs&#22914;&#20309;&#22788;&#29702;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#24187;&#35273;&#34892;&#20026;&#65292;&#21407;&#22240;&#26159;&#36807;&#24230;&#33258;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#38754;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#26102;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#38382;&#65306;&#24403;&#29983;&#25104;&#24187;&#35273;&#22238;&#31572;&#26102;&#65292;&#27169;&#22411;&#26159;&#21542;&#34920;&#31034;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#20107;&#23454;&#65311;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#28872;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#36755;&#20837;&#26597;&#35810;&#30340;&#21487;&#22238;&#31572;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#31532;&#19968;&#20010;&#35299;&#30721;&#30340;&#26631;&#35760;&#30340;&#34920;&#31034;&#24448;&#24448;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25351;&#31034;&#31526;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20808;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20026;&#24320;&#21457;&#26356;&#22909;&#22320;&#36981;&#23432;&#20107;&#23454;&#29983;&#25104;&#30340;&#25913;&#36827;&#35299;&#30721;&#25216;&#26415;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22899;&#20070;&#21551;&#21457;&#30340;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;AI Nushu&#65292;&#36890;&#36807;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35270;&#35282;&#65292;&#32467;&#21512;&#20013;&#22269;&#25991;&#21270;&#36951;&#20135;&#21644;&#22899;&#26435;&#20027;&#20041;&#35270;&#35282;&#65292;&#36890;&#36807;&#20004;&#20010;AI&#20195;&#29702;&#20154;&#30340;&#21512;&#20316;&#21019;&#36896;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#20013;&#25991;&#20889;&#20316;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.11870</link><description>&lt;p&gt;
AI Nushu: &#35745;&#31639;&#35821;&#35328;&#23398;&#35270;&#35282;&#19979;&#22992;&#22969;&#22242;&#32467;&#20013;&#30340;&#35821;&#35328;&#24418;&#25104;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics. (arXiv:2310.11870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22899;&#20070;&#21551;&#21457;&#30340;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;AI Nushu&#65292;&#36890;&#36807;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35270;&#35282;&#65292;&#32467;&#21512;&#20013;&#22269;&#25991;&#21270;&#36951;&#20135;&#21644;&#22899;&#26435;&#20027;&#20041;&#35270;&#35282;&#65292;&#36890;&#36807;&#20004;&#20010;AI&#20195;&#29702;&#20154;&#30340;&#21512;&#20316;&#21019;&#36896;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#20013;&#25991;&#20889;&#20316;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;AI Nushu&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#22899;&#20070;&#65288;&#22899;&#24615;&#19987;&#29992;&#25991;&#23383;&#65289;&#21551;&#21457;&#30340;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;&#65292;&#22899;&#20070;&#26159;&#21476;&#20195;&#20013;&#22269;&#22899;&#24615;&#22312;&#19968;&#20010;&#29238;&#26435;&#31038;&#20250;&#20013;&#34987;&#35748;&#20026;&#26159;&#25991;&#30450;&#32780;&#21019;&#36896;&#24182;&#29420;&#33258;&#20351;&#29992;&#30340;&#29420;&#29305;&#35821;&#35328;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#24335;&#35013;&#32622;&#20013;&#65292;&#20004;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#20154;&#36890;&#36807;&#23545;&#20013;&#25991;&#23383;&#20856;&#21644;&#22899;&#20070;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#65292;&#19981;&#26029;&#35266;&#23519;&#29615;&#22659;&#24182;&#36827;&#34892;&#20132;&#27969;&#65292;&#21512;&#20316;&#21019;&#36896;&#19968;&#20010;&#26631;&#20934;&#30340;&#20013;&#25991;&#20889;&#20316;&#31995;&#32479;&#12290;&#23427;&#20174;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#31181;&#33402;&#26415;&#24615;&#35299;&#37322;&#65292;&#23558;AI&#25216;&#26415;&#19982;&#20013;&#22269;&#25991;&#21270;&#36951;&#20135;&#21644;&#22899;&#26435;&#20027;&#20041;&#35270;&#35282;&#30456;&#32467;&#21512;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#38750;&#35199;&#26041;&#25991;&#23383;&#30340;&#21019;&#20316;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents "AI Nushu," an emerging language system inspired by Nushu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were thought to be illiterate under a patriarchal society. In this interactive installation, two artificial intelligence (AI) agents are trained in the Chinese dictionary and the Nushu corpus. By continually observing their environment and communicating, these agents collaborate towards creating a standard writing system to encode Chinese. It offers an artistic interpretation of the creation of a non-western script from a computational linguistics perspective, integrating AI technology with Chinese cultural heritage and a feminist viewpoint.
&lt;/p&gt;</description></item><item><title>&#26412;&#25163;&#20876;&#26159;&#19968;&#26412;&#23454;&#36341;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#22788;&#29702;&#25991;&#26412;&#26631;&#27880;&#20219;&#21153;&#12290;&#23427;&#25552;&#20379;&#31616;&#26126;&#25212;&#35201;&#30340;&#20171;&#32461;&#65292;&#29702;&#35770;&#27010;&#24565;&#30340;&#27010;&#36848;&#20197;&#21450;&#23454;&#29992;&#24314;&#35758;&#65292;&#24182;&#35302;&#21450;&#19994;&#21153;&#12289;&#36947;&#24503;&#21644;&#30417;&#31649;&#38382;&#39064;&#12290;&#36866;&#21512;&#21508;&#31181;&#32844;&#19994;&#30340;&#20154;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.11780</link><description>&lt;p&gt;
&#25991;&#26412;&#26631;&#27880;&#25163;&#20876;&#65306;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#23454;&#36341;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Text Annotation Handbook: A Practical Guide for Machine Learning Projects. (arXiv:2310.11780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25163;&#20876;&#26159;&#19968;&#26412;&#23454;&#36341;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#22788;&#29702;&#25991;&#26412;&#26631;&#27880;&#20219;&#21153;&#12290;&#23427;&#25552;&#20379;&#31616;&#26126;&#25212;&#35201;&#30340;&#20171;&#32461;&#65292;&#29702;&#35770;&#27010;&#24565;&#30340;&#27010;&#36848;&#20197;&#21450;&#23454;&#29992;&#24314;&#35758;&#65292;&#24182;&#35302;&#21450;&#19994;&#21153;&#12289;&#36947;&#24503;&#21644;&#30417;&#31649;&#38382;&#39064;&#12290;&#36866;&#21512;&#21508;&#31181;&#32844;&#19994;&#30340;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25163;&#20876;&#26159;&#19968;&#26412;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25991;&#26412;&#26631;&#27880;&#20219;&#21153;&#30340;&#23454;&#36341;&#25351;&#21335;&#12290;&#23427;&#25552;&#20379;&#20102;&#23545;&#35813;&#20027;&#39064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#29702;&#35770;&#27010;&#24565;&#30340;&#27010;&#36848;&#20197;&#21450;&#23454;&#29992;&#24314;&#35758;&#12290;&#28085;&#30422;&#30340;&#20027;&#39064;&#20027;&#35201;&#26159;&#25216;&#26415;&#24615;&#30340;&#65292;&#20294;&#20063;&#35302;&#21450;&#20102;&#19994;&#21153;&#12289;&#36947;&#24503;&#21644;&#30417;&#31649;&#38382;&#39064;&#12290;&#37325;&#28857;&#22312;&#20110;&#26131;&#35835;&#24615;&#21644;&#31616;&#26126;&#24615;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#24615;&#21644;&#31185;&#23398;&#20005;&#35880;&#24615;&#12290;&#26377;&#26631;&#27880;&#32463;&#39564;&#21644;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#26159;&#26377;&#29992;&#20294;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#35813;&#25991;&#26723;&#21487;&#20197;&#20316;&#20026;&#21508;&#31181;&#32844;&#19994;&#30340;&#22242;&#38431;&#39046;&#23548;&#12289;&#39033;&#30446;&#32463;&#29702;&#12289;IT&#26550;&#26500;&#24072;&#12289;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#24072;&#30340;&#20837;&#38376;&#20070;&#25110;&#21442;&#32771;&#20070;&#12290;
&lt;/p&gt;
&lt;p&gt;
This handbook is a hands-on guide on how to approach text annotation tasks. It provides a gentle introduction to the topic, an overview of theoretical concepts as well as practical advice. The topics covered are mostly technical, but business, ethical and regulatory issues are also touched upon. The focus lies on readability and conciseness rather than completeness and scientific rigor. Experience with annotation and knowledge of machine learning are useful but not required. The document may serve as a primer or reference book for a wide range of professions such as team leaders, project managers, IT architects, software developers and machine learning engineers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26816;&#27979;&#21051;&#26495;&#21360;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;&#26550;&#26500;&#65292;&#21487;&#33258;&#20027;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#20419;&#36827;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#20110;&#21830;&#19994;&#20135;&#21697;&#21644;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.11778</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26816;&#27979;&#38544;&#24615;&#21051;&#26495;&#21360;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26816;&#27979;&#21051;&#26495;&#21360;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;&#26550;&#26500;&#65292;&#21487;&#33258;&#20027;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#20419;&#36827;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#20110;&#21830;&#19994;&#20135;&#21697;&#21644;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#30340;&#28608;&#22686;&#21152;&#36895;&#20102;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21830;&#19994;&#20135;&#21697;&#20013;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#37319;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#20986;&#33394;&#30340;AIGC&#20135;&#21697;&#22312;&#28040;&#36153;&#32773;&#20013;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#21644;&#28608;&#21457;&#20102;&#28909;&#24773;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#24378;&#21270;&#29616;&#26377;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#21463;&#21040;&#35821;&#35328;&#20195;&#29702;&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#20195;&#29702;&#26550;&#26500;&#12290;&#36825;&#31181;&#22810;&#21151;&#33021;&#30340;&#20195;&#29702;&#26550;&#26500;&#33021;&#22815;&#36866;&#24212;&#33258;&#30001;&#24418;&#24335;&#30340;&#26816;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#33258;&#20027;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#20419;&#36827;&#25972;&#20010;&#36807;&#31243;&#65292;&#20174;&#29983;&#25104;&#30456;&#24212;&#30340;&#25351;&#20196;&#21644;&#22270;&#20687;&#21040;&#26816;&#27979;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#22522;&#20110;&#22810;&#20010;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#30340;&#22522;&#20934;&#65292;&#24182;&#23558;&#36825;&#31181;&#26550;&#26500;&#24212;&#29992;&#20110;&#21830;&#19994;&#20135;&#21697;&#21644;&#27969;&#34892;&#30340;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge in the research of diffusion models has accelerated the adoption of text-to-image models in various Artificial Intelligence Generated Content (AIGC) commercial products. While these exceptional AIGC products are gaining increasing recognition and sparking enthusiasm among consumers, the questions regarding whether, when, and how these models might unintentionally reinforce existing societal stereotypes remain largely unaddressed. Motivated by recent advancements in language agents, here we introduce a novel agent architecture tailored for stereotype detection in text-to-image models. This versatile agent architecture is capable of accommodating free-form detection tasks and can autonomously invoke various tools to facilitate the entire process, from generating corresponding instructions and images, to detecting stereotypes. We build the stereotype-relevant benchmark based on multiple open-text datasets, and apply this architecture to commercial products and popular ope
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#31995;&#26041;&#38754;&#26377;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11772</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#26469;&#25913;&#36827;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#31995;&#26041;&#38754;&#26377;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#23545;&#20110;&#33719;&#21462;&#32467;&#26500;&#21270;&#30340;&#38271;&#25991;&#26723;&#21644;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#33021;&#22815;&#33258;&#21160;&#20174;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#20013;&#25506;&#32034;&#20027;&#39064;&#36716;&#21464;&#30340;&#32447;&#32034;&#65292;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#31070;&#32463;&#27169;&#22411;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#30340;&#21457;&#23637;&#65292;&#20294;&#23545;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#26356;&#28145;&#23618;&#27425;&#30340;&#20851;&#31995;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22686;&#24378;&#20102;&#26377;&#30417;&#30563;&#27169;&#22411;&#20174;&#32467;&#26500;&#21644;&#30456;&#20284;&#24615;&#20004;&#20010;&#26041;&#38754;&#25429;&#25417;&#19968;&#33268;&#24615;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20027;&#39064;&#21010;&#20998;&#24615;&#33021;&#65292;&#21253;&#25324;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#65288;TSSP&#65289;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65288;CSSL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;TSSP&#20219;&#21153;&#65292;&#36890;&#36807;&#23398;&#20064;&#26080;&#24207;&#25991;&#26723;&#20013;&#30456;&#37051;&#21477;&#23376;&#30340;&#21407;&#22987;&#20851;&#31995;&#65292;&#24378;&#21046;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#65292;&#35813;&#26080;&#24207;&#25991;&#26723;&#30001;&#21516;&#26102;&#30772;&#22351;&#20027;&#39064;&#21644;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation is critical for obtaining structured long documents and improving downstream tasks like information retrieval. Due to its ability of automatically exploring clues of topic shift from a large amount of labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship of semantic coherence and topic segmentation underexplored. Therefore, this paper enhances the supervised model's ability to capture coherence from both structure and similarity perspectives to further improve the topic segmentation performance, including the Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations of adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at the topic and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#23545;&#29790;&#20856;&#32844;&#20301;&#24191;&#21578;&#36827;&#34892;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#21019;&#24314;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25253;&#21578;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11769</link><description>&lt;p&gt;
&#20855;&#26377;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#27880;&#37322;&#32844;&#20301;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
Annotated Job Ads with Named Entity Recognition. (arXiv:2310.11769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#23545;&#29790;&#20856;&#32844;&#20301;&#24191;&#21578;&#36827;&#34892;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#21019;&#24314;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25253;&#21578;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#31579;&#36873;&#29790;&#20856;&#32844;&#20301;&#24191;&#21578;&#20013;&#30340;&#21508;&#31181;&#26377;&#29992;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#25307;&#32856;&#32773;&#35201;&#27714;&#30340;&#25216;&#33021;&#65289;&#12290;&#35813;&#27169;&#22411;&#26159;&#36890;&#36807;&#23545;KB-BERT&#36827;&#34892;&#24494;&#35843;&#32780;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#26159;&#21019;&#24314;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#37319;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#27880;&#37322;&#36807;&#31243;&#26356;&#39640;&#25928;&#65292;&#24182;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#32467;&#26524;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have trained a named entity recognition (NER) model that screens Swedish job ads for different kinds of useful information (e.g. skills required from a job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we faced was the creation of a labelled dataset, which required manual annotation. This paper gives an overview of the methods we employed to make the annotation process more efficient and to ensure high quality data. We also report on the performance of the resulting model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#19978;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#31867;&#20284;&#26696;&#20363;&#21644;&#22810;&#39033;&#36873;&#25321;&#30340;&#26041;&#24335;&#25552;&#39640;&#20854;&#22312;&#19987;&#23478;&#27861;&#24459;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36739;&#24369;&#30340;LLMs&#20174;&#24378;&#22823;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#33719;&#24471;&#30340;&#25910;&#30410;&#26377;&#38480;&#65292;&#20174;&#32780;&#23548;&#33268;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#32489;&#25928;&#36229;&#36234;LLM+IR&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.11761</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#19978;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction. (arXiv:2310.11761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#19978;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#31867;&#20284;&#26696;&#20363;&#21644;&#22810;&#39033;&#36873;&#25321;&#30340;&#26041;&#24335;&#25552;&#39640;&#20854;&#22312;&#19987;&#23478;&#27861;&#24459;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36739;&#24369;&#30340;LLMs&#20174;&#24378;&#22823;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#33719;&#24471;&#30340;&#25910;&#30410;&#26377;&#38480;&#65292;&#20174;&#32780;&#23548;&#33268;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#32489;&#25928;&#36229;&#36234;LLM+IR&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#65288;&#22914;&#27861;&#24459;&#39046;&#22495;&#65289;&#20013;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;GPT-4&#22312;&#27861;&#24459;&#35780;&#20272;&#26041;&#38754;&#30340;&#20105;&#35758;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#29616;&#23454;&#27861;&#24459;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#36136;&#30097;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35843;&#26597;&#23427;&#20204;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;LLMs&#30340;&#23454;&#29992;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;LLMs&#21487;&#20197;&#21333;&#29420;&#22238;&#31572;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#25110;&#19982;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#37197;&#21512;&#65292;&#20174;&#31867;&#20284;&#26696;&#20363;&#20013;&#23398;&#20064;&#25110;&#35299;&#20915;&#31616;&#21270;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#26696;&#20363;&#21644;&#22810;&#39033;&#36873;&#25321;&#36873;&#39033;&#65288;&#21363;&#25552;&#31034;&#20013;&#21253;&#21547;&#30340;&#26631;&#31614;&#20505;&#36873;&#39033;&#65289;&#21487;&#20197;&#24110;&#21161;LLMs&#22238;&#24518;&#36215;&#23545;&#19987;&#23478;&#27861;&#24459;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#24726;&#35770;&#65292;&#21363;&#30001;&#20110;&#36739;&#24369;&#30340;LLMs&#20174;&#24378;&#22823;&#30340;IR&#31995;&#32479;&#33719;&#24471;&#30340;&#25910;&#30410;&#26377;&#38480;&#65292;&#23548;&#33268;IR&#31995;&#32479;&#30340;&#32489;&#25928;&#36229;&#36807;LLM+IR&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#35282;&#33394;&#21464;&#24471;&#37325;&#35201;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#24773;&#32490;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26377;&#25152;&#24046;&#24322;&#65292;&#31361;&#26174;&#20102;&#20869;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11753</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#24773;&#32490;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26377;&#25152;&#24046;&#24322;&#65292;&#31361;&#26174;&#20102;&#20869;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20132;&#20114;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#25968;&#25454;&#26631;&#27880;&#21644;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#31561;&#21508;&#31181;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#20294;&#20854;&#22312;&#26356;&#24494;&#22937;&#30340;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#22312;&#27492;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#23427;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#24773;&#32490;&#26631;&#31614;&#19978;&#30340;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#20855;&#26377;&#21512;&#29702;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#26126;&#26174;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#32490;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#26174;&#20102;&#20869;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#28508;&#22312;&#30340;&#20559;&#24046;&#12290;&#25968;&#25454;&#38598;&#21644;&#24773;&#32490;&#26631;&#31614;&#30340;&#36873;&#25321;&#23545;ChatGPT&#30340;&#24773;&#32490;&#35782;&#21035;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#24494;&#35843;&#25552;&#21319;ChatGPT&#24773;&#32490;&#35782;&#21035;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report explores the ability of ChatGPT in recognizing emotions from text, which can be the basis of various applications like interactive chatbots, data annotation, and mental health analysis. While prior research has shown ChatGPT's basic ability in sentiment analysis, its performance in more nuanced emotion recognition is not yet explored. Here, we conducted experiments to evaluate its performance of emotion recognition across different datasets and emotion labels. Our findings indicate a reasonable level of reproducibility in its performance, with noticeable improvement through fine-tuning. However, the performance varies with different emotion labels and datasets, highlighting an inherent instability and possible bias. The choice of dataset and emotion labels significantly impacts ChatGPT's emotion recognition performance. This paper sheds light on the importance of dataset and label selection, and the potential of fine-tuning in enhancing ChatGPT's emotion recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2310.11732</link><description>&lt;p&gt;
&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#30740;&#31350;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;LM&#30456;&#27604;&#65292;&#22312;&#36755;&#20986;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#23545;&#40784;&#36807;&#31243;&#23545;&#22810;&#36873;&#35774;&#32622;&#19979;LM&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#23545;&#40784;LM&#22312;&#26657;&#20934;&#26041;&#38754;&#19982;&#20854;&#39044;&#35757;&#32451;&#23545;&#24212;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20102;&#35748;&#30495;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#65292;LM&#23384;&#22312;&#20004;&#31181;&#26126;&#26174;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#36127;&#36131;&#31572;&#26696;&#20915;&#31574;&#21644;LM&#30340;&#26684;&#24335;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#40784;&#26041;&#26696;&#20013;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23545;&#40784;LM&#30340;&#26657;&#20934;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#23545;&#40784;LM&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#24120;&#35265;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc cal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.11722</link><description>&lt;p&gt;
&#37327;&#21270;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#65306;&#19968;&#39033;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#35786;&#26029;&#24314;&#35758;&#65292;&#20174;&#32780;&#38761;&#26032;&#29992;&#25143;&#33258;&#35786;&#26029;&#30340;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;GPT-4&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#25110;&#20854;&#36890;&#36807;&#21307;&#23398;&#32771;&#35797;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#37327;&#21270;&#23384;&#20648;&#22312;LLMs&#35760;&#24518;&#20013;&#30340;&#20581;&#24247;&#30456;&#20851;&#21407;&#23376;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#32780;&#36825;&#26159;LLMs&#25552;&#20379;&#26356;&#20934;&#30830;&#24314;&#35758;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#29992;&#25143;&#33258;&#35786;&#26029;&#26597;&#35810;&#20013;&#26368;&#24120;&#35265;&#30340;&#21407;&#23376;&#30693;&#35782;&#31867;&#22411;&#65292;&#20849;17&#31181;&#21407;&#23376;&#31867;&#22411;&#21644;14048&#26465;&#21407;&#23376;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#22312;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#65292;&#36890;&#29992;LLMs&#30340;&#34920;&#29616;&#20248;&#20110;&#19987;&#19994;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#37117;&#26159;&#39532;&#23617;&#31934;&#65292;&#21363;&#22312;&#28041;&#21450;&#29992;&#25143;&#35201;&#27714;&#26102;&#24635;&#26159;&#36814;&#21512;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2310.11721</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding. (arXiv:2310.11721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21453;&#23556;&#35843;&#25972;"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#25913;&#36827;&#21644;&#21028;&#26029;&#33021;&#21147;&#26469;&#35299;&#20915;LLMs&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20511;&#21161;Oracle LLM&#22238;&#25910;&#35757;&#32451;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11716</link><description>&lt;p&gt;
&#21453;&#23556;&#35843;&#25972;&#65306;&#25968;&#25454;&#22238;&#25910;&#25913;&#36827;&#20102;LLM&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning. (arXiv:2310.11716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21453;&#23556;&#35843;&#25972;"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#25913;&#36827;&#21644;&#21028;&#26029;&#33021;&#21147;&#26469;&#35299;&#20915;LLMs&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20511;&#21161;Oracle LLM&#22238;&#25910;&#35757;&#32451;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#33539;&#22260;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#25913;&#36827;LLMs&#30340;&#36755;&#20986;&#25511;&#21046;&#21644;&#19982;&#36755;&#20837;&#30340;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#20960;&#39033;&#30740;&#31350;&#25152;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#35757;&#32451;&#38598;&#20013;&#30340;&#20302;&#36136;&#37327;&#25968;&#25454;&#36890;&#24120;&#23545;&#25351;&#20196;&#35843;&#25972;&#26377;&#23475;&#65292;&#23548;&#33268;LLMs&#30340;&#36755;&#20986;&#19981;&#19968;&#33268;&#29978;&#33267;&#35823;&#23548;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21453;&#23556;&#35843;&#25972;&#8221;&#65292;&#36890;&#36807;LLMs&#30340;&#33258;&#25105;&#25913;&#36827;&#21644;&#21028;&#26029;&#33021;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20511;&#21161;&#19968;&#20010;Oracle LLM&#26469;&#22238;&#25910;&#21407;&#22987;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#20869;&#30465;&#21644;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#25351;&#20196;&#21644;&#21709;&#24212;&#30340;&#36136;&#37327;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#22238;&#25910;&#25968;&#25454;&#35757;&#32451;&#30340;LLMs&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#20248;&#20110;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11715</link><description>&lt;p&gt;
&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11715
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#32454;&#31890;&#24230;NER&#22330;&#26223;&#19979;&#24120;&#24120;&#38754;&#20020;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21487;&#20197;&#24212;&#29992;K-shot&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#24403;&#27880;&#37322;&#25968;&#37327;&#36229;&#36807;&#20960;&#21313;&#20010;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#24448;&#24448;&#36798;&#21040;&#39281;&#21644;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#30340;&#26631;&#27880;&#12290;&#19968;&#31181;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#39044;&#35757;&#32451;&#65292;&#23427;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#26080;&#27861;&#30452;&#25509;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23613;&#31649;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#24456;&#21487;&#33021;&#26159;&#31895;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#30340;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#65288;F2C&#65289;&#26144;&#23556;&#30697;&#38453;&#30340;&#32454;&#31890;&#24230;NER&#27169;&#22411;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#19982;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#30340;&#31895;&#31890;&#24230;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-gr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.11710</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#29992;&#20110;&#22810;&#27169;&#24577;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11710
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#35821;&#26159;&#19968;&#31181;&#30001;&#33041;&#25439;&#20260;&#24341;&#36215;&#30340;&#35821;&#35328;&#38556;&#30861;&#65292;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#30340;&#22833;&#35821;&#31867;&#22411;&#65292;&#22914;Broca&#22833;&#35821;&#21644;Wernicke&#22833;&#35821;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#22833;&#35821;&#30340;&#26041;&#27861;&#65292;&#20154;&#20204;&#24182;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20998;&#26512;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#23545;&#20110;&#21306;&#20998;&#22833;&#35821;&#31867;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#35821;&#38899;&#21644;&#30456;&#24212;&#30340;&#25163;&#21183;&#27169;&#24335;&#36827;&#34892;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#31181;&#22833;&#35821;&#31867;&#22411;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#23545;&#25163;&#21183;&#20449;&#24687;&#25935;&#24863;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25163;&#21183;&#29305;&#24449;&#20248;&#20110;&#22768;&#23398;&#29305;&#24449;&#65292;&#31361;&#26174;&#20102;&#25163;&#21183;&#34920;&#36798;&#22312;&#26816;&#27979;&#22833;&#35821;&#31867;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#24773;&#22659;&#22810;&#27169;&#24577;&#20013;&#21560;&#25910;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#20027;&#35270;&#39057;&#12289;&#35821;&#38899;&#21644;&#35821;&#22659;&#20998;&#26512;&#23454;&#29616;&#22686;&#24378;&#30340;&#29366;&#24577;&#20272;&#35745;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36866;&#24212;&#24615;&#26356;&#24378;&#30340;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.11699</link><description>&lt;p&gt;
MISAR&#65306;&#19968;&#20010;&#20855;&#26377;&#22686;&#24378;&#29616;&#23454;&#30340;&#22810;&#27169;&#24335;&#25945;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MISAR: A Multimodal Instructional System with Augmented Reality. (arXiv:2310.11699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#24773;&#22659;&#22810;&#27169;&#24577;&#20013;&#21560;&#25910;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#20027;&#35270;&#39057;&#12289;&#35821;&#38899;&#21644;&#35821;&#22659;&#20998;&#26512;&#23454;&#29616;&#22686;&#24378;&#30340;&#29366;&#24577;&#20272;&#35745;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36866;&#24212;&#24615;&#26356;&#24378;&#30340;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#38656;&#35201;&#23558;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#35821;&#35328;&#36890;&#36947;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#20248;&#21270;&#20154;&#26426;&#20132;&#20114;&#12290;&#23613;&#31649;&#21548;&#35273;&#21644;&#35270;&#35273;&#36755;&#20837;&#26377;&#21161;&#20110;&#23454;&#26102;&#21644;&#24773;&#22659;&#24863;&#23548;&#21521;&#29992;&#25143;&#25351;&#23548;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#26469;&#21560;&#25910;&#26469;&#33258;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#24773;&#22659;&#22810;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#38024;&#23545;AR&#20013;&#20219;&#21153;&#25191;&#34892;&#37327;&#21270;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#20027;&#35270;&#35282;&#35270;&#39057;&#12289;&#35821;&#38899;&#21644;&#35821;&#22659;&#20998;&#26512;&#12290;LLMs&#30340;&#38598;&#25104;&#20419;&#36827;&#20102;&#22686;&#24378;&#30340;&#29366;&#24577;&#20272;&#35745;&#65292;&#36808;&#21521;&#26356;&#36866;&#24212;&#24615;&#30340;AR&#31995;&#32479;&#12290;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#28436;&#31034;&#23558;&#22312;https://github.com/nguyennm1024/misar&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmented reality (AR) requires the seamless integration of visual, auditory, and linguistic channels for optimized human-computer interaction. While auditory and visual inputs facilitate real-time and contextual user guidance, the potential of large language models (LLMs) in this landscape remains largely untapped. Our study introduces an innovative method harnessing LLMs to assimilate information from visual, auditory, and contextual modalities. Focusing on the unique challenge of task performance quantification in AR, we utilize egocentric video, speech, and context analysis. The integration of LLMs facilitates enhanced state estimation, marking a step towards more adaptive AR systems. Code, dataset, and demo will be available at https://github.com/nguyennm1024/misar.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11689</link><description>&lt;p&gt;
&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#25913;&#36827;LLMs&#20013;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#20173;&#28982;&#38480;&#20110;&#20854;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#36873;&#25321;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#22312;LLMs&#19981;&#30830;&#23450;&#26102;&#20351;&#20854;&#36991;&#20813;&#39044;&#27979;&#32780;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20351;&#29992;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25913;&#36827;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#22312;CoQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;AUACC&#20174;91.23%&#25552;&#39640;&#21040;92.63%&#65292;&#24182;&#23558;AURO
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;softmax&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;softmax&#27880;&#24847;&#21147;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.11685</link><description>&lt;p&gt;
Softmax&#30340;&#20248;&#36234;&#24615;&#65306;&#25581;&#31034;&#20854;&#30456;&#23545;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention. (arXiv:2310.11685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;softmax&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;softmax&#27880;&#24847;&#21147;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;Transformer&#26550;&#26500;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#20013;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#21033;&#29992;softmax&#20989;&#25968;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#26631;&#35760;&#20132;&#20114;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#36890;&#36807;&#32447;&#24615;&#22797;&#26434;&#24230;&#36817;&#20284;softmax&#25805;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#27604;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#38477;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;softmax&#27880;&#24847;&#21147;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.  Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.  In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#26500;&#24314;&#25551;&#36848;&#24615;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21477;&#23376;&#24182;&#36827;&#34892;&#20851;&#31995;&#25628;&#32034;&#21644;&#23548;&#33322;&#12290;&#24182;&#19988;&#35813;&#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#38405;&#35835;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;COVID-19&#30740;&#31350;&#20013;&#24212;&#29992;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#20854;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11681</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#25551;&#36848;&#24615;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Descriptive Knowledge Graph in Biomedical Domain. (arXiv:2310.11681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#26500;&#24314;&#25551;&#36848;&#24615;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21477;&#23376;&#24182;&#36827;&#34892;&#20851;&#31995;&#25628;&#32034;&#21644;&#23548;&#33322;&#12290;&#24182;&#19988;&#35813;&#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#38405;&#35835;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;COVID-19&#30740;&#31350;&#20013;&#24212;&#29992;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#20854;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#29983;&#25104;&#26377;&#20449;&#24687;&#37327;&#21644;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#20419;&#36827;&#26377;&#25928;&#30340;&#20851;&#31995;&#30693;&#35782;&#25628;&#32034;&#12290;&#19982;&#20043;&#21069;&#26816;&#32034;&#38750;&#30456;&#20851;&#27573;&#33853;&#30340;&#25628;&#32034;&#24341;&#25806;&#25110;&#25506;&#32034;&#31995;&#32479;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#25551;&#36848;&#24615;&#21477;&#23376;&#32452;&#32455;&#20026;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#30456;&#20851;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#30001;&#21270;&#23398;&#29289;&#36136;&#27835;&#30103;&#30340;&#30142;&#30149;&#65289;&#25110;&#38388;&#25509;&#30456;&#20851;&#30340;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#27835;&#30103;&#26576;&#31181;&#30142;&#30149;&#30340;&#28508;&#22312;&#33647;&#29289;&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#20351;&#29992;ChatGPT&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#20851;&#31995;&#21512;&#25104;&#27169;&#22411;&#20174;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#20013;&#29983;&#25104;&#31616;&#27905;&#21487;&#38752;&#30340;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#38405;&#35835;&#30340;&#38656;&#27714;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#33719;&#24471;&#39640;&#32423;&#30693;&#35782;&#21644;&#35814;&#32454;&#21442;&#32771;&#65292;&#24182;&#21487;&#20197;&#20132;&#20114;&#22320;&#23548;&#33322;&#21040;&#24863;&#20852;&#36259;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#25105;&#20204;&#31995;&#32479;&#22312;COVID-19&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#35828;&#26126;&#20854;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#21046;&#31572;&#26696;&#33539;&#22260;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22806;&#37096;&#30693;&#35782;&#24211;&#19978;&#36845;&#20195;&#26816;&#32034;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#24110;&#21161;&#25214;&#21040;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#22312;&#24120;&#35782;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11672</link><description>&lt;p&gt;
&#26080;&#38480;&#21046;&#31572;&#26696;&#33539;&#22260;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#21046;&#31572;&#26696;&#33539;&#22260;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22806;&#37096;&#30693;&#35782;&#24211;&#19978;&#36845;&#20195;&#26816;&#32034;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#24110;&#21161;&#25214;&#21040;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#22312;&#24120;&#35782;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#34987;&#23450;&#20041;&#20026;&#22312;&#19981;&#25552;&#20379;1&#65289;&#31572;&#26696;&#20505;&#36873;&#21517;&#21333;&#21644;2&#65289;&#39044;&#23450;&#20041;&#31572;&#26696;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#12290;&#23558;&#24120;&#35782;&#38382;&#39064;&#36716;&#21270;&#20026;&#38382;&#31572;&#24418;&#24335;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23398;&#20064;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#35268;&#26041;&#27861;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#19981;&#22826;&#36866;&#29992;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#19968;&#20010;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#31572;&#26696;&#33539;&#22260;&#25110;&#23569;&#25968;&#20505;&#36873;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#38656;&#35201;&#36890;&#36807;&#22312;&#26497;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#25628;&#32034;&#26469;&#39044;&#27979;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#36825;&#32473;&#25105;&#20204;&#30340;&#38382;&#39064;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22806;&#37096;&#30693;&#35782;&#24211;&#19978;&#36845;&#20195;&#22320;&#26816;&#32034;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#30417;&#30563;&#12290;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#25214;&#21040;&#26368;&#20934;&#30830;&#30340;&#24120;&#35782;&#38382;&#39064;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35782;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense ben
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20146;&#21644;&#24230;&#21644;&#22810;&#26679;&#24615;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#25913;&#36827;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixEdit&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#31574;&#30053;&#24615;&#21644;&#21160;&#24577;&#22320;&#22686;&#24378;&#30495;&#23454;&#25968;&#25454;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#21333;&#35821;&#35328;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.11671</link><description>&lt;p&gt;
MixEdit:&#20877;&#25506;&#25968;&#25454;&#22686;&#24378;&#21450;&#20854;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction. (arXiv:2310.11671v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20146;&#21644;&#24230;&#21644;&#22810;&#26679;&#24615;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#25913;&#36827;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixEdit&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#31574;&#30053;&#24615;&#21644;&#21160;&#24577;&#22320;&#22686;&#24378;&#30495;&#23454;&#25968;&#25454;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#21333;&#35821;&#35328;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#29983;&#25104;&#20266;&#25968;&#25454;&#24050;&#34987;&#35777;&#26126;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#39046;&#22495;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#26159;&#26377;&#25928;&#30340;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#21508;&#31181;&#22686;&#24378;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26159;&#22522;&#20110;&#20004;&#20010;&#21551;&#21457;&#24335;&#30340;&#65292;&#21363;&#22686;&#21152;&#20266;&#25968;&#25454;&#30340;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#30340;&#22522;&#30784;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#38416;&#26126;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#25913;&#36827;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#35299;&#37322;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24230;&#37327;&#26041;&#27861;&#65306;&#20146;&#21644;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19968;&#20010;&#20855;&#26377;&#39640;&#20146;&#21644;&#24230;&#21644;&#36866;&#24403;&#22810;&#26679;&#24615;&#30340;&#20248;&#31168;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MixEdit&#65292;&#19968;&#31181;&#33021;&#22815;&#31574;&#30053;&#24615;&#21644;&#21160;&#24577;&#22320;&#22686;&#24378;&#30495;&#23454;&#25968;&#25454;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21333;&#35821;&#35328;&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation through generating pseudo data has been proven effective in mitigating the challenge of data scarcity in the field of Grammatical Error Correction (GEC). Various augmentation strategies have been widely explored, most of which are motivated by two heuristics, i.e., increasing the distribution similarity and diversity of pseudo data. However, the underlying mechanism responsible for the effectiveness of these strategies remains poorly understood. In this paper, we aim to clarify how data augmentation improves GEC models. To this end, we introduce two interpretable and computationally efficient measures: Affinity and Diversity. Our findings indicate that an excellent GEC data augmentation strategy characterized by high Affinity and appropriate Diversity can better improve the performance of GEC models. Based on this observation, we propose MixEdit, a data augmentation approach that strategically and dynamically augments realistic data, without requiring extra monolingua
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11667</link><description>&lt;p&gt;
SOTOPIA: &#20132;&#20114;&#24335;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11667
&lt;/p&gt;
&lt;p&gt;
SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#30340;&#23384;&#22312;&#65307;&#25105;&#20204;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#36861;&#27714;&#31038;&#20132;&#30446;&#26631;&#65292;&#36825;&#26159;&#31038;&#20132;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SOTOPIA&#65292;&#19968;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#31038;&#20132;&#20114;&#21160;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#20154;&#25198;&#28436;&#35282;&#33394;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30456;&#20114;&#21327;&#20316;&#12289;&#21512;&#20316;&#12289;&#20132;&#27969;&#21644;&#31454;&#20105;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#31038;&#20132;&#30446;&#26631;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;LLM-based&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20869;&#30340;&#35282;&#33394;&#25198;&#28436;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;SOTOPIA-Eval&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;SOTOPIA&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#20102;SOTOPIA&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#21363;SOTOPIA-hard&#65292;&#23545;&#25152;&#26377;&#27169;&#22411;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RoBERTa&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#36827;&#34892;&#33521;&#35821;&#35835;&#20889;&#33021;&#21147;&#32771;&#35797;&#30340;&#29616;&#22330;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#32771;&#29983;&#25968;&#25454;&#26377;&#19968;&#23450;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11655</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#29616;&#22330;&#27979;&#35797;&#30340;&#39033;&#30446;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#21464;&#24418;&#37329;&#21018;
&lt;/p&gt;
&lt;p&gt;
Field-testing items using artificial intelligence: Natural language processing with transformers. (arXiv:2310.11655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11655
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RoBERTa&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#36827;&#34892;&#33521;&#35821;&#35835;&#20889;&#33021;&#21147;&#32771;&#35797;&#30340;&#29616;&#22330;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#32771;&#29983;&#25968;&#25454;&#26377;&#19968;&#23450;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20116;&#21315;&#20010;RoBERTa&#27169;&#22411;&#30340;&#21464;&#24322;&#20307;&#65292;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#35821;&#35328;&#30340;&#20154;&#24037;&#26234;&#33021;&#8220;&#21464;&#24418;&#37329;&#21018;&#8221;&#65292;&#23436;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;29&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33521;&#35821;&#35835;&#20889;&#33021;&#21147;&#32771;&#35797;&#12290;&#21033;&#29992;&#25968;&#25454;&#35745;&#31639;&#20102;&#36825;&#20123;&#39064;&#30446;&#30340;&#24515;&#29702;&#27979;&#37327;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#19982;&#20154;&#31867;&#32771;&#29983;&#25968;&#25454;&#30340;&#32467;&#26524;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Five thousand variations of the RoBERTa model, an artificially intelligent "transformer" that can understand text language, completed an English literacy exam with 29 multiple-choice questions. Data were used to calculate the psychometric properties of the items, which showed some degree of agreement to those obtained from human examinee data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24544;&#23454;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FFLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FFLM&#22312;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#21644;&#24544;&#23454;&#24615;&#35780;&#32423;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;24&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.11648</link><description>&lt;p&gt;
&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24544;&#23454;&#24615;&#35780;&#20272;&#30340;&#25991;&#26412;&#25688;&#35201;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model. (arXiv:2310.11648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24544;&#23454;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FFLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FFLM&#22312;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#21644;&#24544;&#23454;&#24615;&#35780;&#32423;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;24&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#20294;&#25688;&#35201;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20351;&#29992;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#25110;&#39046;&#22495;&#20869;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#35780;&#20272;&#24544;&#23454;&#24615;&#65292;&#35201;&#20040;&#20351;&#29992;&#31867;&#20284;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24544;&#23454;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FFLM&#65292;&#23427;&#26159;&#22522;&#20110;&#27010;&#29575;&#21464;&#21270;&#30340;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26159;&#22522;&#20110;&#19968;&#20010;&#35266;&#28857;&#65306;&#22312;&#36755;&#20986;&#30340;&#25991;&#26412;&#21069;&#21152;&#19978;&#19982;&#36755;&#20986;&#19968;&#33268;&#30340;&#19968;&#27573;&#25991;&#26412;&#23558;&#22686;&#21152;&#39044;&#27979;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FFLM&#22312;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#21644;&#24544;&#23454;&#24615;&#35780;&#32423;&#19978;&#19982;ChatGPT&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;24&#20493;&#12290;FFLM&#36824;&#22312;&#20854;&#20182;&#24378;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#36890;&#29992;&#21644;&#29305;&#23450;&#39046;&#22495;&#20013;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26159;&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.11638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Systematic Assessment of Factual Knowledge in Large Language Models. (arXiv:2310.11638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#36890;&#29992;&#21644;&#29305;&#23450;&#39046;&#22495;&#20013;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26159;&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38382;&#31572;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#28085;&#30422;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#29992;&#39046;&#22495;&#65292;&#36825;&#21487;&#33021;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#37325;&#21472;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#32473;&#23450;KG&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#39044;&#26399;&#31572;&#26696;&#65292;&#28982;&#21518;&#35780;&#20272;LLMs&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#36890;&#29992;&#39046;&#22495;&#21644;&#29305;&#23450;&#39046;&#22495;&#20013;&#31995;&#32479;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#19982;KGs&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;ChatGPT&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#22987;&#32456;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;LLMs&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#25351;&#20196;&#24494;&#35843;&#12289;&#39046;&#22495;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#19978;&#23545;&#26032;&#39062;&#35299;&#37322;&#30340;&#27010;&#25324;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26032;&#39062;&#35299;&#37322;&#65292;&#23588;&#20854;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#38271;&#23545;&#35805;&#20013;&#33719;&#21462;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11634</link><description>&lt;p&gt;
MAGNIFICo&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#19978;&#23545;&#26032;&#39062;&#35299;&#37322;&#30340;&#27010;&#25324;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations. (arXiv:2310.11634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#19978;&#23545;&#26032;&#39062;&#35299;&#37322;&#30340;&#27010;&#25324;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26032;&#39062;&#35299;&#37322;&#65292;&#23588;&#20854;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#38271;&#23545;&#35805;&#20013;&#33719;&#21462;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#19968;&#31181;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21363;&#23558;&#26032;&#39062;&#30340;&#35299;&#37322;&#36171;&#20104;&#35821;&#35328;&#34920;&#36798;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#23398;&#20064;&#26032;&#21333;&#35789;&#24182;&#29702;&#35299;&#31038;&#32676;&#29305;&#23450;&#30340;&#20869;&#28085;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#30693;&#35782;&#25130;&#26029;&#65292;&#24182;&#19988;&#37325;&#22797;&#24494;&#35843;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#65292;LLMs&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23398;&#20064;&#26032;&#39062;&#35299;&#37322;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;LLMs&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33719;&#21462;&#26032;&#39062;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MAGNIFICo&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#26694;&#26550;&#20869;&#23454;&#29616;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#23427;&#21253;&#25324;&#22810;&#26679;&#30340;&#20196;&#29260;&#21644;&#25552;&#31034;&#35774;&#32622;&#65292;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;MAGNIFICo&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#23637;&#29616;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20197;&#21450;&#38271;&#23545;&#35805;&#20013;&#30340;&#35752;&#35770;&#20013;&#29702;&#35299;&#26032;&#39062;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20063;&#20984;&#26174;&#20986;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, partic
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;"&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;"&#26041;&#26696;&#65292;&#21033;&#29992;&#21333;&#35789;&#36793;&#30028;&#23558;&#23383;&#33410;/&#23383;&#31526;&#27719;&#32858;&#25104;&#21333;&#35789;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#25913;&#21892;&#26631;&#35760;&#21270;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11628</link><description>&lt;p&gt;
&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#35789;&#27744;&#21270;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11628
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;"&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;"&#26041;&#26696;&#65292;&#21033;&#29992;&#21333;&#35789;&#36793;&#30028;&#23558;&#23383;&#33410;/&#23383;&#31526;&#27719;&#32858;&#25104;&#21333;&#35789;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#25913;&#21892;&#26631;&#35760;&#21270;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23558;&#25991;&#26412;&#26631;&#35760;&#21270;&#20026;&#23376;&#35789;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#30340;&#12289;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#23383;&#31526;&#32452;&#21512;&#25104;&#26356;&#38271;&#30340;&#34920;&#23618;&#23383;&#31526;&#20018;&#65288;&#22914; 'ing'&#65289;&#25110;&#25972;&#20010;&#21333;&#35789;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#21453;&#22797;&#23637;&#31034;&#20102;&#36825;&#31181;&#26631;&#35760;&#21270;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#33521;&#25991;&#30340;&#25991;&#26723;&#21644;&#34920;&#31034;&#25968;&#23383;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23383;&#33410;/&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#21046;&#36739;&#23569;&#65292;&#20294;&#22312;&#33258;&#25105;&#27880;&#24847;&#35745;&#31639;&#20013;&#23384;&#22312;&#24207;&#21015;&#25551;&#36848;&#38271;&#24230;&#22686;&#21152;&#21644;&#21518;&#32493;&#20108;&#27425;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#23545;&#36825;&#20123;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#22266;&#23450;&#22823;&#23567;&#21367;&#31215;&#21387;&#32553;&#21644;&#38480;&#21046;&#30340;&#23581;&#35797;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#23436;&#20840;&#24573;&#30053;&#20102;&#21333;&#35789;&#36793;&#30028;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#8220;&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;&#8221;&#26041;&#26696;&#65292;&#21033;&#29992;&#21333;&#35789;&#36793;&#30028;&#23558;&#23383;&#33410;/&#23383;&#31526;&#27719;&#32858;&#25104;&#21333;&#35789;&#34920;&#31034;&#24418;&#24335;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#20027;&#35201;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20877;&#23545;&#27599;&#20010;&#21333;&#35789;&#24182;&#34892;&#35299;&#30721;&#20010;&#21035;&#30340;&#23383;&#31526;/&#23383;&#33410;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderatel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.11616</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;&#65306;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#29702;&#35770;&#22312;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#29289;&#31181;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;Open LLM Leaderboard&#65288;&#21253;&#21547;1,232&#20010;&#27169;&#22411;&#65289;&#21644;General Language Understanding Evaluation&#65288;GLUE&#65289;Leaderboard&#65288;&#21253;&#21547;88&#20010;&#27169;&#22411;&#65289;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#19968;&#32500;&#24615;&#21644;&#39640;&#24230;&#31283;&#23450;&#24615;&#30340;g&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;g&#20043;&#38388;&#30340;&#20013;&#24230;&#30456;&#20851;&#24615;&#20026;0.48&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;g&#22240;&#23376;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#65292;&#20026;&#26356;&#24378;&#22823;&#12289;&#22522;&#20110;g&#22240;&#23376;&#30340;&#27169;&#22411;&#33021;&#21147;&#35780;&#20272;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#20174;&#24515;&#29702;&#27979;&#37327;&#30340;&#35282;&#24230;&#29702;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#23545;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11604</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20104;&#20302;&#32423;&#25216;&#33021;&#36873;&#25321;&#26102;&#33021;&#22815;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;LLMs&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#29992;&#20110;&#20302;&#32423;&#36712;&#36857;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#35843;&#26597;&#20102;&#24403;&#32473;&#20104;LLM&#65288;GPT-4&#65289;&#20165;&#33021;&#35775;&#38382;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#26102;&#65292;&#23427;&#33021;&#21542;&#30452;&#25509;&#39044;&#27979;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#29992;&#20110;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#27809;&#26377;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#12289;&#36816;&#21160;&#21407;&#35821;&#25110;&#22806;&#37096;&#36712;&#36857;&#20248;&#21270;&#22120;&#65292;&#23427;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#22914;&#8220;&#25171;&#24320;&#29942;&#30422;&#8221;&#21644;&#8220;&#29992;&#28023;&#32501;&#25830;&#25325;&#30424;&#23376;&#8221;&#65292;&#20197;&#21450;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#25552;&#31034;&#20013;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#36136;&#37327;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#32780;&#20154;&#24037;&#21028;&#26029;&#21448;&#26114;&#36149;&#19988;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#27979;&#37327;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36825;&#19977;&#20010;&#37325;&#35201;&#35821;&#20041;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11593</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#36136;&#37327;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#32780;&#20154;&#24037;&#21028;&#26029;&#21448;&#26114;&#36149;&#19988;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#27979;&#37327;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36825;&#19977;&#20010;&#37325;&#35201;&#35821;&#20041;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#20010;&#20154;&#32972;&#26223;&#20132;&#20184;&#20869;&#23481;&#30340;&#19987;&#38376;&#26426;&#21046;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#65288;&#22914;BLEU&#21644;ROUGE&#65289;&#20027;&#35201;&#34913;&#37327;&#19982;&#20154;&#24037;&#21442;&#32771;&#25991;&#26412;&#30340;&#35789;&#27719;&#30456;&#20284;&#24230;&#65292;&#24182;&#19981;&#33021;&#21306;&#20998;&#20010;&#24615;&#21270;&#19982;&#20854;&#20182;&#24494;&#22937;&#30340;&#35821;&#20041;&#26041;&#38754;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#24037;&#21028;&#26029;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20010;&#24615;&#21270;&#35780;&#20272;&#39046;&#22495;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35780;&#20272;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#26816;&#39564;&#23427;&#20204;&#29702;&#35299;&#32454;&#33268;&#30340;&#29992;&#25143;&#32972;&#26223;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AuPEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#25991;&#26412;&#30340;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#19977;&#20010;&#20027;&#35201;&#35821;&#20041;&#26041;&#38754;&#25552;&#21462;&#24182;&#33258;&#21160;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#21644;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#36890;&#36807;GATE&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.11589</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#33719;&#21462;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#21644;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#36890;&#36807;GATE&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26631;&#27880;&#31034;&#20363;&#25110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#25191;&#34892;&#30446;&#26631;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#22312;&#36873;&#25321;&#31034;&#20363;&#25110;&#25776;&#20889;&#25552;&#31034;&#26102;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#8212;&#8212;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#24322;&#24120;&#24773;&#20917;&#12289;&#35201;&#27714;&#31934;&#30830;&#34920;&#36798;&#27169;&#31946;&#20559;&#22909;&#25110;&#38656;&#35201;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#35748;&#30693;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;*&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;*&#26469;&#24341;&#23548;&#20219;&#21153;&#35268;&#33539;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;**&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;**&#65306;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#24182;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#30740;&#31350;&#20102;GATE&#65306;&#30005;&#23376;&#37038;&#20214;&#39564;&#35777;&#12289;&#20869;&#23481;&#25512;&#33616;&#21644;&#36947;&#24503;&#25512;&#29702;&#12290;&#22312;&#39044;&#20808;&#27880;&#20876;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#31034;&#25191;&#34892;GATE&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#24320;&#25918;&#24335;&#38382;&#39064;&#25110;&#21512;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#36793;&#30028;&#26696;&#20363;&#65289;&#25152;&#24341;&#21457;&#30340;&#21709;&#24212;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#29992;&#25143;&#25253;&#21578;&#31216;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#24341;&#23548;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24110;&#21161;&#20182;&#20204;&#34920;&#36798;&#20559;&#22909;&#21644;&#25351;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitat
&lt;/p&gt;</description></item><item><title>BasahaCorpus &#26159;&#19968;&#20010;&#25193;&#23637;&#33778;&#24459;&#23486;&#20013;&#22830;&#35821;&#31995;&#35821;&#35328;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#35821;&#26009;&#24211;&#65292;&#21033;&#29992;&#34920;&#23618;&#29305;&#24449;&#12289;&#38899;&#33410;&#27169;&#24335;&#21644;n-gram&#37325;&#21472;&#29305;&#24449;&#35757;&#32451;&#20102;ARA&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#21270;&#36328;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65307;&#30740;&#31350;&#21457;&#29616;&#36328;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.11584</link><description>&lt;p&gt;
BasahaCorpus: &#33778;&#24459;&#23486;&#20013;&#22830;&#35821;&#31995;&#35821;&#35328;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#25193;&#23637;&#35821;&#35328;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages. (arXiv:2310.11584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11584
&lt;/p&gt;
&lt;p&gt;
BasahaCorpus &#26159;&#19968;&#20010;&#25193;&#23637;&#33778;&#24459;&#23486;&#20013;&#22830;&#35821;&#31995;&#35821;&#35328;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#35821;&#26009;&#24211;&#65292;&#21033;&#29992;&#34920;&#23618;&#29305;&#24449;&#12289;&#38899;&#33410;&#27169;&#24335;&#21644;n-gram&#37325;&#21472;&#29305;&#24449;&#35757;&#32451;&#20102;ARA&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#21270;&#36328;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65307;&#30740;&#31350;&#21457;&#29616;&#36328;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65288;ARA&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#33521;&#35821;&#31561;&#36164;&#28304;&#36739;&#20016;&#23500;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#21457;&#24067;&#20102;BasahaCorpus&#20316;&#20026;&#19968;&#20010;&#26088;&#22312;&#25299;&#23637;&#33778;&#24459;&#23486;&#20302;&#36164;&#28304;&#35821;&#35328;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#21487;&#29992;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#20513;&#35758;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#32534;&#35793;&#20102;&#19968;&#20010;&#30001;Hiligaynon&#65292;Minasbate&#65292;Karay-a&#21644;Rinconada&#22235;&#31181;&#35821;&#35328;&#32534;&#20889;&#30340;&#30701;&#31687;&#23567;&#35828;&#25925;&#20107;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#35821;&#35328;&#23646;&#20110;&#33778;&#24459;&#23486;&#20013;&#22830;&#35821;&#31995;&#23478;&#26063;&#26641;&#30340;&#23376;&#20998;&#25903;&#65292;&#25105;&#20204;&#20351;&#29992;&#34920;&#23618;&#29305;&#24449;&#12289;&#38899;&#33410;&#27169;&#24335;&#21644;n-gram&#37325;&#21472;&#29305;&#24449;&#35757;&#32451;ARA&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#21270;&#36328;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#22312;&#23478;&#26063;&#26641;&#20013;&#30340;&#20301;&#32622;&#20197;&#22686;&#21152;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#25903;&#25345;&#20808;&#21069;&#24037;&#20316;&#23637;&#31034;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada -- languages belonging to the Central Philippine family tree subgroup -- to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. We also propose a new hierarchical cross-lingual modeling approach that takes advantage of a language's placement in the family tree to increase the amount of available training data. Our study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.11571</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19968;&#20010;&#22909;&#38382;&#39064;&#65311;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#19982;&#20107;&#23454;&#32423;&#36974;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#38382;&#26159;&#29616;&#23454;&#29983;&#27963;&#20013;&#21512;&#20316;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#27861;&#24459;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#29992;&#25143;&#24773;&#20917;&#30340;&#20855;&#20307;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20250;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#21521;&#29992;&#25143;&#25110;&#31532;&#19977;&#26041;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#12290;&#38646;-shot&#32842;&#22825;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;TOA&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20027;&#35201;&#22522;&#20110;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#38382;&#39064;&#26159;&#21542;&#23545;&#25104;&#21151;&#30340;&#21512;&#20316;&#26377;&#24110;&#21161;&#12290;&#20026;&#20102;&#33021;&#22815;&#35757;&#32451;&#21644;&#35780;&#20272;TOA&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#35810;&#38382;&#30340;&#23450;&#20041;&#21644;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#33021;&#22815;&#20026;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30465;&#30053;&#29305;&#23450;&#30340;&#37096;&#20998;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#23545;&#40784;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#20559;&#22909;&#20998;&#35299;&#20026;&#22810;&#20010;&#32500;&#24230;&#65292;&#21487;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#21512;&#24182;&#36827;&#34892;&#26377;&#25928;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.11564</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#27748;&#65306;&#36890;&#36807;&#20107;&#21518;&#21512;&#24182;&#21442;&#25968;&#36827;&#34892;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging. (arXiv:2310.11564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#23545;&#40784;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#20559;&#22909;&#20998;&#35299;&#20026;&#22810;&#20010;&#32500;&#24230;&#65292;&#21487;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#21512;&#24182;&#36827;&#34892;&#26377;&#25928;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#33021;&#22815;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#19968;&#33324;&#30340;&#12289;&#32508;&#21512;&#30340;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#23545;&#20110;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20010;&#20307;&#35266;&#28857;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLPHF&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#23558;&#23545;&#40784;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#38382;&#39064;&#65292;&#20197;&#19982;&#22810;&#20010;&#65288;&#26377;&#26102;&#30456;&#20114;&#20914;&#31361;&#30340;&#65289;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;&#19982;&#24378;&#21333;&#30446;&#26631;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#20559;&#22909;&#20998;&#35299;&#20026;&#22810;&#20010;&#32500;&#24230;&#21487;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#23545;&#40784;&#12290;&#36825;&#20123;&#32500;&#24230;&#26159;&#22522;&#20110;&#29992;&#25143;&#22768;&#26126;&#20026;&#29702;&#24819;&#30340;&#20010;&#24615;&#21270;&#29305;&#24449;&#36827;&#34892;&#23450;&#20041;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#20998;&#24067;&#24335;&#35757;&#32451;&#36827;&#34892;&#39640;&#25928;&#29420;&#31435;&#22320;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#21512;&#24182;&#36827;&#34892;&#20107;&#21518;&#26377;&#25928;&#22320;&#32452;&#21512;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/joeljang/RLPHF&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#24182;&#29983;&#25104;&#23453;&#36149;&#27880;&#37322;&#65292;&#36866;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#12290;</title><link>http://arxiv.org/abs/2310.11541</link><description>&lt;p&gt;
MUST&amp;P-SRL: &#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#24182;&#29983;&#25104;&#23453;&#36149;&#27880;&#37322;&#65292;&#36866;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#31181;&#35821;&#35328;&#20013;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#65292;&#24182;&#35774;&#35745;&#19982;&#24378;&#21046;&#23545;&#40784;&#24037;&#20855;Montreal Forced Aligner&#65288;MFA&#65289;&#20860;&#23481;&#12290;&#22312;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#38899;&#26631;&#36716;&#24405;&#12289;&#37325;&#38899;&#26631;&#35760;&#21644;&#32479;&#19968;&#30340;&#33258;&#21160;&#38899;&#33410;&#21270;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#24320;&#28304;&#32452;&#20214;&#21644;&#36164;&#28304;&#26500;&#24314;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#38899;&#33410;&#21270;&#22810;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#30340;&#21333;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;CMU ARCTIC&#25968;&#25454;&#38598;&#30340;&#36716;&#24405;&#20013;&#65292;&#29983;&#25104;&#20102;&#26377;&#21161;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#30340;&#23453;&#36149;&#27880;&#37322;&#65292;&#22312;&#32447;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.11532</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#31454;&#20105;&#24615;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#19987;&#27880;&#20110;&#21333;&#19968;&#25968;&#25454;&#39046;&#22495;&#19981;&#21516;&#65292;LLMs&#30340;&#23835;&#36215;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#26082;&#33021;&#25512;&#21160;&#26368;&#20808;&#36827;&#30340;ASR&#24615;&#33021;&#30340;&#26497;&#38480;&#65292;&#21448;&#33021;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#21644;LLM&#25552;&#31034;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;ASR&#20551;&#35774;&#30340;N&#20010;&#26368;&#20339;&#21015;&#34920;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#65292;&#24182;&#36827;&#34892;&#32622;&#20449;&#24230;&#26816;&#26597;&#65307;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25552;&#31034;&#23545;&#31532;&#19968;&#38454;&#27573;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#32467;&#26524;&#36827;&#34892;ASR&#38169;&#35823;&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#31454;&#20105;&#24615;ASR&#31995;&#32479;&#65292;&#22312;WER&#19978;&#21462;&#24471;&#20102;10%~20%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11523</link><description>&lt;p&gt;
&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#21019;&#24847;&#20889;&#20316;&#65292;&#37117;&#38656;&#35201;&#32454;&#33268;&#20837;&#24494;&#30340;&#20027;&#35266;&#21028;&#26029;&#65292;&#36825;&#20123;&#21028;&#26029;&#22312;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#31639;&#27861;&#22312;&#27599;&#20010;&#32676;&#20307;&#19978;&#23545;&#40784;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#32780;&#35328;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#32676;&#20307;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#22312;GPO&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#25193;&#20805;&#22522;&#26412;LLM&#65292;&#29992;&#20110;&#39044;&#27979;&#32676;&#20307;&#23545;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22359;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22810;&#20010;&#32676;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#22312;&#19977;&#20010;&#20154;&#31867;&#24847;&#35265;&#36866;&#24212;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;GPO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#20351;&#29992;ROUGE&#24471;&#20998;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33021;&#21147;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.11520</link><description>&lt;p&gt;
&#33258;&#21160;&#26032;&#38395;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Automatic News Summerization. (arXiv:2310.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#20351;&#29992;ROUGE&#24471;&#20998;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33021;&#21147;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#38024;&#23545;&#21253;&#25324;&#26032;&#38395;&#25991;&#31456;&#22312;&#20869;&#30340;&#22823;&#22411;&#25991;&#26412;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#23545;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;ROUGE&#24471;&#20998;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;CNN-Daily Mail&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26032;&#38395;&#25991;&#31456;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#21442;&#32771;&#25688;&#35201;&#12290;&#35780;&#20272;&#20351;&#29992;ROUGE&#24471;&#20998;&#26469;&#35780;&#20272;&#29983;&#25104;&#25688;&#35201;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;&#22312;&#35780;&#20272;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#38598;&#25104;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33021;&#21147;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing is booming with its applications in the real world, one of which is Text Summarization for large texts including news articles. This research paper provides an extensive comparative evaluation of extractive and abstractive approaches for news text summarization, with an emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail dataset, which consists of news articles and human-generated reference summaries. The evaluation employs ROUGE scores to assess the efficacy and quality of generated summaries. After Evaluation, we integrate the best-performing models on a web application to assess their real-world capabilities and user experience.
&lt;/p&gt;</description></item><item><title>Self-RAG&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.11511</link><description>&lt;p&gt;
Self-RAG: &#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#23398;&#20064;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11511
&lt;/p&gt;
&lt;p&gt;
Self-RAG&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23427;&#20204;&#25152;&#21253;&#21547;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#22240;&#27492;&#24448;&#24448;&#20250;&#20135;&#29983;&#21547;&#26377;&#20107;&#23454;&#19981;&#20934;&#30830;&#24615;&#30340;&#21709;&#24212;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#22686;&#24378;LM&#30340;&#20020;&#26102;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19981;&#21152;&#36873;&#25321;&#22320;&#26816;&#32034;&#24182;&#32467;&#21512;&#19968;&#23450;&#25968;&#37327;&#30340;&#26816;&#32034;&#27573;&#33853;&#65292;&#32780;&#19981;&#32771;&#34385;&#26816;&#32034;&#26159;&#21542;&#24517;&#35201;&#25110;&#27573;&#33853;&#26159;&#21542;&#30456;&#20851;&#65292;&#20250;&#38477;&#20302;LM&#30340;&#22810;&#21151;&#33021;&#24615;&#25110;&#23548;&#33268;&#26080;&#25928;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Reflective Retrieval-Augmented Generation &#65288;Self-RAG&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;LM&#30340;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#24847;LM&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#26631;&#35760;&#65292;&#31216;&#20026;&#21453;&#24605;&#26631;&#35760;&#65292;&#29983;&#25104;&#21644;&#21453;&#24605;&#26816;&#32034;&#30340;&#27573;&#33853;&#21644;&#33258;&#36523;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;CoMPosT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#27169;&#25311;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20854;&#26159;&#21542;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.11501</link><description>&lt;p&gt;
CoMPosT: LLM&#27169;&#25311;&#20013;&#30340;&#28459;&#30011;&#34920;&#29616;&#29305;&#24449;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations. (arXiv:2310.11501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11501
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;CoMPosT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#27169;&#25311;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20854;&#26159;&#21542;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27169;&#25311;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#22312;&#31038;&#20250;&#31185;&#23398;&#23454;&#39564;&#21644;&#33286;&#35770;&#35843;&#26597;&#31561;&#24773;&#22659;&#20013;&#30340;&#21453;&#24212;&#65292;&#20197;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30830;&#23450;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;&#25110;&#35780;&#20272;&#36825;&#31181;LLM&#27169;&#25311;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;LLM&#27169;&#25311;&#26159;&#23545;&#20182;&#20204;&#25152;&#27169;&#25311;&#30340;&#20154;&#29289;&#36827;&#34892;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#34920;&#29616;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#20154;&#30340;&#22810;&#32500;&#24615;&#24182;&#24310;&#32493;&#20102;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoMPosT&#26694;&#26550;&#65292;&#29992;&#22235;&#20010;&#32500;&#24230;&#26469;&#25551;&#36848;LLM&#27169;&#25311;&#65306;&#35821;&#22659;&#12289;&#27169;&#22411;&#12289;&#35282;&#33394;&#21644;&#20027;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#34913;&#37327;&#24320;&#25918;&#24335;LLM&#27169;&#25311;&#23545;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#20004;&#20010;&#26631;&#20934;&#26469;&#23450;&#20041;&#65306;&#20010;&#24615;&#21270;&#21644;&#22840;&#24352;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;LLM&#27169;&#25311;&#24037;&#20316;&#20013;&#30340;&#22330;&#26223;&#20013;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;GPT-4&#65292;&#29305;&#23450;&#32676;&#20307;&#65288;&#25919;&#27835;&#21644;&#36793;&#32536;&#32676;&#20307;&#65289;&#21644;
&lt;/p&gt;
&lt;p&gt;
Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24314;&#27169;clickbait&#29616;&#35937;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11465</link><description>&lt;p&gt;
BaitBuster-Bangla:&#19968;&#20010;&#21253;&#21547;&#22810;&#29305;&#24449;&#21644;&#22810;&#27169;&#24577;&#20998;&#26512;&#30340;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;Clickbait&#26816;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24314;&#27169;clickbait&#29616;&#35937;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;YouTube API&#21644;Python&#32593;&#32476;&#33258;&#21160;&#21270;&#26694;&#26550;&#33258;&#21160;&#25910;&#38598;&#20102;253,070&#20010;&#25968;&#25454;&#28857;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;58&#20010;&#23391;&#21152;&#25289;&#35821;YouTube&#39057;&#36947;&#30340;&#21333;&#20010;&#35270;&#39057;&#30340;18&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#20998;&#31867;&#20026;&#20803;&#25968;&#25454;&#12289;&#20027;&#35201;&#20869;&#23481;&#12289;&#21442;&#19982;&#32479;&#35745;&#21644;&#26631;&#31614;&#12290;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#39044;&#22788;&#29702;&#65292;&#21435;&#22122;&#22768;&#12289;&#21435;&#37325;&#22797;&#21644;&#21435;&#20559;&#24046;&#65292;&#30830;&#20445;&#20102;&#26080;&#20559;&#20506;&#21644;&#21487;&#38752;&#30340;&#20998;&#26512;&#12290;&#20316;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#24378;&#22823;&#30340;&#23391;&#21152;&#25289;&#35821;clickbait&#35821;&#26009;&#24211;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20182;&#20204;&#24076;&#26395;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25512;&#36827;clickbait&#29616;&#35937;&#30340;&#24314;&#27169;&#12290;&#23427;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#20351;&#24471;&#21487;&#20197;&#23545;clickbait&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#20869;&#23481;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#35821;&#35328;&#32500;&#24230;&#65292;&#20197;&#24320;&#21457;&#20855;&#26377;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#26356;&#22797;&#26434;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a large multi-modal Bangla YouTube clickbait dataset consisting of 253,070 data points collected through an automated process using the YouTube API and Python web automation frameworks. The dataset contains 18 diverse features categorized into metadata, primary content, engagement statistics, and labels for individual videos from 58 Bangla YouTube channels. A rigorous preprocessing step has been applied to denoise, deduplicate, and remove bias from the features, ensuring unbiased and reliable analysis. As the largest and most robust clickbait corpus in Bangla to date, this dataset provides significant value for natural language processing and data science researchers seeking to advance modeling of clickbait phenomena in low-resource languages. Its multi-modal nature allows for comprehensive analyses of clickbait across content, user interactions, and linguistic dimensions to develop more sophisticated detection methods with cross-linguistic applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2310.10706</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#30340;&#33021;&#37327;&#65306;&#36890;&#36807;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#35270;&#35282;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#32034;&#20154;&#31867;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;LLMs&#36827;&#34892;&#20889;&#20316;&#65292;&#24182;&#20102;&#35299;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#21644;&#20449;&#20219;&#24230;&#65292;&#25105;&#20204;&#22312;LLM&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#24120;&#35265;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#24341;&#23548;&#31995;&#32479;&#65292;&#20174;&#31995;&#32479;&#36755;&#20986;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#21518;&#26399;&#32534;&#36753;&#36755;&#20986;&#65289;&#12290;&#23613;&#31649;LLMs&#21333;&#29420;&#21487;&#20197;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#20294;&#24179;&#22343;&#32780;&#35328;&#65292;&#20154;&#31867;&#30340;&#25511;&#21046;&#26159;&#38656;&#35201;&#30340;&#65292;&#20197;&#20462;&#22797;&#19981;&#21487;&#21462;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#21508;&#31181;&#20132;&#20114;&#26041;&#27861;&#20013;&#65292;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#22686;&#21152;&#20102;&#26368;&#22810;&#30340;&#25928;&#30410;&#65292;&#20195;&#20215;&#26368;&#20302;&#65288;&#26102;&#38388;&#21644;&#31934;&#21147;&#65289;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#21327;&#21161;&#24182;&#27809;&#26377;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#65292;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10520</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#35299;&#20915;&#20102;&#33719;&#21462;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;DST&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#22635;&#27133;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26356;&#26032;&#31574;&#30053;&#26469;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#38543;&#30528;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ParsingDST&#65292;&#19968;&#31181;&#26032;&#30340;In-Context Learning&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#26356;&#26032;&#31574;&#30053;&#29992;&#20110;&#38646;&#26679;&#26412;DST&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#23558;&#21407;&#22987;&#23545;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;JSON&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#29366;&#24577;&#26469;&#37325;&#26032;&#23450;&#20041;DST&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#22810;&#30340;&#27169;&#22359;&#26469;&#30830;&#20445;&#25991;&#26412;&#21040;JSON&#36807;&#31243;&#20013;&#26356;&#26032;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;DST&#26041;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27133;&#20934;&#30830;&#24230;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;ICL&#26041;&#27861;&#30456;&#27604;&#21576;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;MPT-7b-instruct, Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;text-davinci-003&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.10449</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;: MPT-7b-instruct&#12289;Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;MPT-7b-instruct, Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;text-davinci-003&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#29983;&#25104;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21319;&#25688;&#35201;&#25216;&#26415;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;MPT-7b-instruct&#65292;falcon-7b-instruct&#21644;OpenAI ChatGPT text-davinci-003&#27169;&#22411;&#65289;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35832;&#22914;&#21452;&#35821;&#35780;&#20272;&#34913;&#37327;&#65288;BLEU&#65289;&#20998;&#25968;&#65292;&#38754;&#21521;&#22238;&#24518;&#30340;&#35270;&#35282;&#35780;&#20272;&#65288;ROUGE&#65289;&#20998;&#25968;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#20998;&#25968;&#31561;&#24191;&#27867;&#25509;&#21463;&#30340;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;&#26681;&#25454;&#23454;&#39564;&#65292;text-davinci-003&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#26412;&#27425;&#30740;&#31350;&#28041;&#21450;CNN Daily Mail&#21644;XSum&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#20840;&#38754;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.09716</link><description>&lt;p&gt;
&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#30340;&#20449;&#24687;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09716
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#37325;&#20889;&#22312;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#29420;&#31435;&#24418;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20154;&#24037;&#37325;&#20889;&#30340;&#26597;&#35810;&#20316;&#20026;&#26631;&#31614;&#26469;&#35757;&#32451;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#37325;&#20889;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#20449;&#24687;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#37325;&#35201;&#29305;&#24615;&#26469;&#23450;&#20041;&#35268;&#33539;&#30340;&#37325;&#20889;&#65292;&#24182;&#23558;&#20854;&#20840;&#37096;&#32435;&#20837;&#25351;&#20196;&#20013;&#12290;&#27492;&#22806;&#65292;&#24403;&#21021;&#22987;&#26597;&#35810;&#37325;&#20889;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMs&#30340;&#37325;&#20889;&#32534;&#36753;&#22120;&#30340;&#35282;&#33394;&#65292;&#24418;&#25104;&#19968;&#20010;&#8220;&#37325;&#20889;-&#32534;&#36753;&#8221;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;LLMs&#30340;&#37325;&#20889;&#33021;&#21147;&#25552;&#28860;&#25104;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#37325;&#20889;&#24310;&#36831;&#12290;&#25105;&#20204;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can y
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;&#65292;&#23558;&#26641;&#32467;&#26500;&#25972;&#21512;&#21040;&#25968;&#23398;&#26041;&#31243;&#29983;&#25104;&#20013;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#39034;&#24207;&#26041;&#27861;&#24573;&#35270;&#24182;&#34892;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09619</link><description>&lt;p&gt;
&#25968;&#23398;&#26041;&#31243;&#29983;&#25104;&#30340;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;&#65292;&#23558;&#26641;&#32467;&#26500;&#25972;&#21512;&#21040;&#25968;&#23398;&#26041;&#31243;&#29983;&#25104;&#20013;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#39034;&#24207;&#26041;&#27861;&#24573;&#35270;&#24182;&#34892;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25968;&#23398;&#26041;&#31243;&#38656;&#35201;&#20934;&#30830;&#29702;&#35299;&#25968;&#23398;&#34920;&#36798;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#26631;&#35760;&#32423;&#29983;&#25104;&#21644;&#34920;&#36798;&#24335;&#32423;&#29983;&#25104;&#20004;&#31867;&#12290;&#21069;&#32773;&#23558;&#26041;&#31243;&#35270;&#20026;&#25968;&#23398;&#35821;&#35328;&#65292;&#25353;&#39034;&#24207;&#29983;&#25104;&#25968;&#23398;&#26631;&#35760;&#12290;&#34920;&#36798;&#24335;&#32423;&#26041;&#27861;&#36880;&#20010;&#29983;&#25104;&#27599;&#20010;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#34920;&#36798;&#24335;&#34920;&#31034;&#19968;&#20010;&#27714;&#35299;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#20043;&#38388;&#33258;&#28982;&#23384;&#22312;&#24182;&#34892;&#25110;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#24403;&#21069;&#30340;&#39034;&#24207;&#26041;&#27861;&#24573;&#35270;&#20102;&#36825;&#20123;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26641;&#32467;&#26500;&#25972;&#21512;&#21040;&#34920;&#36798;&#24335;&#32423;&#29983;&#25104;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;&#12290;&#20026;&#20102;&#29983;&#25104;&#19968;&#20010;&#20197;&#34920;&#36798;&#24335;&#20026;&#33410;&#28857;&#30340;&#26641;&#65292;&#25105;&#20204;&#37319;&#29992;&#36880;&#23618;&#24182;&#34892;&#35299;&#30721;&#31574;&#30053;&#65306;&#22312;&#27599;&#19968;&#23618;&#21516;&#26102;&#35299;&#30721;&#22810;&#20010;&#29420;&#31435;&#34920;&#36798;&#24335;&#65288;&#21494;&#33410;&#28857;&#65289;&#65292;&#24182;&#37325;&#22797;&#36880;&#23618;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;&#65292;&#20197;&#39034;&#24207;&#29983;&#25104;&#20381;&#36182;&#20110;&#20854;&#20182;&#34920;&#36798;&#24335;&#30340;&#29238;&#33410;&#28857;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Beside
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23545;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09238</link><description>&lt;p&gt;
BanglaNLP&#22312;BLP-2023&#20219;&#21153;2&#20013;&#30340;&#34920;&#29616;: &#23545;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;Transformer&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23545;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bangla&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;&#26469;&#33258;&#21360;&#24230;&#21644;&#23391;&#21152;&#25289;&#22269;&#30340;2.34&#20159;&#27597;&#35821;&#20351;&#29992;&#32773;&#12290;&#36825;&#31181;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#25317;&#26377;&#20016;&#23500;&#30340;&#25991;&#23398;&#20256;&#32479;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#35821;&#35328;&#29305;&#23450;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20854;&#35821;&#35328;&#20016;&#23500;&#24615;&#21644;&#21382;&#21490;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35821;&#38899;&#31038;&#21306;&#20013;&#65292;Bangla&#20173;&#34987;&#24402;&#31867;&#20026;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#25105;&#20204;&#22312;BLP&#30740;&#35752;&#20250;&#30340;&#20219;&#21153;2&#65288;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#24773;&#24863;&#20998;&#26512;&#65289;&#20013;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#36801;&#31227;&#23398;&#20064;&#30830;&#23454;&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;&#24403;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#36825;&#19968;&#28857;&#21464;&#24471;&#26126;&#26174;&#65292;&#32780;&#36825;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experiment with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetune a model which has already been finetuned on twitter data for sentiment analysis task and that finetuned model performs the best among all other models. We also perform a deta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26377;&#32447;&#30005;&#35270;&#33410;&#30446;&#25552;&#21450;&#30340;&#20027;&#39064;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#31435;&#22330;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#26469;&#34920;&#24449;&#20854;&#20559;&#35265;&#12290;&#22312;2020&#24180;&#30340;&#26377;&#32447;&#30005;&#35270;&#36716;&#24405;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#33410;&#30446;&#32858;&#31867;&#19982;&#33410;&#30446;&#25152;&#23646;&#30340;&#26377;&#32447;&#30005;&#35270;&#32593;&#32476;&#20445;&#25345;&#19968;&#33268;&#12290;&#25581;&#31034;&#20102;&#23458;&#35266;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#21644;&#34920;&#24449;&#38476;&#29983;&#23186;&#20307;&#29615;&#22659;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09166</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#26469;&#34920;&#24449;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Developing a Natural Language Understanding Model to Characterize Cable News Bias. (arXiv:2310.09166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26377;&#32447;&#30005;&#35270;&#33410;&#30446;&#25552;&#21450;&#30340;&#20027;&#39064;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#31435;&#22330;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#26469;&#34920;&#24449;&#20854;&#20559;&#35265;&#12290;&#22312;2020&#24180;&#30340;&#26377;&#32447;&#30005;&#35270;&#36716;&#24405;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#33410;&#30446;&#32858;&#31867;&#19982;&#33410;&#30446;&#25152;&#23646;&#30340;&#26377;&#32447;&#30005;&#35270;&#32593;&#32476;&#20445;&#25345;&#19968;&#33268;&#12290;&#25581;&#31034;&#20102;&#23458;&#35266;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#21644;&#34920;&#24449;&#38476;&#29983;&#23186;&#20307;&#29615;&#22659;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#20559;&#35265;&#22312;&#31038;&#20250;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20173;&#28982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#38752;&#20154;&#24037;&#36755;&#20837;&#21644;&#20027;&#35266;&#35780;&#20272;&#26469;&#26631;&#35760;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#26377;&#32447;&#30005;&#35270;&#30740;&#31350;&#26356;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#20154;&#24037;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#34920;&#24449;&#26377;&#32447;&#30005;&#35270;&#33410;&#30446;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20998;&#26512;&#25552;&#21450;&#30340;&#20027;&#39064;&#20197;&#21450;&#36890;&#36807;&#31435;&#22330;&#20998;&#26512;&#26469;&#35752;&#35770;&#36825;&#20123;&#20027;&#39064;&#30340;&#26041;&#24335;&#65292;&#20197;&#20415;&#23558;&#20855;&#26377;&#31867;&#20284;&#20559;&#35265;&#30340;&#33410;&#30446;&#36827;&#34892;&#32858;&#31867;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;2020&#24180;&#30340;&#26377;&#32447;&#30005;&#35270;&#36716;&#24405;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33410;&#30446;&#32858;&#31867;&#38543;&#26102;&#38388;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#22823;&#33268;&#23545;&#24212;&#20110;&#33410;&#30446;&#25152;&#23646;&#30340;&#26377;&#32447;&#30005;&#35270;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#25581;&#31034;&#20102;&#26410;&#26469;&#23458;&#35266;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#21644;&#34920;&#24449;&#38476;&#29983;&#23186;&#20307;&#29615;&#22659;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Media bias has been extensively studied by both social and computational sciences. However, current work still has a large reliance on human input and subjective assessment to label biases. This is especially true for cable news research. To address these issues, we develop an unsupervised machine learning method to characterize the bias of cable news programs without any human input. This method relies on the analysis of what topics are mentioned through Named Entity Recognition and how those topics are discussed through Stance Analysis in order to cluster programs with similar biases together. Applying our method to 2020 cable news transcripts, we find that program clusters are consistent over time and roughly correspond to the cable news network of the program. This method reveals the potential for future tools to objectively assess media bias and characterize unfamiliar media environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32467;&#26500;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;LLMs&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#19981;&#21516;&#35748;&#30693;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08172</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#30693;&#35782;&#32467;&#26500;&#65306;&#19968;&#31181;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach. (arXiv:2310.08172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32467;&#26500;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;LLMs&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#19981;&#21516;&#35748;&#30693;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19981;&#20165;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36824;&#23637;&#31034;&#20102;&#26234;&#33021;&#30340;&#28779;&#33457;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#20154;&#31867;&#32771;&#35797;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#25972;&#20307;&#30693;&#35782;&#32467;&#26500;&#30340;&#35748;&#30693;&#30740;&#31350;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#25945;&#32946;&#35786;&#26029;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;MoocRadar&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#24067;&#40065;&#22982;&#20998;&#31867;&#27861;&#36827;&#34892;&#32454;&#33268;&#27880;&#37322;&#30340;&#20154;&#31867;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25581;&#31034;LLMs&#30340;&#30693;&#35782;&#32467;&#26500;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#35843;&#26597;LLMs&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#20854;&#35748;&#30693;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#29031;&#20142;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#21152;&#26126;&#30830;&#21644;&#26377;&#25928;&#22320;&#20419;&#36827;LLMs&#30340;&#24320;&#21457;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21518;&#26524;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#35780;&#20272;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.07521</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#24615;&#35843;&#26597;&#65306;&#30693;&#35782;&#12289;&#26816;&#32034;&#21644;&#39046;&#22495;&#19987;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21518;&#26524;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#35780;&#20272;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20107;&#23454;&#24615;&#12290;&#30001;&#20110;LLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#24615;&#38382;&#39064;&#23450;&#20041;&#20026;LLMs&#20135;&#29983;&#19982;&#24050;&#30830;&#31435;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#30340;&#21547;&#20041;&#65292;&#31361;&#20986;&#20102;&#20107;&#23454;&#38169;&#35823;&#22312;LLMs&#36755;&#20986;&#20013;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#21518;&#26524;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#23384;&#20648;&#21644;&#22788;&#29702;&#20107;&#23454;&#30340;&#26426;&#21046;&#65292;&#23547;&#25214;&#20107;&#23454;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#38543;&#21518;&#36716;&#21521;&#35780;&#20272;LLM&#20107;&#23454;&#24615;&#30340;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20851;&#38190;&#25351;&#26631;&#12289;&#22522;&#20934;&#21644;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#20027;&#35201;&#30340;LLM&#37197;&#32622;&#65292;&#29420;&#31435;LLMs&#21644;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#30340;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#35814;&#32454;&#20171;&#32461;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential 
&lt;/p&gt;</description></item><item><title>Cognate Transformer&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#21644;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#22810;&#24207;&#21015;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#21382;&#21490;&#35821;&#35328;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.07487</link><description>&lt;p&gt;
Cognate Transformer&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#21644;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction. (arXiv:2310.07487v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07487
&lt;/p&gt;
&lt;p&gt;
Cognate Transformer&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#21644;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#22522;&#20110;&#22810;&#24207;&#21015;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35745;&#31639;&#21382;&#21490;&#35821;&#35328;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#37325;&#24314;&#26159;&#21382;&#21490;&#35821;&#35328;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#36890;&#36807;&#35266;&#23519;&#23376;&#35821;&#35328;&#30340;&#21516;&#28304;&#35789;&#27719;&#65292;&#30830;&#23450;&#31062;&#20808;&#35821;&#35328;&#30340;&#21407;&#22987;&#35789;&#27719;&#12290;&#35745;&#31639;&#21382;&#21490;&#35821;&#35328;&#23398;&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23398;&#20064;&#29616;&#26377;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;MSA Transformer&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#37325;&#24314;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#21629;&#21517;&#20026;Cognate Transformer&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21363;&#21516;&#28304;&#21453;&#23556;&#39044;&#27979;&#65292;&#26681;&#25454;&#20854;&#20182;&#23376;&#35821;&#35328;&#30340;&#21516;&#28304;&#35789;&#27719;&#26469;&#39044;&#27979;&#23376;&#35821;&#35328;&#20013;&#30340;&#21453;&#23556;&#35789;&#27719;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction, where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model o
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#29983;&#25104;&#26356;&#20855;&#36830;&#36143;&#24615;&#12289;&#36259;&#21619;&#24615;&#21644;&#33258;&#28982;&#24615;&#30340;&#21465;&#20107;&#12290;</title><link>http://arxiv.org/abs/2310.05295</link><description>&lt;p&gt;
&#20351;&#29992;&#38382;&#39064;-&#31572;&#26696;&#35745;&#21010;&#30340;&#35270;&#35273;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Visual Storytelling with Question-Answer Plans. (arXiv:2310.05295v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#29983;&#25104;&#26356;&#20855;&#36830;&#36143;&#24615;&#12289;&#36259;&#21619;&#24615;&#21644;&#33258;&#28982;&#24615;&#30340;&#21465;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21465;&#20107;&#26088;&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#20107;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#30528;&#37325;&#20110;&#25913;&#36827;&#22270;&#20687;&#24207;&#21015;&#30340;&#34920;&#24449;&#65292;&#20363;&#22914;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#28304;&#25110;&#20808;&#36827;&#30340;&#22270;&#32467;&#26500;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#24448;&#24448;&#37325;&#22797;&#12289;&#19981;&#21512;&#36923;&#36753;&#19988;&#32570;&#20047;&#32454;&#33410;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#36798;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35745;&#21010;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22270;&#20687;&#24207;&#21015;&#36716;&#21270;&#20026;&#35270;&#35273;&#21069;&#32512;&#65292;&#21363;&#36830;&#32493;&#23884;&#20837;&#30340;&#24207;&#21015;&#65292;&#21487;&#20197;&#30001;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#12290;&#23427;&#36824;&#21033;&#29992;&#19968;&#31995;&#21015;&#38382;&#39064;-&#31572;&#26696;&#23545;&#20316;&#20026;&#34013;&#22270;&#35745;&#21010;&#65292;&#36873;&#25321;&#26174;&#33879;&#30340;&#35270;&#35273;&#27010;&#24565;&#24182;&#30830;&#23450;&#23427;&#20204;&#22914;&#20309;&#32452;&#21512;&#25104;&#21465;&#20107;&#12290;&#22312;VIST&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#22522;&#20110;&#34013;&#22270;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25925;&#20107;&#19982;&#31454;&#20105;&#22522;&#20934;&#21644;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#26356;&#36830;&#36143;&#65292;&#26356;&#26377;&#36259;&#65292;&#26356;&#33258;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crystal&#30340;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#20869;&#30465;&#30693;&#35782;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04921</link><description>&lt;p&gt;
Crystal: &#20197;&#33258;&#25105;&#21453;&#39304;&#20026;&#22686;&#24378;&#30340;&#20869;&#30465;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crystal&#30340;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#20869;&#30465;&#30693;&#35782;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20854;&#20013;&#25512;&#29702;&#36807;&#31243;&#30340;&#22522;&#30784;&#30693;&#35782;&#26126;&#30830;&#34920;&#36798;&#21644;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#29616;&#65292;&#21253;&#25324;"&#24605;&#32500;&#38142;"&#21450;&#20854;&#21464;&#31181;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#24120;&#35782;&#25512;&#29702;&#20013;&#25152;&#38656;&#30340;&#20869;&#30465;&#24615;&#36136;&#65292;&#20063;&#26410;&#33021;&#35299;&#37322;&#30693;&#35782;&#29983;&#25104;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120; Crystal&#12290;&#20026;&#20102;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#65292;&#23427;&#39318;&#20808;&#20869;&#30465;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#38472;&#36848;&#65292;&#28982;&#21518;&#22522;&#20110;&#20808;&#21069;&#20869;&#30465;&#30340;&#30693;&#35782;&#36827;&#34892;&#30693;&#24773;&#39044;&#27979;&#12290;&#27169;&#22411;&#30340;&#30693;&#35782;&#20869;&#30465;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#27169;&#24335;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35843;&#25972;&#65292;&#20854;&#20013;&#22870;&#21169;&#26469;&#33258;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including "chain-of-thought" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#35843;&#26597;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#28508;&#22312;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00313</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;: &#23545;&#34920;&#31034;&#30340;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#24335;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations. (arXiv:2310.00313v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#35843;&#26597;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#28508;&#22312;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#20219;&#21153;&#31034;&#20363;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25913;&#36827;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;Llama-2 70B&#21644;Vicuna 13B&#20013;&#30340;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#30340;&#21464;&#21270;&#20197;&#21450;&#36825;&#20123;&#21464;&#21270;&#22914;&#20309;&#35843;&#35299;&#34892;&#20026;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#22914;&#34920;&#31034;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#25506;&#27979;&#21644;&#27880;&#24847;&#21147;&#27604;&#29575;&#20998;&#26512;&#65288;ARA&#65292;&#34913;&#37327;&#20851;&#27880;&#30456;&#20851;&#19982;&#26080;&#20851;&#20449;&#24687;&#30340;&#27604;&#29575;&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#20855;&#26377;&#26465;&#20214;&#20043;&#38388;&#20808;&#39564;&#20851;&#31995;&#30340;&#20219;&#21153;&#65306;&#38405;&#35835;&#29702;&#35299;&#65292;&#32447;&#24615;&#22238;&#24402;&#21644;&#23545;&#25239;&#25552;&#31034;&#27880;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#34920;&#31034;&#20013;&#39044;&#26399;&#30456;&#20284;&#24615;&#30340;&#20551;&#35774;&#65292;&#20197;&#30740;&#31350;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#20013;&#30340;&#28508;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#25351;&#26631;&#23545;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#30340;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#24573;&#30053;&#20102;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.12444</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#25928;&#26524;&#30340;&#37327;&#21270;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#25351;&#26631;&#23545;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#30340;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#24573;&#30053;&#20102;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23558;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24739;&#32773;&#25252;&#29702;&#36716;&#21464;&#20026;&#26356;&#20010;&#24615;&#21270;&#12289;&#39640;&#25928;&#21644;&#31215;&#26497;&#30340;&#36807;&#31243;&#65292;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#26041;&#24335;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#20114;&#21160;&#23545;&#35805;&#27169;&#22411;&#65292;&#24456;&#21487;&#33021;&#25512;&#21160;&#21307;&#30103;&#20445;&#20581;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#36716;&#22411;&#12290;&#36890;&#36807;&#25552;&#20379;&#35786;&#26029;&#12289;&#20010;&#24615;&#21270;&#29983;&#27963;&#26041;&#24335;&#24314;&#35758;&#21644;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#31561;&#21508;&#31181;&#26381;&#21153;&#65292;&#30446;&#26631;&#26159;&#22823;&#24133;&#24230;&#25552;&#39640;&#24739;&#32773;&#30340;&#20581;&#24247;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#21307;&#30103;&#24212;&#29992;&#30340;&#29983;&#21629;&#20851;&#38190;&#24615;&#35201;&#27714;&#24314;&#31435;&#32479;&#19968;&#20840;&#38754;&#30340;&#23545;&#35805;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#24050;&#26377;&#30340;&#38024;&#23545;&#21508;&#31181;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#29702;&#35299;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#21450;&#20854;&#22312;&#20419;&#36827;&#24739;&#32773;&#31119;&#31049;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25351;&#26631;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-ce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08813</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#23545;&#20110;&#25581;&#31034;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#30740;&#31350;&#29983;&#29289;&#21151;&#33021;&#21644;&#22797;&#26434;&#30142;&#30149;&#30340;&#22522;&#26412;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#28304;&#30340;&#31574;&#21010;&#29983;&#29289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#23436;&#25972;&#19988;&#32500;&#25252;&#24037;&#20316;&#32321;&#37325;&#65292;&#22240;&#27492;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#20174;&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36825;&#20123;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12289;&#36890;&#36335;&#21644;&#22522;&#22240;&#35843;&#25511;&#20851;&#31995;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#38142;&#25509;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25345;&#32493;&#23618;&#29305;&#23450;&#24494;&#35843;&#21644;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#26469;&#25913;&#21892;&#24503;&#35821;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#36739;&#23567;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07280</link><description>&lt;p&gt;
&#22238;&#25918;&#20197;&#22238;&#24518;&#65306;&#38024;&#23545;&#24503;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#25345;&#32493;&#23618;&#29305;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25345;&#32493;&#23618;&#29305;&#23450;&#24494;&#35843;&#21644;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#26469;&#25913;&#21892;&#24503;&#35821;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#36739;&#23567;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#22312;&#24341;&#20837;&#26080;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#35757;&#32451;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#25913;&#36827;&#20173;&#28982;&#20165;&#38480;&#20110;&#26576;&#20123;&#35821;&#35328;&#21644;&#35828;&#35805;&#32773;&#12290;&#36801;&#31227;&#23398;&#20064;&#20351;&#24471;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#19981;&#20165;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36824;&#21253;&#25324;&#26356;&#29305;&#23450;&#30340;&#35828;&#35805;&#32773;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#23545;&#26032;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#21407;&#22987;&#39046;&#22495;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;ASR&#27169;&#22411;&#22312;&#36739;&#23567;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#26377;&#22810;&#22909;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#30340;&#24503;&#35821;&#39640;&#32423;&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#65288;SVC-de&#65289;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20923;&#32467;&#27169;&#22411;&#30340;&#37096;&#20998;&#26469;&#20445;&#30041;&#22810;&#23569;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#21152;ASR&#27169;&#22411;&#23545;&#24494;&#35843;&#39046;&#22495;&#20043;&#22806;&#30340;&#35789;&#27719;&#21644;&#35828;&#35805;&#32773;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24212;&#29992;&#32463;&#39564;&#22238;&#25918;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for conti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15895</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#65306;&#22810;&#26679;&#24615;&#21644;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#32487;&#25215;&#20102;LLM&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#23646;&#24615;&#30340;&#25552;&#31034;(&#20363;&#22914;&#25351;&#23450;&#38271;&#24230;&#21644;&#39118;&#26684;&#31561;&#23646;&#24615;)&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#26377;&#28508;&#21147;&#20135;&#29983;&#22810;&#26679;&#21644;&#24402;&#22240;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#24615;&#21270;&#25552;&#31034;&#22312;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#39033;&#21253;&#25324;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#31561;&#20851;&#38190;&#26041;&#38754;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#39318;&#20808;&#65292;&#31995;&#32479;&#24615;&#20559;&#24046;&#22312;&#29983;&#25104;&#25968;&#25454;&#20013;&#23384;&#22312;&#65307;&#20854;&#27425;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65307;&#26368;&#21518;&#65292;&#36827;&#34892;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14577</link><description>&lt;p&gt;
&#24046;&#24322;&#24615;&#36974;&#25377;&#65306;&#36873;&#25321;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36974;&#25377;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#65292;&#29305;&#21035;&#26159;&#36974;&#25377;&#39044;&#27979;&#30446;&#26631;&#30340;&#30446;&#26631;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#38543;&#26426;&#22320;&#36827;&#34892;&#26631;&#35760;&#21644;&#36974;&#25377;&#65292;&#32780;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#24378;&#28872;&#30340;&#30452;&#35273;&#35748;&#20026;&#65292;&#20915;&#23450;&#20160;&#20040;&#38656;&#35201;&#36974;&#25377;&#21487;&#20197;&#23454;&#36136;&#24615;&#22320;&#25913;&#21892;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24046;&#24322;&#36974;&#25377;(Difference-Masking)&#65292;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#36974;&#25377;&#20160;&#20040;&#30340;&#26041;&#27861;&#65292;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#23454;&#29616;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24046;&#24322;&#36974;&#25377;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#20219;&#21153;&#30340;&#32487;&#32493;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#20248;&#20110;&#22522;&#32447;&#12290;&#24046;&#24322;&#24615;&#36974;&#25377;&#30340;&#36328;&#20219;&#21153;&#36866;&#29992;&#24615;&#25903;&#25345;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#39044;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#37325;&#26032;&#26500;&#26550;&#20026;&#32534;&#31243;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#33021;&#22815;&#35299;&#20915;&#26102;&#25928;&#24615;&#38382;&#39064;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.14221</link><description>&lt;p&gt;
&#23558;&#38382;&#39064;&#22238;&#31572;&#20316;&#20026;&#35299;&#20915;&#26102;&#25928;&#38382;&#39064;&#30340;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Question Answering as Programming for Solving Time-Sensitive Questions. (arXiv:2305.14221v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#37325;&#26032;&#26500;&#26550;&#20026;&#32534;&#31243;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#33021;&#22815;&#35299;&#20915;&#26102;&#25928;&#24615;&#38382;&#39064;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#25105;&#20204;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#30340;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#65292;&#24403;&#38382;&#39064;&#30340;&#26102;&#38388;&#38480;&#21046;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#31572;&#26696;&#21487;&#33021;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26174;&#31034;&#20986;&#22312;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#30340;&#26174;&#33879;&#26234;&#33021;&#65292;&#32780;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#19978;&#36848;&#38382;&#39064;&#20173;&#28982;&#23545;&#29616;&#26377;LLM&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;LLM&#26080;&#27861;&#22522;&#20110;&#34920;&#38754;&#32423;&#25991;&#26412;&#35821;&#20041;&#36827;&#34892;&#20005;&#26684;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#19981;&#26159;&#35201;&#27714;LLM&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#26159;&#23558;&#8220;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#8221;&#37325;&#26032;&#26500;&#26550;&#20026;&#8220;&#32534;&#31243;&#20219;&#21153;&#8221;&#65288;QAaP&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;LLM&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25105;&#20204;&#21162;&#21147; harness LLM models to craft programs that can solve time-sensitive questions.
&lt;/p&gt;
&lt;p&gt;
Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task $\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, by leveraging modern LLMs' superior capability in understanding both natural language and programming language, we endeavor to harne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#35328;&#25968;&#25454;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21333;&#35821;&#35328;&#25968;&#25454;&#36890;&#24120;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#20294;&#27169;&#22411;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#23481;&#24525;&#24615;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#12290;&#22238;&#35793;&#22312;&#25968;&#25454;&#28304;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;&#21435;&#22122;&#33258;&#32534;&#30721;&#30340;&#25928;&#26524;&#19981;&#22914;&#20808;&#21069;&#25253;&#21578;&#30340;&#22909;&#12290;&#35268;&#27169;&#23545;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.14124</link><description>&lt;p&gt;
&#21333;&#35821;&#35328;&#25968;&#25454;&#20309;&#26102;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#65306;&#39046;&#22495;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale. (arXiv:2305.14124v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#35328;&#25968;&#25454;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21333;&#35821;&#35328;&#25968;&#25454;&#36890;&#24120;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#20294;&#27169;&#22411;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#23481;&#24525;&#24615;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#12290;&#22238;&#35793;&#22312;&#25968;&#25454;&#28304;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;&#21435;&#22122;&#33258;&#32534;&#30721;&#30340;&#25928;&#26524;&#19981;&#22914;&#20808;&#21069;&#25253;&#21578;&#30340;&#22909;&#12290;&#35268;&#27169;&#23545;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#26159;&#36890;&#36807;&#28151;&#21512;&#24179;&#34892;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#32763;&#35793;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#23545;&#20110;&#21253;&#21547;&#21333;&#35821;&#35328;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#27861;&#30340;&#34920;&#29616;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21435;&#22122;&#33258;&#32534;&#30721;&#65288;DAE&#65289;&#21644;&#22238;&#35793;&#65288;BT&#65289;&#22312;&#19981;&#21516;&#25968;&#25454;&#26465;&#20214;&#21644;&#27169;&#22411;&#35268;&#27169;&#19979;&#23545;MMT&#30340;&#24433;&#21709;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#24182;&#32771;&#34385;&#20102;&#35768;&#22810;&#21333;&#35821;&#35328;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#39046;&#22495;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21333;&#35821;&#35328;&#25968;&#25454;&#36890;&#24120;&#26377;&#21161;&#20110;MMT&#65292;&#20294;&#27169;&#22411;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#23481;&#24525;&#24615;&#20986;&#20046;&#24847;&#26009;&#22320;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#12290;&#24403;&#24179;&#34892;&#12289;&#21333;&#35821;&#35328;&#21644;&#27979;&#35797;&#25968;&#25454;&#28304;&#30456;&#20284;&#26102;&#65292;&#22238;&#35793;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;DAE&#30340;&#25928;&#26524;&#19981;&#22914;&#20808;&#21069;&#25253;&#21578;&#30340;&#22909;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35268;&#27169;&#65288;&#20174;90M&#21040;1.6B&#21442;&#25968;&#65289;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23427;&#23545;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods
&lt;/p&gt;</description></item><item><title>MADNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26041;&#23545;&#35805;&#29983;&#25104;&#20013;&#26368;&#22823;&#21270;&#22320;&#22336;&#25512;&#26029;&#26399;&#26395;&#20540;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#28508;&#22312;&#36793;&#26469;&#30830;&#20445;&#23545;&#35805;&#29255;&#27573;&#20043;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#35299;&#20915;&#22320;&#22336;&#26631;&#31614;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12733</link><description>&lt;p&gt;
MADNet: &#22810;&#26041;&#23545;&#35805;&#29983;&#25104;&#20013;&#26368;&#22823;&#21270;&#22320;&#22336;&#25512;&#26029;&#26399;&#26395;&#20540;
&lt;/p&gt;
&lt;p&gt;
MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation. (arXiv:2305.12733v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12733
&lt;/p&gt;
&lt;p&gt;
MADNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26041;&#23545;&#35805;&#29983;&#25104;&#20013;&#26368;&#22823;&#21270;&#22320;&#22336;&#25512;&#26029;&#26399;&#26395;&#20540;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#28508;&#22312;&#36793;&#26469;&#30830;&#20445;&#23545;&#35805;&#29255;&#27573;&#20043;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#35299;&#20915;&#22320;&#22336;&#26631;&#31614;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26041;&#23545;&#35805;&#36827;&#34892;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#21644;&#22270;&#24418;&#21270;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36807;&#22810;&#22320;&#20381;&#36182;&#20110;&#24517;&#35201;&#30340;&#22320;&#22336;&#26631;&#31614;&#65292;&#24182;&#19988;&#21482;&#33021;&#24212;&#29992;&#20110;&#27599;&#20010;&#35805;&#35821;&#37117;&#24517;&#39035;&#26631;&#35760;&#26377;&#22320;&#22336;&#26631;&#31614;&#30340;&#29702;&#24819;&#24773;&#20917;&#12290;&#20026;&#20102;&#30740;&#31350;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#24120;&#35265;&#30340;&#22320;&#22336;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MADNet&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#21270;&#22320;&#22336;&#25512;&#26029;&#26399;&#26395;&#20540;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29983;&#25104;&#12290;&#22312;&#32570;&#23569;&#23569;&#37327;&#22320;&#22336;&#26631;&#31614;&#30340;&#22810;&#26041;&#23545;&#35805;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#26500;&#24314;&#19968;&#20010;&#36830;&#36143;&#30340;&#23545;&#35805;&#22270;&#65292;&#32780;&#21482;&#33021;&#24418;&#25104;&#20960;&#20010;&#29420;&#31435;&#30340;&#23545;&#35805;&#30862;&#29255;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#23545;&#35805;&#30862;&#29255;&#20043;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#39069;&#22806;&#31867;&#22411;&#30340;&#28508;&#22312;&#36793;&#20197;&#23436;&#25104;&#19968;&#20010;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23545;&#37027;&#20123;&#27809;&#26377;&#22320;&#22336;&#26631;&#31614;&#30340;&#35805;&#35821;&#36827;&#34892;&#36793;&#31867;&#22411;&#30456;&#20851;&#30340;&#28040;&#24687;&#20256;&#36882;&#30340;&#20248;&#21270;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling multi-party conversations (MPCs) with graph neural networks has been proven effective at capturing complicated and graphical information flows. However, existing methods rely heavily on the necessary addressee labels and can only be applied to an ideal setting where each utterance must be tagged with an addressee label. To study the scarcity of addressee labels which is a common issue in MPCs, we propose MADNet that maximizes addressee deduction expectation in heterogeneous graph neural networks for MPC generation. Given an MPC with a few addressee labels missing, existing methods fail to build a consecutively connected conversation graph, but only a few separate conversation fragments instead. To ensure message passing between these conversation fragments, four additional types of latent edges are designed to complete a fully-connected graph. Besides, to optimize the edge-type-dependent message passing for those utterances without addressee labels, an Expectation-Maximization
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#21477;&#23376;&#23545;&#40784;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.12532</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilingual Simplification of Medical Texts. (arXiv:2305.12532v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#21477;&#23376;&#23545;&#40784;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20135;&#29983;&#22797;&#26434;&#25991;&#26412;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#39033;&#20219;&#21153;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#26032;&#30340;&#21307;&#23398;&#21457;&#29616;&#36890;&#24120;&#36890;&#36807;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#30340;&#25991;&#31456;&#36827;&#34892;&#20256;&#25773;&#12290;&#36825;&#20026;&#23547;&#27714;&#26368;&#26032;&#21307;&#23398;&#21457;&#29616;&#20449;&#24687;&#30340;&#26222;&#36890;&#20154;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#20581;&#24247;&#32032;&#20859;&#30340;&#25552;&#39640;&#12290;&#22823;&#37096;&#20998;&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#30340;&#29616;&#26377;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#23548;&#33268;&#36825;&#20123;&#35777;&#25454;&#21482;&#33021;&#29992;&#19968;&#31181;&#35821;&#35328;&#65288;&#36890;&#24120;&#26159;&#33521;&#35821;&#65289;&#25552;&#20379;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#22235;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#27874;&#26031;&#35821;&#65289;&#30340;&#21477;&#23376;&#23545;&#40784;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MultiCochrane, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages, with extensive human assessments and analyses. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570; PGIM &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992; ChatGPT &#20316;&#20026;&#38544;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312; MNER &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12212</link><description>&lt;p&gt;
Prompt ChatGPT &#22312; MNER &#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110; ChatGPT &#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#30340;&#25913;&#36827;&#24335;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT. (arXiv:2305.12212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570; PGIM &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992; ChatGPT &#20316;&#20026;&#38544;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312; MNER &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22270;&#20687;&#30340;&#32447;&#32034;&#26469;&#22686;&#24378;&#25991;&#26412;&#23454;&#20307;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26368;&#22823;&#21270;&#22270;&#20687;&#30456;&#20851;&#20449;&#24687;&#30340;&#21033;&#29992;&#25110;&#23558;&#22806;&#37096;&#30693;&#35782;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#20013;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#21521;&#27169;&#22411;&#25552;&#20379;&#30456;&#20851;&#22806;&#37096;&#30693;&#35782;&#30340;&#24517;&#35201;&#24615;&#65292;&#35201;&#20040;&#24341;&#20837;&#30340;&#22806;&#37096;&#30693;&#35782;&#23384;&#22312;&#37325;&#22797;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#31216;&#20026; Prompt ChatGPT In MNER (PGIM)&#12290;&#25105;&#20204;&#21033;&#29992; ChatGPT &#20316;&#20026;&#38544;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312; MNER &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing research in this domain has primarily focused on maximizing the utilization of potentially relevant information in images or incorporating external knowledge from explicit knowledge bases (KBs). However, these methods either neglect the necessity of providing the model with relevant external knowledge, or the retrieved external knowledge suffers from high redundancy. To address these problems, we propose a conceptually simple two-stage framework called Prompt ChatGPT In MNER (PGIM) in this paper. We leverage ChatGPT as an implicit knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks. Specifically, we first utilize a Multimodal Similar Example Awareness module to select suitable examples from a small number of manually annotated samples. These examples are then integrated into a form
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMPSITION&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;CG&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12169</link><description>&lt;p&gt;
&#23398;&#20064;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#20197;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization. (arXiv:2305.12169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMPSITION&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;CG&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;Seq2Seq&#65289;&#27169;&#22411;&#22312;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#20219;&#21153;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#31995;&#32479;&#24615;&#22320;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#24050;&#30693;&#32452;&#20214;&#32452;&#21512;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#38459;&#30861;CG&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#32534;&#30721;&#22120;&#26368;&#19978;&#23618;&#30340;&#34920;&#31034;&#26159;&#32416;&#32544;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#34987;&#19981;&#36866;&#24403;&#22320;&#25197;&#26354;&#20102;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#22320;&#32452;&#21512;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#65292;&#23601;&#20687;&#20154;&#31867;&#25152;&#20570;&#30340;&#37027;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#20182;&#20204;&#21457;&#29616;&#30340;&#34920;&#31034;&#32416;&#32544;&#38382;&#39064;&#19981;&#20840;&#38754;&#65292;&#24182;&#36827;&#19968;&#27493;&#20551;&#35774;&#20256;&#36882;&#21040;&#19981;&#21516;&#35299;&#30721;&#22120;&#23618;&#30340;&#28304;&#38190;&#20540;&#34920;&#31034;&#20063;&#26159;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#30452;&#35273;&#21644;&#21463;&#20154;&#31867;CG&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COMPSITION&#65288;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#20197;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;CG&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#12290;COMPSITION&#36890;&#36807;&#20998;&#21035;&#24314;&#27169;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20960;&#20309;&#34920;&#31034;&#27169;&#22359;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#65292;&#26174;&#24335;&#22320;&#32452;&#21512;&#32534;&#30721;&#22120;&#30340;&#26368;&#19978;&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COMPSITION&#22312;&#21512;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;CG&#20219;&#21153;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that sequence-to-sequence (Seq2Seq) models are limited in solving the compositional generalization (CG) tasks, failing to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing semantic information at token-level, rather than composing the syntactic and semantic representations of sequences appropriately as humans do. In addition, we consider the representation entanglement problem they found is not comprehensive, and further hypothesize that source keys and values representations passing into different decoder layers are also entangled. Staring from this intuition and inspired by humans' strategies for CG, we propose COMPSITION (Compose Syntactic and Seman
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20272;&#20102;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#19979;&#33521;&#25991;&#25991;&#26412;&#30340;&#27010;&#29575;&#25552;&#20986;&#30340;&#29109;&#29575;&#24658;&#23450;&#21407;&#29702;&#65292;&#26410;&#33021;&#25214;&#21040;&#26126;&#26174;&#30340;&#25903;&#25345;&#29109;&#29575;&#24658;&#23450;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.12084</link><description>&lt;p&gt;
&#37325;&#35775;&#25991;&#26412;&#29109;&#29575;&#24658;&#23450;
&lt;/p&gt;
&lt;p&gt;
Revisiting Entropy Rate Constancy in Text. (arXiv:2305.12084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20272;&#20102;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#19979;&#33521;&#25991;&#25991;&#26412;&#30340;&#27010;&#29575;&#25552;&#20986;&#30340;&#29109;&#29575;&#24658;&#23450;&#21407;&#29702;&#65292;&#26410;&#33021;&#25214;&#21040;&#26126;&#26174;&#30340;&#25903;&#25345;&#29109;&#29575;&#24658;&#23450;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#65288;UID&#65289;&#20551;&#35828;&#34920;&#26126;&#65292;&#20154;&#31867;&#20542;&#21521;&#20110;&#22312;&#35805;&#35821;&#25110;&#35805;&#35821;&#20013;&#22823;&#33268;&#22343;&#21248;&#20998;&#24067;&#20449;&#24687;&#12290;&#25903;&#25345;UID&#20551;&#35828;&#30340;&#26089;&#26399;&#35777;&#25454;&#26469;&#33258;Genzel&#65286;Charniak&#65288;2002&#65289;&#65292;&#20182;&#20204;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#19979;&#33521;&#25991;&#25991;&#26412;&#30340;&#27010;&#29575;&#25552;&#20986;&#20102;&#29109;&#29575;&#24658;&#23450;&#21407;&#29702;&#12290;&#26412;&#25991;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20272;Genzel&#65286;Charniak&#65288;2002&#65289;&#30340;&#35828;&#27861;&#65292;&#26410;&#33021;&#25214;&#21040;&#26126;&#26174;&#30340;&#25903;&#25345;&#29109;&#29575;&#24658;&#23450;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35821;&#35328;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#20551;&#35828;&#21644;&#26356;&#24191;&#27867;&#30340;&#26377;&#25928;&#20256;&#25773;&#35821;&#35328;&#29702;&#35770;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The uniform information density (UID) hypothesis states that humans tend to distribute information roughly evenly across an utterance or discourse. Early evidence in support of the UID hypothesis came from Genzel &amp; Charniak (2002), which proposed an entropy rate constancy principle based on the probability of English text under n-gram language models. We re-evaluate the claims of Genzel &amp; Charniak (2002) with neural language models, failing to find clear evidence in support of entropy rate constancy. We conduct a range of experiments across datasets, model sizes, and languages and discuss implications for the uniform information density hypothesis and linguistic theories of efficient communication more broadly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.11828</link><description>&lt;p&gt;
LLM&#22312;&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#23545;&#20110;&#21046;&#23450;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#21046;&#20316;&#36825;&#26679;&#30340;&#32508;&#36848;&#24456;&#36153;&#21147;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#24456;&#22810;&#38382;&#39064;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#32508;&#36848;&#65292;&#21363;&#20351;&#36825;&#20123;&#32508;&#36848;&#21487;&#29992;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#21487;&#33021;&#24050;&#32463;&#36807;&#26102;&#12290;&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#38271;&#31687;&#25991;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#30340;&#35825;&#20154;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26500;&#25110;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#65292;LLM&#26377;&#26102;&#20250;&#20135;&#29983;&#19981;&#20934;&#30830;&#65288;&#29978;&#33267;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65289;&#30340;&#25991;&#26412;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#20351;LLM&#22312;&#26368;&#22909;&#24773;&#20917;&#19979;&#26080;&#27861;&#20351;&#29992;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#21361;&#38505;&#12290;&#23545;&#20110;LLM&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#30340;&#22823;&#22810;&#25968;&#35752;&#35770;&#19982;&#20855;&#20307;&#24212;&#29992;&#33073;&#31163;&#20102;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23450;&#24615;&#25551;&#36848;LLM&#22312;&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;16&#20301;&#22269;&#38469;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11595</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;&#65306;&#36890;&#36807;&#36777;&#35770;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#36890;&#35782;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21508;&#31181;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#25506;&#32034;&#20004;&#20010;&#25110;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#21644;&#31934;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#22312;7&#20010;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;LLMs&#19981;&#20165;&#36890;&#36807;&#22949;&#21327;&#21644;&#21453;&#39539;&#21464;&#24471;&#26356;&#20855;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.11508</link><description>&lt;p&gt;
&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20026;&#24739;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#38382;&#31572;&#39046;&#22495;&#20855;&#26377;&#26480;&#20986;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20855;&#22791;&#20102;&#23545;&#24120;&#35782;&#30340;&#20016;&#23500;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35786;&#26029;&#31574;&#30053;&#65292;LLMs&#26080;&#27861;&#30452;&#25509;&#29992;&#20110;&#35786;&#26029;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;&#21478;&#19968;&#31181;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#24320;&#21457;&#19968;&#20010;&#25554;&#20214;&#65292;&#36171;&#20104;LLMs&#25191;&#34892;&#21307;&#30103;&#23545;&#35805;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PlugMed&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#20004;&#20010;&#27169;&#22359;&#20419;&#36827;&#20102;LLMs&#30340;&#24688;&#24403;&#23545;&#35805;&#21160;&#20316;&#65306;&#25552;&#31034;&#29983;&#25104;&#65288;PG&#65289;&#27169;&#22359;&#21644;&#22238;&#22797;&#25490;&#21517;&#65288;RR&#65289;&#27169;&#22359;&#12290;PG&#27169;&#22359;&#26088;&#22312;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35282;&#24230;&#25429;&#33719;&#23545;&#35805;&#20449;&#24687;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#24230;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue systems aim to provide accurate answers to patients, necessitating specific domain knowledge. Recent advancements in Large Language Models (LLMs) have demonstrated their exceptional capabilities in the medical Q&amp;A domain, indicating a rich understanding of common sense. However, LLMs are insufficient for direct diagnosis due to the absence of diagnostic strategies. The conventional approach to address this challenge involves expensive fine-tuning of LLMs. Alternatively, a more appealing solution is the development of a plugin that empowers LLMs to perform medical conversation tasks. Drawing inspiration from in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue actions by LLMs through two modules: the prompt generation (PG) module and the response ranking (RR) module. The PG module is designed to capture dialogue information from both global and local perspectives. It selects suitable prompts by assessing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cross-modality Data Augmentation&#65288;XmDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;XmDA&#22312;&#35813;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11096</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cross-modality Data Augmentation&#65288;XmDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;XmDA&#22312;&#35813;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#26088;&#22312;&#30452;&#25509;&#23558;&#25163;&#35821;&#35270;&#39057;&#36716;&#25442;&#20026;&#21475;&#35821;&#25991;&#26412;&#65292;&#26080;&#38656;&#20013;&#38388;&#34920;&#31034;&#12290;&#21463;&#25163;&#35821;&#35270;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#25361;&#25112;&#65292;&#36825;&#19968;&#20219;&#21153;&#19968;&#30452;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36328;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;XmDA&#65289;&#8221;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65288;&#21363;&#35270;&#39057;&#21040;&#25991;&#26412;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;XmDA&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#36328;&#27169;&#24577;&#28151;&#21512;&#21644;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#12290;&#21069;&#32773;&#26126;&#30830;&#22320;&#20419;&#36827;&#25163;&#35821;&#35270;&#39057;&#29305;&#24449;&#21644;&#25163;&#35821;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20197;&#24357;&#21512;&#27169;&#24577;&#24046;&#36317;&#12290;&#21518;&#32773;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#29983;&#25104;&#30693;&#35782;&#26469;&#25351;&#23548;&#21475;&#35821;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;LIBRISIGN&#21644;WLASL&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XmDA&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end sign language translation (SLT) aims to convert sign language videos into spoken language texts directly without intermediate representations. It has been a challenging task due to the modality gap between sign videos and texts and the data scarcity of labeled data. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the sign gloss translation model. Specifically, XmDA consists of two key components, namely, cross-modality mix-up and cross-modality knowledge distillation. The former explicitly encourages the alignment between sign video features and gloss embeddings to bridge the modality gap. The latter utilizes the generation knowledge from gloss-to-text teacher models to guide the spoken language text generation. Experimental results on two widely used SLT datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#36866;&#29992;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#27979;&#35797;&#25968;&#25454;&#65292;&#20165;&#20801;&#35768;&#29305;&#23450;&#27966;&#29983;&#21457;&#24067;&#65307;&#65288;2&#65289;&#23545;&#20110;API&#25345;&#26377;&#26041;&#65292;&#35201;&#27714;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#20445;&#25252;&#27979;&#35797;&#25968;&#25454;&#65292;&#19981;&#20572;&#27490;&#35780;&#20272;&#30452;&#21040;&#36798;&#21040;&#35201;&#27714;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#38656;&#36991;&#20813;&#26576;&#20123;&#32467;&#26524;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10160</link><description>&lt;p&gt;
&#19981;&#35201;&#29992;&#26126;&#25991;&#19978;&#20256;&#27979;&#35797;&#25968;&#25454;&#65306;&#20943;&#36731;&#25968;&#25454;&#22806;&#27844;&#23545;&#20110;&#35780;&#20272;&#22522;&#20934;&#30340;&#25345;&#32493;&#24433;&#21709;&#30340;&#23454;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#36866;&#29992;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#27979;&#35797;&#25968;&#25454;&#65292;&#20165;&#20801;&#35768;&#29305;&#23450;&#27966;&#29983;&#21457;&#24067;&#65307;&#65288;2&#65289;&#23545;&#20110;API&#25345;&#26377;&#26041;&#65292;&#35201;&#27714;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#20445;&#25252;&#27979;&#35797;&#25968;&#25454;&#65292;&#19981;&#20572;&#27490;&#35780;&#20272;&#30452;&#21040;&#36798;&#21040;&#35201;&#27714;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#38656;&#36991;&#20813;&#26576;&#20123;&#32467;&#26524;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#21160;&#29228;&#32593;&#36164;&#26009;&#24211;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#25968;&#25454;&#22806;&#27844;&#21464;&#24471;&#24120;&#35265;&#19988;&#37096;&#20998;&#38590;&#20197;&#24212;&#23545;&#12290;&#23545;&#20110;&#37027;&#20123;&#19981;&#20250;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20854;&#25968;&#25454;&#25104;&#20026;&#20102;&#21830;&#19994;&#26426;&#23494;&#65292;&#21363;&#20351;&#22312;&#20844;&#24320;&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#29305;&#23450;&#27979;&#35797;&#23454;&#20363;&#26159;&#21542;&#34987;&#27844;&#38706;&#20063;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#19977;&#20010;&#21487;&#34892;&#30340;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20351;&#29992;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#24182;&#38480;&#21046;&#27966;&#29983;&#21457;&#24067;&#30340;&#35768;&#21487;&#65307;&#65288;2&#65289;&#35201;&#27714;&#25345;&#26377;API&#35757;&#32451;&#25968;&#25454;&#30340;&#20844;&#21496;&#37319;&#29992;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#24182;&#25298;&#32477;&#35780;&#20272;&#65292;&#30452;&#21040;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#26080;&#35823;&#20026;&#27490;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#37027;&#20040;&#38656;&#36991;&#20813;&#22312;&#32593;&#32476;&#25628;&#32034;&#20013;&#20986;&#29616;&#21253;&#21547;&#27491;&#30830;&#25552;&#21462;&#37096;&#20998;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination has become especially prevalent and challenging with the rise of models pretrained on very large, automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to ascertain whether a particular test instance has been compromised. Strategies such as live leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met; (3) in case of test data based on internet text, avoid data which appears with its soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.03695</link><description>&lt;p&gt;
Vera&#65306;&#19968;&#20010;&#29992;&#20110;&#36890;&#29992;&#24120;&#35782;&#35821;&#21477;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#33618;&#35884;&#21644;&#24847;&#22806;&#30340;&#24120;&#35782;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#21453;&#24605;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;Vera&#65292;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;19&#20010;QA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#21019;&#24314;&#30340;&#32422;700&#19975;&#26465;&#24120;&#35782;&#35821;&#21477;&#20197;&#21450;&#19977;&#20010;&#35757;&#32451;&#30446;&#26631;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;Vera&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21508;&#31181;&#24120;&#35782;&#39046;&#22495;&#20013;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#35821;&#21477;&#12290;&#24403;&#24212;&#29992;&#20110;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#37325;&#29992;&#20110;&#24120;&#35782;&#39564;&#35777;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;Vera&#22312;&#36807;&#28388;LM&#29983;&#25104;&#30340;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge an
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102; TempoSum &#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#22522;&#20934;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01951</link><description>&lt;p&gt;
TempoSum&#65306;&#35780;&#20272;&#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization. (arXiv:2305.01951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102; TempoSum &#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#22522;&#20934;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#26377;&#30340;&#25277;&#35937;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26377; promising &#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25688;&#35201;&#22522;&#20934;&#19982;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#22312;&#26102;&#38388;&#19978;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#21487;&#33021;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25152;&#35760;&#24518;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#30693;&#35782;&#21487;&#33021;&#24456;&#24555;&#23601;&#36807;&#26102;&#65292;&#36825;&#20250;&#24433;&#21709;&#21040;&#23427;&#20204;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#20102;&#35299;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; TempoSum&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20174; 2010 &#24180;&#21040; 2022 &#24180;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#26041;&#27861;&#19981;&#33021;&#21487;&#38752;&#22320;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RexUIE&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#36882;&#24402;&#26426;&#21046;&#21644;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#65292;&#23454;&#29616;&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;(UIE)&#12290;&#19982;&#20197;&#24448;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;RexUIE&#22312;&#25552;&#21462;&#19981;&#21516;&#27169;&#24335;&#26102;&#19981;&#20250;&#20986;&#29616;&#20914;&#31361;&#65292;&#24182;&#19988;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#22120;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.14770</link><description>&lt;p&gt;
RexUIE: &#19968;&#31181;&#24102;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RexUIE&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#36882;&#24402;&#26426;&#21046;&#21644;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#65292;&#23454;&#29616;&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;(UIE)&#12290;&#19982;&#20197;&#24448;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;RexUIE&#22312;&#25552;&#21462;&#19981;&#21516;&#27169;&#24335;&#26102;&#19981;&#20250;&#20986;&#29616;&#20914;&#31361;&#65292;&#24182;&#19988;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#22120;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;(UIE)&#22240;&#20854;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#12289;&#24322;&#26500;&#32467;&#26500;&#21644;&#38656;&#27714;&#29305;&#23450;&#27169;&#24335;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20165;&#36890;&#36807;&#32479;&#19968;&#19968;&#20123;&#20219;&#21153;(&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;)&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#19981;&#33021;&#30495;&#27491;&#31216;&#20043;&#20026;UIE&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#24403;&#25552;&#21462;&#20854;&#20182;&#24120;&#35265;&#27169;&#24335;&#65288;&#22914;&#22235;&#20803;&#32452;&#21644;&#20116;&#20803;&#32452;&#65289;&#26102;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#20010;&#38544;&#24335;&#32467;&#26500;&#27169;&#24335;&#35828;&#26126;&#31526;&#65292;&#22312;&#31867;&#22411;&#20043;&#38388;&#24314;&#31435;&#38169;&#35823;&#30340;&#38142;&#25509;&#65292;&#20351;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#27491;&#24335;&#30340;UIE&#20844;&#24335;&#65292;&#28085;&#30422;&#20102;&#20960;&#20046;&#25152;&#26377;&#25552;&#21462;&#27169;&#24335;&#12290;&#25105;&#20204;&#26159;&#39318;&#27425;&#20026;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#24335;&#24341;&#20837;UIE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RexUIE&#65292;&#23427;&#26159;&#19968;&#31181;&#24102;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110; UIE&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;RexUIE&#36890;&#36807;&#36882;&#24402;&#26426;&#21046;&#32479;&#19968;&#25552;&#21462;&#36807;&#31243;&#65292;&#36866;&#24212;&#22810;&#31181;&#25552;&#21462;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#22120;&#65292;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#32473;&#23450;&#22330;&#26223;&#30340;&#32467;&#26500;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#30340;&#27867;&#21270;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal Information Extraction (UIE) is an area of interest due to the challenges posed by varying targets, heterogeneous structures, and demand-specific schemas. However, previous works have only achieved limited success by unifying a few tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE), which fall short of being authentic UIE models particularly when extracting other general schemas such as quadruples and quintuples. Additionally, these models used an implicit structural schema instructor, which could lead to incorrect links between types, hindering the model's generalization and performance in low-resource scenarios. In this paper, we redefine the authentic UIE with a formal formulation that encompasses almost all extraction schemas. To the best of our knowledge, we are the first to introduce UIE for any kind of schemas. In addition, we propose RexUIE, which is a Recursive Method with Explicit Schema Instructor for UIE. To avoid interference between diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65306;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;&#38382;&#39064;&#35299;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#36339;&#38382;&#39064;&#35299;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#24605;&#32771;&#27493;&#39588;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#25165;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#22810;&#20010;&#38142;&#26465;&#34987;&#25277;&#26679;&#24182;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#20013;&#38388;&#27493;&#39588;&#26412;&#36523;&#34987;&#20002;&#24323;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32771;&#34385;&#38142;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#39044;&#27979;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340; Multi-Chain Reasoning (MCR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36229;&#36234;&#22810;&#20010;&#24605;&#32771;&#38142;&#65292;&#32780;&#19981;&#26159;&#32858;&#21512;&#22238;&#31572;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#39044;&#27979;&#31572;&#26696;&#26102;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;MCR&#30340;&#35299;&#37322;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
&lt;/p&gt;</description></item><item><title>SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11060</link><description>&lt;p&gt;
SkillGPT: &#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11060
&lt;/p&gt;
&lt;p&gt;
SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SkillGPT&#65292;&#19968;&#31181;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20174;&#33258;&#30001;&#39118;&#26684;&#32844;&#20301;&#25551;&#36848;&#21644;&#29992;&#25143;&#36164;&#26009;&#20013;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270; (SES) &#30340;&#24037;&#20855;&#12290;&#19982;&#22823;&#22810;&#25968;&#31867;&#20284;&#20219;&#21153;&#30340;&#20197;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;SkillGPT &#30452;&#25509;&#20351;&#29992;&#26368;&#26032;&#30340;&#23545;&#35805; LLM &#36827;&#34892;&#26631;&#20934;&#25216;&#33021;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#26469;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#20813;&#36153; SkillGPT &#35753;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#35805;&#22411; SES&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SkillGPT, a tool for skill extraction and standardization (SES) from free-style job descriptions and user profiles with an open-source Large Language Model (LLM) as backbone. Most previous methods for similar tasks either need supervision or rely on heavy data-preprocessing and feature engineering. Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes a LLM to perform its tasks in steps via summarization and vector similarity search, to balance speed with precision. The backbone LLM of SkillGPT is based on Llama, free for academic use and thus useful for exploratory research and prototype development. Hence, our cost-free SkillGPT gives users the convenience of conversational SES, efficiently and reliably.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.13408</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#25442;&#28151;&#28102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#65292;&#32780;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#22810;&#31181;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#29992;&#20110;&#35782;&#21035;&#24694;&#24847;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#21019;&#24314;&#25110;&#23398;&#26415;&#25220;&#34989;)&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#36890;&#36807;&#27700;&#21360;&#25110;&#32479;&#35745;&#24322;&#24120;&#28857;&#12290;&#26412;&#25991;&#25506;&#31350;&#36825;&#20123;&#25991;&#26412;&#26816;&#27979;&#31639;&#27861;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#21547;&#20041;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;11B&#21442;&#25968;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;(DIPPER)&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#27573;&#33853;&#36827;&#34892;&#35821;&#20041;&#36716;&#25442;&#65292;&#21487;&#36873;&#25321;&#21033;&#29992;&#21608;&#22260;&#25991;&#26412;(&#20363;&#22914;&#29992;&#25143;&#20889;&#30340;&#25552;&#31034;)&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;DIPPER&#36824;&#20351;&#29992;&#26631;&#37327;&#26059;&#38062;&#26469;&#25511;&#21046;&#35821;&#20041;&#36716;&#25442;&#20013;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#37325;&#26032;&#25490;&#21015;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;DIPPER&#26469;&#36827;&#34892;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#36716;&#25442;&#65292;&#25104;&#21151;&#22320;&#28151;&#28102;&#20102;&#22810;&#20010;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26816;&#27979;&#12289;GPTZero&#12289;DetectGPT&#21644;OpenAI&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#20363;&#22914;&#65292;DIPPER&#23558;DetectGPT&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#20174;70.3%&#38477;&#33267;4.6%&#65288;&#22312;&#24658;&#23450;&#30340;1%&#35823;&#25253;&#29575;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#25552;&#31034;&#28151;&#21512;&#27169;&#22411;&#65288;MSP&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#32780;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MSP&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#33258;&#28982;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#31614;&#35821;&#20041;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.01580</link><description>&lt;p&gt;
&#21487;&#25511;&#25968;&#25454;&#29983;&#25104;&#30340;&#36719;&#25552;&#31034;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixture of Soft Prompts for Controllable Data Generation. (arXiv:2303.01580v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#25552;&#31034;&#28151;&#21512;&#27169;&#22411;&#65288;MSP&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#32780;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MSP&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#33258;&#28982;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#31614;&#35821;&#20041;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#30446;&#26631;&#36755;&#20986;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#27169;&#24335;&#26102;&#26377;&#25928;&#22320;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#23558;&#36755;&#20986;&#26684;&#24335;&#38480;&#21046;&#22312;&#26377;&#38480;&#30340;&#26412;&#20307;&#19978;&#65292;&#23548;&#33268;&#21363;&#20351;&#26159;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#20063;&#38590;&#20197;&#24212;&#23545;&#65292;&#22240;&#20026;&#23427;&#20204;&#20174;&#26410;&#34987;&#35757;&#32451;&#22312;&#32771;&#34385;&#36825;&#31181;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;LLM&#30340;&#30452;&#25509;&#39044;&#27979;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#36825;&#24120;&#24120;&#26159;&#30001;&#20110;&#39046;&#22495;&#31227;&#20301;&#21644;&#36164;&#28304;&#38480;&#21046;&#36896;&#25104;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#24037;&#20855;&#32780;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#65292;&#20174;&#32780;&#39072;&#35206;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36719;&#25552;&#31034;&#28151;&#21512;&#27169;&#22411;&#65288;MSP&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#24212;&#29992;&#21435;&#22122;&#26426;&#21046;&#26469;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#33258;&#28982;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#31614;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;MSP&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Lang2LTL&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21644;&#36719;&#20214;&#21253;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;LTL&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#12290;&#36890;&#36807;&#22312;&#27809;&#26377;&#20808;&#21069;&#35821;&#35328;&#25968;&#25454;&#30340;&#29615;&#22659;&#20013;&#20840;&#38754;&#35780;&#20272;Lang2LTL&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;21&#20010;&#22478;&#24066;&#32423;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#19982;&#21508;&#31181;&#26102;&#24577;&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#30340;&#26368;&#20808;&#36827;&#33021;&#21147;&#12290;&#24182;&#19988;&#36890;&#36807;&#23637;&#31034;&#29289;&#29702;&#26426;&#22120;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#29615;&#22659;&#20013;&#21487;&#20197;&#36981;&#24490;52&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#23548;&#33322;&#21629;&#20196;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;Lang2LTL&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.11649</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#20026;&#26102;&#38388;&#20219;&#21153;&#24314;&#31435;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments. (arXiv:2302.11649v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Lang2LTL&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21644;&#36719;&#20214;&#21253;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;LTL&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#12290;&#36890;&#36807;&#22312;&#27809;&#26377;&#20808;&#21069;&#35821;&#35328;&#25968;&#25454;&#30340;&#29615;&#22659;&#20013;&#20840;&#38754;&#35780;&#20272;Lang2LTL&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;21&#20010;&#22478;&#24066;&#32423;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#19982;&#21508;&#31181;&#26102;&#24577;&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#30340;&#26368;&#20808;&#36827;&#33021;&#21147;&#12290;&#24182;&#19988;&#36890;&#36807;&#23637;&#31034;&#29289;&#29702;&#26426;&#22120;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#29615;&#22659;&#20013;&#21487;&#20197;&#36981;&#24490;52&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#23548;&#33322;&#21629;&#20196;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;Lang2LTL&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753; (LTL) &#36827;&#34892;&#20851;&#32852;&#65292;&#21033;&#29992;&#20854;&#26126;&#30830;&#30340;&#35821;&#20041;&#26469;&#25512;&#29702;&#38271;&#31243;&#20219;&#21153;&#21644;&#39564;&#35777;&#26102;&#38388;&#32422;&#26463;&#30340;&#28385;&#36275;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26469;&#33258;&#29305;&#23450;&#29615;&#22659;&#21644;&#29992;&#20110;&#29702;&#35299;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#21629;&#20196;&#30340;&#22320;&#26631;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Lang2LTL&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#31995;&#32479;&#21644;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#27809;&#26377;&#20808;&#21069;&#35821;&#35328;&#25968;&#25454;&#30340;&#29615;&#22659;&#20013;&#23558;&#26102;&#24577;&#23548;&#33322;&#21629;&#20196;&#19982;LTL&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#12290;&#25105;&#20204;&#23545;Lang2LTL&#36827;&#34892;&#20102;&#20116;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;Lang2LTL&#22312;21&#20010;&#22478;&#24066;&#32423;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#21333;&#20010;&#27169;&#22411;&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;&#21508;&#31181;&#26102;&#24577;&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#30340;&#26368;&#20808;&#36827;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20351;&#29992;Lang2LTL&#30340;&#29289;&#29702;&#26426;&#22120;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#29615;&#22659;&#20013;&#36981;&#24490;52&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#23548;&#33322;&#21629;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#26469;&#25913;&#36827;&#26816;&#32034;&#25928;&#26524;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09473</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#31232;&#30095;&#22810;&#31890;&#24230;&#23398;&#20064;&#36827;&#34892;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#26469;&#25913;&#36827;&#26816;&#32034;&#25928;&#26524;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65292;&#26368;&#36817;&#22312;&#25506;&#32034;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#23398;&#20064;&#35270;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#31232;&#30095;&#27010;&#24565;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#27599;&#20010;&#27010;&#24565;&#37117;&#23545;&#24212;&#19968;&#20123;&#35789;&#35821;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#25105;&#20204;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#21644;&#26356;&#26032;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#21644;&#23545;&#40784;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#22810;&#31890;&#24230;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#24103;&#34920;&#31034;&#26041;&#27861;&#32435;&#20837;&#27169;&#22411;&#65292;&#26356;&#22909;&#22320;&#23545;&#35270;&#39057;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#21644;&#35745;&#31639;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S3MA&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/yim&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at https://github.com/yim
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02676</link><description>&lt;p&gt;
&#22238;&#39038;&#38142;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36825;&#26679;&#25165;&#33021;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#24182;&#31526;&#21512;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#26469;&#29702;&#35299;&#21644;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26159;&#22522;&#20110;&#34987;&#20154;&#31867;&#27880;&#37322;&#32773;&#21916;&#27426;&#30340;&#25163;&#21160;&#25361;&#36873;&#30340;&#27169;&#22411;&#29983;&#25104;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#19988;&#26222;&#36941;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#22870;&#21169;&#20989;&#25968;&#19981;&#23436;&#32654;&#21644;&#26497;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#8220;&#22238;&#39038;&#38142;&#8221;&#65292;&#23427;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#21463;&#21040;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#20197;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#30340;&#24191;&#27867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#31867;&#22411;&#30340;&#21453;&#39304;&#36716;&#25442;&#25104;&#21477;&#23376;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>Seq2Seq-SC&#26159;&#19968;&#20010;&#20855;&#22791;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#28040;&#24687;&#21547;&#20041;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20026;&#35821;&#20041;&#36890;&#20449;&#30340;&#21457;&#23637;&#20197;&#21450;6G&#32593;&#32476;&#20013;&#19982;&#26410;&#26469;&#26080;&#32447;&#31995;&#32479;&#30340;&#25972;&#21512;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.15237</link><description>&lt;p&gt;
Seq2Seq-SC:&#20855;&#26377;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. (arXiv:2210.15237v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15237
&lt;/p&gt;
&lt;p&gt;
Seq2Seq-SC&#26159;&#19968;&#20010;&#20855;&#22791;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#28040;&#24687;&#21547;&#20041;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20026;&#35821;&#20041;&#36890;&#20449;&#30340;&#21457;&#23637;&#20197;&#21450;6G&#32593;&#32476;&#20013;&#19982;&#26410;&#26469;&#26080;&#32447;&#31995;&#32479;&#30340;&#25972;&#21512;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;seq2seq-SC&#30340;&#29616;&#23454;&#35821;&#20041;&#32593;&#32476;&#65292;&#26088;&#22312;&#19982;5G NR&#20860;&#23481;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#36890;&#29992;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19987;&#27880;&#20110;&#35821;&#20041;&#36890;&#20449;&#20013;&#28040;&#24687;&#30340;&#21547;&#20041;&#26469;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#21483;&#20570;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#36890;&#36807;BLEU&#27979;&#37327;&#35789;&#27719;&#30456;&#20284;&#24615;&#65292;&#36890;&#36807;SBERT&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;seq2seq-SC&#22312;&#25552;&#21462;&#35821;&#20041;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#35821;&#20041;&#36890;&#20449;&#30340;&#25345;&#32493;&#36827;&#27493;&#20197;&#21450;&#22312;6G&#32593;&#32476;&#20013;&#19982;&#26410;&#26469;&#26080;&#32447;&#31995;&#32479;&#30340;&#28508;&#22312;&#25972;&#21512;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a realistic semantic network called seq2seq-SC, designed to be compatible with 5G NR and capable of working with generalized text datasets using a pre-trained language model. The goal is to achieve unprecedented communication efficiency by focusing on the meaning of messages in semantic communication. We employ a performance metric called semantic similarity, measured by BLEU for lexical similarity and SBERT for semantic similarity. Our findings demonstrate that seq2seq-SC outperforms previous models in extracting semantically meaningful information while maintaining superior performance. This study paves the way for continued advancements in semantic communication and its prospective incorporation with future wireless systems in 6G networks.
&lt;/p&gt;</description></item><item><title>YATO&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#26512;&#65292;&#21487;&#32452;&#21512;&#19981;&#21516;&#30340;&#29305;&#24449;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#21183;&#65292;&#20419;&#36827;&#20102;&#20808;&#36827;NLP&#27169;&#22411;&#30340;&#22797;&#29616;&#21644;&#25913;&#36827;&#65292;&#20197;&#21450;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#25512;&#21160;&#12290;</title><link>http://arxiv.org/abs/2209.13877</link><description>&lt;p&gt;
YATO: &#21478;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#20998;&#26512;&#24320;&#28304;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13877
&lt;/p&gt;
&lt;p&gt;
YATO&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#26512;&#65292;&#21487;&#32452;&#21512;&#19981;&#21516;&#30340;&#29305;&#24449;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#21183;&#65292;&#20419;&#36827;&#20102;&#20808;&#36827;NLP&#27169;&#22411;&#30340;&#22797;&#29616;&#21644;&#25913;&#36827;&#65292;&#20197;&#21450;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;YATO&#65292;&#19968;&#20010;&#24320;&#28304;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#26512;&#30340;&#24037;&#20855;&#21253;&#12290;&#19982;&#29616;&#26377;&#30340;&#37325;&#24230;&#24037;&#31243;&#21270;&#24037;&#20855;&#21253;&#21644;&#24179;&#21488;&#19981;&#21516;&#65292;YATO&#36731;&#37327;&#19988;&#29992;&#25143;&#21451;&#22909;&#65292;&#36866;&#29992;&#20110;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;YATO&#35774;&#35745;&#25104;&#20998;&#23618;&#32467;&#26500;&#65292;&#25903;&#25345;&#33258;&#30001;&#32452;&#21512;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#29305;&#24449;&#31867;&#22411;&#65292;&#21253;&#25324;1&#65289;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65292;RNN&#31561;&#65289;&#65307;2&#65289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#65292;RoBERTa&#65292;ELECTRA&#31561;&#65289;&#65307;&#21644;3&#65289;&#36890;&#36807;&#31616;&#21333;&#21487;&#37197;&#32622;&#30340;&#25991;&#20214;&#23454;&#29616;&#30340;&#29992;&#25143;&#23450;&#21046;&#21270;&#31070;&#32463;&#29305;&#24449;&#12290;&#30001;&#20110;&#28789;&#27963;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#20248;&#21183;&#65292;YATO&#21487;&#20197;&#20419;&#36827;&#24555;&#36895;&#22797;&#29616;&#21644;&#25913;&#36827;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#24182;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20195;&#30721;&#12289;&#31034;&#20363;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/jiesutd/YATO&#20844;&#24320;&#33719;&#21462;&#12290;&#36824;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;https://youtu.be/tSjjf5BzfQg&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports free combinations of three types of widely used features including 1) traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a simple configurable file. Benefiting from the advantages of flexibility and ease of use, YATO can facilitate fast reproduction and refinement of state-of-the-art NLP models, and promote the cross-disciplinary applications of NLP techniques. The code, examples, and documentation are publicly available at https://github.com/jiesutd/YATO. A demo video is also available at https://youtu.be/tSjjf5BzfQg.
&lt;/p&gt;</description></item></channel></rss>