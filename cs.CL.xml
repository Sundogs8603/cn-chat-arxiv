<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>FPT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#29305;&#24449;&#23884;&#20837;&#35757;&#32451;&#36719;&#25552;&#31034;&#24182;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#25913;&#21892;&#20102;&#21487;&#35835;&#24615;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02772</link><description>&lt;p&gt;
FPT:&#29305;&#24449;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#23569;&#26679;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FPT: Feature Prompt Tuning for Few-shot Readability Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02772
&lt;/p&gt;
&lt;p&gt;
FPT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#29305;&#24449;&#23884;&#20837;&#35757;&#32451;&#36719;&#25552;&#31034;&#24182;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#25913;&#21892;&#20102;&#21487;&#35835;&#24615;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#35835;&#24615;&#35780;&#20272;&#20219;&#21153;&#20013;&#65292;&#20256;&#32479;&#30340;&#25552;&#31034;&#26041;&#27861;&#32570;&#20047;&#20851;&#38190;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#32780;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#20851;&#20110;&#21033;&#29992;&#35821;&#35328;&#29305;&#24449;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#20855;&#26377;&#38750;&#31283;&#20581;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#21363;&#20855;&#26377;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#30340;&#29305;&#24449;&#25552;&#31034;&#35843;&#25972;&#65288;FPT&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21487;&#35757;&#32451;&#30340;&#36719;&#25552;&#31034;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#25490;&#21517;&#39034;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;FTP&#26041;&#27861;&#19981;&#20165;&#22312;&#20808;&#21069;&#26368;&#20339;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26041;&#27861;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02772v1 Announce Type: new  Abstract: Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses th
&lt;/p&gt;</description></item><item><title>KazParC&#26159;&#19968;&#20010;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#65292;&#36824;&#24320;&#21457;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;Tilmash&#12290;</title><link>https://arxiv.org/abs/2403.19399</link><description>&lt;p&gt;
KazParC&#65306;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#21704;&#33832;&#20811;&#24179;&#34892;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
KazParC: Kazakh Parallel Corpus for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19399
&lt;/p&gt;
&lt;p&gt;
KazParC&#26159;&#19968;&#20010;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#65292;&#36824;&#24320;&#21457;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;Tilmash&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;KazParC&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#32780;&#35774;&#35745;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#20316;&#20026;&#20854;&#31867;&#21035;&#20013;&#39318;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#35821;&#26009;&#24211;&#65292;KazParC&#21253;&#21547;&#20102;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#22312;&#20154;&#31867;&#35793;&#32773;&#30340;&#21327;&#21161;&#19979;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#25193;&#23637;&#21040;&#20102;&#21457;&#23637;&#19968;&#20010;&#34987;&#26165;&#31216;&#20026;Tilmash&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;Tilmash&#30340;&#34920;&#29616;&#19982;&#24037;&#19994;&#24040;&#22836;&#65292;&#22914;Google&#32763;&#35793;&#21644;Yandex&#32763;&#35793;&#65292;&#22312;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;BLEU&#21644;chrF&#65289;&#30340;&#22522;&#30784;&#19978;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36234;&#12290;KazParC&#21644;Tilmash&#22343;&#21487;&#36890;&#36807;&#25105;&#20204;&#30340;GitHub&#23384;&#20648;&#24211;&#20197;&#30693;&#35782;&#20849;&#20139;&#32626;&#21517;4.0&#22269;&#38469;&#35768;&#21487;&#65288;CC BY 4.0&#65289;&#20844;&#24320;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19399v1 Announce Type: new  Abstract: We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.
&lt;/p&gt;</description></item><item><title>KazSAnDRA&#26159;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#26368;&#22823;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21253;&#25324;&#24320;&#21457;&#21644;&#35780;&#20272;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#21151;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19335</link><description>&lt;p&gt;
KazSAnDRA&#65306;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#35780;&#35770;&#21644;&#24577;&#24230;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19335
&lt;/p&gt;
&lt;p&gt;
KazSAnDRA&#26159;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#26368;&#22823;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21253;&#25324;&#24320;&#21457;&#21644;&#35780;&#20272;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#21151;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KazSAnDRA&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#65292;&#26159;&#31532;&#19968;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;KazSAnDRA&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;18&#19975;&#38646;64&#26465;&#35780;&#35770;&#30340;&#24191;&#27867;&#25910;&#38598;&#65292;&#24182;&#21253;&#25324;&#20174;1&#21040;5&#30340;&#25968;&#23383;&#35780;&#20998;&#65292;&#25552;&#20379;&#20102;&#23458;&#25143;&#24577;&#24230;&#30340;&#23450;&#37327;&#34920;&#31034;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#24320;&#21457;&#21644;&#35780;&#20272;&#22235;&#20010;&#29992;&#20110;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#31867;&#30340;&#33258;&#21160;&#21270;&#12290;&#23454;&#39564;&#20998;&#26512;&#21253;&#25324;&#32771;&#34385;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#35780;&#20272;&#12290;&#26368;&#25104;&#21151;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#30340;F1&#20998;&#21035;&#36798;&#21040;&#20102;0.81&#21644;0.39&#12290;&#25968;&#25454;&#38598;&#21644;&#20248;&#21270;&#27169;&#22411;&#26159;&#24320;&#25918;&#33719;&#21462;&#30340;&#65292;&#21487;&#22312;&#30693;&#35782;&#20849;&#20139;&#32626;&#21517;4.0&#22269;&#38469;&#35768;&#21487;&#19979;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19335v1 Announce Type: new  Abstract: This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment analysis that is the first and largest publicly available dataset of its kind. KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes. The study also pursued the automation of Kazakh sentiment classification through the development and evaluation of four machine learning models trained for both polarity classification and score classification. Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios. The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets. The dataset and fine-tuned models are open access and available for download under the Creative Commons Attribution 4.0 International Lic
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00226</link><description>&lt;p&gt;
&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#35789;&#27719;&#30340;&#26102;&#38388;&#35821;&#20041;&#21464;&#21270;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24517;&#39035;&#23545;&#26102;&#38388;&#25935;&#24863;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#65288;SCD&#65289;&#20219;&#21153;&#32771;&#34385;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;$C_1$&#21644;$C_2$&#20043;&#38388;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#35789;$w$&#26159;&#21542;&#25913;&#21464;&#20102;&#21547;&#20041;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#26377;&#30340;Word-in-Context&#65288;WiC&#65289;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20004;&#38454;&#27573;SCD&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20110;&#30446;&#26631;&#35789;$w$&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#24863;&#30693;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#34920;&#31034;&#32473;&#23450;&#35821;&#26009;&#24211;&#20013;&#25152;&#36873;&#21477;&#23376;&#20013;$w$&#30340;&#21547;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#24863;&#30693;&#24863;&#30693;&#36317;&#31163;&#24230;&#37327;&#65292;&#27604;&#36739;&#30446;&#26631;&#35789;&#22312;$C_1$&#21644;$C_2$&#20013;&#30340;&#25152;&#26377;&#20986;&#29616;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#23545;&#22810;&#20010;SCD&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#25552;&#20986;&#30340;&#22810;&#31181;&#35821;&#35328;&#30340;SCD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00226v1 Announce Type: new  Abstract: Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, esta
&lt;/p&gt;</description></item><item><title>$\lambda$-ECLIPSE&#36890;&#36807;&#21033;&#29992;CLIP&#28508;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22810;&#27010;&#24565;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20943;&#23567;&#20102;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#19968;&#33268;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05195</link><description>&lt;p&gt;
$\lambda$-ECLIPSE: &#36890;&#36807;&#21033;&#29992;CLIP&#28508;&#31354;&#38388;&#65292;&#22522;&#20110;&#22810;&#27010;&#24565;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05195
&lt;/p&gt;
&lt;p&gt;
$\lambda$-ECLIPSE&#36890;&#36807;&#21033;&#29992;CLIP&#28508;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22810;&#27010;&#24565;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20943;&#23567;&#20102;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#19968;&#33268;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;(P-T2I)&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#20027;&#39064;&#30340;T2I&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20027;&#35201;&#30340;&#29942;&#39048;&#21253;&#25324;&#65306;1) &#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;2) &#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#65292;&#20197;&#21450;3) &#24179;&#34913;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#26500;&#22270;&#23545;&#40784;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#37325;&#26032;&#38416;&#36848;&#20102;T2I&#25193;&#25955;&#27169;&#22411;&#30340;&#26680;&#24515;&#29702;&#24565;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#20027;&#35201;&#22320;&#65292;&#24403;&#20195;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;T2I&#26041;&#27861;&#20381;&#36182;&#20110;&#28508;&#31354;&#38388;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#23454;&#29616;T2I&#26144;&#23556;&#12290;&#34429;&#28982;LDMs&#25552;&#20379;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#20294;P-T2I&#26041;&#27861;&#23545;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#30340;&#20381;&#36182;&#26174;&#33879;&#22686;&#21152;&#20102;&#36164;&#28304;&#38656;&#27714;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#24182;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#25165;&#33021;&#24471;&#21040;&#19968;&#20010;&#25152;&#38656;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;ECLIPSE&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#20855;&#36164;&#28304;&#25928;&#29575;&#30340;&#35757;&#32451;UnCLIP-based T2I&#27169;&#22411;&#30340;&#36335;&#24452;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25193;&#25955;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in personalized text-to-image (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05125</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#19982;LLMs
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Clinical Trial Patient Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#25512;&#20986;&#26032;&#33647;&#30340;&#20851;&#38190;&#38590;&#39064;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#31526;&#21512;&#35797;&#39564;&#20837;&#36873;&#26631;&#20934;&#30340;&#24739;&#32773;&#26159;&#39640;&#24230;&#25163;&#21160;&#30340;&#65292;&#27599;&#20301;&#24739;&#32773;&#38656;&#33457;&#36153;&#38271;&#36798;1&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#31579;&#36873;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#19968;&#20010;&#24739;&#32773;&#30340;&#30149;&#21490;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#35780;&#20272;&#35813;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#19968;&#32452;&#21253;&#21547;&#26631;&#20934;&#65288;&#20063;&#20197;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#25351;&#23450;&#65289;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#31995;&#32479;&#22312;n2c2 2018&#38431;&#21015;&#36873;&#25321;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#65292;&#35813;&#31574;&#30053;&#19982;&#29616;&#29366;&#30456;&#27604;&#21487;&#20197;&#23558;&#24739;&#32773;&#21305;&#37197;&#26102;&#38388;&#21644;&#25104;&#26412;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#20943;&#23569;&#20102;&#21305;&#37197;&#28040;&#38500;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2306.08543</link><description>&lt;p&gt;
MiniLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniLLM: Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;KD&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#30333;&#30418;&#20998;&#31867;&#27169;&#22411;&#25110;&#35757;&#32451;&#23567;&#27169;&#22411;&#26469;&#27169;&#20223;&#22914;ChatGPT&#20043;&#31867;&#30340;&#40657;&#30418;&#27169;&#22411;API&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#30333;&#30418;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#23567;&#27169;&#22411;&#20013;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38543;&#30528;&#24320;&#28304;LLMs&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;KD&#26041;&#27861;&#65292;&#23558;LLMs&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;</title><link>http://arxiv.org/abs/2401.01283</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#33258;&#21160;&#35780;&#20272;&#30340;&#21442;&#32771;&#25991;&#29486;&#36136;&#37327;&#21644;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#32763;&#35793;&#26469;&#30830;&#23450;&#31995;&#32479;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#39046;&#22495;&#20869;&#30340;&#20849;&#35782;&#35748;&#20026;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#24212;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#21487;&#20197;&#25351;&#23548;&#35745;&#21010;&#25910;&#38598;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21442;&#32771;&#25991;&#29486;&#30340;&#20174;&#19994;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#33021;&#22815;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#35780;&#20215;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#30340;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#21046;&#20316;&#25104;&#26412;&#26356;&#39640;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#65292;&#24212;&#35813;&#25910;&#38598;&#21738;&#20123;&#21442;&#32771;&#25991;&#29486;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation metrics often use human translations to determine the quality system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#24182;&#23558;&#21442;&#25968;&#32858;&#21512;&#21644;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#35299;&#20915;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#27861;&#24459;LLMs&#24341;&#21457;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#21644;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;FL&#26041;&#27861;&#25928;&#26524;&#38477;&#20302;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.08173</link><description>&lt;p&gt;
FedJudge: &#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedJudge: Federated Legal Large Language Model. (arXiv:2309.08173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#24182;&#23558;&#21442;&#25968;&#32858;&#21512;&#21644;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#35299;&#20915;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#27861;&#24459;LLMs&#24341;&#21457;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#21644;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;FL&#26041;&#27861;&#25928;&#26524;&#38477;&#20302;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#26234;&#33021;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#36741;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21644;&#26222;&#36890;&#20154;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27861;&#24459;LLMs&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#24341;&#21457;&#20102;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#27861;&#24459;&#25968;&#25454;&#20998;&#25955;&#22312;&#21253;&#21547;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#30340;&#21508;&#20010;&#26426;&#26500;&#20043;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#23558;&#27861;&#24459;LLMs&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;FL&#65292;&#27861;&#24459;LLMs&#21487;&#20197;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#20854;&#21442;&#25968;&#34987;&#32858;&#21512;&#24182;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#65292;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#32780;&#26080;&#38656;&#30452;&#25509;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#38459;&#30861;&#20102;LLMs&#22312;FL&#29615;&#22659;&#20013;&#30340;&#20840;&#38754;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#25968;&#25454;&#30340;&#20998;&#24067;&#20559;&#31227;&#20943;&#23569;&#20102;FL&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained prominence in the field of Legal Intelligence, offering potential applications in assisting legal professionals and laymen. However, the centralized training of these Legal LLMs raises data privacy concerns, as legal data is distributed among various institutions containing sensitive individual information. This paper addresses this challenge by exploring the integration of Legal LLMs with Federated Learning (FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on devices or clients, and their parameters are aggregated and distributed on a central server, ensuring data privacy without directly sharing raw data. However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods. To this end, in this paper, we propose the first Federated Legal Large Language Model (FedJudge) framework, which fine-tunes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09820</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#21452;&#38454;&#27573;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification. (arXiv:2304.09820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#32570;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#23427;&#21033;&#29992;&#25110;&#37325;&#29992;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#20016;&#23500;&#26631;&#35760;&#25968;&#25454;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#19987;&#27880;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#35201;&#20040;&#24573;&#30053;&#21487;&#33021;&#23384;&#22312;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#24182;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#30340;&#39046;&#22495;&#24863;&#30693;&#29305;&#24449;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#21644;&#26469;&#33258;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#65288;SSD&#65289;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20854;&#24615;&#33021;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#22343;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain text classification aims to adapt models to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with self-supervised distillation (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#23398;&#32773;&#22312;&#36923;&#36753;&#35821;&#35328;&#36716;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06186</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#23398;&#32773;&#30340;&#65288;&#38750;&#65289;&#24418;&#24335;&#21270;&#21644;&#33258;&#28982;&#35770;&#35777;&#32451;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using large language models for (de-)formalization and natural argumentation exercises for beginner's students. (arXiv:2304.06186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#23398;&#32773;&#22312;&#36923;&#36753;&#35821;&#35328;&#36716;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#25991;&#26412;&#36798;&#33452;&#22855;-003&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#32416;&#27491;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#19982;&#21629;&#39064;&#36923;&#36753;&#35821;&#35328;&#21644;&#19968;&#38454;&#35859;&#35789;&#36923;&#36753;&#35821;&#35328;&#20043;&#38388;&#36716;&#21270;&#30340;&#32451;&#20064;; &#21644;&#65288;ii&#65289;&#22312;&#38750;&#25968;&#23398;&#22330;&#26223;&#19979;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#31616;&#21333;&#35770;&#28857;&#30340;&#32451;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe two systems that use text-davinci-003, a large language model, for the automatized correction of (i) exercises in translating back and forth between natural language and the languages of propositional logic and first-order predicate logic and (ii) exercises in writing simple arguments in natural language in non-mathematical scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17513</link><description>&lt;p&gt;
&#36890;&#36807;GPT-3&#33258;&#21160;&#24418;&#24335;&#21270;&#25552;&#39640;Diproche CNL&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving the Diproche CNL through autoformalization via GPT-3. (arXiv:2303.17513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diproche&#31995;&#32479;&#26159;&#19968;&#27454;&#38024;&#23545;&#24503;&#35821;&#25511;&#21046;&#35821;&#35328;&#29255;&#27573;&#30340;&#33258;&#21160;&#21270;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#26088;&#22312;&#29992;&#20110;&#25945;&#23398;&#24212;&#29992;&#65292;&#22312;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#35777;&#26126;&#26102;&#20351;&#29992;&#12290;&#35813;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#20351;&#29992;&#19968;&#31181;&#25511;&#21046;&#33258;&#28982;&#35821;&#35328;&#65292;&#20854;Prolog&#24418;&#24335;&#21270;&#20363;&#31243;&#24050;&#32463;&#32534;&#20889;&#22909;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Diproche system is an automated proof checker for texts written in a controlled fragment of German, designed for didactical applications in classes introducing students to proofs for the first time. The first version of the system used a controlled natural language for which a Prolog formalization routine was written. In this paper, we explore the possibility of prompting large language models for autoformalization in the context of Diproche, with encouraging first results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item></channel></rss>