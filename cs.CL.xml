<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#27597;&#35821;&#32773;&#20351;&#29992;AI&#20889;&#20316;&#21161;&#25163;&#26102;&#32570;&#20047;&#35299;&#37322;&#12289;&#38590;&#20197;&#35780;&#20272;&#36716;&#35793;&#24314;&#35758;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22235;&#20010;&#22686;&#21152;&#35299;&#37322;&#30340;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#65292;&#26088;&#22312;&#24110;&#21161;&#38750;&#27597;&#35821;&#29992;&#25143;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#36716;&#35793;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.02625</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#33521;&#35821;&#27597;&#35821;&#32773;&#30340;&#21487;&#35299;&#37322;AI&#20889;&#20316;&#21161;&#25163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable AI Writing Assistants for Non-native English Speakers. (arXiv:2304.02625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#27597;&#35821;&#32773;&#20351;&#29992;AI&#20889;&#20316;&#21161;&#25163;&#26102;&#32570;&#20047;&#35299;&#37322;&#12289;&#38590;&#20197;&#35780;&#20272;&#36716;&#35793;&#24314;&#35758;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22235;&#20010;&#22686;&#21152;&#35299;&#37322;&#30340;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#65292;&#26088;&#22312;&#24110;&#21161;&#38750;&#27597;&#35821;&#29992;&#25143;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#36716;&#35793;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;15&#21517;&#33521;&#35821;&#38750;&#27597;&#35821;&#32773;&#30340;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#38750;&#27597;&#35821;&#32773;&#20351;&#29992;AI&#20889;&#20316;&#21161;&#25163;&#36827;&#34892;&#36716;&#35793;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;AI&#20889;&#20316;&#21161;&#25163;&#29983;&#25104;&#30340;&#24314;&#35758;&#36716;&#35793;&#32570;&#20047;&#35299;&#37322;&#65292;&#36825;&#20123;&#29992;&#25143;&#22312;&#35780;&#20272;&#36716;&#35793;&#25991;&#26412;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35843;&#26597;&#20102;&#36825;&#20123;&#29992;&#25143;&#22312;&#32570;&#23569;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#35775;&#35848;&#20013;&#21457;&#29616;&#30340;&#38750;&#27597;&#35821;&#29992;&#25143;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#30340;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35299;&#37322;&#26469;&#25913;&#21892;&#38750;&#27597;&#35821;&#29992;&#25143;&#20351;&#29992;AI&#20889;&#20316;&#21161;&#25163;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We highlight the challenges faced by non-native speakers when using AI writing assistants to paraphrase text. Through an interview study with 15 non-native English speakers (NNESs) with varying levels of English proficiency, we observe that they face difficulties in assessing paraphrased texts generated by AI writing assistants, largely due to the lack of explanations accompanying the suggested paraphrases. Furthermore, we examine their strategies to assess AI-generated texts in the absence of such explanations. Drawing on the needs of NNESs identified in our interview, we propose four potential user interfaces to enhance the writing experience of NNESs using AI writing assistants. The proposed designs focus on incorporating explanations to better support NNESs in understanding and evaluating the AI-generated paraphrasing suggestions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;AI&#25903;&#25345;&#30340;&#35828;&#26126;&#24615;&#20889;&#20316;&#65292;&#24182;&#25351;&#20986;&#23427;&#20855;&#26377;&#29420;&#29305;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#21487;&#20197;&#24102;&#26469;&#39640;&#24230;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02623</link><description>&lt;p&gt;
&#36229;&#36234;&#25688;&#35201;&#65306;&#35774;&#35745;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#35828;&#26126;&#24615;&#20889;&#20316;&#20219;&#21153;&#30340;AI&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks. (arXiv:2304.02623v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02623
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;AI&#25903;&#25345;&#30340;&#35828;&#26126;&#24615;&#20889;&#20316;&#65292;&#24182;&#25351;&#20986;&#23427;&#20855;&#26377;&#29420;&#29305;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#21487;&#20197;&#24102;&#26469;&#39640;&#24230;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#35774;&#35745;&#21644;&#24320;&#21457;&#26032;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#24102;&#26469;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#19968;&#26032;&#25216;&#26415;&#21487;&#20197;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25913;&#21464;&#20889;&#20316;&#65292;&#22914;&#21019;&#24847;&#20889;&#20316;&#20013;&#30340;&#26500;&#24605;&#12289;&#32534;&#36753;&#25903;&#25345;&#21644;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;AI&#25903;&#25345;&#30340;&#35828;&#26126;&#24615;&#20889;&#20316;&#65292;&#21253;&#25324;&#23398;&#32773;&#25776;&#20889;&#25991;&#29486;&#32508;&#36848;&#25110;&#21307;&#29983;&#25776;&#20889;&#36827;&#23637;&#25253;&#21578;&#31561;&#29616;&#23454;&#20219;&#21153;&#65292;&#30456;&#23545;&#19981;&#22815;&#30740;&#31350;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20026;&#35828;&#26126;&#24615;&#20889;&#20316;&#24320;&#21457;AI&#25903;&#25345;&#20855;&#26377;&#29420;&#29305;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#21487;&#20197;&#24102;&#26469;&#39640;&#24230;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#35828;&#26126;&#24615;&#20889;&#20316;&#25551;&#36848;&#20026;&#22522;&#20110;&#35777;&#25454;&#21644;&#29983;&#25104;&#30693;&#35782;&#30340;&#65306;&#23427;&#21253;&#21547;&#22806;&#37096;&#25991;&#26723;&#30340;&#24635;&#32467;&#20197;&#21450;&#26032;&#20449;&#24687;&#25110;&#30693;&#35782;&#12290;&#23427;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20316;&#32773;&#22312;&#19968;&#32452;&#28304;&#25991;&#20214;&#19978;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#36807;&#31243;&#30340;&#20135;&#29289;&#65292;&#38405;&#35835;&#12289;&#24605;&#32771;&#21644;&#20889;&#20316;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#21551;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have introduced exciting new opportunities and challenges in designing and developing new AI-assisted writing support tools. Recent work has shown that leveraging this new technology can transform writing in many scenarios such as ideation during creative writing, editing support, and summarization. However, AI-supported expository writing--including real-world tasks like scholars writing literature reviews or doctors writing progress notes--is relatively understudied. In this position paper, we argue that developing AI supports for expository writing has unique and exciting research challenges and can lead to high real-world impacts. We characterize expository writing as evidence-based and knowledge-generating: it contains summaries of external documents as well as new information or knowledge. It can be seen as the product of authors' sensemaking process over a set of source documents, and the interplay between reading, reflection, and writing opens up new oppor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02541</link><description>&lt;p&gt;
PWESuite&#65306;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#21521;&#37327;&#31354;&#38388;&#30340;&#21333;&#35789;&#23884;&#20837;&#26159;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#12290;&#22823;&#22810;&#25968;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#32534;&#30721;&#35821;&#20041;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#30340;&#35821;&#38899;&#20449;&#24687;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21457;&#22768;&#29305;&#24449;&#26500;&#24314;&#35821;&#38899;&#30693;&#24773;&#21333;&#35789;&#23884;&#20837;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#20197;&#40723;&#21169;&#20854;&#31038;&#21306;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#20351;&#29992;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#23398;&#20064;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26041;&#38754;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#20869;&#22312;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#22914;&#21333;&#35789;&#26816;&#32034;&#21644;&#19982;&#22768;&#38899;&#30456;&#20284;&#24615;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22806;&#22312;&#34920;&#29616;&#65292;&#22914;&#38901;&#24459;&#21644;&#21516;&#28304;&#26816;&#27979;&#21644;&#22768;&#38899;&#31867;&#27604;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20219;&#21153;&#22871;&#20214;&#23558;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#24182;&#25552;&#20379;&#26410;&#26469;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;ChatGPT&#23478;&#26063;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#34429;&#28982;Fine-tuned&#30340;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#24182;&#26410;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36229;&#36234;&#38382;&#31572;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#23450;&#21046;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;Fine-tuning&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.02496</link><description>&lt;p&gt;
ChatGPT&#23478;&#26063;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25512;&#29702;&#21644;&#20998;&#31867;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. (arXiv:2304.02496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;ChatGPT&#23478;&#26063;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#34429;&#28982;Fine-tuned&#30340;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#24182;&#26410;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36229;&#36234;&#38382;&#31572;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#23450;&#21046;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;Fine-tuning&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25552;&#21319;&#23637;&#29616;&#20102;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#20854;&#22312;&#26356;&#20855;&#20307;&#30340;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#23478;&#26063;&#27169;&#22411;&#65288;GPT-3.5s&#65292;GPT-4&#65289;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;OpenAI API&#20844;&#20849;&#25509;&#21475;&#20013;&#19981;&#33021;&#20256;&#36882;&#24739;&#32773;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#36807;10000&#20010;&#26679;&#26412;&#20316;&#20026;&#20004;&#20010;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#20195;&#29702;&#36827;&#34892;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26159;&#23558;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20020;&#24202;&#21644;&#25919;&#31574;&#24314;&#35758;&#38472;&#36848;&#24402;&#31867;&#20026;&#20581;&#24247;&#24314;&#35758;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#26159;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#26816;&#27979;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31616;&#21333;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#30340;&#35789;&#34955;&#27169;&#22411;&#65289;&#21644;Fine-tuned&#30340;BioBERT&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23613;&#31649;ChatGPT&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;Fine-tuned&#30340;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#24182;&#26410;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36229;&#36234;&#38382;&#31572;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26356;&#23450;&#21046;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;Fine-tuning&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#19981;&#23545;&#31216;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#37327;&#21270;&#22320;&#35777;&#26126;&#65292;&#19982;&#21517;&#35789;&#30456;&#27604;&#65292;&#21160;&#35789;&#26356;&#38590;&#20064;&#24471;&#65292;&#38656;&#35201;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2304.02492</link><description>&lt;p&gt;
&#37327;&#21270;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22797;&#26434;&#24230;&#22312;&#21160;&#35789;&#20064;&#24471;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition. (arXiv:2304.02492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#19981;&#23545;&#31216;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#37327;&#21270;&#22320;&#35777;&#26126;&#65292;&#19982;&#21517;&#35789;&#30456;&#27604;&#65292;&#21160;&#35789;&#26356;&#38590;&#20064;&#24471;&#65292;&#38656;&#35201;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#23398;&#20064;&#21517;&#35789;&#30340;&#24847;&#20041;&#36890;&#24120;&#27604;&#23398;&#20064;&#21160;&#35789;&#30340;&#24847;&#20041;&#26089;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#26159;&#30001;&#20110;&#35821;&#35328;&#25152;&#25351;&#30340;&#19990;&#30028;&#20013;&#31867;&#21035;&#30340;&#35270;&#35273;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#35821;&#35328;&#26412;&#36523;&#30340;&#32467;&#26500;&#65292;&#36824;&#26159;&#20004;&#31181;&#20449;&#24687;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25152;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#28304;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#21333;&#35789;&#34920;&#31034;&#65292;&#23450;&#37327;&#22320;&#27979;&#35797;&#20102;&#20851;&#20110;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#30340;&#36825;&#19977;&#20010;&#20551;&#35828;&#12290;&#36890;&#36807;&#26816;&#26597;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#65292;&#19982;&#21517;&#35789;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#21160;&#35789;&#30340;&#34920;&#31034;&#22312;&#22495;&#20869;&#36890;&#24120;&#26356;&#21152;&#21464;&#21270;&#21644;&#19981;&#21487;&#36776;&#35782;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#23398;&#20064;&#23454;&#20363;&#65292;&#37027;&#20040;&#21160;&#35789;&#31995;&#32479;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#27604;&#21517;&#35789;&#31995;&#32479;&#20013;&#30340;&#19981;&#22826;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#35821;&#35328;&#21457;&#23637;&#30340;&#36807;&#31243;&#31867;&#20284;&#65292;&#22914;&#26524;&#22312;&#23398;&#20064;&#26399;&#38388;&#26377;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#65292;&#37027;&#20040;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#37117;&#21464;&#24471;&#26356;&#21152;&#21563;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#30340;&#19981;&#23545;&#31216;&#24615;&#33267;&#23569;&#37096;&#20998;&#24402;&#22240;&#20110;&#26356;&#22797;&#26434;&#30340;&#21160;&#35789;&#21547;&#20041;&#65292;&#36825;&#38656;&#35201;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35270;&#35273;&#22797;&#26434;&#24230;&#25110;&#35821;&#35328;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn the meanings of nouns earlier than the meanings of verbs. However, it is unclear whether this asymmetry is a result of complexity in the visual structure of categories in the world to which language refers, the structure of language itself, or the interplay between the two sources of information. We quantitatively test these three hypotheses regarding early verb learning by employing visual and linguistic representations of words sourced from large-scale pre-trained artificial neural networks. Examining the structure of both visual and linguistic embedding spaces, we find, first, that the representation of verbs is generally more variable and less discriminable within domain than the representation of nouns. Second, we find that if only one learning instance per category is available, visual and linguistic representations are less well aligned in the verb system than in the noun system. However, in parallel with the course of human language development, if mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21704;&#24076;&#38543;&#26426;&#25237;&#24433;&#21644;&#37327;&#21270;&#25216;&#26415;&#26377;&#25928;&#37327;&#21270;&#19978;&#19979;&#25991;&#21270;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#38477;&#20302;&#23384;&#20648;&#31354;&#38388;&#30340;&#24320;&#38144;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#22312;&#22810;&#31181;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.02481</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;&#22522;&#20110;&#21704;&#24076;&#30340;&#38543;&#26426;&#25237;&#24433;&#65292;&#29992;&#20110;&#26377;&#25928;&#37327;&#21270;&#19978;&#19979;&#25991;&#21270;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21704;&#24076;&#38543;&#26426;&#25237;&#24433;&#21644;&#37327;&#21270;&#25216;&#26415;&#26377;&#25928;&#37327;&#21270;&#19978;&#19979;&#25991;&#21270;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#38477;&#20302;&#23384;&#20648;&#31354;&#38388;&#30340;&#24320;&#38144;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#22312;&#22810;&#31181;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#23545;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#36890;&#24120;&#38656;&#35201;&#39640;&#25928;&#30340;&#35774;&#32622;&#12290;&#23613;&#31649;&#39044;&#20808;&#35745;&#31639;&#25968;&#25454;&#34920;&#31034;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#32531;&#23384;&#21487;&#20197;&#20943;&#23569;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#37327;&#65292;&#20294;&#36825;&#20250;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#19978;&#25152;&#38656;&#30340;&#23384;&#20648;&#37327;&#38543;&#23454;&#20363;&#25968;&#37327;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#20854;&#27425;&#65292;&#38656;&#35201;&#21457;&#36865;&#22823;&#37327;&#25968;&#25454;&#30340;&#24102;&#23485;&#21040;&#36793;&#32536;&#35774;&#22791;&#12290;&#20026;&#20102;&#20943;&#23569;&#39044;&#20808;&#35745;&#31639;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#23384;&#20648;&#31354;&#38388;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#36229;&#24179;&#38754;&#25237;&#24433;&#12290;&#20026;&#20102;&#23558;&#23427;&#20204;&#30340;&#22823;&#23567;&#36827;&#19968;&#27493;&#32553;&#23567;&#33267;98.96&#65285;&#65292;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#28014;&#28857;&#34920;&#31034;&#37327;&#21270;&#20026;&#20108;&#36827;&#21046;&#21521;&#37327;&#12290;&#23613;&#31649;&#22823;&#23567;&#22823;&#22823;&#32553;&#23567;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#23884;&#20837;&#23545;&#22810;&#31181;&#20445;&#30041;&#20102;94&#65285;-99&#65285;&#28014;&#28857;&#20540;&#30340;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#35757;&#32451;&#27169;&#22411;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and inference on edge devices often requires an efficient setup due to computational limitations. While pre-computing data representations and caching them on a server can mitigate extensive edge device computation, this leads to two challenges. First, the amount of storage required on the server that scales linearly with the number of instances. Second, the bandwidth required to send extensively large amounts of data to an edge device. To reduce the memory footprint of pre-computed data representations, we propose a simple, yet effective approach that uses randomly initialized hyperplane projections. To further reduce their size by up to 98.96%, we quantize the resulting floating-point representations into binary vectors. Despite the greatly reduced size, we show that the embeddings remain effective for training models across various English and German sentence classification tasks that retain 94%--99% of their floating-point.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#27492;&#31867;&#24037;&#20855;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.02478</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#29983;&#20889;&#20316;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#65306;AI&#33021;&#36215;&#21040;&#20160;&#20040;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02478
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#27492;&#31867;&#24037;&#20855;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#30340;&#23398;&#29983;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#25552;&#39640;&#20182;&#20204;&#30340;&#20889;&#20316;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#23398;&#29983;&#30340;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#23548;&#33268;&#26356;&#39640;&#36136;&#37327;&#30340;&#20889;&#20316;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;23&#21517;&#39321;&#28207;&#20013;&#23398;&#29983;&#25776;&#20889;&#25925;&#20107;&#65288;&#21253;&#21547;&#33258;&#24049;&#30340;&#25991;&#23383;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#65289;&#30340;&#23581;&#35797;&#12290;&#20154;&#31867;&#19987;&#23478;&#23545;&#36825;&#20123;&#25925;&#20107;&#36827;&#34892;&#20102;&#20869;&#23481;&#12289;&#35821;&#35328;&#21644;&#32452;&#32455;&#26041;&#38754;&#30340;&#35780;&#20998;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25925;&#20107;&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#32455;&#32467;&#26500;&#21644;&#21477;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#25191;&#34892;&#20102;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#21644;&#32858;&#31867;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#35789;&#35821;&#30340;&#25968;&#37327;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35789;&#35821;&#30340;&#25968;&#37327;&#23545;&#20998;&#25968;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#19982;&#21516;&#40836;&#20154;&#30456;&#27604;&#65292;&#23398;&#29983;&#30340;&#20889;&#20316;&#21487;&#20197;&#20998;&#20026;&#25797;&#38271;&#21644;&#19981;&#25797;&#38271;&#20351;&#29992;&#26356;&#22810;&#25110;&#26356;&#23569;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#20004;&#32452;&#12290;&#32858;&#31867;&#27604;&#36739;&#26174;&#31034;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#36807;&#24230;&#20381;&#36182;&#36825;&#31181;&#24037;&#20855;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
English as foreign language_EFL_students' use of text generated from artificial intelligence_AI_natural language generation_NLG_tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higher-quality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of clusters reveal some benefit of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20854;&#20182;&#20027;&#35201;&#31639;&#27861;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#22914;&#20309;&#26080;&#32541;&#22320;&#24357;&#21512;&#20102;&#35821;&#35328;&#29983;&#25104;&#21644;&#30693;&#35782;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39564;&#35777;ChatGPT&#32467;&#26524;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.02468</link><description>&lt;p&gt;
CHATGPT&#30340;&#27604;&#36739;&#20998;&#26512;&#21450;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of CHATGPT and the evolution of language models. (arXiv:2304.02468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20854;&#20182;&#20027;&#35201;&#31639;&#27861;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#22914;&#20309;&#26080;&#32541;&#22320;&#24357;&#21512;&#20102;&#35821;&#35328;&#29983;&#25104;&#21644;&#30693;&#35782;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39564;&#35777;ChatGPT&#32467;&#26524;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20986;&#29616;&#20197;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20154;&#20204;&#23545;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#34920;&#29616;&#32473;&#20104;&#20102;&#26497;&#22823;&#30340;&#27491;&#38754;&#31038;&#20250;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;ChatGPT&#30340;&#32988;&#21033;&#22312;&#20110;&#23427;&#22914;&#20309;&#26080;&#32541;&#22320;&#24357;&#21512;&#20102;&#35821;&#35328;&#29983;&#25104;&#21644;&#30693;&#35782;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#22797;&#21046;&#20154;&#31867;&#23545;&#30693;&#35782;&#39046;&#22495;&#30340;&#30452;&#35273;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;NLP&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#24819;&#27861;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#26426;&#22120;&#25688;&#35201;&#12289;&#38382;&#31572;&#20197;&#21450;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#8220;&#33258;&#21457;&#36136;&#37327;&#8221;&#65288;SQ&#65289;&#24471;&#20998;&#27604;&#36739;&#20102;ChatGPT&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#20027;&#35201;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36824;&#27010;&#36848;&#20102;&#19968;&#31181;&#39564;&#35777;ChatGPT&#30340;&#35770;&#28857;&#21644;&#32467;&#26524;&#30340;&#31574;&#30053;&#65292;&#20316;&#20026;&#23433;&#20840;&#12289;&#22823;&#35268;&#27169;&#37319;&#29992;LLMs&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in Large Language Models (LLMs) has increased drastically since the emergence of ChatGPT and the outstanding positive societal response to the ease with which it performs tasks in Natural Language Processing (NLP). The triumph of ChatGPT, however, is how it seamlessly bridges the divide between language generation and knowledge models. In some cases, it provides anecdotal evidence of a framework for replicating human intuition over a knowledge domain. This paper highlights the prevailing ideas in NLP, including machine translation, machine summarization, question-answering, and language generation, and compares the performance of ChatGPT with the major algorithms in each of these categories using the Spontaneous Quality (SQ) score. A strategy for validating the arguments and results of ChatGPT is presented summarily as an example of safe, large-scale adoption of LLMs.
&lt;/p&gt;</description></item><item><title>ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.02426</link><description>&lt;p&gt;
ParroT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32842;&#22825;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02426
&lt;/p&gt;
&lt;p&gt;
ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#21644; GPT-4 &#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#32842;&#22825;&#36807;&#31243;&#20013;&#23436;&#25104;&#21508;&#31181;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#30340;API&#35775;&#38382;&#65292;&#36825;&#20026;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ParroT &#26694;&#26550;&#65292;&#22522;&#20110;&#24320;&#28304;LLM&#65288;&#22914;LLaMA-7b&#65289;&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#26469;&#22686;&#24378;&#21644;&#35268;&#33539;&#32842;&#22825;&#32763;&#35793;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ParroT&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#30340;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837; "Hint " &#23383;&#27573;&#20197;&#21152;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25351;&#20196;&#31867;&#22411;&#26469;&#24494;&#35843; ParroT &#27169;&#22411;&#65292;&#21253;&#25324;&#32763;&#35793;&#25351;&#20196;&#12289;&#23545;&#27604;&#25351;&#20196;&#21644;&#35823;&#24046;&#24341;&#23548;&#25351;&#20196;&#12290;&#22312;&#20004;&#20010; Flores &#23376;&#38598;&#21644; WMT22 &#27979;&#35797;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a "Hint" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on two Flores subsets and WMT22 test sets suggest that tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#30005;&#35270;&#21095;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20010;&#20307;&#20010;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#35777;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.02313</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#65306;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Personality-aware Human-centric Multimodal Reasoning: A New Task. (arXiv:2304.02313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#30005;&#35270;&#21095;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20010;&#20307;&#20010;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#35777;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#29702;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#35832;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#22810;&#27169;&#24577;&#20449;&#21495;&#20013;&#36827;&#34892;&#25512;&#29702;&#21644;&#21028;&#26029;&#65292;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#19981;&#21516;&#20010;&#24615;&#30340;&#20154;&#21487;&#33021;&#23545;&#21516;&#19968;&#24773;&#22659;&#20570;&#20986;&#19981;&#21516;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#20010;&#24615;&#36825;&#19968;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65288;Personality-aware HMR&#65289;&#65292;&#24182;&#26681;&#25454;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#30005;&#35270;&#21095;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39044;&#27979;&#29305;&#23450;&#26102;&#21051;&#29305;&#23450;&#20154;&#29289;&#30340;&#34892;&#20026;&#65292;&#22522;&#20110;&#20854;&#36807;&#21435;&#21644;&#26410;&#26469;&#26102;&#21051;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;Myers-Briggs&#31867;&#22411;&#25351;&#26631;&#65288;MBTI&#65289;&#34987;&#27880;&#37322;&#24182;&#29992;&#20110;&#34920;&#31034;&#20010;&#20307;&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#31181;&#22522;&#32447;&#26041;&#27861;&#26469;&#22522;&#20934;&#27979;&#35797;&#35813;&#20219;&#21153;&#65292;&#20854;&#20013;&#20004;&#31181;&#26159;&#20174;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#19968;&#31181;&#26159;&#26032;&#25552;&#20986;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21069;&#26223;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#20010;&#24615;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal reasoning, an area of artificial intelligence that aims at make inferences from multimodal signals such as vision, language and speech, has drawn more and more attention in recent years. People with different personalities may respond differently to the same situation. However, such individual personalities were ignored in the previous studies. In this work, we introduce a new Personality-aware Human-centric Multimodal Reasoning (Personality-aware HMR) task, and accordingly construct a new dataset based on The Big Bang Theory television shows, to predict the behavior of a specific person at a specific moment, given the multimodal information of its past and future moments. The Myers-Briggs Type Indicator (MBTI) was annotated and utilized in the task to represent individuals' personalities. We benchmark the task by proposing three baseline methods, two were adapted from the related tasks and one was newly proposed for our task. The experimental results demonstrate that person
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02247</link><description>&lt;p&gt;
&#12298;&#35299;&#24320;&#32467;&#26500;&#19982;&#39118;&#26684;&#30340;&#32445;&#24102;&#65306;&#36890;&#36807;&#35825;&#23548;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12299;
&lt;/p&gt;
&lt;p&gt;
Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#25919;&#27835;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#35201;&#24046;&#36317;&#36827;&#34892;&#30740;&#31350;&#12290;&#20808;&#21069;&#36827;&#34892;&#30417;&#30563;&#24335;&#25991;&#26723;&#20998;&#31867;&#30340;&#24037;&#20316;&#21487;&#33021;&#20250;&#20559;&#21521;&#21508;&#32593;&#31449;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#21477;&#23376;&#32423;&#35821;&#20041;&#21644;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31181;&#26356;&#24378;&#22823;&#21644;&#19981;&#21463;&#39118;&#26684;&#24433;&#21709;&#30340;&#26816;&#27979;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22836;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#27880;&#24847;&#21147;&#22836;&#30340;&#19981;&#21516;&#38598;&#21512;&#26377;&#25928;&#22320;&#32534;&#30721;&#38271;&#25991;&#26723;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#36825;&#31181;&#22495;&#20381;&#36182;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26032;&#38395;&#20013;&#24120;&#29992;&#30340;&#35805;&#35821;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Ericson&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#20154;&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#20027;&#21160;&#38382;&#39064;&#32454;&#21270;&#21644;&#25512;&#33616;&#30340;&#24847;&#22270;&#25512;&#26029;&#21644;&#23545;&#35805;&#31649;&#29702;&#27169;&#22411;&#65292;&#22312;Alexa Prize&#20013;&#32463;&#36807;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#25628;&#32034;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02233</link><description>&lt;p&gt;
Ericson: &#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Ericson: An Interactive Open-Domain Conversational Search Agent. (arXiv:2304.02233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Ericson&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#20154;&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#20027;&#21160;&#38382;&#39064;&#32454;&#21270;&#21644;&#25512;&#33616;&#30340;&#24847;&#22270;&#25512;&#26029;&#21644;&#23545;&#35805;&#31649;&#29702;&#27169;&#22411;&#65292;&#22312;Alexa Prize&#20013;&#32463;&#36807;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#25628;&#32034;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#65288;ODCS&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#12289;&#26368;&#26032;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#33258;&#28982;&#30340;&#23545;&#35805;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#32454;&#21270;&#24182;&#26368;&#32456;&#22238;&#31572;&#20449;&#24687;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#21644;&#24378;&#22823;&#30340;ODCS&#20195;&#29702;&#20154;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;ODCS&#31995;&#32479;Ericson&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#20027;&#21160;&#38382;&#39064;&#32454;&#21270;&#21644;&#25512;&#33616;&#30340;&#24847;&#22270;&#25512;&#26029;&#21644;&#23545;&#35805;&#31649;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20122;&#39532;&#36874;Alexa Prize&#20013;&#32463;&#36807;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#19982;&#25968;&#21315;&#21517;Alexa&#29992;&#25143;&#36827;&#34892;&#20102;&#23454;&#26102;&#23545;&#35805;&#65292;&#20174;&#32780;&#20026;&#20998;&#26512;ODCS&#31995;&#32479;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#22522;&#30784;&#25552;&#20379;&#20102;&#32463;&#39564;&#20381;&#25454;&#12290;&#25105;&#20204;&#30340;&#20132;&#20114;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#31934;&#30830;&#30340;&#24847;&#22270;&#20998;&#31867;&#12289;&#40723;&#21169;&#29992;&#25143;&#21442;&#19982;&#21644;&#20180;&#32454;&#30340;&#20027;&#21160;&#25512;&#33616;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#20570;&#20986;&#20102;&#26368;&#22823;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29616;&#26377;&#25628;&#32034;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain conversational search (ODCS) aims to provide valuable, up-to-date information, while maintaining natural conversations to help users refine and ultimately answer information needs. However, creating an effective and robust ODCS agent is challenging. In this paper, we present a fully functional ODCS system, Ericson, which includes state-of-the-art question answering and information retrieval components, as well as intent inference and dialogue management models for proactive question refinement and recommendations. Our system was stress-tested in the Amazon Alexa Prize, by engaging in live conversations with thousands of Alexa users, thus providing empirical basis for the analysis of the ODCS system in real settings. Our interaction data analysis revealed that accurate intent classification, encouraging user engagement, and careful proactive recommendations contribute most to the users satisfaction. Our study further identifies limitations of the existing search techniques, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.02210</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Chat-GPT&#21487;&#20197;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#29983;&#25104;&#36830;&#36143;&#65292;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#27969;&#30021;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#25552;&#20379;&#20102;LLMs&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20851;&#27880;&#19977;&#20010;&#26041;&#38754;&#65306;1&#65289;&#35821;&#31687;&#24863;&#30693;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#25552;&#31034;&#23545;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#21644;&#35821;&#31687;&#29616;&#35937;&#30340;&#24433;&#21709;&#65307;2&#65289;&#32763;&#35793;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#27604;&#36739;Chat-GPT&#19982;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#30340;&#32763;&#35793;&#24615;&#33021;&#65307;3&#65289;&#35821;&#31687;&#24314;&#27169;&#33021;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#20013;&#32534;&#30721;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#24182;&#30740;&#31350;&#22521;&#35757;&#25216;&#26415;&#23545;&#35821;&#31687;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35780;&#20272;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;1&#65289;&#21033;&#29992;&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#65292;ChatGPT&#22312;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#65307;2&#65289;&#20462;&#25913;&#26126;&#30830;&#38024;&#23545;&#35821;&#31687;&#29616;&#35937;&#30340;&#25552;&#31034;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65307;3&#65289;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#22521;&#35757;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#37319;&#29992;ChatGPT&#36741;&#21161;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#20855;&#26377;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#32763;&#35793;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.02182</link><description>&lt;p&gt;
&#21457;&#25496;ChatGPT&#32763;&#35793;&#30340;&#33021;&#21147;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of ChatGPT for Translation: An Empirical Study. (arXiv:2304.02182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#37319;&#29992;ChatGPT&#36741;&#21161;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#20855;&#26377;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#32763;&#35793;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;ChatGPT&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#32763;&#35793;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#20351;&#29992;ChatGPT&#36741;&#21161;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#32763;&#35793;&#20013;&#37319;&#29992;&#20102;&#20960;&#20010;&#32763;&#35793;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#35774;&#35745;&#22909;&#30340;&#32763;&#35793;&#25552;&#31034;&#30340;ChatGPT&#21487;&#20197;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#20013;&#36798;&#21040;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#32763;&#35793;&#19978;&#20005;&#37325;&#28382;&#21518;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25991;&#26412;&#23545;&#32763;&#35793;&#36136;&#37327;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#30456;&#23545;&#20110;&#19987;&#19994;&#31995;&#32479;&#34920;&#29616;&#26356;&#21152;&#20248;&#24322;&#12290;&#25105;&#20204;&#36824;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#32763;&#35793;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;ChatGPT&#33021;&#22815;&#36798;&#21040;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation. Machine translation is an important and extensively studied task in the field of natural language processing, which heavily relies on the abilities of language understanding and generation. Thus, in this paper, we explore how to assist machine translation with ChatGPT. We adopt several translation prompts on a wide range of translations. Our experimental results show that ChatGPT with designed translation prompts can achieve comparable or better performance over professional translation systems for high-resource language translations but lags behind significantly on low-resource translations. We further evaluate the translation quality using multiple references, and ChatGPT achieves superior performance compared to the professional systems. We also conduct experiments on domain-specific translations, the final results show that ChatGPT is able to compre
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#21311;&#21517;&#21270;&#22312; COVID-19 &#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#23545;&#35821;&#38899;&#35786;&#26029;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02181</link><description>&lt;p&gt;
&#20851;&#20110;&#22768;&#38899;&#21311;&#21517;&#21270;&#23545;&#22522;&#20110;&#35821;&#38899;&#30340;COVID-19&#26816;&#27979;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection. (arXiv:2304.02181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#21311;&#21517;&#21270;&#22312; COVID-19 &#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#23545;&#35821;&#38899;&#35786;&#26029;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20174;&#20010;&#20154;&#21161;&#29702;&#12289;&#24773;&#24863;&#35745;&#31639;&#21040;&#36828;&#31243;&#30142;&#30149;&#35786;&#26029;&#12290;&#30001;&#20110;&#22768;&#38899;&#21516;&#26102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35821;&#29992;&#20449;&#24687;&#65288;&#22914;&#35821;&#38899;&#38899;&#35843;&#12289;&#35821;&#35843;&#12289;&#35821;&#36895;&#12289;&#22768;&#38899;&#22823;&#23567;&#65289;&#65292;&#22240;&#27492;&#20445;&#25252;&#35828;&#35805;&#32773;&#30340;&#38544;&#31169;&#21644;&#36523;&#20221;&#30340;&#22768;&#38899;&#21311;&#21517;&#21270;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#22768;&#38899;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65292;&#37325;&#28857;&#26159;&#21435;&#38500;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24773;&#24863;&#35745;&#31639;&#21644;&#30142;&#30149;&#30417;&#27979;&#24212;&#29992;&#32780;&#35328;&#65292;&#35821;&#29992;&#20869;&#23481;&#21487;&#33021;&#26356;&#20026;&#20851;&#38190;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#21311;&#21517;&#21270;&#21487;&#33021;&#23545;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#19987;&#27880;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#20581;&#24247;&#30417;&#27979;&#24212;&#29992;&#65306;&#22522;&#20110;&#35821;&#38899;&#30340;COVID-19&#35786;&#26029;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#21450;&#20854;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;COVID-19&#35786;&#26029;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances seen in deep learning, voice-based applications are burgeoning, ranging from personal assistants, affective computing, to remote disease diagnostics. As the voice contains both linguistic and paralinguistic information (e.g., vocal pitch, intonation, speech rate, loudness), there is growing interest in voice anonymization to preserve speaker privacy and identity. Voice privacy challenges have emerged over the last few years and focus has been placed on removing speaker identity while keeping linguistic content intact. For affective computing and disease monitoring applications, however, the paralinguistic content may be more critical. Unfortunately, the effects that anonymization may have on these systems are still largely unknown. In this paper, we fill this gap and focus on one particular health monitoring application: speech-based COVID-19 diagnosis. We test two popular anonymization methods and their impact on five different state-of-the-art COVID-19 diagnostic system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.02168</link><description>&lt;p&gt;
I2I: &#29992;&#25913;&#36827;&#30340;&#30693;&#35782;&#21021;&#22987;&#21270;&#36716;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25509;&#22120;&#26159;&#24310;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#27169;&#22359;&#38169;&#22833;&#20102;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Improvise to Initialize (I2I) &#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#65292;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102; I2I &#22312; CLiMB&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992; I2I &#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#22987;&#32456;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20419;&#36827;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#36827;&#30340; AdapterFusion&#65292;I2I &#20063;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#32780;&#19981;&#20135;&#29983;&#30456;&#20851;&#30340;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#22810;&#20010;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#24182;&#22312;&#23545;&#40784;&#25968;&#25454;&#38598;&#29305;&#24449;&#31354;&#38388;&#21644;&#22788;&#29702;&#26631;&#31614;&#20559;&#31227;&#26041;&#38754;&#20570;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#36947;&#24503;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02144</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#36947;&#24503;&#23398;&#20064;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Data Fusion Framework for Multi-Domain Morality Learning. (arXiv:2304.02144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#22810;&#20010;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#24182;&#22312;&#23545;&#40784;&#25968;&#25454;&#38598;&#29305;&#24449;&#31354;&#38388;&#21644;&#22788;&#29702;&#26631;&#31614;&#20559;&#31227;&#26041;&#38754;&#20570;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#36947;&#24503;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#36947;&#24503;&#24773;&#24863;&#65292;&#20174;&#32780;&#21019;&#36896;&#20102;&#30740;&#31350;&#36947;&#24503;&#22312;&#20154;&#31867;&#29983;&#27963;&#20013;&#25198;&#28436;&#35282;&#33394;&#30340;&#26032;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#22240;&#20026;&#25968;&#25454;&#37319;&#38598;&#26041;&#27861;&#12289;&#39046;&#22495;&#12289;&#20027;&#39064;&#12289;&#26631;&#27880;&#32773;&#35828;&#26126;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#32780;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#31616;&#21333;&#22320;&#22312;&#35757;&#32451;&#20013;&#32858;&#21512;&#22810;&#20010;&#36825;&#31181;&#24322;&#36136;&#24615;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20010;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#32852;&#21512;&#35757;&#32451;&#30340;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#37319;&#29992;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;&#25968;&#25454;&#38598;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#40784;&#65292;&#21516;&#26102;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26469;&#22788;&#29702;&#26631;&#31614;&#20559;&#31227;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can be trained to recognize the moral sentiment of text, creating new opportunities to study the role of morality in human life. As interest in language and morality has grown, several ground truth datasets with moral annotations have been released. However, these datasets vary in the method of data collection, domain, topics, instructions for annotators, etc. Simply aggregating such heterogeneous datasets during training can yield models that fail to generalize well. We describe a data fusion framework for training on multiple heterogeneous datasets that improve performance and generalizability. The model uses domain adversarial training to align the datasets in feature space and a weighted loss function to deal with label shift. We show that the proposed framework achieves state-of-the-art performance in different datasets compared to prior works in morality inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30528;&#37325;&#35752;&#35770;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.02138</link><description>&lt;p&gt;
&#22303;&#26408;&#40550;&#40521;&#20256;&#22855;(GPT)&#65306;&#21033;&#29992;&#21450;&#26102;&#24037;&#31243;&#20811;&#26381;GPT&#24187;&#35273;&#20197;&#22312;&#23721;&#22303;&#24037;&#31243;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. (arXiv:2304.02138v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30528;&#37325;&#35752;&#35770;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#21253;&#25324;&#23721;&#22303;&#24037;&#31243;&#22312;&#20869;&#30340;&#21508;&#20010;&#34892;&#19994;&#12290; &#20294;&#26159;&#65292;GPT&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#21548;&#36215;&#26469;&#24456;&#26377;&#36947;&#29702;&#20294;&#38169;&#35823;&#30340;&#36755;&#20986;&#65292;&#23548;&#33268;&#24187;&#35273;&#20135;&#29983;&#12290; &#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#21644;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#26041;&#38754;&#65292;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290; &#25105;&#20204;&#25506;&#35752;&#20102;&#19982;LLM&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#24182;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#22312;&#30830;&#20445;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29305;&#23450;&#20110;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#24320;&#21457;&#20197;&#21450;LLM&#25104;&#20026;&#22797;&#26434;&#20219;&#21153;&#65288;&#20363;&#22914;&#25968;&#25454;&#20998;&#26512;&#21644;&#35774;&#35745;&#65289;&#30340;&#33258;&#28982;&#30028;&#38754;&#30340;&#28508;&#21147;&#12290; &#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290; &#36890;&#36807;&#23558;GPT&#38598;&#25104;&#21040;&#23721;&#22303;&#24037;&#31243;&#24037;&#20316;&#27969;&#20013;&#65292;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#31616;&#21270;&#20182;&#20204;&#30340;&#24037;&#20316;&#24182;&#21457;&#23637;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of large language models (LLMs), such as OpenAI's ChatGPT, could revolutionized various industries, including geotechnical engineering. However, GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations. In this article, we discuss the importance of prompt engineering in mitigating these risks and harnessing the full potential of GPT for geotechnical applications. We explore the challenges and pitfalls associated with LLMs and highlight the role of context in ensuring accurate and valuable responses. Furthermore, we examine the development of context-specific search engines and the potential of LLMs to become a natural interface for complex tasks, such as data analysis and design. We also develop a unified interface using natural language to handle complex geotechnical engineering tasks and data analysis. By integrating GPT into geotechnical engineering workflows, professionals can streamline their work and develop sustain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23545;&#40784;&#35270;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.02080</link><description>&lt;p&gt;
&#26080;&#38656;&#23545;&#40784;&#35270;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#25193;&#23637;&#20934;&#30830;&#30340;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data. (arXiv:2304.02080v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23545;&#40784;&#35270;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#22312;&#22270;&#24418;&#25991;&#26412;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#20026;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#38598;&#21644;&#25366;&#25496;&#25216;&#26415;&#23384;&#22312;&#22810;&#20010;&#38480;&#21046;&#65292;&#22914;&#23545;&#40784;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12289;&#25968;&#25454;&#32570;&#20047;&#22810;&#26679;&#24615;&#20197;&#21450;&#23545;&#40784;&#25968;&#25454;&#30340;&#37319;&#38598;&#38590;&#24230;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#24182;&#34892;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual back
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#19988;&#28145;&#30340;Transformer&#20013;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20449;&#21495;&#20256;&#25773;&#65292;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#23485;&#24230;&#32553;&#25918;&#24314;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.02034</link><description>&lt;p&gt;
&#21021;&#22987;&#21270;&#26102;Transformer&#30340;&#26377;&#25928;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Effective Theory of Transformers at Initialization. (arXiv:2304.02034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#19988;&#28145;&#30340;Transformer&#20013;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20449;&#21495;&#20256;&#25773;&#65292;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#23485;&#24230;&#32553;&#25918;&#24314;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23485;&#19988;&#28145;&#30340;Transformer&#65288;&#21363;&#20351;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#22359;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#30340;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65289;&#20013;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20449;&#21495;&#20256;&#25773;&#36827;&#34892;&#20102;&#26377;&#25928;&#29702;&#35770;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#24314;&#35758;&#36825;&#20123;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#37319;&#29992;&#29305;&#23450;&#30340;&#23485;&#24230;&#32553;&#25918;&#12290;&#25105;&#20204;&#38543;&#21518;&#37319;&#29992;&#36825;&#20123;&#24314;&#35758;&#65292;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;Transformer&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, i.e., residual neural networks with multi-head self-attention blocks and multilayer perceptron blocks. This analysis suggests particular width scalings of initialization and training hyperparameters for these models. We then take up such suggestions, training Vision and Language Transformers in practical setups.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#37327;LMMs&#23398;&#26415;&#25991;&#29486;&#30340;&#35745;&#37327;&#23398;&#21644;&#35805;&#35821;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#32773;&#12289;&#23454;&#36341;&#32773;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;LMMs&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.02020</link><description>&lt;p&gt;
2017&#24180;&#33267;2023&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Bibliometric Review of Large Language Models Research from 2017 to 2023. (arXiv:2304.02020v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#37327;LMMs&#23398;&#26415;&#25991;&#29486;&#30340;&#35745;&#37327;&#23398;&#21644;&#35805;&#35821;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#32773;&#12289;&#23454;&#36341;&#32773;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;LMMs&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#31867;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20854;&#29983;&#25104;&#20154;&#31867;&#21270;&#35821;&#35328;&#30340;&#33021;&#21147;&#21644;&#38761;&#21629;&#24615;&#30340;&#31185;&#25216;&#28508;&#21147;&#65292;&#24050;&#25104;&#20026;&#23398;&#26415;&#30740;&#31350;&#39046;&#22495;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#28909;&#38376;&#35838;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#26415;&#25991;&#29486;&#30340;&#35745;&#37327;&#23398;&#21644;&#35805;&#35821;&#20998;&#26512;&#65292;&#32508;&#21512;&#20102;&#36229;&#36807;5000&#31687;&#25991;&#31456;&#65292;&#20026;&#30740;&#31350;&#32773;&#12289;&#23454;&#36341;&#32773;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;LMMs&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26159;&#24403;&#21069;LMMs&#30740;&#31350;&#39046;&#22495;&#30340;&#19968;&#20221;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2017&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#36235;&#21183;&#21450;&#30740;&#31350;&#27169;&#24335;&#19982;&#21512;&#20316;&#27169;&#24335;&#30340;&#29305;&#28857;&#65292;&#20998;&#26512;&#20102;LMMs&#30740;&#31350;&#20013;&#30340;&#26680;&#24515;&#31639;&#27861;&#24320;&#21457;&#21644;NLP&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;LLMs&#22312;&#21307;&#23398;&#12289;&#24037;&#31243;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#25991;&#23398;&#31185;&#31561;&#19981;&#21516;&#39046;&#22495;&#21644;&#24212;&#29992;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#36824;&#25581;&#31034;&#20102;LMMs&#30740;&#31350;&#39046;&#22495;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dyn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.02017</link><description>&lt;p&gt;
&#35299;&#38145;ChatGPT&#30340;&#28508;&#21147;&#65306;&#23545;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#12289;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#21307;&#30103;&#35786;&#26029;&#27835;&#30103;&#12290;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#20934;&#30830;&#24615;&#20351;&#20854;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;ChatGPT&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20854;&#20542;&#21521;&#20110;&#20135;&#29983;&#26377;&#20559;&#35265;&#30340;&#21709;&#24212;&#20197;&#21450;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#35821;&#35328;&#27169;&#24335;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;ChatGPT&#21450;&#20854;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#35265;&#35299;&#65292;&#20026;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#23545;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#30340;&#24433;&#21709;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#21644;&#19987;&#19994;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;API&#23558;&#23545;&#35937;&#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#20174;&#32780;&#29983;&#25104;&#36866;&#21512;&#20110;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#26032;&#39062;&#30340;&#33756;&#35889;&#21345;&#12290;</title><link>http://arxiv.org/abs/2304.02016</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#22810;&#27169;&#22359;&#30340;AI&#22823;&#21416;&#65306;&#22522;&#20110;&#22270;&#20687;&#30340;&#22797;&#26434;&#33756;&#35889;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery. (arXiv:2304.02016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#21644;&#19987;&#19994;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;API&#23558;&#23545;&#35937;&#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#20174;&#32780;&#29983;&#25104;&#36866;&#21512;&#20110;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#26032;&#39062;&#30340;&#33756;&#35889;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31038;&#21306;&#37319;&#29992;&#22810;&#24863;&#23448;&#25110;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#25512;&#36827;&#36825;&#19968;&#20195;AI&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#23558;&#35821;&#35328;&#21644;&#22270;&#20687;&#30456;&#32467;&#21512;&#20195;&#34920;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#29087;&#24713;&#26041;&#27861;&#65292;&#20363;&#22914;&#20174;&#25551;&#36848;&#20013;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#25110;&#22270;&#20687;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#21333;&#29255;&#24335;&#26041;&#27861;&#19982;&#22522;&#20110;&#37319;&#29992;&#22270;&#20687;&#27169;&#22411;&#26631;&#35760;&#23545;&#35937;&#30340;&#36731;&#37327;&#32423;&#21644;&#19987;&#19994;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#28982;&#21518;&#20018;&#34892;&#25552;&#20132;&#27492;&#32467;&#26524;&#23545;&#35937;&#21015;&#34920;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#12290;&#22810;&#20010;API&#30340;&#20351;&#29992;&#20351;&#24471;&#27491;&#30830;&#23545;&#35937;&#21015;&#34920;&#30340;&#24179;&#22343;&#31934;&#24230;&#36798;&#21040;95%&#20197;&#19978;&#65292;&#36825;&#20123;&#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#26368;&#26032;&#30340;Open AI&#25991;&#26412;&#29983;&#25104;&#22120;(GPT-4)&#12290;&#20026;&#20102;&#28436;&#31034;API&#20316;&#20026;&#27169;&#22359;&#21270;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#29992;&#25143;&#25293;&#19979;&#20912;&#31665;&#20013;&#26377;&#21738;&#20123;&#39135;&#26448;&#30340;&#29031;&#29255;&#65292;&#28982;&#21518;&#29983;&#25104;&#36866;&#21512;&#20110;&#25104;&#26412;&#12289;&#20934;&#22791;&#26102;&#38388;&#12289;&#39278;&#39135;&#38480;&#21046;&#12289;&#20998;&#37327;&#22823;&#23567;&#21644;&#22810;&#20010;&#22240;&#32032;&#30340;&#26032;&#39062;&#33756;&#35889;&#21345;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The AI community has embraced multi-sensory or multi-modal approaches to advance this generation of AI models to resemble expected intelligent understanding. Combining language and imagery represents a familiar method for specific tasks like image captioning or generation from descriptions. This paper compares these monolithic approaches to a lightweight and specialized method based on employing image models to label objects, then serially submitting this resulting object list to a large language model (LLM). This use of multiple Application Programming Interfaces (APIs) enables better than 95% mean average precision for correct object lists, which serve as input to the latest Open AI text generator (GPT-4). To demonstrate the API as a modular alternative, we solve the problem of a user taking a picture of ingredients available in a refrigerator, then generating novel recipe cards tailored to complex constraints on cost, preparation time, dietary restrictions, portion sizes, and multip
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#26415;&#25968;&#25454;&#38598; MATH 401 &#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#24182;&#20855;&#20307;&#27979;&#35797;&#20102; GPT-4&#12289;ChatGPT&#12289;InstrctGPT&#12289;Galactica &#21644; LLaMA &#31561;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.02015</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How well do Large Language Models perform in Arithmetic tasks?. (arXiv:2304.02015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02015
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#26415;&#25968;&#25454;&#38598; MATH 401 &#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#24182;&#20855;&#20307;&#27979;&#35797;&#20102; GPT-4&#12289;ChatGPT&#12289;InstrctGPT&#12289;Galactica &#21644; LLaMA &#31561;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20855;&#22791;&#20102;&#36830;&#36143;&#24605;&#36335;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36880;&#27493;&#35299;&#31572;&#25968;&#23398;&#38382;&#39064;&#12290;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#19981;&#20165;&#38656;&#35201;&#36890;&#36807;&#24605;&#32500;&#36830;&#36143;&#30340;&#33021;&#21147;&#20998;&#35299;&#38382;&#39064;&#65292;&#36824;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#27493;&#39588;&#27491;&#30830;&#35745;&#31639;&#31639;&#26415;&#34920;&#36798;&#24335;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#19987;&#38376;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#26415;&#25968;&#25454;&#38598; MATH 401&#65292;&#29992;&#20110;&#27979;&#35797;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324; GPT-4&#12289;ChatGPT&#12289;InstrctGPT&#12289;Galactica &#21644; LLaMA&#65292;&#20854;&#20013;&#28041;&#21450;&#21508;&#31181;&#31639;&#26415;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;MATH 401 &#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20195;&#30721;&#24050;&#22312; \url{https://github.com/GanjinZero/math401-llm} &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \url{https://github.com/GanjinZero/math401-llm}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.01905</link><description>&lt;p&gt;
&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#26102;&#30340;&#39640;&#25928;&#21796;&#37266;&#35789;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#39640;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#24182;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#21033;&#29992;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#21738;&#20010;&#27880;&#24847;&#21147;&#32593;&#32476;&#25191;&#34892;&#36755;&#20837;&#38899;&#39057;&#24103;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#36335;&#24452;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#20316;&#32773;&#26377;&#25928;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20351;&#29992;&#20316;&#32773;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#26102;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21452;&#20851;&#27880;&#32593;&#32476;&#21487;&#20197;&#23558;&#21796;&#37266;&#35789;&#38899;&#39057;&#24103;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;$90\%$&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#20165;&#22686;&#21152;$1\%$&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;F1&#24471;&#20998;$16\%$&#65292;&#24182;&#23558;&#19968;&#33324;&#30340;&#32597;&#35265;&#35789;&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;$3\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.01492</link><description>&lt;p&gt;
&#32479;&#19968;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#19982;&#20256;&#25773;&#32467;&#26500;&#29992;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#35875;&#35328;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20256;&#25773;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#31639;&#27861;&#23637;&#31034;&#20102;&#22312;&#21069;&#20960;&#22825;&#26032;&#38395;&#19978;&#33391;&#22909;&#24615;&#33021;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#24456;&#38590;&#21457;&#29616;&#19982;&#39044;&#26399;&#20107;&#20214;&#26377;&#20851;&#30340;&#35875;&#35328;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#65288;&#21363;&#20302;&#36164;&#28304;&#29615;&#22659;&#65289;&#20013;&#20256;&#25773;&#30340;&#35875;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#35875;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#26080;&#21521;&#25299;&#25169;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#32479;&#19968;&#23545;&#27604;&#33539;&#24335;&#36827;&#34892;Multi-scale&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#30830;&#22320;&#31361;&#30772;&#20102;&#39046;&#22495;&#21644;/&#25110;&#35821;&#35328;&#38382;&#39064;&#30340;&#38556;&#30861;&#65292;&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01487</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#65292;&#36824;&#26159;&#19981;&#32842;&#22825;GPT&#65306;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;GPT&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20840;&#29699;&#24863;&#30693;&#12290;&#38543;&#30528;&#32842;&#22825;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#20182;&#20204;&#30340;&#35823;&#29992;&#30340;&#25285;&#24551;&#20063;&#22686;&#21152;&#20102;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#28040;&#24687;&#65292;&#25220;&#34989;&#65292;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#65292;&#27450;&#39575;&#21644;&#27450;&#35784;&#12290;&#22240;&#27492;&#65292;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#21040;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#20123;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#32479;&#35745;&#29305;&#24449;&#25110;&#21477;&#27861;&#27169;&#24335;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#21253;&#21547;&#35821;&#20041;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#26368;&#26032;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#21644;&#29616;&#20195;&#21270;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20854;&#20182;&#26410;&#19987;&#38376;&#22768;&#31216;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#32534;&#20889;&#21644;&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#30495;&#23454;&#24847;&#22270;&#30340;&#23545;&#35805;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#22522;&#20110;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#33258;&#21160;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#34920;&#21644;&#22312;&#27599;&#20010;&#22238;&#21512;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;&#20195;&#29702;&#21709;&#24212;&#12290;&#35813;&#30740;&#31350;&#23545;&#29616;&#26377;&#27169;&#22411;&#25345;&#32493;&#25552;&#20986;&#25361;&#25112;&#65292;&#40723;&#21169;&#26356;&#22810;&#22522;&#20110;&#23545;&#35805;&#30340;&#25968;&#25454;&#26816;&#32034;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.01412</link><description>&lt;p&gt;
StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65306;&#36890;&#36807;&#30495;&#23454;&#24847;&#22270;&#30340;&#23545;&#35805;&#26816;&#32034;&#25968;&#25454;&#34920;
&lt;/p&gt;
&lt;p&gt;
The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01412
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#30495;&#23454;&#24847;&#22270;&#30340;&#23545;&#35805;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#22522;&#20110;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#33258;&#21160;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#34920;&#21644;&#22312;&#27599;&#20010;&#22238;&#21512;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;&#20195;&#29702;&#21709;&#24212;&#12290;&#35813;&#30740;&#31350;&#23545;&#29616;&#26377;&#27169;&#22411;&#25345;&#32493;&#25552;&#20986;&#25361;&#25112;&#65292;&#40723;&#21169;&#26356;&#22810;&#22522;&#20110;&#23545;&#35805;&#30340;&#25968;&#25454;&#26816;&#32034;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;StatCan&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;19379&#27425;&#20195;&#34920;Statistics Canada&#30340;&#20195;&#29702;&#21644;&#22312;&#32447;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#36716;&#25442;&#65292;&#28041;&#21450;&#30495;&#23454;&#24847;&#22270;&#65292;&#20351;&#29992;&#33521;&#35821;&#25110;&#27861;&#35821;&#36827;&#34892;&#65292;&#20195;&#29702;&#20250;&#26816;&#32034;&#21040;5000&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#34920;&#20043;&#19968;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#65288;1&#65289;&#22522;&#20110;&#27491;&#22312;&#36827;&#34892;&#30340;&#23545;&#35805;&#33258;&#21160;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#34920;&#65292;&#65288;2&#65289;&#22312;&#27599;&#20010;&#22238;&#21512;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;&#20195;&#29702;&#21709;&#24212;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24378;&#22522;&#32447;&#26469;&#30740;&#31350;&#27599;&#20010;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#21106;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#38590;&#20197;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#23545;&#35805;&#20013;&#65292;&#24403;&#25105;&#20204;&#20174;&#39564;&#35777;&#38598;&#31227;&#21160;&#21040;&#27979;&#35797;&#38598;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#37117;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#22312;&#20309;&#26102;&#36820;&#22238;&#34920;&#26684;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#20219;&#21153;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#22522;&#20110;&#23545;&#35805;&#30340;&#25968;&#25454;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we enc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#20351;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01352</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#20351;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#35821;&#35328;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#26102;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#35768;&#22810;&#35821;&#35328;&#65292;&#21253;&#25324;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#22312;&#27861;&#35821;&#12289;&#20420;&#35821;&#21644;&#20122;&#32654;&#23612;&#20122;&#35821;&#31561;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple cross-lingual plagiarism detection method applicable to a large number of languages. The presented approach leverages open multilingual thesauri for candidate retrieval task and pre-trained multilingual BERT-based language models for detailed analysis. The method does not rely on machine translation and word sense disambiguation when in use, and therefore is suitable for a large number of languages, including under-resourced languages. The effectiveness of the proposed approach is demonstrated for several existing and new benchmarks, achieving state-of-the-art results for French, Russian, and Armenian languages.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01240</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;
&lt;/p&gt;
&lt;p&gt;
Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#35775;&#38382;&#21307;&#30103;&#36164;&#28304;&#30340;&#24120;&#35265;&#21407;&#22240;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#24515;&#29702;&#20581;&#24247;&#30340;&#37325;&#21472;&#26041;&#38754;&#12290;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#30740;&#31350;&#27492;&#37325;&#21472;&#30340;&#33391;&#22909;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30140;&#30171;&#30340;&#22823;&#37327;&#20449;&#24687;&#20445;&#23384;&#22312;&#36825;&#20123;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#20013;&#65292;&#30001;&#20110;&#20854;&#27495;&#20041;&#24615;&#65292;&#30140;&#30171;&#30340;&#25552;&#21450;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#12290;&#26412;&#39033;&#30446;&#20351;&#29992;&#21311;&#21517;&#30340;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#35752;&#35770;&#24739;&#32773;&#30140;&#30171;&#25110;&#19981;&#35752;&#35770;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30140;&#30171;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#36755;&#20986;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#12290;&#20849;&#25163;&#21160;&#19977;&#37325;&#27880;&#37322;&#20102;1,985&#20221;&#25991;&#20214;&#65292;&#20197;&#21019;&#24314;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#19977;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#20026;0.787&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01238</link><description>&lt;p&gt;
Spam-T5&#65306;&#22522;&#20110;&#23567;&#26679;&#26412;&#30340;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BERT-like&#12289;Sentence Transformers&#21644;Seq2Seq&#65289;&#20197;&#21450;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;LightGBM&#65289;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65288;&#23436;&#25972;&#35757;&#32451;&#38598;&#21644;&#23567;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; &#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;LLMs&#20248;&#20110;&#22522;&#32447;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;LLMs&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#32463;&#24120;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19987;&#38376;&#20026;&#26816;&#27979;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#32780;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Spam-T5&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;AUDIT&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#21644;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#35299;&#20915;&#20102;&#20808;&#21069;&#25193;&#25955;-based&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00830</link><description>&lt;p&gt;
AUDIT&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models. (arXiv:2304.00830v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00830
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;AUDIT&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#21644;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#35299;&#20915;&#20102;&#20808;&#21069;&#25193;&#25955;-based&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#32534;&#36753;&#21487;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#65292;&#20363;&#22914;&#28155;&#21152;&#32972;&#26223;&#38899;&#25928;&#12289;&#26367;&#25442;&#20048;&#22120;&#21644;&#20462;&#22797;&#25439;&#22351;&#30340;&#38899;&#39057;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20197;&#36755;&#20986;&#38899;&#39057;&#30340;&#25991;&#26412;&#35828;&#26126;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#21644;&#21435;&#22122;&#36807;&#31243;&#26469;&#23454;&#29616;&#38646;-shot&#38899;&#39057;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;1&#65289;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#35757;&#32451;&#29992;&#20110;&#32534;&#36753;&#20219;&#21153;&#65292;&#19981;&#33021;&#20445;&#35777;&#33391;&#22909;&#30340;&#32534;&#36753;&#25928;&#26524;&#65307;2&#65289;&#23427;&#20204;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#20462;&#25913;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#38899;&#39057;&#29255;&#27573;&#65307;3&#65289;&#20182;&#20204;&#38656;&#35201;&#36755;&#20986;&#38899;&#39057;&#30340;&#23436;&#25972;&#25551;&#36848;&#65292;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#25110;&#24517;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUDIT&#65292;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;AUDIT&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#35774;&#35745;&#29305;&#28857;&#65306;1&#65289;&#25105;&#20204;&#20026;&#19981;&#21516;&#30340;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#26500;&#24314;&#19977;&#20803;&#35757;&#32451;&#25968;&#25454;&#65288;&#25351;&#20196;&#65292;&#36755;&#20837;&#38899;&#39057;&#65292;&#36755;&#20986;&#38899;&#39057;&#65289;&#24182;&#20351;&#29992;&#25351;&#20196;&#21644;&#36755;&#20837;&#65288;&#35201;&#32534;&#36753;&#30340;&#38899;&#39057;&#65289;&#38899;&#39057;&#23545;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65307;2&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#20351;&#29992;&#24456;&#23569;&#37327;&#30340;&#32534;&#36753;&#20219;&#21153;&#38899;&#39057;&#25968;&#25454;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65307;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#65292;&#33258;&#21160;&#23558;&#36755;&#20837;&#25351;&#20196;&#19982;&#35201;&#32534;&#36753;&#30340;&#30456;&#24212;&#38899;&#39057;&#29255;&#27573;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AUDIT&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edite
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#12289;ChatGPT&#21644;GPT-3&#22312;&#26085;&#26412;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;GPT-4&#20248;&#20110;&#20854;&#20182;&#20004;&#32773;&#65292;&#21576;&#29616;&#20986;LLMs&#22312;&#19982;&#33521;&#35821;&#36828;&#31163;&#30340;&#35821;&#35328;&#20013;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;LLM API&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#24314;&#35758;&#23454;&#26045;&#31105;&#27490;&#30340;&#21307;&#30103;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2303.18027</link><description>&lt;p&gt;
&#22312;&#26085;&#26412;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#35780;&#20272;GPT-4&#21644;ChatGPT
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations. (arXiv:2303.18027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#12289;ChatGPT&#21644;GPT-3&#22312;&#26085;&#26412;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;GPT-4&#20248;&#20110;&#20854;&#20182;&#20004;&#32773;&#65292;&#21576;&#29616;&#20986;LLMs&#22312;&#19982;&#33521;&#35821;&#36828;&#31163;&#30340;&#35821;&#35328;&#20013;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;LLM API&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#24314;&#35758;&#23454;&#26045;&#31105;&#27490;&#30340;&#21307;&#30103;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#23427;&#20204;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#30340;&#34892;&#20026;&#12289;&#22833;&#35823;&#21644;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM API&#65288;ChatGPT&#12289;GPT-3&#21644;GPT-4&#65289;&#22312;&#36807;&#21435;5&#24180;&#30340;&#26085;&#26412;&#22269;&#23478;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#21253;&#25324;&#20197;&#26085;&#35821;&#20026;&#27597;&#35821;&#30340;NLP&#30740;&#31350;&#20154;&#21592;&#21644;&#22312;&#26085;&#26412;&#24037;&#20316;&#30340;&#19968;&#21517;&#23454;&#36341;&#24515;&#33039;&#30149;&#21307;&#24072;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPT-4&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;GPT-3&#65292;&#24182;&#36890;&#36807;&#20102;&#25152;&#26377;&#20116;&#24180;&#30340;&#32771;&#35797;&#65292;&#31361;&#26174;LLMs&#22312;&#19982;&#33521;&#35821;&#36828;&#31163;&#30340;&#35821;&#35328;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#36824;&#26292;&#38706;&#20102;&#24403;&#21069;LLM API&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;LLMs&#26377;&#26102;&#20250;&#36873;&#25321;&#22312;&#26085;&#26412;&#21307;&#30103;&#23454;&#36341;&#20013;&#24212;&#35813;&#20005;&#26684;&#36991;&#20813;&#30340;&#31105;&#27490;&#36873;&#25321;&#65292;&#20363;&#22914;&#24314;&#35758;&#23454;&#26045;&#23433;&#20048;&#27515;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;API&#25104;&#26412;&#26222;&#36941;&#36739;&#39640;&#65292;&#26368;&#22823;&#19978;&#19979;&#25991;&#22823;&#23567;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) gain popularity among speakers of diverse languages, we believe that it is crucial to benchmark them to better understand model behaviors, failures, and limitations in languages beyond English. In this work, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national medical licensing examinations from the past five years. Our team comprises native Japanese-speaking NLP researchers and a practicing cardiologist based in Japan. Our experiments show that GPT-4 outperforms ChatGPT and GPT-3 and passes all five years of the exams, highlighting LLMs' potential in a language that is typologically distant from English. However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size is smaller fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;ASR&#20013;&#30340;Transformer&#27169;&#22411;&#30340;&#22359;&#37325;&#29992;&#31574;&#30053;&#24182;&#37197;&#20197;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#32039;&#20945;&#65292;&#21487;&#36866;&#24212;&#24615;&#26356;&#24378;&#65292;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.13072</link><description>&lt;p&gt;
&#36229;&#36234;&#36890;&#29992;Transformer&#65306;&#33258;&#36866;&#24212;&#27169;&#22359;&#22312;ASR&#20013;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit. (arXiv:2303.13072v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;ASR&#20013;&#30340;Transformer&#27169;&#22411;&#30340;&#22359;&#37325;&#29992;&#31574;&#30053;&#24182;&#37197;&#20197;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#32039;&#20945;&#65292;&#21487;&#36866;&#24212;&#24615;&#26356;&#24378;&#65292;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290; Transformer&#27169;&#22411;&#20351;&#24471;&#33021;&#22815;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#31471;&#21040;&#31471;&#30340;ASR&#31995;&#32479;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#19968;&#20010;&#32570;&#28857;&#65292;&#21363;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#20811;&#26381;&#36890;&#29992;Transformer&#27169;&#22411;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#24212;&#29992;ASR&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#37325;&#29992;Transformer&#27169;&#22411;&#20013;&#30340;&#22359;&#20316;&#20026;&#23567;&#23610;&#23544;ASR&#31995;&#32479;&#20351;&#29992;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#26082;&#28385;&#36275;&#36164;&#28304;&#38480;&#21046;&#30340;&#30446;&#26631;&#65292;&#21448;&#19981;&#20250;&#24433;&#21709;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35821;&#38899;Transformer&#65288;BRST&#65289;&#30340;&#22359;&#37325;&#29992;&#31574;&#30053;&#26469;&#22686;&#24378;&#21442;&#25968;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#37197;&#22120;&#27169;&#22359;&#65288;ADM&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#21482;&#38656;&#35201;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#32039;&#20945;&#21644;&#21487;&#36866;&#24212;&#30340;&#27169;&#22411;&#26469;&#30830;&#20445;&#27599;&#20010;&#37325;&#29992;&#22359;&#30340;&#38506;&#20276;&#12290;&#25105;&#20204;&#22312;Aishell-1&#21644;CommonVoice&#26816;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;BRST&#23545;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#31561;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have recently made significant achievements in the application of end-to-end (E2E) automatic speech recognition (ASR). It is possible to deploy the E2E ASR system on smart devices with the help of Transformer-based models. While these models still have the disadvantage of requiring a large number of model parameters. To overcome the drawback of universal Transformer models for the application of ASR on edge devices, we propose a solution that can reuse the block in Transformer models for the occasion of the small footprint ASR system, which meets the objective of accommodating resource limitations without compromising recognition accuracy. Specifically, we design a novel block-reusing strategy for speech Transformer (BRST) to enhance the effectiveness of parameters and propose an adapter module (ADM) that can produce a compact and adaptable model with only a few additional trainable parameters accompanying each reusing block. We conducted an experiment with the
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.12126</link><description>&lt;p&gt;
KHAN&#65306;&#22522;&#20110;&#30693;&#35782;&#30340;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20934;&#30830;&#30340;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#20943;&#36731;&#22238;&#22768;&#23460;&#25928;&#24212;&#65292;&#21363;&#20154;&#20204;&#33853;&#20837;&#20854;&#24605;&#24819;&#65292;&#24378;&#21270;&#20854;&#29616;&#26377;&#20449;&#24565;&#12290;&#20197;&#24448;&#20851;&#20110;&#25919;&#27835;&#31435;&#22330;&#38382;&#39064;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#65288;1&#65289;&#35782;&#21035;&#21487;&#20197;&#21453;&#26144;&#26032;&#38395;&#25991;&#31456;&#25919;&#27835;&#31435;&#22330;&#30340;&#25919;&#27835;&#22240;&#32032;&#21644;&#65288;2&#65289;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#22240;&#32032;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32463;&#39564;&#19978;&#25104;&#21151;&#20102;&#65292;&#20294;&#22312;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#20013;&#20854;&#35782;&#21035;&#30340;&#22240;&#32032;&#30340;&#26377;&#25928;&#24615;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#35843;&#26597;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#35266;&#23519;&#21040;&#26032;&#38395;&#25991;&#31456;&#30340;&#29615;&#22659;&#21644;&#35821;&#35843;&#65288;&#38544;&#21547;&#65289;&#20197;&#21450;&#25991;&#31456;&#20013;&#28041;&#21450;&#30340;&#29616;&#23454;&#23454;&#20307;&#30340;&#22806;&#37096;&#30693;&#35782;&#65288;&#26174;&#24335;&#65289;&#22312;&#30830;&#23450;&#20854;&#25919;&#27835;&#31435;&#22330;&#26041;&#38754;&#26159;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65288;KHAN&#65289;&#65292;&#37319;&#29992;&#65288;1&#65289;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#65288;2&#65289;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#20379;&#20851;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#20449;&#24687;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierar
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23558;&#21477;&#23376;&#20301;&#32622;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#30340;&#24819;&#27861;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#20351;&#29992;context-discounted loss&#23545;&#33521;&#35821;&#21040;&#20420;&#35821;&#32763;&#35793;&#26102;&#26377;&#30410;&#65292;&#20294;&#22312;&#33521;&#35821;&#21040;&#24503;&#35821;&#32763;&#35793;&#20013;&#26080;&#26174;&#33879;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.06459</link><description>&lt;p&gt;
&#21033;&#29992;&#25340;&#25509;&#23558;&#21477;&#23376;&#20301;&#32622;&#32534;&#30721;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation. (arXiv:2302.06459v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23558;&#21477;&#23376;&#20301;&#32622;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#30340;&#24819;&#27861;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#20351;&#29992;context-discounted loss&#23545;&#33521;&#35821;&#21040;&#20420;&#35821;&#32763;&#35793;&#26102;&#26377;&#30410;&#65292;&#20294;&#22312;&#33521;&#35821;&#21040;&#24503;&#35821;&#32763;&#35793;&#20013;&#26080;&#26174;&#33879;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26631;&#20934;Transformer&#26550;&#26500;&#22788;&#29702;&#36830;&#32493;&#21477;&#23376;&#25340;&#25509;&#21487;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32473;&#27169;&#22411;&#25552;&#20379;&#20851;&#20110;&#25340;&#25509;&#31383;&#21475;&#20013;&#21477;&#23376;&#20301;&#32622;&#26174;&#24335;&#20449;&#24687;&#30340;&#30452;&#35273;&#24819;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;&#32534;&#30721;&#21477;&#23376;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#20197;context-discounted loss&#65288;Lupo et al.&#65292;2022&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#21017;Transformer&#26550;&#26500;&#22312;&#33521;&#35821;&#21040;&#20420;&#35821;&#32763;&#35793;&#20013;&#21463;&#30410;&#20110;&#26576;&#20123;&#21477;&#23376;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#33521;&#35821;&#21040;&#24503;&#35821;&#32763;&#35793;&#20013;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#21516;&#26679;&#30340;&#25910;&#30410;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#35777;&#30740;&#31350;&#38656;&#35201;&#23450;&#20041;&#27492;&#26041;&#27861;&#26377;&#30410;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-aware translation can be achieved by processing a concatenation of consecutive sentences with the standard Transformer architecture. This paper investigates the intuitive idea of providing the model with explicit information about the position of the sentences contained in the concatenation window. We compare various methods to encode sentence positions into token representations, including novel methods. Our results show that the Transformer benefits from certain sentence position encoding methods on English to Russian translation if trained with a context-discounted loss (Lupo et al., 2022). However, the same benefits are not observed in English to German. Further empirical efforts are necessary to define the conditions under which the proposed approach is beneficial.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;pAbT5&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20026;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#37197;&#23545;&#24773;&#20917;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.02748</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;pAbT5&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20026;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#37197;&#23545;&#24773;&#20917;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#21151;&#33021;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20165;&#38480;&#20110;&#21333;&#19968;&#24207;&#21015;&#30340;&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#32780;&#35768;&#22810;&#29983;&#29289;&#23398;&#29615;&#22659;&#28041;&#21450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;pAbT5&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;T5&#30340;&#26550;&#26500;&#23558;&#25239;&#20307;&#38142;&#37197;&#23545;&#24314;&#27169;&#20026;&#27491;&#21521;&#21644;&#21453;&#21521;&#32763;&#35793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;pAbT5&#36890;&#36807;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#38142;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21487;&#21464;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#20854;&#19979;&#19968;&#20010;&#35789;&#35821;&#30340;&#39044;&#27979;&#27010;&#29575;&#19982;&#24207;&#21015;&#27604;&#23545;&#30340;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#19968;&#33268;&#12290;&#20687;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20854;&#20182;&#30740;&#31350;&#19968;&#26679;&#65292;pAbT5&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;pAbT5&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#29983;&#25104;&#24335;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (LMs) have been successful in sequence, structural and functional predictions. However, currently, protein LMs are limited to encoder- or decoder-only architectures for single sequences while many biological contexts involve protein-protein interactions. Here, we introduce pAbT5, which models antibody chain pairing as forward- and back-translations using a T5-based architecture. We show that pAbT5 accurately reflects chain pairing through sequence generation. Our protein LM generates variable-length sequences and its next-word prediction probability agrees with position-specific scoring matrix from sequence alignment. Like other works in protein LM, pAbT5 performs state-of-the-art unsupervised prediction on experimental measurements. To the best of our knowledge, pAbT5 is the first generative encoder-decoder protein LM for protein-protein interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20923;&#32467;ViTs&#22312;&#27809;&#26377;&#24494;&#35843;&#20219;&#20309;&#21407;&#22987;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#25512;&#24191;&#21040;&#38899;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;LAVISH&#30340;&#36866;&#37197;&#22120;&#21644;&#23569;&#25968;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26377;&#25928;&#34701;&#21512;&#35270;&#35273;&#21644;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#22312;&#20351;&#29992;&#36739;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21508;&#31181;&#38899;&#20687;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.07983</link><description>&lt;p&gt;
&#35270;&#35273;Transformer&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#38899;&#20687;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers are Parameter-Efficient Audio-Visual Learners. (arXiv:2212.07983v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20923;&#32467;ViTs&#22312;&#27809;&#26377;&#24494;&#35843;&#20219;&#20309;&#21407;&#22987;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#25512;&#24191;&#21040;&#38899;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;LAVISH&#30340;&#36866;&#37197;&#22120;&#21644;&#23569;&#25968;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26377;&#25928;&#34701;&#21512;&#35270;&#35273;&#21644;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#22312;&#20351;&#29992;&#36739;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21508;&#31181;&#38899;&#20687;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20165;&#22312;&#35270;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;ViTs&#22312;&#27809;&#26377;&#24494;&#35843;&#20219;&#20309;&#21407;&#22987;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#20854;&#25512;&#24191;&#21040;&#38899;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;latent audio-visual hybrid&#65288;LAVISH&#65289;&#30340;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#20923;&#32467;ViT&#23618;&#27880;&#20837;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#35843;&#36866;&#29992;&#20110;&#38899;&#20687;&#20219;&#21153;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34701;&#21512;&#35270;&#35273;&#21644;&#38899;&#39057;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;LAVISH&#36866;&#37197;&#22120;&#20351;&#29992;&#19968;&#23567;&#32452;&#28508;&#22312;&#20196;&#29260;&#65292;&#24418;&#25104;&#19968;&#20010;&#27880;&#24847;&#29942;&#39048;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#26631;&#20934;&#20132;&#21449;&#20851;&#27880;&#30340;&#20108;&#27425;&#25104;&#26412;&#12290;&#19982;&#29616;&#26377;&#30340;&#27169;&#24577;&#29305;&#23450;&#38899;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#21487;&#35843;&#21442;&#25968;&#24182;&#19988;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#25110;&#22806;&#37096;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#38899;&#20687;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://genj&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have achieved impressive results on various computer vision tasks in the last several years. In this work, we study the capability of frozen ViTs, pretrained only on visual data, to generalize to audio-visual data without finetuning any of its original parameters. To do so, we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT. To efficiently fuse visual and audio cues, our LAVISH adapter uses a small set of latent tokens, which form an attention bottleneck, thus, eliminating the quadratic cost of standard cross-attention. Compared to the existing modality-specific audio-visual methods, our approach achieves competitive or even better performance on various audio-visual tasks while using fewer tunable parameters and without relying on costly audio pretraining or external audio encoders. Our code is available at https://genj
&lt;/p&gt;</description></item><item><title>DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11337</link><description>&lt;p&gt;
DreamArtist: &#36890;&#36807;&#23545;&#27604;prompt-tuning&#23454;&#29616;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11337
&lt;/p&gt;
&lt;p&gt;
DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#21512;&#25104;&#39640;&#36136;&#37327;&#12289;&#29305;&#24449;&#20016;&#23500;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#26032;&#39118;&#26684;&#12289;&#29289;&#20307;&#23454;&#20307;&#31561;&#65289;&#26102;&#24120;&#24120;&#38754;&#20020;&#22256;&#38590;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#37319;&#29992;&#24494;&#35843;&#25110;prompt-tuning&#31574;&#30053;&#26469;&#25945;&#25480;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20174;&#21442;&#32771;&#22270;&#20687;&#38598;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#27425;&#24212;&#29992;&#20013;&#65292;&#36825;&#23545;&#20110;&#20445;&#25345;&#29983;&#25104;&#21487;&#25511;&#24615;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;DreamArtist&#65292;&#23427;&#37319;&#29992;&#20102;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamArtist&#32467;&#21512;&#20102;&#27491;&#36127;&#23884;&#20837;&#24182;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#12290;&#27491;&#23884;&#20837;&#31215;&#26497;&#22320;&#25429;&#25417;&#21442;&#32771;&#22270;&#20687;&#30340;&#26174;&#30528;&#29305;&#24449;&#26469;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#36127;&#23884;&#20837;&#21017;&#24378;&#21046;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#24615;&#22270;&#20687;&#20197;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.  To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#27969;&#31243;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#23558;&#27861;&#24459;&#21327;&#35758;&#36716;&#25442;&#25104;&#26234;&#33021;&#21512;&#32422;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;NER&#27969;&#31243;&#21644;&#38382;&#31572;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#27169;&#26495;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986; CiceroMark &#21644;&#25552;&#21462; Concerto &#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.08954</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23558;&#27861;&#24459;&#21327;&#35758;&#36716;&#25442;&#25104;&#26234;&#33021;&#21512;&#32422;
&lt;/p&gt;
&lt;p&gt;
Conversion of Legal Agreements into Smart Legal Contracts using NLP. (arXiv:2210.08954v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#27969;&#31243;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#23558;&#27861;&#24459;&#21327;&#35758;&#36716;&#25442;&#25104;&#26234;&#33021;&#21512;&#32422;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;NER&#27969;&#31243;&#21644;&#38382;&#31572;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#27169;&#26495;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986; CiceroMark &#21644;&#25552;&#21462; Concerto &#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#21512;&#32422;&#26159;&#19968;&#31181;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#21644;&#21487;&#35745;&#31639;&#32452;&#20214;&#30340;&#25968;&#23383;&#21270;&#21327;&#35758;&#12290;Accord Project &#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#26234;&#33021;&#21512;&#32422;&#26694;&#26550;&#65292;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;Cicero&#12289;Concerto&#21644;Ergo&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#38656;&#35201;&#24459;&#24072;&#12289;&#31243;&#24207;&#21592;&#21644;&#23458;&#25143;&#20849;&#21516;&#21162;&#21147;&#25165;&#33021;&#20351;&#29992; Accord Project &#21019;&#24314;&#21487;&#29992;&#30340;&#26234;&#33021;&#21512;&#32422;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23558;&#27861;&#24459;&#21512;&#21516;&#36716;&#25442;&#20026; Accord Project &#30340; Concerto &#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#21270;&#26234;&#33021;&#21512;&#32422;&#30340;&#21019;&#24314;&#36807;&#31243;&#12290;&#22312;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#27969;&#31243;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340; NER &#27969;&#31243;&#21487;&#20197;&#27491;&#30830;&#35782;&#21035; Accord Project &#27169;&#26495;&#25991;&#26412;&#20013;&#30340; CiceroMark&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;0.8&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38382;&#31572;&#26041;&#27861;&#21487;&#20197;&#20174;&#27169;&#26495;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#19977;&#20998;&#20043;&#19968;&#30340; Concerto &#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#27969;&#31243;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#21487;&#33021;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010; Web &#25509;&#21475;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#26234;&#33021;&#21512;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Smart Legal Contract (SLC) is a specialized digital agreement comprising natural language and computable components. The Accord Project provides an open-source SLC framework containing three main modules: Cicero, Concerto, and Ergo. Currently, we need lawyers, programmers, and clients to work together with great effort to create a usable SLC using the Accord Project. This paper proposes a pipeline to automate the SLC creation process with several Natural Language Processing (NLP) models to convert law contracts to the Accord Project's Concerto model. After evaluating the proposed pipeline, we discovered that our NER pipeline accurately detects CiceroMark from Accord Project template text with an accuracy of 0.8. Additionally, our Question Answering method can extract one-third of the Concerto variables from the template text. We also delve into some limitations and possible future research for the proposed pipeline. Finally, we describe a web interface enabling users to build SLCs. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViL-Sum&#65292;&#29992;&#20110;&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ViL-Sum&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11303</link><description>&lt;p&gt;
&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViL-Sum&#65292;&#29992;&#20110;&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ViL-Sum&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#25688;&#35201;&#26041;&#27861;&#37319;&#29992;&#32423;&#32852;&#26041;&#24335;&#65306;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;, &#28982;&#21518;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#35821;&#35328;&#34920;&#31034;&#30456;&#34701;&#21512;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#12290;&#32423;&#32852;&#30340;&#26041;&#24335;&#26080;&#27861;&#25429;&#25417;&#22270;&#20687;&#21644;&#27573;&#33853;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#30340;&#25688;&#35201;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViL-Sum&#65292;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#27573;&#33853;&#32423;&#21035;&#30340;&#35270;&#35273;-&#35821;&#35328;&#35821;&#20041;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#12290;ViL-Sum&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32852;&#21512;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20219;&#21153;&#65306;&#22270;&#20687;&#37325;&#25490;&#24207;&#21644;&#22270;&#20687;&#36873;&#25321;&#12290;&#32852;&#21512;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#25429;&#25417;&#20102;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#37325;&#25490;&#24207;&#20219;&#21153;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#27573;&#33853;&#32423;&#21035;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#32780;&#36873;&#25321;&#20219;&#21153;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#36873;&#25321;&#19982;&#25688;&#35201;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ViL-Sum&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features, then these features are fused with language representations to generate the summary with an encoder-decoder model. The cascaded way cannot capture the semantic alignments between images and paragraphs, which are crucial to a precise summary. In this paper, we propose ViL-Sum to jointly model paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment and Multi-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal encoder with two well-designed tasks, image reordering and image selection. The joint multi-modal encoder captures the interactions between modalities, where the reordering task guides the model to learn paragraph-level semantic alignment and the selection task guides the model to selected summary-related images in the final summary. Experimental results show that our proposed ViL-Sum significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#24615;&#20998;&#25968;&#65292;&#23427;&#20204;&#26159;&#23545;&#26368;&#23567;&#21644;&#38543;&#26426;&#22522;&#32447;&#37117;&#36827;&#34892;&#20102;&#24402;&#19968;&#21270;&#22788;&#29702;&#30340;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#22312;&#35789;&#38271;&#26041;&#38754;&#39640;&#24230;&#20248;&#21270;&#65292;&#32780;Zipf&#30340;&#32553;&#20889;&#23450;&#24459;&#30830;&#23454;&#26159;&#21387;&#32553;&#30340;&#19968;&#31181;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.10384</link><description>&lt;p&gt;
&#35789;&#38271;&#30340;&#26368;&#20248;&#24615;&#65306;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The optimality of word lengths. Theoretical foundations and an empirical study. (arXiv:2208.10384v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#24615;&#20998;&#25968;&#65292;&#23427;&#20204;&#26159;&#23545;&#26368;&#23567;&#21644;&#38543;&#26426;&#22522;&#32447;&#37117;&#36827;&#34892;&#20102;&#24402;&#19968;&#21270;&#22788;&#29702;&#30340;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#22312;&#35789;&#38271;&#26041;&#38754;&#39640;&#24230;&#20248;&#21270;&#65292;&#32780;Zipf&#30340;&#32553;&#20889;&#23450;&#24459;&#30830;&#23454;&#26159;&#21387;&#32553;&#30340;&#19968;&#31181;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Zipf&#30340;&#32553;&#20889;&#23450;&#24459;&#34920;&#26126;&#65292;&#26356;&#24120;&#35265;&#30340;&#35789;&#26356;&#30701;&#65292;&#36825;&#34987;&#35270;&#20026;&#21387;&#32553;&#30340;&#19968;&#31181;&#34920;&#29616;&#8212;&#8212;&#33258;&#28982;&#20132;&#27969;&#30340;&#26222;&#36941;&#21407;&#21017;&#20043;&#19968;&#26159;&#24418;&#24335;&#38271;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#34429;&#28982;&#35821;&#35328;&#20248;&#21270;&#30340;&#35828;&#27861;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#20294;&#34913;&#37327;&#35821;&#35328;&#20248;&#21270;&#31243;&#24230;&#30340;&#23581;&#35797;&#21364;&#30456;&#24403;&#31232;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#24615;&#20998;&#25968;&#65292;&#23427;&#20204;&#26159;&#23545;&#26368;&#23567;&#21644;&#38543;&#26426;&#22522;&#32447;&#37117;&#36827;&#34892;&#20102;&#24402;&#19968;&#21270;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#20998;&#25968;&#20197;&#21450;&#20854;&#20182;&#20998;&#25968;&#30340;&#29702;&#35770;&#21644;&#32479;&#35745;&#20248;&#32570;&#28857;&#12290;&#21033;&#29992;&#26368;&#20339;&#20998;&#25968;&#65292;&#25105;&#20204;&#39318;&#27425;&#37327;&#21270;&#20102;&#35821;&#35328;&#20013;&#35789;&#38271;&#30340;&#26368;&#20248;&#24615;&#31243;&#24230;&#12290;&#36825;&#34920;&#26126;&#24403;&#20197;&#23383;&#31526;&#35745;&#31639;&#35789;&#38271;&#26102;&#65292;&#35821;&#35328;&#24179;&#22343;&#20248;&#21270;&#21040;62&#25110;67&#65285;&#65288;&#21462;&#20915;&#20110;&#25968;&#25454;&#26469;&#28304;&#65289;&#65292;&#24403;&#20197;&#26102;&#38388;&#35745;&#31639;&#35789;&#38271;&#26102;&#65292;&#24179;&#22343;&#20026;65&#65285;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#21475;&#35821;&#21333;&#35789;&#25345;&#32493;&#26102;&#38388;&#27604;&#20070;&#38754;&#35789;&#38271;&#26356;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#22312;&#35789;&#38271;&#26041;&#38754;&#39640;&#24230;&#20248;&#21270;&#65292;&#32780;Zipf&#30340;&#32553;&#20889;&#23450;&#24459;&#30830;&#23454;&#26159;&#21387;&#32553;&#30340;&#19968;&#31181;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zipf's law of abbreviation, namely the tendency of more frequent words to be shorter, has been viewed as a manifestation of compression, i.e. the minimization of the length of forms -- a universal principle of natural communication. Although the claim that languages are optimized has become trendy, attempts to measure the degree of optimization of languages have been rather scarce. Here we present two optimality scores that are dualy normalized, namely, they are normalized with respect to both the minimum and the random baseline. We analyze the theoretical and statistical pros and cons of these and other scores. Harnessing the best score, we quantify for the first time the degree of optimality of word lengths in languages. This indicates that languages are optimized to 62 or 67 percent on average (depending on the source) when word lengths are measured in characters, and to 65 percent on average when word lengths are measured in time. In general, spoken word durations are more optimize
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2203.05711</link><description>&lt;p&gt;
&#30005;&#24433;&#21465;&#36848;&#25688;&#35201;&#65306;&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#26377;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#20854;&#20013;&#21253;&#21547;5,193&#20010;&#27969;&#34892;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#25688;&#35201;&#12290;SYMON&#25429;&#25417;&#20102;&#30001;&#20154;&#31867;&#21019;&#20316;&#32773;&#21046;&#20316;&#30340;&#38754;&#21521;&#20154;&#31867;&#35266;&#20247;&#30340;&#33258;&#28982;&#25925;&#20107;&#21465;&#36848;&#35270;&#39057;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#22411;&#21644;&#33258;&#28982;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;SYMON&#20855;&#26377;&#39640;&#35206;&#30422;&#30340;&#22810;&#27169;&#24577;&#25925;&#20107;&#20107;&#20214;&#12289;&#20016;&#23500;&#30340;&#24515;&#29702;&#29366;&#24577;&#25551;&#36848;&#21644;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#22823;&#35821;&#20041;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#21644;&#30005;&#24433;&#25688;&#35201;&#35270;&#39057;&#30340;&#38646;&#26679;&#26412;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#22312;&#25925;&#20107;&#29702;&#35299;&#20013;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SYMON&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#36827;&#23637;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
&lt;/p&gt;</description></item><item><title>&#25163;&#35821;&#21040;&#21475;&#35821;&#33258;&#21160;&#32763;&#35793;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#30340;&#30740;&#31350;&#29616;&#29366;&#24471;&#21040;&#20102;&#24635;&#32467;&#65292;&#20294;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#38656;&#35201;&#36827;&#19968;&#27493;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2202.03086</link><description>&lt;p&gt;
&#20174;&#25163;&#35821;&#21040;&#21475;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03086
&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#21040;&#21475;&#35821;&#33258;&#21160;&#32763;&#35793;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#30340;&#30740;&#31350;&#29616;&#29366;&#24471;&#21040;&#20102;&#24635;&#32467;&#65292;&#20294;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#38656;&#35201;&#36827;&#19968;&#27493;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25163;&#35821;&#21040;&#21475;&#35821;&#30340;&#33258;&#21160;&#32763;&#35793;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#28041;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#35328;&#23398;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#22823;&#22810;&#30001;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#21333;&#29420;&#36827;&#34892;&#12290;&#38543;&#30528;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814; - &#36807;&#21435;&#19977;&#24180;&#20013;&#20851;&#20110;&#25163;&#35821;&#32763;&#35793;&#30340;&#22823;&#37096;&#20998;&#31185;&#23398;&#35770;&#25991;&#24050;&#32463;&#21457;&#34920; - &#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#29366;&#27010;&#36848;&#20197;&#21450;&#19981;&#21516;&#30456;&#20851;&#23398;&#31185;&#30340;&#19968;&#20123;&#24517;&#35201;&#32972;&#26223;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;&#25163;&#35821;&#35821;&#35328;&#23398;&#21644;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#30340;&#20171;&#32461;&#65292;&#20197;&#35828;&#26126;&#33258;&#21160;&#25163;&#35821;&#32763;&#35793;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#35828;&#26126;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21475;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic translation from signed to spoken languages is an interdisciplinary research domain, lying on the intersection of computer vision, machine translation and linguistics. Nevertheless, research in this domain is performed mostly by computer scientists in isolation. As the domain is becoming increasingly popular - the majority of scientific papers on the topic of sign language translation have been published in the past three years - we provide an overview of the state of the art as well as some required background in the different related disciplines. We give a high-level introduction to sign language linguistics and machine translation to illustrate the requirements of automatic sign language translation. We present a systematic literature review to illustrate the state of the art in the domain and then, harking back to the requirements, lay out several challenges for future research. We find that significant advances have been made on the shoulders of spoken language machine t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#20013;&#38388;&#23618;&#23545;&#20110;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#20316;&#29992;&#65292;&#34920;&#26126;&#21024;&#38500;&#20013;&#38388;&#23618;&#25968;&#37327;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19981;&#21463;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2012.11881</link><description>&lt;p&gt;
&#26080;&#20998;&#21106;&#20851;&#27880;&#65306;BERT&#26159;&#21542;&#38656;&#35201;&#20013;&#38388;&#23618;&#65311;
&lt;/p&gt;
&lt;p&gt;
Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.11881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#20013;&#38388;&#23618;&#23545;&#20110;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#20316;&#29992;&#65292;&#34920;&#26126;&#21024;&#38500;&#20013;&#38388;&#23618;&#25968;&#37327;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19981;&#21463;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20363;&#22914;&#38405;&#35835;&#29702;&#35299;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#12290;&#25152;&#26377;&#22522;&#20110;BERT&#30340;&#26550;&#26500;&#37117;&#20855;&#26377;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#22359;&#65292;&#21518;&#36319;&#19968;&#20010;&#20013;&#38388;&#23618;&#22359;&#20316;&#20026;&#22522;&#26412;&#26500;&#24314;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#21253;&#21547;&#36825;&#20123;&#20013;&#38388;&#23618;&#30340;&#24378;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20013;&#38388;&#23618;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20943;&#23569;&#20013;&#38388;&#23618;&#25968;&#37327;&#24182;&#20462;&#25913;BERT-BASE&#30340;&#26550;&#26500;&#20250;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#23567;&#25439;&#22833;&#65292;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20013;&#24515;&#21270;&#20869;&#26680;&#23545;&#40784;&#21644;&#25506;&#27979;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#33719;&#24471;&#23545;&#25105;&#20204;&#30340;&#26550;&#26500;&#20462;&#25913;&#30340;&#27934;&#35265;&#65292;&#24182;&#35777;&#26126;&#21024;&#38500;&#20013;&#38388;&#23618;&#23545;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#27809;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use centered kernel alignment and probing linear classifiers to gain insight into our architectural modifications and justify that removal of intermediate l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coke&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19978;&#19979;&#25991;&#30693;&#35782;&#21160;&#24577;&#36873;&#25321;&#21644;&#23884;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#36991;&#20813;&#23545;&#36755;&#20837;&#25991;&#26412;&#21305;&#37197;&#25928;&#26524;&#24046;&#30340;&#20887;&#20313;&#21644;&#27169;&#31946;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2009.13964</link><description>&lt;p&gt;
CokeBERT: &#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#36873;&#25321;&#19982;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.13964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coke&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19978;&#19979;&#25991;&#30693;&#35782;&#21160;&#24577;&#36873;&#25321;&#21644;&#23884;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#36991;&#20813;&#23545;&#36755;&#20837;&#25991;&#26412;&#21305;&#37197;&#25928;&#26524;&#24046;&#30340;&#20887;&#20313;&#21644;&#27169;&#31946;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22806;&#37096;&#24322;&#26500;&#30693;&#35782;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#24182;&#22312;&#21508;&#31181;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#30693;&#35782;&#22686;&#24378;&#30340;PLMs&#20165;&#23884;&#20837;KG&#20013;&#30340;&#38745;&#24577;&#23376;&#22270;&#65288;&#8220;&#30693;&#35782;&#19978;&#19979;&#25991;&#8221;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;PLMs&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#25991;&#26412;&#65288;&#8220;&#25991;&#26412;&#19978;&#19979;&#25991;&#8221;&#65289;&#21160;&#24577;&#21464;&#21270;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;Coke&#65292;&#29992;&#20110;&#20026;PLMs&#21160;&#24577;&#36873;&#25321;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#26681;&#25454;&#25991;&#26412;&#19978;&#19979;&#25991;&#23884;&#20837;&#30693;&#35782;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#21487;&#20197;&#36991;&#20813;KG&#20013;&#30340;&#20887;&#20313;&#21644;&#27169;&#31946;&#30693;&#35782;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#21305;&#37197;&#25928;&#26524;&#19981;&#20339;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Coke&#22312;&#20856;&#22411;&#30340;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#20248;&#20110;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#34920;&#26126;&#20102;&#21033;&#29992;&#21160;&#24577;&#30693;&#35782;&#19978;&#19979;&#25991;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#24615;&#33021;&#26041;&#38754;&#30340;&#25913;&#36827;&#65292;&#25152;&#36873;&#25321;&#30340;&#21160;&#24577;&#30693;&#35782;&#33021;&#22815;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25351;&#23548;&#28145;&#20837;&#29702;&#35299;PLMs&#25152;&#23398;&#20064;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs ("knowledge context"), regardless of that the knowledge required by PLMs may change dynamically according to specific text ("textual context"). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowle
&lt;/p&gt;</description></item></channel></rss>