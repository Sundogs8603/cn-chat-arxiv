<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;BioLaySumm 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#24320;&#21457;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21487;&#25511;&#25110;&#19981;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#29983;&#25104;&#24120;&#35268;&#25688;&#35201;&#65292;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#24120;&#35268;&#25688;&#35201;&#21644;&#21487;&#35835;&#24615;&#25511;&#21046;&#30340;&#25688;&#35201;&#12290;&#20849;&#26377;20&#20010;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.17332</link><description>&lt;p&gt;
&#20851;&#20110;BioLaySumm 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#32508;&#36848;&#65306;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#30340;&#31616;&#21270;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles. (arXiv:2309.17332v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17332
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;BioLaySumm 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#24320;&#21457;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21487;&#25511;&#25110;&#19981;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#29983;&#25104;&#24120;&#35268;&#25688;&#35201;&#65292;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#24120;&#35268;&#25688;&#35201;&#21644;&#21487;&#35835;&#24615;&#25511;&#21046;&#30340;&#25688;&#35201;&#12290;&#20849;&#26377;20&#20010;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;ACL 2023&#30340;BioNLP&#30740;&#35752;&#20250;&#19978;&#20030;&#21150;&#30340;&#20851;&#20110;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#31616;&#21270;&#25688;&#35201;&#30340;&#20849;&#20139;&#20219;&#21153;&#65288;BioLaySumm&#65289;&#30340;&#32467;&#26524;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#29983;&#25104;&#8220;&#24120;&#35268;&#25688;&#35201;&#8221;&#65288;&#21363;&#21487;&#29702;&#35299;&#32473;&#38750;&#25216;&#26415;&#20154;&#21592;&#30340;&#25688;&#35201;&#65289;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411;&#65292;&#26080;&#35770;&#22312;&#21487;&#25511;&#25110;&#19981;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;1&#65289;&#24120;&#35268;&#25688;&#35201;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#26681;&#25454;&#25552;&#20379;&#30340;&#23436;&#25972;&#25991;&#31456;&#25991;&#26412;&#21644;&#23545;&#24212;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#65292;&#26500;&#24314;&#20135;&#29983;&#24120;&#35268;&#25688;&#35201;&#30340;&#27169;&#22411;&#65307;2&#65289;&#21487;&#35835;&#24615;&#25511;&#21046;&#30340;&#25688;&#35201;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#26681;&#25454;&#25991;&#31456;&#30340;&#20027;&#35201;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#35757;&#32451;&#27169;&#22411;&#26469;&#29983;&#25104;&#25216;&#26415;&#25688;&#35201;&#21644;&#24120;&#35268;&#25688;&#35201;&#12290;&#38500;&#20102;&#24635;&#20307;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;BioLaySumm&#20849;&#20139;&#20219;&#21153;&#30340;&#35774;&#32622;&#21644;&#35265;&#35299;&#65292;&#20849;&#26377;20&#20010;&#21442;&#19982;&#22242;&#38431;&#21442;&#19982;&#20102;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023. The goal of this shared task is to develop abstractive summarisation models capable of generating "lay summaries" (i.e., summaries that are comprehensible to non-technical audiences) in both a controllable and non-controllable setting. There are two subtasks: 1) Lay Summarisation, where the goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input; and 2) Readability-controlled Summarisation, where the goal is for participants to train models to generate both the technical abstract and the lay summary, given an article's main text as input. In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21517;&#20026;DLCCP&#65292;&#29992;&#20110;&#38750;&#19987;&#19994;&#25551;&#36848;&#30340;&#25351;&#25511;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;DLCCP&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#19987;&#19994;&#29992;&#25143;&#30340;&#25991;&#26412;&#39118;&#26684;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#20869;&#23481;&#21644;&#39118;&#26684;&#34920;&#31034;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17313</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#38750;&#19987;&#19994;&#25551;&#36848;&#30340;&#25351;&#25511;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions. (arXiv:2309.17313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21517;&#20026;DLCCP&#65292;&#29992;&#20110;&#38750;&#19987;&#19994;&#25551;&#36848;&#30340;&#25351;&#25511;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;DLCCP&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#19987;&#19994;&#29992;&#25143;&#30340;&#25991;&#26412;&#39118;&#26684;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#20869;&#23481;&#21644;&#39118;&#26684;&#34920;&#31034;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#19987;&#19994;&#27861;&#24459;&#35821;&#35328;&#39118;&#26684;&#65288;PLLS&#65289;&#25991;&#26412;&#65292;&#22312;&#25351;&#25511;&#39044;&#27979;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#38750;&#19987;&#19994;&#29992;&#25143;&#23545;&#36825;&#26679;&#30340;&#39044;&#27979;&#26381;&#21153;&#20063;&#26377;&#22686;&#38271;&#30340;&#38656;&#27714;&#12290;PLLS&#25991;&#26412;&#21644;&#38750;PLLS&#25991;&#26412;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#30001;&#38750;&#19987;&#19994;&#20154;&#22763;&#34920;&#36798;&#65292;&#36825;&#38477;&#20302;&#20102;&#24403;&#21069;SOTA&#27169;&#22411;&#22312;&#38750;PLLS&#25991;&#26412;&#19978;&#30340;&#24615;&#33021;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;&#25351;&#25511;&#31867;&#21035;&#30340;&#38750;PLLS&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Disentangled Legal Content for Charge Prediction&#65288;DLCCP&#65289;&#30340;&#26032;&#39062;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;FSDA&#65289;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#20165;&#20165;&#36827;&#34892;&#23454;&#20363;&#32423;&#21035;&#23545;&#40784;&#32780;&#19981;&#32771;&#34385;&#23384;&#22312;&#20110;&#28508;&#22312;&#29305;&#24449;&#20013;&#30340;&#25991;&#26412;&#39118;&#26684;&#20449;&#24687;&#30340;FSDA&#24037;&#20316;&#30456;&#27604;&#65292;DLCCP&#65288;1&#65289;&#20998;&#31163;&#20869;&#23481;&#21644;&#39118;&#26684;&#34920;&#31034;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#27861;&#24459;&#20869;&#23481;&#65292;&#24182;&#20351;&#29992;&#20180;&#32454;&#35774;&#35745;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#31354;&#38388;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#65288;2&#65289;&#20351;&#29992;&#32452;&#25104;&#30340;&#26041;&#27861;&#26469;&#21152;&#24378;&#23545;&#39118;&#26684;&#19968;&#33268;&#24615;&#30340;&#24314;&#27169;&#65292;&#24182;&#22312;&#22495;&#36866;&#24212;&#36807;&#31243;&#20013;&#21033;&#29992;&#29305;&#23450;&#20110;&#39118;&#26684;&#30340;&#36716;&#25442;&#21644;&#20849;&#20139;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works considering professional legal-linguistic style (PLLS) texts have shown promising results on the charge prediction task. However, unprofessional users also show an increasing demand on such a prediction service. There is a clear domain discrepancy between PLLS texts and non-PLLS texts expressed by those laypersons, which degrades the current SOTA models' performance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data for most charge classes. This paper proposes a novel few-shot domain adaptation (FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP). Compared with existing FSDA works, which solely perform instance-level alignment without considering the negative impact of text style information existing in latent features, DLCCP (1) disentangles the content and style representations for better domain-invariant legal content learning with carefully designed optimization goals for content and style spaces and, (2) employs the constitutiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21487;&#25511;&#30340;&#27861;&#24459;&#24847;&#35265;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#27979;&#30340;&#35770;&#35777;&#35282;&#33394;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#25688;&#35201;&#65292;&#20855;&#26377;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17280</link><description>&lt;p&gt;
STRONG - &#32467;&#26500;&#21487;&#25511;&#30340;&#27861;&#24459;&#24847;&#35265;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STRONG -- Structure Controllable Legal Opinion Summary Generation. (arXiv:2309.17280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21487;&#25511;&#30340;&#27861;&#24459;&#24847;&#35265;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#27979;&#30340;&#35770;&#35777;&#35282;&#33394;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#25688;&#35201;&#65292;&#20855;&#26377;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38271;&#31687;&#27861;&#24459;&#24847;&#35265;&#32467;&#26500;&#21487;&#25511;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25991;&#26723;&#30340;&#35770;&#35777;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#27979;&#30340;&#35770;&#35777;&#35282;&#33394;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19968;&#23450;&#32467;&#26500;&#27169;&#24335;&#30340;&#36830;&#36143;&#25688;&#35201;&#12290;&#25105;&#20204;&#22312;&#27861;&#24459;&#24847;&#35265;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;ROUGE&#12289;BERTScore&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for the structure controllable summarization of long legal opinions that considers the argument structure of the document. Our approach involves using predicted argument role information to guide the model in generating coherent summaries that follow a provided structure pattern. We demonstrate the effectiveness of our approach on a dataset of legal opinions and show that it outperforms several strong baselines with respect to ROUGE, BERTScore, and structure similarity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.17272</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#20013;&#30340;&#33021;&#21147;&#36890;&#36807;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#20840;&#38754;&#22320;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#26694;&#26550;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;LLM&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#35282;&#24230;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#21333;&#20010;&#35282;&#24230;&#20869;&#30340;&#20869;&#19968;&#33268;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#23545;&#32473;&#23450;&#26597;&#35810;&#20174;&#21508;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#26681;&#25454;&#36825;&#20123;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33521;&#35821;ASR&#33258;&#23450;&#20041;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#31232;&#26377;&#21644;&#36229;&#20986;&#35789;&#27719;&#65288;OOV&#65289;&#30701;&#35821;&#65292;&#24182;&#20171;&#32461;&#20102;&#27880;&#20837;&#22256;&#38590;&#36127;&#26679;&#26412;&#20559;&#24046;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#21644;&#35823;&#25253;&#35686;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.17267</link><description>&lt;p&gt;
Wiki-En-ASR-Adapt: &#29992;&#20110;&#33521;&#35821;ASR&#33258;&#23450;&#20041;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Wiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR Customization. (arXiv:2309.17267v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33521;&#35821;ASR&#33258;&#23450;&#20041;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#31232;&#26377;&#21644;&#36229;&#20986;&#35789;&#27719;&#65288;OOV&#65289;&#30701;&#35821;&#65292;&#24182;&#20171;&#32461;&#20102;&#27880;&#20837;&#22256;&#38590;&#36127;&#26679;&#26412;&#20559;&#24046;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#21644;&#35823;&#25253;&#35686;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#24320;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#19978;&#19979;&#25991;&#25340;&#20889;&#26816;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#31232;&#26377;&#21644;&#36229;&#33073;&#35789;&#27719;&#65288;OOV&#65289;&#30701;&#35821;&#65292;&#22914;&#19987;&#26377;&#21517;&#35789;&#25110;&#26415;&#35821;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#21019;&#24314;&#25968;&#30334;&#19975;&#20010;&#30495;&#23454;&#20363;&#23376;&#65292;&#29992;&#20110;&#27169;&#25311;&#30772;&#25439;&#30340;ASR&#20551;&#35774;&#21644;&#27169;&#25311;&#38750;&#24179;&#20961;&#20559;&#24046;&#21015;&#34920;&#20197;&#36827;&#34892;&#23450;&#21046;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21521;&#35757;&#32451;&#31034;&#20363;&#30340;&#27169;&#25311;&#20559;&#24046;&#21015;&#34920;&#27880;&#20837;&#20004;&#31181;&#8220;&#22256;&#38590;&#36127;&#26679;&#26412;&#8221;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#33258;&#21160;&#25366;&#25496;&#36825;&#20123;&#22256;&#38590;&#36127;&#26679;&#26412;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24320;&#28304;&#23450;&#21046;&#27169;&#22411;&#30340;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#27880;&#20837;&#22256;&#38590;&#36127;&#26679;&#26412;&#20559;&#24046;&#30701;&#35821;&#21487;&#20197;&#20943;&#23567;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#21644;&#35823;&#25253;&#35686;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a first large-scale public synthetic dataset for contextual spellchecking customization of automatic speech recognition (ASR) with focus on diverse rare and out-of-vocabulary (OOV) phrases, such as proper names or terms. The proposed approach allows creating millions of realistic examples of corrupted ASR hypotheses and simulate non-trivial biasing lists for the customization task. Furthermore, we propose injecting two types of ``hard negatives" to the simulated biasing lists in training examples and describe our procedures to automatically mine them. We report experiments with training an open-source customization model on the proposed dataset and show that the injection of hard negative biasing phrases decreases WER and the number of false alarms.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17255</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#65306;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#26159;&#30740;&#31350;&#29983;&#29289;&#21644;&#29983;&#21629;&#36807;&#31243;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#30456;&#20851;&#23398;&#31185;&#12290;&#29983;&#21629;&#31185;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#20135;&#29983;&#21644;&#28040;&#36153;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20855;&#26377;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#12290;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20854;&#20013;&#28041;&#21450;&#30340;&#31185;&#23398;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#25512;&#21160;&#20102;&#24212;&#29992;&#20808;&#36827;&#30340;&#30693;&#35782;&#39537;&#21160;&#25216;&#26415;&#26469;&#31649;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25512;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#21644;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#31649;&#29702;&#65292;&#20197;&#21450;&#22312;&#26032;&#21457;&#29616;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.17234</link><description>&lt;p&gt;
LLM-&#36777;&#35770;: &#20351;&#29992;&#20132;&#20114;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21830;&#28216;&#25103;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#25104;&#21151;&#35848;&#21028;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-4&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#26469;&#35299;&#20915;&#21487;&#33021;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#24773;&#20917;&#30340;&#29616;&#23454;&#20219;&#21153;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#26377;&#38480;&#30340;&#29702;&#35299;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#30001;&#20110;&#35848;&#21028;&#21644;&#22949;&#21327;&#26159;&#25105;&#20204;&#26085;&#24120;&#27807;&#36890;&#21644;&#21512;&#20316;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35780;&#20998;&#30340;&#35848;&#21028;&#28216;&#25103;&#20316;&#20026;LLMs&#30340;&#26032;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#12289;&#22810;&#38382;&#39064;&#30340;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#35848;&#21028;&#28216;&#25103;&#27979;&#35797;&#24179;&#21488;&#65292;&#38590;&#24230;&#21487;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#20855;&#22791;&#24378;&#22823;&#30340;&#31639;&#26415;&#12289;&#25512;&#29702;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#21516;&#26102;&#26080;&#32541;&#22320;&#25972;&#21512;&#23427;&#20204;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#38646;-shot&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#20154;&#21487;&#20197;&#36827;&#34892;&#35848;&#21028;&#24182;&#25345;&#32493;&#36798;&#25104;&#25104;&#21151;&#20132;&#26131;&#12290;&#25105;&#20204;&#29992;&#22810;&#20010;&#25351;&#26631;&#37327;&#21270;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;GPT-4&#19982;&#21407;&#25991;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;FP8&#32447;&#24615;&#23618;&#30340;&#32553;&#25918;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21160;&#24577;&#26356;&#26032;&#27599;&#20010;&#24352;&#37327;&#30340;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#30340;&#23610;&#24230;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17224</link><description>&lt;p&gt;
&#20351;&#29992;8&#20301;&#28014;&#28857;&#25968;&#35757;&#32451;&#21644;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training and inference of large language models using 8-bit floating point. (arXiv:2309.17224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;FP8&#32447;&#24615;&#23618;&#30340;&#32553;&#25918;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21160;&#24577;&#26356;&#26032;&#27599;&#20010;&#24352;&#37327;&#30340;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#30340;&#23610;&#24230;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FP8&#26684;&#24335;&#27491;&#22312;&#21463;&#21040;&#38738;&#30544;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#26029;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23427;&#20204;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;&#35880;&#24910;&#36873;&#25321;&#32553;&#25918;&#20197;&#38450;&#27490;&#30001;&#20110;&#36739;&#39640;&#31934;&#24230;&#26684;&#24335;&#30340;&#21160;&#24577;&#33539;&#22260;&#30340;&#20943;&#23569;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#23613;&#31649;&#20851;&#20110;&#36873;&#25321;INT&#26684;&#24335;&#30340;&#36825;&#20123;&#32553;&#25918;&#22240;&#23376;&#30340;&#25991;&#29486;&#24456;&#22810;&#65292;&#20294;&#23545;&#20110;FP8&#26469;&#35828;&#65292;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26356;&#26032;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#30340;&#27599;&#20010;&#24352;&#37327;&#23610;&#24230;&#30340;FP8&#32447;&#24615;&#23618;&#32553;&#25918;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;FP8&#35757;&#32451;&#21644;&#39564;&#35777;GPT&#21644;Llama 2&#31561;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27169;&#22411;&#22823;&#23567;&#33539;&#22260;&#20174;111M&#21040;70B&#19981;&#31561;&#12290;&#20026;&#20102;&#20415;&#20110;&#29702;&#35299;FP8&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#38468;&#24102;&#20102;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#26799;&#24230;&#30340;&#27599;&#20010;&#24352;&#37327;&#23610;&#24230;&#20998;&#24067;&#30340;&#22270;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.
&lt;/p&gt;</description></item><item><title>Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>http://arxiv.org/abs/2309.17179</link><description>&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17179
&lt;/p&gt;
&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36890;&#24120;&#37319;&#29992;&#37319;&#26679;&#25110;&#26463;&#25628;&#32034;&#65292;&#32467;&#21512; Chain-of-Thought (CoT) &#31561;&#25552;&#31034;&#26469;&#25552;&#39640;&#25512;&#29702;&#21644;&#35299;&#30721;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914; Tree-of-Thought (ToT) &#21644; Reasoning via Planning (RAP) &#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24341;&#23548;&#22810;&#27493;&#25512;&#29702;&#65292;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#28608;&#27963;LLM&#20316;&#20026;&#19968;&#20010;&#20215;&#20540;&#20989;&#25968;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AlphaZero&#31867;&#20284;&#30340;&#29992;&#20110;LLM&#30340;&#26641;&#25628;&#32034;&#26694;&#26550; (&#31216;&#20026;TS-LLM)&#65292;&#31995;&#32479;&#22320;&#35828;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21033;&#29992;&#26641;&#25628;&#32034;&#26469;&#25351;&#23548;LLM&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;TS-LLM&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20247;&#19981;&#21516;&#65306;(1)&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36866;&#22320;&#24212;&#29992;&#20110;&#38500;&#20102;&#25512;&#29702;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153; (&#20363;&#22914;RLHF&#23545;&#40784;)&#65292;&#20197;&#21450;&#20219;&#20309;&#22823;&#23567;&#30340;LLM&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
&lt;/p&gt;</description></item><item><title>RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17176</link><description>&lt;p&gt;
RLAdapter&#65306;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17176
&lt;/p&gt;
&lt;p&gt;
RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20915;&#31574;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20026;&#20195;&#29702;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;RL&#31639;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26368;&#20248;&#22320;&#24110;&#21161;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25968;&#25454;&#26469;&#24494;&#35843;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#22256;&#38590;&#65292;&#27604;&#22914;&#26080;&#27861;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#20854;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RLAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;RL&#31639;&#27861;&#21644;LLM&#20043;&#38388;&#24314;&#31435;&#26356;&#22909;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#27604;&#36739;&#20998;&#26512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;Flair&#12289;Trankit&#21644;Spacy&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#32972;&#26223;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.17171</link><description>&lt;p&gt;
&#40857;&#19982;&#22320;&#19979;&#22478;&#39046;&#22495;&#20013;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain. (arXiv:2309.17171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17171
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20998;&#26512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;Flair&#12289;Trankit&#21644;Spacy&#22312;&#40857;&#19982;&#22320;&#19979;&#22478;&#32972;&#26223;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#36890;&#29992;&#33521;&#35821;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22855;&#24187;&#25991;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;NER&#21487;&#20197;&#26816;&#27979;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;10&#20010;NER&#27169;&#22411;&#22312;7&#26412;&#40857;&#19982;&#22320;&#19979;&#22478;(D&amp;D)&#20882;&#38505;&#20070;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#35780;&#20272;&#29305;&#23450;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20070;&#31821;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#20570;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;Flair&#12289;Trankit&#21644;Spacy&#22312;&#35782;&#21035;&#40857;&#19982;&#22320;&#19979;&#22478;&#32972;&#26223;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many NLP tasks, although well-resolved for general English, face challenges in specific domains like fantasy literature. This is evident in Named Entity Recognition (NER), which detects and categorizes entities in text. We analyzed 10 NER models on 7 Dungeons and Dragons (D&amp;D) adventure books to assess domain-specific performance. Using open-source Large Language Models, we annotated named entities in these books and evaluated each model's precision. Our findings indicate that, without modifications, Flair, Trankit, and Spacy outperform others in identifying named entities in the D&amp;D context.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.17169</link><description>&lt;p&gt;
&#35780;&#20272;GPT&#27169;&#22411;&#22312;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#22312;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#30340;&#35786;&#26029;&#20197;&#21450;&#26500;&#24314;&#21327;&#35843;&#25252;&#29702;&#35745;&#21010;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#36807;&#31243;&#20381;&#36182;&#20110;&#20351;&#29992;&#26412;&#20307;&#27010;&#24565;&#23545;&#24739;&#32773;&#26723;&#26696;&#36827;&#34892;&#24314;&#27169;&#21644;&#25972;&#29702;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#31867;&#34920;&#22411;&#26412;&#20307;&#36827;&#34892;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#25903;&#25345;&#36825;&#39033;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#21253;&#25324;&#19971;&#20010;&#19981;&#21516;&#29305;&#24322;&#24615;&#32423;&#21035;&#30340;&#25552;&#31034;&#12289;&#20004;&#20010;GPT&#27169;&#22411;&#65288;gpt-3.5&#21644;gpt-4.0&#65289;&#20197;&#21450;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#34920;&#22411;&#35782;&#21035;&#40644;&#37329;&#26631;&#20934;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#20339;&#36816;&#34892;&#32467;&#26524;&#37319;&#29992;&#20102;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achi
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;</title><link>http://arxiv.org/abs/2309.17157</link><description>&lt;p&gt;
LatticeGen: &#19968;&#31181;&#22312;&#20113;&#19978;&#36827;&#34892;&#38544;&#31169;&#24863;&#30693;&#29983;&#25104;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#38544;&#34255;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#26684;&#23376;&#20013;
&lt;/p&gt;
&lt;p&gt;
LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17157
&lt;/p&gt;
&lt;p&gt;
LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29992;&#25143;-&#26381;&#21153;&#22120;&#20132;&#20114;&#27169;&#24335;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#26381;&#21153;&#22120;&#23436;&#20840;&#25511;&#21046;&#30528;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20351;&#24471;&#24819;&#35201;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20445;&#30041;&#32473;&#33258;&#24049;&#30340;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LatticeGen&#65292;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#22788;&#29702;&#22823;&#37096;&#20998;&#35745;&#31639;&#20219;&#21153;&#65292;&#32780;&#29992;&#25143;&#25511;&#21046;&#37319;&#26679;&#25805;&#20316;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#25143;&#23558;&#30495;&#23454;&#29983;&#25104;&#24207;&#21015;&#19982;&#22122;&#22768;&#26631;&#35760;&#28151;&#21512;&#65292;&#24182;&#38544;&#34255;&#22312;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20013;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#20551;&#35774;&#24694;&#24847;&#26381;&#21153;&#22120;&#30340;&#28508;&#22312;&#25915;&#20987;&#20197;&#21450;&#29992;&#25143;&#22914;&#20309;&#36827;&#34892;&#38450;&#24481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#22797;&#27874;&#26463;&#25628;&#32034;&#25915;&#20987;&#21644;&#28151;&#21512;&#22122;&#22768;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;LatticeGen&#24212;&#29992;&#20110;&#20445;&#25252;&#25552;&#31034;&#21644;&#29983;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20250;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;LatticeGen&#25104;&#21151;&#22320;&#22312;&#24378;&#25915;&#20987;&#19979;&#26174;&#33879;&#20445;&#25252;&#20102;&#30495;&#23454;&#29983;&#25104;&#65288;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#21487;&#33021;&#24341;&#20837;&#20005;&#37325;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#36805;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#24456;&#22909;&#29702;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#20998;&#26512;&#22823;&#37327;&#24320;&#25918;&#24615;&#38754;&#35848;&#25968;&#25454;&#65292;&#20197;&#32599;&#20852;&#20122;&#38590;&#27665;&#22312;&#23391;&#21152;&#25289;&#22269;&#31185;&#20811;&#26031;&#24052;&#25166;&#30340;&#35775;&#35848;&#35760;&#24405;&#20026;&#24212;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;LLMs&#23545;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#26102;&#38656;&#35201;&#38750;&#24120;&#35880;&#24910;&#65292;&#22240;&#20026;&#23384;&#22312;&#24341;&#20837;&#20559;&#35265;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#25105;&#20204;&#36825;&#37324;&#25152;&#25351;&#30340;&#20559;&#35265;&#26159;&#25216;&#26415;&#24847;&#20041;&#19978;&#30340;&#65292;&#21363;LLMs&#22312;&#27880;&#37322;&#35775;&#35848;&#35760;&#24405;&#26102;&#30340;&#38169;&#35823;&#19981;&#26159;&#19982;&#35775;&#35848;&#23545;&#35937;&#30340;&#29305;&#24449;&#26080;&#20851;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#27979;&#37327;&#35823;&#24046;&#21644;&#20559;&#35265;&#65292;&#20248;&#20110;LLMs&#30340;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#24517;&#39035;&#26377;&#19968;&#20123;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#20197;&#35780;&#20272;LLM&#26159;&#21542;&#24341;&#20837;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#22909;&#30340;&#36873;&#25321;&#21487;&#33021;&#26159;&#20351;&#29992;&#36739;&#31616;&#21333;&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#21644;&#36328;&#35821;&#35328;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#36164;&#28304;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#24191;&#20041;&#36328;&#35821;&#35328;&#38382;&#31572;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;mAP@k&#31995;&#25968;&#26469;&#23454;&#29616;&#24179;&#34913;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2309.17134</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#22312;&#36164;&#28304;&#31232;&#32570;&#24773;&#20917;&#19979;&#20419;&#36827;&#24191;&#20041;&#36328;&#35821;&#35328;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation. (arXiv:2309.17134v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17134
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#21644;&#36328;&#35821;&#35328;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#36164;&#28304;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#24191;&#20041;&#36328;&#35821;&#35328;&#38382;&#31572;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;mAP@k&#31995;&#25968;&#26469;&#23454;&#29616;&#24179;&#34913;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#25277;&#21462;&#24335;&#38382;&#31572;&#65288;QA&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#35821;&#35328;&#19978;&#20855;&#26377;&#39640;&#27700;&#24179;&#19988;&#19968;&#33268;&#20998;&#24067;&#30340;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#24191;&#20041;&#36328;&#35821;&#35328;&#36801;&#31227;&#65288;G-XLT&#65289;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#20851;&#27880;&#38382;&#39064;&#35821;&#35328;&#19982;&#19978;&#19979;&#25991;&#35821;&#35328;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#36804;&#20170;&#20026;&#27490;&#36825;&#20010;&#25361;&#25112;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#39640;&#24615;&#33021;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36328;&#35821;&#35328;&#37319;&#26679;&#21644;&#20808;&#36827;&#30340;&#33258;&#25105;&#33976;&#39311;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20960;&#31181;&#35821;&#35328;&#38388;&#36827;&#34892;&#20132;&#21449;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;QA&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;mAP@k&#31995;&#25968;&#26469;&#24494;&#35843;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#65292;&#21160;&#24577;&#35843;&#33410;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#24179;&#34913;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in multilingual extractive Question Answering (QA), models with high and uniformly distributed performance across languages remain challenging, especially for languages with limited resources. We study cross-lingual transfer mainly focusing on the Generalized Cross-Lingual Transfer (G-XLT) task, where the question language differs from the context language - a challenge that has received limited attention thus far. Our approach seeks to enhance cross-lingual QA transfer using a high-performing multilingual model trained on a large-scale dataset, complemented by a few thousand aligned QA examples across languages. Our proposed strategy combines cross-lingual sampling and advanced self-distillation training in generations to tackle the previous challenge. Notably, we introduce the novel mAP@k coefficients to fine-tune self-knowledge distillation loss, dynamically regulating the teacher's model knowledge to perform a balanced and effective knowledge transfer. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#26041;&#27861;&#65288;FLMR&#65289;&#26469;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#12290;FLMR&#36890;&#36807;&#33719;&#21462;&#34917;&#20805;&#30340;&#22270;&#20687;&#34920;&#31034;&#24182;&#20351;&#29992;&#19982;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;RA-VQA&#20013;&#26816;&#32034;&#22120;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.17133</link><description>&lt;p&gt;
&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering. (arXiv:2309.17133v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#26041;&#27861;&#65288;FLMR&#65289;&#26469;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#12290;FLMR&#36890;&#36807;&#33719;&#21462;&#34917;&#20805;&#30340;&#22270;&#20687;&#34920;&#31034;&#24182;&#20351;&#29992;&#19982;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;RA-VQA&#20013;&#26816;&#32034;&#22120;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KB-VQA&#65289;&#35201;&#27714;VQA&#31995;&#32479;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#24211;&#20013;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;RA-VQA&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;KB-VQA&#38382;&#39064;&#65292;&#39318;&#20808;&#20351;&#29992;&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#65288;DPR&#65289;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25991;&#26723;&#22238;&#31572;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#65288;FLMR&#65289;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;RA-VQA&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#12290;FLMR&#35299;&#20915;&#20102;RA-VQA&#26816;&#32034;&#22120;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#65288;1&#65289;&#36890;&#36807;&#22270;&#20687;&#21040;&#25991;&#26412;&#36716;&#25442;&#33719;&#24471;&#30340;&#22270;&#20687;&#34920;&#31034;&#21487;&#33021;&#19981;&#23436;&#25972;&#21644;&#19981;&#20934;&#30830;&#65292;&#65288;2&#65289;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#26159;&#36890;&#36807;&#19968;&#32500;&#23884;&#20837;&#35745;&#31639;&#30340;&#65292;&#21487;&#33021;&#23545;&#26356;&#32454;&#31890;&#24230;&#30340;&#30456;&#20851;&#24615;&#19981;&#25935;&#24863;&#12290;FLMR&#36890;&#36807;&#20351;&#29992;&#19982;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#33719;&#21462;&#34917;&#20805;&#22270;&#20687;&#34920;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from existing knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) relevance scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transforms using a vision model aligned with an existing text-based r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RDF&#30693;&#35782;&#22270;&#21019;&#24314;&#21644;&#29702;&#35299;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20219;&#21153;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#35299;&#26512;&#12289;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#21019;&#24314;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21830;&#19994;&#21487;&#29992;&#21644;&#20813;&#36153;&#30340;&#31163;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.17122</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RDF&#30693;&#35782;&#22270;&#21019;&#24314;&#21644;&#29702;&#35299;&#20013;&#30340;&#33021;&#21147;&#65306;LLMs&#22914;&#20309;&#36827;&#34892;Turtle&#35821;&#35328;&#35299;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?. (arXiv:2309.17122v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RDF&#30693;&#35782;&#22270;&#21019;&#24314;&#21644;&#29702;&#35299;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20219;&#21153;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#35299;&#26512;&#12289;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#21019;&#24314;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21830;&#19994;&#21487;&#29992;&#21644;&#20813;&#36153;&#30340;&#31163;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#34920;&#31034;&#25968;&#25454;&#30340;&#24418;&#24335;&#35821;&#35328;&#20013;&#30340;&#24037;&#20316;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#30693;&#35782;&#22270;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#20116;&#20010;&#20219;&#21153;&#65292;&#20197;&#25506;&#31350;&#23427;&#20204;&#22788;&#29702;Turtle&#35821;&#27861;&#30340;&#30693;&#35782;&#22270;&#35299;&#26512;&#12289;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#21019;&#24314;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#20855;&#26377;&#19981;&#21516;&#30340;&#22797;&#26434;&#24615;&#31243;&#24230;&#65292;&#24182;&#33021;&#38543;&#38382;&#39064;&#35268;&#27169;&#32780;&#25193;&#23637;&#65292;&#24050;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#21160;&#35780;&#20272;&#31995;&#32479;LLM-KG-Bench&#20013;&#12290;&#35780;&#20272;&#21253;&#25324;&#22235;&#20010;&#21830;&#19994;&#21487;&#29992;&#30340;LLMs - GPT-3.5&#12289;GPT-4&#12289;Claude 1.3 &#21644; Claude 2.0&#65292;&#20197;&#21450;&#20004;&#20010;&#21487;&#20813;&#36153;&#20351;&#29992;&#30340;&#31163;&#32447;&#27169;&#22411; GPT4All Vicuna &#21644; GPT4All Falcon 13B&#12290;&#36825;&#39033;&#20998;&#26512;&#28145;&#20837;&#20102;&#35299;&#20102;LLMs&#22312;&#20854;&#22312;R&#20013;&#24212;&#29992;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within R
&lt;/p&gt;</description></item><item><title>SCALE&#26159;&#19968;&#31181;&#23558;&#19987;&#19994;&#32763;&#35793;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#20026;&#32479;&#19968;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;STM&#30340;&#32763;&#35793;&#36827;&#19968;&#27493;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.17061</link><description>&lt;p&gt;
SCALE: &#19981;&#23545;&#31216;&#35821;&#35328;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
SCALE: Synergized Collaboration of Asymmetric Language Translation Engines. (arXiv:2309.17061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17061
&lt;/p&gt;
&lt;p&gt;
SCALE&#26159;&#19968;&#31181;&#23558;&#19987;&#19994;&#32763;&#35793;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#20026;&#32479;&#19968;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;STM&#30340;&#32763;&#35793;&#36827;&#19968;&#27493;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SCALE&#65292;&#19968;&#31181;&#23558;&#32039;&#20945;&#30340;&#19987;&#19994;&#32763;&#35793;&#27169;&#22411;&#65288;STM&#65289;&#21644;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36830;&#25509;&#20026;&#32479;&#19968;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;STM&#30340;&#32763;&#35793;&#24341;&#20837;&#19977;&#20803;&#32452;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#20013;&#65292;SCALE&#23454;&#29616;&#20102;LLM&#30340;&#32454;&#21270;&#21644;&#26530;&#36724;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;LLM&#30340;&#35821;&#35328;&#20559;&#24046;&#21644;STM&#30340;&#24182;&#34892;&#25968;&#25454;&#20559;&#24046;&#65292;&#22312;&#19981;&#29306;&#29298;&#24191;&#27867;&#24615;&#30340;&#21069;&#25552;&#19979;&#22686;&#24378;&#20102;LLM&#30340;&#19987;&#19994;&#24615;&#65292;&#20419;&#36827;&#20102;&#26080;&#38656;&#26114;&#36149;LLM&#31934;&#35843;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;SCALE&#26126;&#26174;&#20248;&#20110;&#23569;&#26679;&#26412;LLMs&#65288;GPT-4&#65289;&#21644;&#19987;&#19994;&#27169;&#22411;&#65288;NLLB&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;Xhosa&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20013;&#65292;SCALE&#30340;&#24615;&#33021;&#25345;&#32493;&#25552;&#21319;4&#20010;BLEURT&#20998;&#25968;&#65292;&#24403;&#37197;&#22791;&#20165;600M&#21442;&#25968;&#30340;&#32039;&#20945;&#27169;&#22411;&#26102;&#65292;SCALE&#22312;COMET&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#23569;&#26679;&#26412;GPT-4&#30340;2.5&#20010;&#20998;&#25968;&#21644;BLEURT&#20998;&#25968;&#19978;3.8&#20010;&#20998;&#25968;&#12290;SCALE&#36824;&#33021;&#26377;&#25928;&#22320;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively expl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#21487;&#35299;&#37322;&#30340;&#38271;&#31687;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;"&#20808;&#26816;&#32034;&#21518;&#38405;&#35835;"&#30340;&#27969;&#31243;&#29983;&#25104;&#35814;&#32454;&#22238;&#31572;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17050</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#21487;&#35299;&#37322;&#30340;&#38271;&#31687;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models. (arXiv:2309.17050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#21487;&#35299;&#37322;&#30340;&#38271;&#31687;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;"&#20808;&#26816;&#32034;&#21518;&#38405;&#35835;"&#30340;&#27969;&#31243;&#29983;&#25104;&#35814;&#32454;&#22238;&#31572;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21487;&#33021;&#22312;&#29983;&#27963;&#20013;&#38754;&#20020;&#27861;&#24459;&#32416;&#32439;&#65292;&#20294;&#20182;&#20204;&#23545;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#30340;&#29702;&#35299;&#19981;&#36275;&#65292;&#24448;&#24448;&#20351;&#20182;&#20204;&#22788;&#20110;&#24369;&#21183;&#22320;&#20301;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#20026;&#36890;&#36807;&#24320;&#21457;&#33258;&#21160;&#21270;&#27861;&#24459;&#25588;&#21161;&#31995;&#32479;&#24357;&#21512;&#36825;&#31181;&#27861;&#24459;&#35782;&#23383;&#24046;&#36317;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#65288;LQA&#65289;&#26041;&#27861;&#24448;&#24448;&#21463;&#38480;&#20110;&#33539;&#22260;&#29421;&#31364;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#27861;&#24459;&#39046;&#22495;&#65292;&#35201;&#20040;&#21482;&#25552;&#20379;&#31616;&#30701;&#26080;&#20449;&#24687;&#37327;&#30340;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#29983;&#25104;&#20219;&#20309;&#27861;&#23450;&#27861;&#24459;&#38382;&#39064;&#30340;&#38271;&#31687;&#22238;&#31572;&#65292;&#37319;&#29992;&#8220;&#20808;&#26816;&#32034;&#21518;&#38405;&#35835;&#8221;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#21457;&#24067;&#20102;&#38271;&#31687;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#65288;LLeQA&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1,868&#20010;&#30001;&#19987;&#23478;&#26631;&#27880;&#30340;&#27861;&#24459;&#38382;&#39064;&#65288;&#20351;&#29992;&#27861;&#35821;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#26681;&#25454;&#30456;&#20851;&#27861;&#24459;&#26465;&#25991;&#25552;&#20379;&#35814;&#32454;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a "retrieve-then-read" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#23383;&#22788;&#29702;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#36164;&#28304;&#27700;&#24179;&#23545;&#20854;&#24433;&#21709;&#30340;&#19978;&#19979;&#25991;&#21270;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;&#20116;&#20010;&#31561;&#32423;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#21482;&#21010;&#20998;&#20026;LRL&#21644;HRL&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17035</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#24433;&#21709;&#25991;&#26412;&#25968;&#23383;&#22788;&#29702;&#30340;&#35821;&#35328;&#36164;&#28304;&#27700;&#24179;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contextualising Levels of Language Resourcedness affecting Digital Processing of Text. (arXiv:2309.17035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#23383;&#22788;&#29702;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#36164;&#28304;&#27700;&#24179;&#23545;&#20854;&#24433;&#21709;&#30340;&#19978;&#19979;&#25991;&#21270;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;&#20116;&#20010;&#31561;&#32423;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#21482;&#21010;&#20998;&#20026;LRL&#21644;HRL&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#39046;&#22495;&#22914;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#24037;&#20855;&#37117;&#28041;&#21450;&#21040;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#22788;&#29702;&#65292;&#20174;&#25968;&#23383;&#21270;&#32440;&#36136;&#25991;&#20214;&#21040;&#35821;&#38899;&#29983;&#25104;&#12290;&#20869;&#23481;&#30340;&#35821;&#35328;&#36890;&#24120;&#34987;&#21010;&#20998;&#20026;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#65288;LRL&#65289;&#25110;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#65288;HRL&#65289;&#12290;&#38750;&#27954;&#35821;&#35328;&#34987;&#35748;&#20026;&#26159;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#65292;&#32780;&#33521;&#35821;&#21017;&#26159;&#36164;&#28304;&#26368;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#20026;&#20102;&#20026;&#36825;&#20123;&#35821;&#35328;&#24320;&#21457;&#36719;&#20214;&#31995;&#32479;&#20197;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#35821;&#35328;&#36164;&#28304;&#12290;&#26412;&#25991;&#35748;&#20026;&#23545;&#20110;&#25152;&#26377;&#35821;&#35328;&#26469;&#35828;&#65292;&#23558;&#20854;&#21010;&#20998;&#20026;LRL&#21644;HRL&#20004;&#31181;&#23545;&#31435;&#30340;&#31867;&#22411;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#36890;&#36807;&#23545;&#31038;&#20250;&#20013;&#35821;&#35328;&#36164;&#28304;&#30340;&#28165;&#26224;&#29702;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30697;&#38453;&#26469;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;"&#38750;&#24120;LRL"&#12289;"LRL"&#12289;"RL"&#12289;"HRL"&#21644;"&#38750;&#24120;HRL"&#12290;&#36825;&#31181;&#21010;&#20998;&#22522;&#20110;&#36830;&#25509;&#24773;&#26223;&#22522;&#30784;&#35774;&#26045;&#12289;&#24773;&#26223;&#23545;&#35805;&#27969;&#12289;&#24773;&#26223;&#30693;&#35782;&#31561;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application domains such as digital humanities and tool like chatbots involve some form of processing natural language, from digitising hardcopies to speech generation. The language of the content is typically characterised as either a low resource language (LRL) or high resource language (HRL), also known as resource-scarce and well-resourced languages, respectively. African languages have been characterized as resource-scarce languages (Bosch et al. 2007; Pretorius &amp; Bosch 2003; Keet &amp; Khumalo 2014) and English is by far the most well-resourced language. Varied language resources are used to develop software systems for these languages to accomplish a wide range of tasks. In this paper we argue that the dichotomous typology LRL and HRL for all languages is problematic. Through a clear understanding of language resources situated in a society, a matrix is developed that characterizes languages as Very LRL, LRL, RL, HRL and Very HRL. The characterization is based on the typology of con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20105;&#35770;&#24615;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#20105;&#35770;&#24615;&#25512;&#29702;&#24615;&#33021;&#19982;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#23494;&#20999;&#30456;&#20851;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#8220;&#20856;&#22411;&#25928;&#24212;&#8221;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#19979;&#35299;&#20915;&#30149;&#24577;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.16938</link><description>&lt;p&gt;
&#25105;&#24076;&#26395;&#20105;&#35770;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20105;&#35770;&#24615;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
I Wish to Have an Argument: Argumentative Reasoning in Large Language Models. (arXiv:2309.16938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20105;&#35770;&#24615;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#20105;&#35770;&#24615;&#25512;&#29702;&#24615;&#33021;&#19982;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#23494;&#20999;&#30456;&#20851;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#8220;&#20856;&#22411;&#25928;&#24212;&#8221;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#19979;&#35299;&#20915;&#30149;&#24577;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20105;&#35770;&#24615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#23454;&#39564;&#26694;&#26550;&#23450;&#20026;&#20105;&#35770;&#25366;&#25496;&#65288;AM&#65289;&#21644;&#20105;&#35770;&#23545;&#25552;&#21462;&#65288;APE&#65289;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#19978;&#25191;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#20219;&#24847;&#26631;&#31614;&#38598;&#65292;&#35821;&#20041;&#22270;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649; LLMs &#22312; AM &#21644; APE &#19978;&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#30340;&#20105;&#35770;&#24615;&#25512;&#29702;&#24615;&#33021;&#38750;&#24120;&#20381;&#36182;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20010;&#8220;&#20856;&#22411;&#25928;&#24212;&#8221;&#65292;&#36807;&#22810;&#30340;&#20856;&#22411;&#23454;&#20363;&#20250;&#23545;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#26368;&#20339;&#25968;&#37327;&#32422;&#20026;4-5&#20010;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#24182;&#19981;&#36866;&#29992;&#20110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#65306;&#25105;&#20204;&#21457;&#29616;&#20856;&#22411;&#25928;&#24212;&#34987;&#25269;&#28040;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#22312;&#30149;&#24577;&#38382;&#39064;&#19979;&#65292;CoT &#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25152;&#25253;&#21578;&#30340;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;&#20105;&#35770;&#24615;&#25512;&#29702;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate the ability of contemporary large language models (LLMs) to perform argumentative reasoning. We frame our experiments in terms of the argument mining (AM) and argument pair extraction (APE) tasks, and evaluate their ability to perform reasoning at increasing levels of abstraction in the input and output representations (e.g., arbitrary label sets, semantic graphs). We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an "exemplar effect", where too many exemplars increasingly become detrimental for task performance, and about 4-5 being the optimal amount. Neither result extends to chain-of-thought (CoT) prompting: we find the exemplar effect to be nullified, and our results suggest that CoT allows for better performance under ill-conditioned problems. We hope that the work reported contributes to the improvement of argument
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#65288;SSHR&#65289;&#20248;&#21270;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#34920;&#31034;&#65292;&#25552;&#21462;&#20986;&#35821;&#35328;&#30456;&#20851;&#24103;&#21644;&#29305;&#23450;&#20869;&#23481;&#65292;&#24182;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#23618;&#27425;&#33719;&#21462;&#26356;&#22810;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.16937</link><description>&lt;p&gt;
SSHR: &#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#25552;&#39640;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#65288;SSHR&#65289;&#20248;&#21270;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#34920;&#31034;&#65292;&#25552;&#21462;&#20986;&#35821;&#35328;&#30456;&#20851;&#24103;&#21644;&#29305;&#23450;&#20869;&#23481;&#65292;&#24182;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#23618;&#27425;&#33719;&#21462;&#26356;&#22810;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22240;&#20854;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#25193;&#23637;&#35821;&#35328;&#35206;&#30422;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22810;&#35821;&#31181;ASR&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SSL&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#34920;&#31034;&#21487;&#33021;&#21253;&#21547;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#19981;&#21516;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23618;&#27425;&#34920;&#31034;&#65288;SSHR&#65289;&#26469;&#20248;&#21270;&#22810;&#35821;&#31181;ASR&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;SSL&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#27425;&#23545;&#20110;&#35821;&#35328;&#30456;&#20851;&#21644;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#30340;&#34920;&#36798;&#24773;&#20917;&#65292;&#21457;&#29616;&#20855;&#26377;&#26356;&#24378;&#30456;&#20851;&#24615;&#30340;&#23618;&#27425;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#20013;&#38388;&#23618;&#20013;&#25552;&#21462;&#35821;&#35328;&#30456;&#20851;&#24103;&#65292;&#24182;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#23548;&#29305;&#23450;&#20869;&#23481;&#30340;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#20132;&#21449;CTC&#26041;&#27861;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#26368;&#32456;&#23618;&#27425;&#33719;&#24471;&#26356;&#22810;&#20869;&#23481;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#65288;C&#65289;&#19978;&#35780;&#20272;&#20102;SSHR&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#36830;&#32493;&#23398;&#20064;&#26469;&#36866;&#24212;&#24615;&#22320;&#26816;&#27979;&#38382;&#39064;&#20869;&#23481;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#26469;&#33258;8&#20010;&#26469;&#28304;&#30340;15&#20010;&#27880;&#37322;&#26550;&#26500;&#30340;84&#20010;&#30456;&#20851;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#36827;&#23637;&#24230;&#37327;&#26631;&#20934;&#65306;&#20248;&#20808;&#32771;&#34385;&#31867;&#21035;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16905</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#23398;&#20064;&#23454;&#29616;&#36866;&#24212;&#24615;&#38382;&#39064;&#20869;&#23481;&#26816;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning. (arXiv:2309.16905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#36830;&#32493;&#23398;&#20064;&#26469;&#36866;&#24212;&#24615;&#22320;&#26816;&#27979;&#38382;&#39064;&#20869;&#23481;&#12290;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#26469;&#33258;8&#20010;&#26469;&#28304;&#30340;15&#20010;&#27880;&#37322;&#26550;&#26500;&#30340;84&#20010;&#30456;&#20851;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#36827;&#23637;&#24230;&#37327;&#26631;&#20934;&#65306;&#20248;&#20808;&#32771;&#34385;&#31867;&#21035;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#38382;&#39064;&#20869;&#23481;&#65292;&#20363;&#22914;&#20167;&#24680;&#35328;&#35770;&#65292;&#26159;&#19968;&#39033;&#22810;&#26041;&#38754;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#65292;&#21463;&#31038;&#20250;&#21160;&#24577;&#12289;&#29992;&#25143;&#32676;&#20307;&#12289;&#20449;&#24687;&#26469;&#28304;&#30340;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#30340;&#28436;&#21464;&#24433;&#21709;&#12290;&#23398;&#26415;&#30028;&#21644;&#19994;&#30028;&#37117;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#24320;&#21457;&#20986;&#25429;&#25417;&#38382;&#39064;&#20869;&#23481;&#21508;&#20010;&#26041;&#38754;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#19981;&#21516;&#30446;&#26631;&#65292;&#36825;&#20123;&#27880;&#37322;&#26159;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#26816;&#27979;&#38382;&#39064;&#20869;&#23481;&#30340;&#36827;&#23637;&#25253;&#21578;&#26159;&#20998;&#25955;&#30340;&#12290;&#38500;&#38750;&#25105;&#20204;&#32771;&#34385;&#38382;&#39064;&#30340;&#21160;&#24577;&#24615;&#65292;&#21542;&#21017;&#36825;&#31181;&#27169;&#24335;&#39044;&#35745;&#20250;&#25345;&#32493;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#25972;&#21512;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#20854;&#21160;&#24577;&#29305;&#24615;&#25171;&#30772;&#36825;&#31181;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#21644;&#26694;&#26550;&#65292;&#29992;&#20110;&#38382;&#39064;&#20869;&#23481;&#26816;&#27979;&#65292;&#21253;&#25324;&#26469;&#33258;8&#20010;&#26469;&#28304;&#30340;15&#20010;&#27880;&#37322;&#26550;&#26500;&#30340;84&#20010;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#36827;&#23637;&#24230;&#37327;&#65306;&#20248;&#20808;&#32771;&#34385;&#31867;&#21035;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting problematic content, such as hate speech, is a multifaceted and ever-changing task, influenced by social dynamics, user populations, diversity of sources, and evolving language. There has been significant efforts, both in academia and in industry, to develop annotated resources that capture various aspects of problematic content. Due to researchers' diverse objectives, the annotations are inconsistent and hence, reports of progress on detection of problematic content are fragmented. This pattern is expected to persist unless we consolidate resources considering the dynamic nature of the problem. We propose integrating the available resources, and leveraging their dynamic nature to break this pattern. In this paper, we introduce a continual learning benchmark and framework for problematic content detection comprising over 84 related tasks encompassing 15 annotation schemas from 8 sources. Our benchmark creates a novel measure of progress: prioritizing the adaptability of class
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#20351;Pepper&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#32654;&#22269;&#25163;&#35821;&#24182;&#23454;&#29616;&#38750;&#35821;&#35328;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#36890;&#36807;&#20248;&#21270;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#27169;&#22411;&#21644;&#26234;&#33021;&#26426;&#22120;&#20154;&#20114;&#21160;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#20849;&#35821;&#25163;&#21183;&#21453;&#24212;&#65292;&#20026;&#26356;&#26377;&#26426;&#21644;&#30452;&#35266;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#35805;&#25552;&#20379;&#22522;&#30784;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#22686;&#24378;&#25163;&#35821;&#29702;&#35299;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16898</link><description>&lt;p&gt;
&#22522;&#20110;Pepper&#12289;&#36731;&#37327;&#32423;Transformer&#21644;LLM&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM. (arXiv:2309.16898v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#20351;Pepper&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#32654;&#22269;&#25163;&#35821;&#24182;&#23454;&#29616;&#38750;&#35821;&#35328;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#36890;&#36807;&#20248;&#21270;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#27169;&#22411;&#21644;&#26234;&#33021;&#26426;&#22120;&#20154;&#20114;&#21160;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#20849;&#35821;&#25163;&#21183;&#21453;&#24212;&#65292;&#20026;&#26356;&#26377;&#26426;&#21644;&#30452;&#35266;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#35805;&#25552;&#20379;&#22522;&#30784;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#22686;&#24378;&#25163;&#35821;&#29702;&#35299;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#20154;&#24418;&#26426;&#22120;&#20154;Pepper&#33021;&#22815;&#29702;&#35299;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#24182;&#20419;&#36827;&#38750;&#35821;&#35328;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#23884;&#20837;&#24335;&#31995;&#32479;&#20248;&#21270;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#27169;&#22411;&#65292;&#30830;&#20445;&#24555;&#36895;&#25163;&#21183;&#35782;&#21035;&#21516;&#26102;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#26234;&#33021;&#26426;&#22120;&#20154;&#20114;&#21160;&#12290;&#36890;&#36807;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#23450;&#21046;&#20132;&#20114;&#65292;&#20351;Pepper&#26426;&#22120;&#20154;&#33021;&#22815;&#20135;&#29983;&#33258;&#28982;&#30340;&#20849;&#35821;&#25163;&#21183;&#21453;&#24212;&#65292;&#20026;&#26356;&#26377;&#26426;&#21644;&#30452;&#35266;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#35805;&#22880;&#23450;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#20307;&#29616;&#20102;&#22312;&#31038;&#20132;&#24863;&#30693;AI&#20132;&#20114;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#21033;&#29992;Pepper&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#31361;&#26174;&#20986;&#22686;&#24378;&#25163;&#35821;&#29702;&#35299;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores using lightweight deep neural network architectures to enable the humanoid robot Pepper to understand American Sign Language (ASL) and facilitate non-verbal human-robot interaction. First, we introduce a lightweight and efficient model for ASL understanding optimized for embedded systems, ensuring rapid sign recognition while conserving computational resources. Building upon this, we employ large language models (LLMs) for intelligent robot interactions. Through intricate prompt engineering, we tailor interactions to allow the Pepper Robot to generate natural Co-Speech Gesture responses, laying the foundation for more organic and intuitive humanoid-robot dialogues. Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model. Leveraging the Pepper Robot's capabilities, we demonstrate the practicality and effectiveness of our approach in real-world scenarios. The results highlight a profound potential for enh
&lt;/p&gt;</description></item><item><title>DeBERTinha&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27493;&#39588;&#35757;&#32451;&#21644;&#24494;&#35843;&#36866;&#24212;DebertaV3 XSmall&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#23427;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#21028;&#26029;&#21477;&#23376;&#30456;&#20851;&#24615;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16844</link><description>&lt;p&gt;
DeBERTinha: &#19968;&#31181;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;DebertaV3 XSmall&#30340;&#22810;&#27493;&#39588;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16844
&lt;/p&gt;
&lt;p&gt;
DeBERTinha&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27493;&#39588;&#35757;&#32451;&#21644;&#24494;&#35843;&#36866;&#24212;DebertaV3 XSmall&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#23427;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#21028;&#26029;&#21477;&#23376;&#30456;&#20851;&#24615;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#33521;&#35821;&#39044;&#35757;&#32451;DebertaV3 XSmall&#27169;&#22411;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#22810;&#27493;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#22312;&#33889;&#33796;&#29273;&#35821;&#19978;&#30340;&#26377;&#25928;&#35843;&#20248;&#12290;&#20351;&#29992;Carolina&#21644;BrWac&#30340;&#21021;&#22987;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#35299;&#20915;&#34920;&#24773;&#31526;&#21495;&#12289;HTML&#26631;&#31614;&#21644;&#32534;&#30721;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;SentencePiece&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;50,000&#20010;token&#30340;&#33889;&#33796;&#29273;&#35821;&#29305;&#23450;&#35789;&#27719;&#34920;&#12290;&#27169;&#22411;&#19981;&#26159;&#20174;&#22836;&#35757;&#32451;&#65292;&#32780;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33521;&#35821;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#32593;&#32476;&#30340;&#22823;&#37096;&#20998;&#65292;&#20165;&#21253;&#25324;&#38543;&#26426;&#23884;&#20837;&#65292;&#20197;&#35782;&#21035;&#20174;&#22836;&#35757;&#32451;&#30340;&#26114;&#36149;&#25104;&#26412;&#12290;&#37319;&#29992;&#26367;&#25442;&#26631;&#35760;&#26816;&#27979;&#20219;&#21153;&#20197;&#19982;DebertaV3&#35757;&#32451;&#30340;&#30456;&#21516;&#26684;&#24335;&#24494;&#35843;&#27169;&#22411;&#12290;&#36866;&#24212;&#30340;&#27169;&#22411;&#31216;&#20026;DeBERTinha&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#21028;&#26029;&#21477;&#23376;&#30456;&#20851;&#24615;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#32988;&#36807;BER&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BER
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#23545;&#35805;&#25968;&#25454;&#24320;&#21457;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#26426;&#22120;&#20154;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#29305;&#24449;&#21644;&#33521;&#35821;&#25945;&#31185;&#20070;&#30340;&#31995;&#32479;&#26448;&#26009;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#21475;&#35821;&#25216;&#24039;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#25945;&#31185;&#20070;&#20027;&#39064;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#23545;&#35805;&#65292;&#36890;&#36807;&#24494;&#35843;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#35838;&#31243;&#39537;&#21160;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24341;&#39046;&#22522;&#20110;&#35838;&#31243;&#30340;&#23545;&#35805;&#21644;&#36866;&#24212;&#29992;&#25143;&#33521;&#35821;&#27700;&#24179;&#26041;&#38754;&#20248;&#20110;ChatGPT&#12290;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#25945;&#31185;&#20070;&#26041;&#27861;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#35838;&#31243;&#30456;&#21305;&#37197;&#24182;&#25552;&#20379;&#29992;&#25143;&#20010;&#24615;&#21270;&#23545;&#35805;&#32451;&#20064;&#30340;&#20114;&#21160;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.16804</link><description>&lt;p&gt;
&#35838;&#31243;&#39537;&#21160;&#30340;&#25945;&#32946;&#26426;&#22120;&#20154;&#65306;&#36890;&#36807;&#32508;&#21512;&#23545;&#35805;&#25968;&#25454;&#24320;&#21457;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data. (arXiv:2309.16804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#23545;&#35805;&#25968;&#25454;&#24320;&#21457;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#26426;&#22120;&#20154;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#29305;&#24449;&#21644;&#33521;&#35821;&#25945;&#31185;&#20070;&#30340;&#31995;&#32479;&#26448;&#26009;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#21475;&#35821;&#25216;&#24039;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#25945;&#31185;&#20070;&#20027;&#39064;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#23545;&#35805;&#65292;&#36890;&#36807;&#24494;&#35843;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#35838;&#31243;&#39537;&#21160;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24341;&#39046;&#22522;&#20110;&#35838;&#31243;&#30340;&#23545;&#35805;&#21644;&#36866;&#24212;&#29992;&#25143;&#33521;&#35821;&#27700;&#24179;&#26041;&#38754;&#20248;&#20110;ChatGPT&#12290;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#25945;&#31185;&#20070;&#26041;&#27861;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#35838;&#31243;&#30456;&#21305;&#37197;&#24182;&#25552;&#20379;&#29992;&#25143;&#20010;&#24615;&#21270;&#23545;&#35805;&#32451;&#20064;&#30340;&#20114;&#21160;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#22330;&#26223;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#25913;&#21464;&#20102;&#23398;&#29983;&#19982;&#35838;&#31243;&#20114;&#21160;&#21644;&#25945;&#24072;&#25480;&#35838;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35838;&#31243;&#39537;&#21160;&#30340;&#25945;&#32946;&#26426;&#22120;&#20154;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20114;&#21160;&#29305;&#24449;&#19982;&#33521;&#35821;&#25945;&#31185;&#20070;&#30340;&#31995;&#32479;&#26448;&#26009;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#21475;&#35821;&#25216;&#24039;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#25945;&#31185;&#20070;&#20013;&#25552;&#21462;&#30456;&#20851;&#20027;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#36825;&#20123;&#20027;&#39064;&#30456;&#20851;&#30340;&#23545;&#35805;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#23545;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#21019;&#24314;&#25105;&#20204;&#30340;&#35838;&#31243;&#39537;&#21160;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24341;&#39046;&#22522;&#20110;&#35838;&#31243;&#30340;&#23545;&#35805;&#21644;&#36866;&#24212;&#29992;&#25143;&#33521;&#35821;&#27700;&#24179;&#26041;&#38754;&#20248;&#20110;ChatGPT&#12290;&#36890;&#36807;&#23558;&#20256;&#32479;&#25945;&#31185;&#20070;&#26041;&#27861;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#35838;&#31243;&#30456;&#21305;&#37197;&#24182;&#25552;&#20379;&#29992;&#25143;&#20010;&#24615;&#21270;&#23545;&#35805;&#32451;&#20064;&#30340;&#20114;&#21160;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice.
&lt;/p&gt;</description></item><item><title>Promptbreeder&#26159;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#36827;&#34892;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16797</link><description>&lt;p&gt;
Promptbreeder: &#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#23454;&#29616;&#33258;&#25105;&#21442;&#29031;&#30340;&#33258;&#25105;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. (arXiv:2309.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16797
&lt;/p&gt;
&lt;p&gt;
Promptbreeder&#26159;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#36827;&#34892;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;Chain-of-Thought Prompting&#36825;&#26679;&#30340;&#27969;&#34892;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#31574;&#30053;&#36890;&#24120;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Promptbreeder&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#25105;&#21442;&#29031;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#29992;&#20110;&#36827;&#21270;&#21644;&#35843;&#25972;&#32473;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#12290;Promptbreeder&#36890;&#36807;LLM&#39537;&#21160;&#65292;&#23545;&#19968;&#32452;&#20219;&#21153;&#25552;&#31034;&#36827;&#34892;&#31361;&#21464;&#65292;&#24182;&#22312;&#35757;&#32451;&#38598;&#19978;&#35780;&#20272;&#20854;&#36866;&#24212;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#25552;&#31034;&#30340;&#31361;&#21464;&#26159;&#30001;LLM&#29983;&#25104;&#24182;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#33258;&#25105;&#25913;&#36827;&#30340;&#31361;&#21464;&#25552;&#31034;&#26469;&#25511;&#21046;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;Promptbreeder&#19981;&#20165;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36824;&#25913;&#36827;&#20102;&#25913;&#36827;&#36825;&#20123;&#20219;&#21153;&#25552;&#31034;&#30340;&#31361;&#21464;&#25552;&#31034;&#12290;Promptbreeder&#22312;&#24120;&#29992;&#30340;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;Chain-of-Thought&#21644;Plan-and-Solve Prompting&#31561;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#65292;&#36890;&#36807;&#22312;Longformer Encoder-Decoder&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16781</link><description>&lt;p&gt;
&#38271;&#36755;&#20837;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Hallucination Reduction in Long Input Text Summarization. (arXiv:2309.16781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#65292;&#36890;&#36807;&#22312;Longformer Encoder-Decoder&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#27169;&#22411;&#29983;&#25104;&#19981;&#34987;&#36755;&#20837;&#28304;&#25991;&#26723;&#25903;&#25345;&#30340;&#20449;&#24687;&#30340;&#29616;&#35937;&#12290;&#24187;&#35273;&#32473;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#26088;&#22312;&#20943;&#23569;&#38271;&#31687;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#38271;&#31687;&#31185;&#23398;&#30740;&#31350;&#25991;&#26723;&#21450;&#20854;&#25688;&#35201;&#30340;PubMed&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;Longformer Encoder-Decoder (LED)&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#21152;&#20837;&#20102;&#25968;&#25454;&#36807;&#28388;&#21644;&#32852;&#21512;&#23454;&#20307;&#21644;&#25688;&#35201;&#29983;&#25104;&#65288;JAENS&#65289;&#25216;&#26415;&#65292;&#20197;&#26368;&#23567;&#21270;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#19979;&#25351;&#26631;&#26469;&#34913;&#37327;&#23454;&#20307;&#32423;&#21035;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65306;&#28304;&#31934;&#30830;&#24230;&#21644;&#30446;&#26631;F1&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LED&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#25968;&#25454;&#36807;&#28388;&#25216;&#26415;&#22522;&#20110;&#19968;&#20123;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination in text summarization refers to the phenomenon where the model generates information that is not supported by the input source document. Hallucination poses significant obstacles to the accuracy and reliability of the generated summaries. In this paper, we aim to reduce hallucinated outputs or hallucinations in summaries of long-form text documents. We have used the PubMed dataset, which contains long scientific research documents and their abstracts. We have incorporated the techniques of data filtering and joint entity and summary generation (JAENS) in the fine-tuning of the Longformer Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the quality of the generated summary. We have used the following metrics to measure factual consistency at the entity level: precision-source, and F1-target. Our experiments show that the fine-tuned LED model performs well in generating the paper abstract. Data filtering techniques based on some preprocessing steps
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;ChatWords&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#35797;&#31995;&#32479;&#65292;&#20026;&#35780;&#20272;ChatGPT&#21644;&#20854;&#20182;NLP AI&#24037;&#20855;&#30340;&#35789;&#27719;&#30693;&#35782;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#23427;&#20204;&#22312;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#30340;&#38480;&#21046;&#21644;&#38169;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16777</link><description>&lt;p&gt;
ChatGPT&#30693;&#36947;&#22810;&#23569;&#20010;&#21333;&#35789;&#65311;&#31572;&#26696;&#26159;ChatWords&#12290;
&lt;/p&gt;
&lt;p&gt;
How many words does ChatGPT know? The answer is ChatWords. (arXiv:2309.16777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;ChatWords&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#35797;&#31995;&#32479;&#65292;&#20026;&#35780;&#20272;ChatGPT&#21644;&#20854;&#20182;NLP AI&#24037;&#20855;&#30340;&#35789;&#27719;&#30693;&#35782;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#23427;&#20204;&#22312;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#30340;&#38480;&#21046;&#21644;&#38169;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20851;&#27880;&#12290;ChatGPT&#30340;&#37319;&#29992;&#29575;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#25968;&#30334;&#19975;&#29992;&#25143;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#39046;&#22495;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;ChatGPT&#23384;&#22312;&#30528;&#38480;&#21046;&#21644;&#38169;&#35273;&#65292;&#20363;&#22914;&#20135;&#29983;&#30475;&#20284;&#21487;&#20449;&#20294;&#23436;&#20840;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#35780;&#20272;ChatGPT&#21644;&#31867;&#20284;&#30340;AI&#24037;&#20855;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#27491;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#36827;&#34892;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;ChatWords&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#35797;&#31995;&#32479;&#65292;&#20026;&#35780;&#20272;ChatGPT&#23545;&#20219;&#24847;&#21333;&#35789;&#38598;&#30340;&#30693;&#35782;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;ChatWords&#35774;&#35745;&#20026;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#20351;&#29992;&#21644;&#36866;&#24212;&#24615;&#24378;&#65292;&#20063;&#21487;&#20197;&#35780;&#20272;&#20854;&#20182;NLP AI&#24037;&#20855;&#12290;ChatWords&#24050;&#20844;&#24320;&#21487;&#29992;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#23545;AI&#24037;&#20855;&#30340;&#35789;&#27719;&#30693;&#35782;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;ChatWords&#30340;&#22909;&#22788;&#65306;&#35780;&#20272;&#23545;AI&#24037;&#20855;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of ChatGPT has put Artificial Intelligence (AI) Natural Language Processing (NLP) in the spotlight. ChatGPT adoption has been exponential with millions of users experimenting with it in a myriad of tasks and application domains with impressive results. However, ChatGPT has limitations and suffers hallucinations, for example producing answers that look plausible but they are completely wrong. Evaluating the performance of ChatGPT and similar AI tools is a complex issue that is being explored from different perspectives. In this work, we contribute to those efforts with ChatWords, an automated test system, to evaluate ChatGPT knowledge of an arbitrary set of words. ChatWords is designed to be extensible, easy to use, and adaptable to evaluate also other NLP AI tools. ChatWords is publicly available and its main goal is to facilitate research on the lexical knowledge of AI tools. The benefits of ChatWords are illustrated with two case studies: evaluating the knowledge tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16770</link><description>&lt;p&gt;
Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#65306;Persona&#24341;&#23548;&#30340;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#35805;AI&#12290;&#28982;&#32780;&#65292;&#35201;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#32972;&#26223;&#25110;&#20010;&#24615;&#21270;&#35843;&#25972;&#30340;&#36741;&#21161;&#20449;&#24687;&#20197;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#20173;&#28982;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#20351;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#30340;&#30740;&#31350;&#20165;&#26377;&#38480;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;AI&#25216;&#26415;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#36741;&#21161;&#25968;&#25454;&#20449;&#21495;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#20132;&#20114;&#25968;&#25454;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#31038;&#20250;&#30830;&#23450;&#22240;&#32032;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#27969;&#31243;&#32534;&#30721;&#26041;&#26696;&#20013;&#30340;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#21442;&#32771;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.16705</link><description>&lt;p&gt;
&#35299;&#30721;&#22270;&#20687;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#25361;&#25112;-&#21709;&#24212;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;Google Bard&#36827;&#34892;&#20102;64&#20010;&#35270;&#35273;&#25361;&#25112;&#65292;&#26088;&#22312;&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25361;&#25112;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#8220;&#35270;&#35273;&#24773;&#22659;&#25512;&#29702;&#8221;&#65292;&#8220;&#35270;&#35273;&#25991;&#26412;&#25512;&#29702;&#8221;&#21644;&#8220;&#19979;&#19968;&#22330;&#26223;&#39044;&#27979;&#8221;&#31561;&#65292;&#20197;&#30830;&#23450;Bard&#22312;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Bard&#20542;&#21521;&#20110;&#26681;&#25454;&#22270;&#29255;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#23450;&#22270;&#29255;&#20013;&#30340;&#32447;&#32034;&#26102;&#12290;&#19982;GPT4&#31561;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;Bard&#20284;&#20046;&#19981;&#20381;&#36182;&#20110;&#20687;Tesseract&#36825;&#26679;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#24211;&#65292;&#32780;&#26159;&#20687;Google Lens&#21644;Visual API&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#35782;&#21035;&#22797;&#26434;&#22270;&#29255;&#20013;&#30340;&#25991;&#26412;&#12290;&#26174;&#30528;&#30340;&#26159;&#65292;Bard&#21487;&#20197;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#35299;&#20915;ChatGPT&#26080;&#27861;&#29702;&#35299;&#30340;&#39564;&#35777;&#30721;&#65292;&#25512;&#33616;&#20351;&#29992;Tesseract&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Bard&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#26080;&#27861;&#37325;&#24314;&#25110;&#20462;&#25913;&#21407;&#22987;&#30340;&#35270;&#35273;&#23545;&#35937;&#26469;&#25903;&#25345;&#20854;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16671</link><description>&lt;p&gt;
&#25581;&#31192;CLIP&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16671
&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#19968;&#31181;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20026;&#29616;&#20195;&#35782;&#21035;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22411;&#27880;&#20837;&#20102;&#27963;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CLIP&#25104;&#21151;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#20854;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;CLIP&#21482;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#25968;&#25454;&#21644;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#30340;&#38750;&#24120;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20854;&#20182;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#36807;&#28388;&#26469;&#37325;&#29616;CLIP&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#22312;&#25581;&#31034;CLIP&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#32473;&#31038;&#21306;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20803;&#25968;&#25454;&#25972;&#29702;&#30340;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;MetaCLIP&#65289;&#12290;MetaCLIP&#36890;&#36807;&#23545;&#20803;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24179;&#34913;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#65288;&#20174;CLIP&#30340;&#27010;&#24565;&#20013;&#24471;&#20986;&#65289;&#20013;&#20135;&#29983;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20005;&#26684;&#38548;&#31163;&#20102;&#27169;&#22411;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#12290;MetaCLIP&#24212;&#29992;&#20110;&#21253;&#21547;400M&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#30340;CommonCrawl&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2309.16396</link><description>&lt;p&gt;
&#23545;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#32508;&#21512;&#35843;&#26597;&#65288;2016-2022&#65289;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Document-level Relation Extraction (2016-2022). (arXiv:2309.16396v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16396
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#65292;&#28041;&#21450;&#35782;&#21035;&#21644;&#25277;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36229;&#36234;&#21477;&#23376;&#36793;&#30028;&#12290;&#19982;&#20256;&#32479;&#30340;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#20219;&#21153;&#22312;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22823;&#35268;&#27169;&#25991;&#26723;&#65288;&#20363;&#22914;&#31185;&#23398;&#35770;&#25991;&#12289;&#27861;&#24459;&#21512;&#21516;&#25110;&#26032;&#38395;&#25991;&#31456;&#65289;&#20013;&#33258;&#21160;&#33719;&#21462;&#20851;&#31995;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20854;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#19981;&#21516;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) is an active area of research in natural language processing (NLP) concerned with identifying and extracting relationships between entities beyond sentence boundaries. Compared to the more traditional sentence-level relation extraction, DocRE provides a broader context for analysis and is more challenging because it involves identifying relationships that may span multiple sentences or paragraphs. This task has gained increased interest as a viable solution to build and populate knowledge bases automatically from unstructured large-scale documents (e.g., scientific papers, legal contracts, or news articles), in order to have a better understanding of relationships between entities. This paper aims to provide a comprehensive overview of recent advances in this field, highlighting its different applications in comparison to sentence-level relation extraction.
&lt;/p&gt;</description></item><item><title>Fast-HuBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#30340;5.2&#20493;&#21152;&#36895;&#65292;&#24182;&#19988;&#22312;&#21069;&#20154;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.13860</link><description>&lt;p&gt;
Fast-HuBERT: &#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning. (arXiv:2309.13860v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13860
&lt;/p&gt;
&lt;p&gt;
Fast-HuBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#30340;5.2&#20493;&#21152;&#36895;&#65292;&#24182;&#19988;&#22312;&#21069;&#20154;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21508;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;SSL&#27169;&#22411;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#65289;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;SSL&#27169;&#22411;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#38754;&#20020;&#30528;&#20849;&#21516;&#30340;&#22256;&#22659;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#28145;&#20837;&#23398;&#26415;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#26412;&#25991;&#20013;&#25552;&#20986;&#30340;Fast-HuBERT&#12290;&#25152;&#25552;&#20986;&#30340;Fast-HuBERT&#21487;&#20197;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#20351;&#29992;8&#20010;V100 GPU&#22312;1.1&#22825;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#21407;&#22987;&#23454;&#29616;&#65292;&#21152;&#36895;&#20102;5.2&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;Fast-HuBERT&#20013;&#30340;&#20004;&#31181;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20043;&#21069;&#24037;&#20316;&#20013;&#25253;&#21578;&#30340;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#24773;&#24863;&#20869;&#23481;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#35789;&#20856;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#26512;&#24773;&#24863;&#20869;&#23481;&#33021;&#22815;&#36229;&#36234;&#20165;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#39118;&#38505;&#24739;&#32773;&#12290;&#36825;&#39033;&#30740;&#31350;&#26174;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13147</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular Disease Risk Prediction via Social Media. (arXiv:2309.13147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#24773;&#24863;&#20869;&#23481;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#35789;&#20856;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#26512;&#24773;&#24863;&#20869;&#23481;&#33021;&#22815;&#36229;&#36234;&#20165;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#39118;&#38505;&#24739;&#32773;&#12290;&#36825;&#39033;&#30740;&#31350;&#26174;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;Twitter&#21644;&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35270;&#25512;&#25991;&#20013;&#20256;&#36798;&#30340;&#24773;&#24863;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19982;CVD&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#35789;&#20856;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#32654;&#22269;18&#20010;&#24030;&#30340;&#25512;&#25991;&#65292;&#28085;&#30422;&#20102;&#38463;&#24052;&#25289;&#22865;&#20122;&#22320;&#21306;&#12290;&#37319;&#29992;VADER&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#29992;&#25143;&#24402;&#31867;&#20026;&#28508;&#22312;&#30340;CVD&#39118;&#38505;&#12290;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#35780;&#20272;&#20010;&#20307;&#30340;CVD&#39118;&#38505;&#65292;&#24182;&#38543;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;CDC&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#65292;F1&#20998;&#25968;&#65292;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#21644;&#31185;&#24681;&#30340;Kappa&#20998;&#25968;&#65288;CK&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#26512;&#25512;&#25991;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#20248;&#20110;&#20165;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#28508;&#22312;&#39118;&#38505;&#24739;&#32773;&#30340;&#20010;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#39044;&#27979;CVD&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers utilize Twitter and sentiment analysis to forecast the risk of Cardiovascular Disease (CVD). We have introduced a novel CVD-related keyword dictionary by scrutinizing the emotions conveyed in tweets. We gathered tweets from eighteen U.S. states, encompassing the Appalachian region. Employing the VADER model for sentiment analysis, we categorized users as potentially at risk for CVD. Machine Learning (ML) models were employed to assess individuals' CVD risk and were subsequently applied to a CDC dataset containing demographic information for comparison. We considered various performance evaluation metrics, including Test Accuracy, Precision, Recall, F1 score, Mathew's Correlation Coefficient (MCC), and Cohen's Kappa (CK) score. Our findings demonstrate that analyzing the emotional content of tweets outperforms the predictive capabilities of demographic data alone, enabling the identification of individuals at potential risk of developing CVD. This research underscores the po
&lt;/p&gt;</description></item><item><title>DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.12625</link><description>&lt;p&gt;
DRG-LLaMA: &#35843;&#20248;LLaMA&#27169;&#22411;&#20197;&#39044;&#27979;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12625
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#20303;&#38498;&#20184;&#36153;&#31995;&#32479;&#20013;&#65292;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#65288;DRG&#65289;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#20998;&#32452;&#36807;&#31243;&#32791;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DRG-LLaMA&#65292;&#19968;&#20010;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25913;&#21892;DRG&#39044;&#27979;&#12290;&#20351;&#29992;Meta&#30340;LLaMA&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;236,192&#20010;MIMIC-IV&#20986;&#38498;&#25688;&#35201;&#19978;&#36827;&#34892;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20248;&#21270;&#12290;&#22312;&#36755;&#20837;&#20196;&#29260;&#38271;&#24230;&#20026;512&#30340;&#24773;&#20917;&#19979;&#65292;DRG-LLaMA-7B&#23454;&#29616;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#20026;0.327&#65292;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#20026;52.0&#65285;&#65292;&#23439;&#24179;&#22343;AUC&#20026;0.986&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;DRG-LLaMA-7B&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#39046;&#20808;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;ClinicalBERT&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;40.3&#65285;&#65292;&#30456;&#23545;&#20110;CAML&#25552;&#39640;&#20102;35.7&#65285;&#12290;&#24403;&#24212;&#29992;DRG-LLaMA&#26469;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#26102;&#65292;&#22522;&#26412;DRG&#30340;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.8&#65285;&#65292;&#32780;CC/MCC&#29366;&#24577;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays a key role but its current assignment process is time-consuming. We introduce DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously reported leading models on this task, demonstrating a relative improvement in macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to CAML. When DRG-LLaMA is applied to predict base DRGs and complication or comorbidity (CC) / major complication or comorbidity (MCC), the top-1 prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status. DRG-L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11696</link><description>&lt;p&gt;
&#24102;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#21327;&#20316;&#30340;&#22686;&#24378;&#35760;&#24518;&#22411;LLM&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT3.5&#65292;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38750;&#20010;&#24615;&#21270;&#29983;&#25104;&#26041;&#24335;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#29305;&#23450;&#32467;&#26524;&#30340;&#20122;&#20248;&#21270;&#12290;&#36890;&#24120;&#65292;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#30693;&#35782;&#21644;&#20559;&#22909;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#12290;&#36825;&#23601;&#38656;&#35201;&#22686;&#24378;&#38754;&#21521;&#29992;&#25143;&#30340;LLM&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#26469;&#23384;&#20648;&#21644;&#26816;&#32034;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#20351;&#29992;&#35760;&#24518;&#27169;&#22359;&#26080;&#27861;&#29702;&#35299;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#19988;&#23436;&#20840;&#35757;&#32451;&#19968;&#20010;LLM&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10966</link><description>&lt;p&gt;
MBR&#21644;QE&#24494;&#35843;&#65306;&#23545;&#26368;&#20339;&#21644;&#26368;&#26114;&#36149;&#30340;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#30340;&#35299;&#30721;&#26041;&#27861;&#30740;&#31350;&#20013;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#27010;&#29575;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22256;&#24785;&#24230;&#19982;&#36136;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26356;&#24378;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#37325;&#25490;&#24207;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#33976;&#39311;&#20102;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#30340;&#36136;&#37327;&#25552;&#21319;&#65292;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;NLG&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#22806;&#37096;LLM&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outp
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#35745;&#31639;&#37327;&#36731;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24378;&#27169;&#22411;&#21644;&#24369;&#27169;&#22411;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.09117</link><description>&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Contrastive Decoding Improves Reasoning in Large Language Models. (arXiv:2309.09117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09117
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#35745;&#31639;&#37327;&#36731;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24378;&#27169;&#22411;&#21644;&#24369;&#27169;&#22411;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#35299;&#30721;&#8212;&#8212;&#19968;&#31181;&#30001;Li&#31561;&#20154;&#25552;&#20986;&#30340;&#31616;&#21333;&#12289;&#35745;&#31639;&#37327;&#36731;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#8212;&#8212;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#36229;&#36807;&#36138;&#23146;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#26368;&#21021;&#34987;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#23545;&#27604;&#35299;&#30721;&#25628;&#32034;&#26368;&#22823;&#21270;&#24378;&#27169;&#22411;&#21644;&#24369;&#27169;&#22411;&#20043;&#38388;&#20284;&#28982;&#24046;&#24322;&#21152;&#26435;&#30340;&#23383;&#31526;&#20018;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#35299;&#30721;&#22312;HellaSwag&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#20351;LLaMA-65B&#36229;&#36807;&#20102;LLaMA 2&#12289;GPT-3.5&#21644;PaLM 2-L&#65292;&#24182;&#19988;&#22312;GSM8K&#25968;&#23398;&#21333;&#35789;&#25512;&#29702;&#22522;&#20934;&#19978;&#36229;&#36807;&#20102;LLaMA 2&#12289;GPT-3.5&#21644;PaLM-540B&#65292;&#27492;&#22806;&#36824;&#22312;&#19968;&#31995;&#21015;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#20102;&#25913;&#36827;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#23545;&#27604;&#35299;&#30721;&#36890;&#36807;&#38450;&#27490;&#26576;&#20123;&#25277;&#35937;&#25512;&#29702;&#38169;&#35823;&#20197;&#21450;&#36991;&#20813;&#22312;&#24605;&#32500;&#38142;&#26465;&#20013;&#22797;&#21046;&#36755;&#20837;&#30340;&#37096;&#20998;&#31561;&#31616;&#21333;&#27169;&#24335;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#23545;&#27604;&#35299;&#30721;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35762;&#25925;&#20107;&#30340;&#24320;&#22836;&#37096;&#20998;&#36890;&#24120;&#20250;&#36981;&#24490;&#21160;&#20316;&#21407;&#21017;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#33021;&#19982;&#35199;&#26041;&#20256;&#32479;&#35762;&#25925;&#20107;&#30340;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.07797</link><description>&lt;p&gt;
&#35762;&#25925;&#20107;&#30340;&#21160;&#24577;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Dynamical Principles of Storytelling. (arXiv:2309.07797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35762;&#25925;&#20107;&#30340;&#24320;&#22836;&#37096;&#20998;&#36890;&#24120;&#20250;&#36981;&#24490;&#21160;&#20316;&#21407;&#21017;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#33021;&#19982;&#35199;&#26041;&#20256;&#32479;&#35762;&#25925;&#20107;&#30340;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#20102;1800&#20010;&#30701;&#31687;&#23567;&#35828;&#30340;&#24320;&#22836;&#37096;&#20998;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#24179;&#22343;&#25925;&#20107;&#30340;&#21069;&#21313;&#20960;&#20010;&#27573;&#33853;&#36981;&#24490;&#20102;arXiv:2309.06600&#20013;&#23450;&#20041;&#30340;&#21160;&#20316;&#21407;&#21017;&#12290;&#24403;&#27573;&#33853;&#30340;&#39034;&#24207;&#34987;&#25171;&#20081;&#26102;&#65292;&#24179;&#22343;&#20540;&#19981;&#20877;&#20855;&#22791;&#36825;&#19968;&#29305;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24320;&#22987;&#19968;&#20010;&#25925;&#20107;&#26102;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#26377;&#19968;&#20010;&#20248;&#20808;&#26041;&#21521;&#65292;&#21487;&#33021;&#19982;&#20122;&#37324;&#22763;&#22810;&#24503;&#22312;&#12298;&#35799;&#23398;&#12299;&#20013;&#25152;&#26263;&#31034;&#30340;&#35199;&#26041;&#20256;&#32479;&#35762;&#25925;&#20107;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
When considering the opening part of 1800 short stories, we find that the first dozen paragraphs of the average narrative follow an action principle as defined in arXiv:2309.06600. When the order of the paragraphs is shuffled, the average no longer exhibits this property. The findings show that there is a preferential direction we take in semantic space when starting a story, possibly related to a common Western storytelling tradition as implied by Aristotle in Poetics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.07311</link><description>&lt;p&gt;
&#25439;&#22833;&#31361;&#28982;&#19979;&#38477;&#65306;&#35821;&#27861;&#20064;&#24471;&#12289;&#30456;&#21464;&#21644;MLM&#20013;&#30340;&#31616;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20391;&#37325;&#20110;&#29702;&#35299;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#21487;&#33021;&#25165;&#33021;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26576;&#20123;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;(MLMs)&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21270;&#26469;&#21152;&#28145;&#25105;&#20204;&#23545;&#26032;&#20852;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#36825;&#26159;MLMs&#20013;&#33258;&#28982;&#24418;&#25104;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#20854;&#20013;&#29305;&#23450;&#30340;Transformer&#22836;&#20542;&#21521;&#20110;&#20851;&#27880;&#29305;&#23450;&#30340;&#21477;&#27861;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;SAS&#65292;&#24182;&#21457;&#29616;&#36825;&#20010;&#31383;&#21475;&#19982;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#21516;&#26102;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;SAS&#20419;&#20351;&#20102;&#38543;&#21518;&#23545;&#35821;&#35328;&#33021;&#21147;&#30340;&#20064;&#24471;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#25805;&#32437;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;SAS&#65292;&#26469;&#30740;&#31350;SAS&#30340;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
&lt;/p&gt;</description></item><item><title>&#21465;&#20107;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;&#65292;&#20854;&#28436;&#21270;&#21487;&#20197;&#29992;&#19968;&#20010;&#34892;&#21160;&#31215;&#20998;&#26469;&#25551;&#36848;&#65292;&#24182;&#19988;&#24179;&#22343;&#36335;&#24452;&#19982;&#34892;&#21160;&#21407;&#29702;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.06600</link><description>&lt;p&gt;
&#21465;&#20107;&#20316;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Narrative as a Dynamical System. (arXiv:2309.06600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06600
&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;&#65292;&#20854;&#28436;&#21270;&#21487;&#20197;&#29992;&#19968;&#20010;&#34892;&#21160;&#31215;&#20998;&#26469;&#25551;&#36848;&#65292;&#24182;&#19988;&#24179;&#22343;&#36335;&#24452;&#19982;&#34892;&#21160;&#21407;&#29702;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#20154;&#31867;&#27963;&#21160;&#30340;&#20840;&#36807;&#31243;&#20197;&#21450;&#21465;&#20107;&#21487;&#20197;&#34987;&#35270;&#20026;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#21160;&#21147;&#31995;&#32479;&#65307;&#19968;&#20010;&#30001;&#34892;&#21160;&#31215;&#20998;&#25551;&#36848;&#20854;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;&#20174;A&#28857;&#21040;B&#28857;&#30340;&#25152;&#26377;&#21487;&#33021;&#36335;&#24452;&#30340;&#24179;&#22343;&#20540;&#30001;&#34892;&#21160;&#30340;&#26497;&#22823;&#20540;&#32473;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#32422;500&#20010;&#19981;&#21516;&#30340;&#21465;&#20107;&#26500;&#24314;&#20102;&#19977;&#26465;&#36825;&#26679;&#30340;&#36335;&#24452;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#22343;&#36335;&#24452;&#19982;&#34892;&#21160;&#21407;&#29702;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing evidence that human activity in general, and narrative in particular, can be treated as a dynamical system in the physics sense; a system whose evolution is described by an action integral, such that the average of all possible paths from point A to point B is given by the extremum of the action. We create by construction three such paths by averaging about 500 different narratives, and we show that the average path is consistent with an action principle.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>Neural-Hidden-CRF&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38544;&#34255;CRF&#23618;&#21644;&#21033;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05086</link><description>&lt;p&gt;
Neural-Hidden-CRF: &#19968;&#31181;&#40065;&#26834;&#30340;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler. (arXiv:2309.05086v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05086
&lt;/p&gt;
&lt;p&gt;
Neural-Hidden-CRF&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38544;&#34255;CRF&#23618;&#21644;&#21033;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#21521;&#22270;&#27169;&#22411; Neural-Hidden-CRF &#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#38382;&#39064;&#12290;&#22312;&#27010;&#29575;&#26080;&#21521;&#22270;&#29702;&#35770;&#30340;&#26694;&#26550;&#19979;&#65292;Neural-Hidden-CRF&#34701;&#21512;&#20102;&#19968;&#20010;&#38544;&#34255;CRF&#23618;&#65292;&#29992;&#20110;&#24314;&#27169;&#21333;&#35789;&#24207;&#21015;&#12289;&#28508;&#22312;&#30495;&#23454;&#24207;&#21015;&#21644;&#24369;&#26631;&#31614;&#24207;&#21015;&#20043;&#38388;&#30340;&#21464;&#37327;&#20851;&#31995;&#24182;&#20855;&#26377;&#20840;&#23616;&#35270;&#35282;&#12290;&#22312;Neural-Hidden-CRF&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;BERT&#25110;&#20854;&#20182;&#28145;&#24230;&#27169;&#22411;&#20026;&#28508;&#22312;&#30495;&#23454;&#24207;&#21015;&#25552;&#20379;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#38544;&#34255;&#30340;CRF&#23618;&#26469;&#25429;&#25417;&#20869;&#37096;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;Neural-Hidden-CRF&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#22312;&#23454;&#36341;&#20013;&#24378;&#22823;&#12290;&#23427;&#22312;&#19968;&#20010;&#20247;&#21253;&#22522;&#20934;&#27979;&#35797;&#21644;&#19977;&#20010;&#24369;&#30417;&#30563;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;&#24179;&#22343;&#27867;&#21270;&#21644;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;CHMM&#30340;2.80 F1&#20998;&#21644;2.23 F1&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference perfo
&lt;/p&gt;</description></item><item><title>Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.16149</link><description>&lt;p&gt;
Jais&#21644;Jais-chat&#65306;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16149
&lt;/p&gt;
&lt;p&gt;
Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Jais&#21644;Jais-chat&#65292;&#36825;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;GPT-3&#30340;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#30340;&#28151;&#21512;&#29289;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;130&#20159;&#20010;&#21442;&#25968;&#65292;&#26681;&#25454;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#33521;&#35821;&#25968;&#25454;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#33521;&#35821;&#26041;&#38754;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35843;&#20248;&#12289;&#23433;&#20840;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24320;&#25918;&#29256;&#26412;--&#22522;&#30784;Jais&#27169;&#22411;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;Jais-chat&#21464;&#31181;--&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;&#35814;&#35265;https://hugging
&lt;/p&gt;
&lt;p&gt;
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36136;&#30097;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23567;&#22411;LLMs&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;HPC&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Tokompiler&#30340;&#26032;&#22411;&#26631;&#35760;&#22120;&#65292;&#29992;&#20110;&#20195;&#30721;&#39044;&#22788;&#29702;&#21644;&#32534;&#35793;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.09440</link><description>&lt;p&gt;
Scope is all you need: Transforming LLMs for HPC Code (arXiv:2308.09440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36136;&#30097;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23567;&#22411;LLMs&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;HPC&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Tokompiler&#30340;&#26032;&#22411;&#26631;&#35760;&#22120;&#65292;&#29992;&#20110;&#20195;&#30721;&#39044;&#22788;&#29702;&#21644;&#32534;&#35793;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#35745;&#31639;&#36164;&#28304;&#30340;&#26356;&#23481;&#26131;&#33719;&#21462;&#65292;AI&#39046;&#22495;&#30340;&#36719;&#20214;&#24320;&#21457;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#24320;&#21457;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#12290;&#21363;&#20351;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#30340;LLMs&#20063;&#38750;&#24120;&#24222;&#22823;&#65288;&#20363;&#22914;&#65292;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#20196;&#20154;&#22256;&#24785;-&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#22312;&#19982;HPC&#19981;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;LLMs&#26469;&#22788;&#29702;HPC&#29305;&#23450;&#20219;&#21153;&#65311;&#22312;&#36825;&#19968;&#31995;&#21015;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36136;&#30097;&#29616;&#26377;LLMs&#25152;&#20570;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36890;&#36807;&#20026;&#29305;&#23450;&#39046;&#22495;&#24320;&#21457;&#26356;&#23567;&#30340;LLMs-&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#29305;&#23450;LLMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;HPC&#20026;&#39046;&#22495;&#24320;&#22987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tokompiler&#30340;&#26032;&#22411;&#26631;&#35760;&#22120;&#65292;&#19987;&#38376;&#29992;&#20110;HPC&#20013;&#30340;&#20195;&#30721;&#39044;&#22788;&#29702;&#21644;&#32534;&#35793;&#20013;&#24515;&#20219;&#21153;&#12290;Tokompiler&#21033;&#29992;&#35821;&#35328;&#21407;&#35821;&#30340;&#30693;&#35782;&#29983;&#25104;&#38754;&#21521;&#35821;&#35328;&#30340;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With easier access to powerful compute resources, there is a growing trend in the field of AI for software development to develop larger and larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size (e.g., billions of parameters) and demand expensive compute resources for training. We found this design choice confusing - why do we need large LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question design choices made by existing LLMs by developing smaller LLMs for specific domains - we call them domain-specific LLMs. Specifically, we start off with HPC as a domain and propose a novel tokenizer named Tokompiler, designed specifically for preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08945</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#26469;&#20943;&#32531;&#26631;&#31614;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#22686;&#21152;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#26631;&#31614;&#20559;&#35265;&#22312;&#21253;&#25324;&#21307;&#30103;&#12289;&#25307;&#32856;&#21644;&#20869;&#23481;&#23457;&#26680;&#22312;&#20869;&#30340;&#37325;&#35201;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#23481;&#26131;&#24102;&#26377;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;&#26631;&#31614;&#20559;&#35265;&#30340;&#23384;&#22312;&#24050;&#32463;&#22312;&#27010;&#24565;&#19978;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20294;&#32570;&#20047;&#24212;&#23545;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#8212;&#8212;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#65288;DeCoLe&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#12290;&#22312;&#28436;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#21518;&#65292;&#25105;&#20204;&#23558;DeCoLe&#24212;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#25361;&#25112;&#30340;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20854;&#25104;&#21151;&#35782;&#21035;&#20986;&#20559;&#35265;&#26631;&#31614;&#24182;&#36229;&#36234;&#31454;&#20105;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;ASR&#38169;&#35823;&#20462;&#27491;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;ChatGPT&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#38646;-shot&#25110;1-shot&#35774;&#32622;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ChatGPT&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20462;&#27491;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04172</link><description>&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;ASR&#38169;&#35823;&#20462;&#27491;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Large Language Models Perform ASR Error Correction?. (arXiv:2307.04172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;ASR&#38169;&#35823;&#20462;&#27491;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;ChatGPT&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#38646;-shot&#25110;1-shot&#35774;&#32622;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ChatGPT&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20462;&#27491;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ASR&#38169;&#35823;&#20462;&#27491;&#22312;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#21518;&#22788;&#29702;&#20013;&#32487;&#32493;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#24213;&#23618;ASR&#31995;&#32479;&#30340;&#35299;&#30721;&#32467;&#26524;&#21644;&#21442;&#32771;&#25991;&#26412;&#36827;&#34892;&#26377;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#32791;&#36153;&#36164;&#28304;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#20999;&#25442;&#24213;&#23618;ASR&#27169;&#22411;&#26102;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#20197;ChatGPT&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#22312;&#38646;-shot&#25110;1-shot&#35774;&#32622;&#19979;&#36827;&#34892;ASR&#38169;&#35823;&#20462;&#27491;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;ASR N-best&#21015;&#34920;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#32422;&#26463;&#38169;&#35823;&#20462;&#27491;&#21644;N-best&#32422;&#26463;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#12290;&#22312;Conformer-Transducer&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;ChatGPT&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20462;&#27491;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.14565</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#25351;&#20196;&#35843;&#25972;&#26469;&#20943;&#36731;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#20219;&#21153;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#24456;&#23481;&#26131;&#22312;&#25551;&#36848;&#22270;&#20687;&#21644;&#20154;&#31867;&#25351;&#20196;&#26102;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#24187;&#35273;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;LRV-Instruction&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;GPT4&#29983;&#25104;&#30340;12&#19975;&#20010;&#35270;&#35273;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;16&#20010;&#24320;&#25918;&#24335;&#25351;&#20196;&#21644;&#31572;&#26696;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#27491;&#25351;&#20196;&#26679;&#26412;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LRV-Instruction&#20197;&#21253;&#21547;&#26356;&#22810;&#38024;&#23545;&#26356;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27491;&#36127;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#36127;&#25351;&#20196;&#22312;&#20004;&#20010;&#35821;&#20041;&#23618;&#27425;&#19978;&#35774;&#35745;&#65306;&#65288;i&#65289;&#19981;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#21644;&#65288;ii&#65289;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;LMM&#25152;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;GPT4&#36741;&#21161;&#30340;&#35270;&#35273;&#25351;&#20196;&#35780;&#20272;&#65288;GAVIE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02797</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#23545;&#25277;&#35937;&#31526;&#21495;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.19402</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;&#29992;&#20110;&#24378;&#20581;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19402
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#34920;&#29616;&#20986;&#20998;&#32452;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#22914;&#21327;&#21464;&#37327;&#12290;ContextViT&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#26469;&#32534;&#30721;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#65292;&#20801;&#35768;&#27169;&#22411;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;Context-ViT&#23558;&#20849;&#20139;&#30456;&#21516;&#21327;&#21464;&#37327;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#35813;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#24182;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20687;&#20196;&#29260;&#20013;&#65292;&#20197;&#25429;&#33719;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#20026;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#25512;&#26029;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#39044;&#27979;&#36825;&#20123;&#20196;&#29260;&#65292;&#21482;&#38656;&#35201;&#32473;&#20986;&#19968;&#20123;&#26469;&#33258;&#32452;&#20998;&#24067;&#30340;&#26679;&#26412;&#21363;&#21487;&#65292;&#20351;&#24471;ContextViT&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#27979;&#35797;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#35828;&#26126;&#20102;ContextViT&#30340;&#24615;&#33021;&#12290;&#22312;&#30417;&#30563;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#19982;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992;10%&#30340;&#26631;&#35760;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#36890;&#36807;&#24341;&#23548;&#36873;&#25321;&#26469;&#33258;&#23569;&#25968;&#32676;&#20307;&#30340;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18149</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#31561;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#36896;&#34394;&#20551;&#30340;&#23398;&#26415;&#25991;&#26412;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#25512;&#29305;&#31561;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26816;&#27979;&#36825;&#20123;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;ML&#20998;&#31867;&#22120;&#12289;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#21487;&#30693;&#26041;&#27861;&#21644;&#31934;&#35843;&#30340;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#26816;&#27979;&#22120;&#22312;&#26500;&#24314;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25991;&#26412;&#38271;&#24230;&#30340;&#22240;&#32032;&#65306;&#30701;&#25991;&#26412;&#30340;&#32570;&#20047;&#20449;&#24687;&#29305;&#24449;&#65292;&#20351;&#20854;&#26356;&#38590;&#26816;&#27979;&#12290;&#38024;&#23545;&#22810;&#23610;&#24230;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#65288;MPU&#65289;&#35757;&#32451;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25215;&#35748;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20855;&#26377;&#31867;&#20154;&#23646;&#24615;&#65292;&#24182;&#23558;&#25991;&#26412;&#20998;&#31867;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#27491;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#36825;&#20123;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20026;"unlabeled"&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#27491;&#36127;&#26679;&#26412;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14718</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#36136;&#37327;&#25110;&#39118;&#26684;&#38480;&#21046;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#21253;&#25324;&#23398;&#20064;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25968;&#25454;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#8220;&#20302;&#36136;&#37327;&#8221;&#25968;&#25454;&#21644;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#20307;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#36807;&#28388;&#20250;&#21024;&#38500;&#26377;&#20215;&#20540;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#32780;&#25968;&#25454;&#25910;&#38598;&#21644;RLHF&#19981;&#26029;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25110;LM&#25506;&#32034;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#8220;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;RL&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#20247;&#21253;&#21644;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;LM&#25928;&#29992;&#21527;&#65311;&#8221;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21320;&#39184;&#24378;&#21270;&#23398;&#20064;&#65288;LoL-RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#26469;&#23398;&#20064;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;1&#27493;RL&#28216;&#25103;&#12290; LoL-RL&#21487;&#20197;&#24494;&#35843;LM&#65292;&#20197;&#20248;&#21270;&#20219;&#24847;&#22522;&#20110;&#20998;&#31867;&#22120;&#25110;&#20154;&#23450;&#20041;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#27169;&#22411;&#30340;&#20116;&#20010;&#19981;&#21516;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09807</link><description>&lt;p&gt;
&#20851;&#20110;transformer&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#21487;&#36801;&#31227;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26597;&#35810;&#23545;&#27169;&#22411;&#23398;&#20064;&#26368;&#26377;&#30410;&#30340;&#31034;&#20363;&#26469;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#20110;&#24494;&#35843;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#19981;&#28165;&#26970;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#20027;&#21160;&#23398;&#20064;&#25910;&#30410;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36866;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#31215;&#26497;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#22312;&#20351;&#29992;&#19981;&#21516;PLM&#35757;&#32451;&#26102;&#33021;&#21542;&#20445;&#25345;AL&#25910;&#30410;&#12290;&#25105;&#20204;&#23558;AL&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#19982;&#19981;&#21516;PLMs&#26597;&#35810;&#21040;&#30340;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#34920;&#26126;&#20855;&#26377;&#31867;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;AL&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#20855;&#26377;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#33719;&#21462;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#26356;&#21463;&#21040;AL&#26041;&#27861;&#30340;&#36873;&#25321;&#32780;&#38750;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07893</link><description>&lt;p&gt;
PESTS: &#27874;&#26031;&#35821;-&#33521;&#35821;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#32452;&#20214;&#12290;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#21333;&#35789;&#12289;&#30701;&#35821;&#12289;&#27573;&#33853;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24456;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#35201;&#27714;&#22312;&#28304;&#21644;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#20379;&#20855;&#26377;&#19968;&#23450;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23545;&#12290;&#35768;&#22810;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#26469;&#24357;&#34917;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#19981;&#21487;&#29992;&#30340;&#19981;&#36275;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#30340;&#35823;&#24046;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#29305;&#24449;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#26102;&#65292;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27599;&#31181;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05735</link><description>&lt;p&gt;
&#23398;&#20064;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27599;&#31181;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a sparse multilingual ASR model, which explicitly learns the parameters for each language by activating language-specific sub-networks, and enables knowledge transfer for lower-resource languages via joint multilingual training. The proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26377;&#25928;&#22320;&#21387;&#32553;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;ASR&#20013;&#65292;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#21487;&#33021;&#20250;&#23548;&#33268;&#26576;&#20123;&#35821;&#35328;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#20026;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#25513;&#30721;&#21487;&#33021;&#19981;&#36866;&#21512;&#25152;&#26377;&#35821;&#35328;&#24182;&#19988;&#20002;&#24323;&#37325;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASR&#36335;&#24452;&#65292;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#23427;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#65288;&#8220;&#36335;&#24452;&#8221;&#65289;&#65292;&#20197;&#20415;&#20026;&#27599;&#31181;&#35821;&#35328;&#26174;&#24335;&#22320;&#23398;&#20064;&#21442;&#25968;&#12290;&#36890;&#36807;&#37325;&#21472;&#30340;&#23376;&#32593;&#32476;&#65292;&#20849;&#20139;&#21442;&#25968;&#36824;&#21487;&#20197;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;ASR&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#27969;&#24335;RNN-T&#27169;&#22411;&#22312;4&#31181;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ASR&#36335;&#24452;&#27169;&#22411;&#20248;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;&#21333;&#35821;&#31232;&#30095;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks ("pathways"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#30340;&#20449;&#24687;&#23494;&#24230;&#36234;&#39640;&#65292;&#20132;&#27969;&#36895;&#24230;&#36234;&#24555;&#65292;&#20294;&#23545;&#35805;&#24191;&#24230;&#20943;&#23569;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#32467;&#26500;&#23545;&#20154;&#31867;&#20132;&#27969;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.08491</link><description>&lt;p&gt;
&#20449;&#24687;&#23494;&#24230;&#26356;&#39640;&#30340;&#20154;&#31867;&#35821;&#35328;&#21152;&#24555;&#20102;&#20132;&#27969;&#36895;&#24230;&#65292;&#20294;&#20943;&#23569;&#20102;&#23545;&#35805;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Languages with Greater Information Density Increase Communication Speed, but Decrease Conversation Breadth. (arXiv:2112.08491v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08491
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#20449;&#24687;&#23494;&#24230;&#36234;&#39640;&#65292;&#20132;&#27969;&#36895;&#24230;&#36234;&#24555;&#65292;&#20294;&#23545;&#35805;&#24191;&#24230;&#20943;&#23569;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#32467;&#26500;&#23545;&#20154;&#31867;&#20132;&#27969;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#22312;&#32534;&#30721;&#35821;&#20041;&#20449;&#24687;&#65288;&#20363;&#22914;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#39068;&#33394;&#12289;&#20154;&#30340;&#36523;&#20307;&#37096;&#20301;&#21644;&#27963;&#21160;&#65289;&#26041;&#38754;&#23384;&#22312;&#24191;&#27867;&#30340;&#24046;&#24322;&#65292;&#20294;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#20840;&#23616;&#32467;&#26500;&#21450;&#20854;&#19982;&#20154;&#31867;&#20132;&#27969;&#30340;&#20851;&#31995;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#32422;1000&#31181;&#35821;&#35328;&#30340;&#26679;&#26412;&#20013;&#65292;&#35821;&#35328;&#22312;&#35789;&#27719;&#20013;&#32534;&#30721;&#20449;&#24687;&#30340;&#23494;&#24230;&#23384;&#22312;&#24191;&#27867;&#21464;&#21270;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#20449;&#24687;&#23494;&#24230;&#19982;&#35821;&#20041;&#20449;&#24687;&#30340;&#23494;&#38598;&#37197;&#32622;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36861;&#36394;&#20102;&#35821;&#35328;&#20449;&#24687;&#23494;&#24230;&#19982;&#20132;&#27969;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#20449;&#24687;&#23494;&#24230;&#26356;&#39640;&#30340;&#35821;&#35328;&#20542;&#21521;&#20110;&#65288;1&#65289;&#26356;&#24555;&#30340;&#20132;&#27969;&#65292;&#20294;&#65288;2&#65289;&#22312;&#35805;&#39064;&#19978;&#27010;&#24565;&#19978;&#26356;&#29421;&#31364;&#30340;&#23545;&#35805;&#65292;&#23545;&#35805;&#20013;&#26356;&#28145;&#20837;&#22320;&#35752;&#35770;&#20102;&#35805;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#20154;&#31867;&#20132;&#27969;&#28192;&#36947;&#19978;&#30340;&#19968;&#20010;&#37325;&#35201;&#21464;&#24322;&#28304;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#32467;&#26500;&#23545;&#20154;&#31867;&#20132;&#27969;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human languages vary widely in how they encode information within circumscribed semantic domains (e.g., time, space, color, human body parts and activities), but little is known about the global structure of semantic information and nothing about its relation to human communication. We first show that across a sample of ~1,000 languages, there is broad variation in how densely languages encode information into their words. Second, we show that this language information density is associated with a denser configuration of semantic information. Finally, we trace the relationship between language information density and patterns of communication, showing that informationally denser languages tend toward (1) faster communication, but (2) conceptually narrower conversations within which topics of conversation are discussed at greater depth. These results highlight an important source of variation across the human communicative channel, revealing that the structure of language shapes the nat
&lt;/p&gt;</description></item></channel></rss>