<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.18969</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models from Concept to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18969
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#26497;&#22823;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#26368;&#21021;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;GPT&#31995;&#21015;&#12290;&#36825;&#39033;&#25506;&#32034;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#39537;&#21160;&#24037;&#20855;&#22312;&#25913;&#38761;&#20256;&#32479;&#32534;&#30721;&#21644;&#38382;&#39064;&#35299;&#20915;&#31561;&#20219;&#21153;&#19978;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#36328;&#36234;&#19981;&#21516;&#34892;&#19994;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#24320;&#36767;&#26032;&#36335;&#24452;&#12290;&#20174;&#20195;&#30721;&#35299;&#37322;&#21644;&#22270;&#20687;&#25551;&#36848;&#21040;&#20419;&#36827;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#25645;&#24314;&#21644;&#25512;&#36827;&#35745;&#31639;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20307;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;</title><link>https://arxiv.org/abs/2403.07008</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20215;&#27491;&#30830;: &#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoEval Done Right: Using Synthetic Data for Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#20351;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21487;&#20197;&#20351;&#29992;AI&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#27492;&#31867;&#30446;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#37327;&#65292;&#36825;&#19968;&#36807;&#31243;&#31216;&#20026;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20559;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.03121</link><description>&lt;p&gt;
&#24868;&#24594;&#30340;&#30007;&#24615;&#65292;&#24754;&#20260;&#30340;&#22899;&#24615;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#32490;&#24402;&#22240;&#20013;&#21453;&#26144;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24615;&#21035;&#30340;&#12290;&#34429;&#28982;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24773;&#32490;&#20998;&#26512;&#26041;&#38754;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#31354;&#30333;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20250;&#35805;&#35821;&#20013;&#65292;&#24773;&#32490;&#21644;&#24615;&#21035;&#23494;&#20999;&#30456;&#20851;&#12290;&#20363;&#22914;&#65292;&#22899;&#24615;&#32463;&#24120;&#34987;&#35748;&#20026;&#26356;&#20855;&#31227;&#24773;&#33021;&#21147;&#65292;&#32780;&#30007;&#24615;&#30340;&#24868;&#24594;&#26356;&#21463;&#31038;&#20250;&#25509;&#21463;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#24320;&#28304;&#21644;&#23553;&#38381;&#28304;&#65289;&#36827;&#34892;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#30340;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#24773;&#32490;&#26159;&#21542;&#20855;&#26377;&#24615;&#21035;&#29305;&#24449;&#65292;&#20197;&#21450;&#36825;&#20123;&#21464;&#21270;&#26159;&#21542;&#22522;&#20110;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#37319;&#29992;&#24615;&#21035;&#21270;&#35282;&#33394;&#24182;&#23558;&#24773;&#32490;&#24402;&#22240;&#20110;&#31867;&#20284;&#8220;&#24403;&#25105;&#19982;&#20146;&#36817;&#30340;&#20154;&#21457;&#29983;&#20005;&#37325;&#20105;&#25191;&#8221;&#36825;&#26679;&#30340;&#20107;&#20214;&#12290;&#28982;&#21518;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#29983;&#25104;&#30340;&#24773;&#32490;&#19982;&#24615;&#21035;-&#20107;&#20214;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#19968;&#33268;&#23637;&#29616;&#20986;&#24615;&#21035;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03121v1 Announce Type: new  Abstract: Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25506;&#32034;LLMs&#22312;&#26032;&#38395;&#26631;&#39064;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;&#20013;&#19981;&#21516;&#32423;&#21035;&#25552;&#31034;&#35268;&#33539;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00418</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#26032;&#38395;&#26631;&#39064;&#30340;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;&#65306;&#25506;&#32034;&#19981;&#21516;&#32423;&#21035;&#30340;&#25552;&#31034;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25506;&#32034;LLMs&#22312;&#26032;&#38395;&#26631;&#39064;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;&#20013;&#19981;&#21516;&#32423;&#21035;&#25552;&#31034;&#35268;&#33539;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#24120;&#24120;&#36890;&#36807;&#26377;&#24847;&#35782;&#22320;&#20197;&#29305;&#23450;&#26041;&#24335;&#25551;&#32472;&#23454;&#20307;&#26469;&#24341;&#21457;&#24773;&#24863;&#65292;&#36825;&#20351;&#24471;&#26032;&#38395;&#26631;&#39064;&#30340;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;(TSA)&#25104;&#20026;&#19968;&#39033;&#20540;&#24471;&#20570;&#20294;&#20855;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;&#24494;&#35843;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23637;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;TSA&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#32972;&#26223;&#30693;&#35782;&#26377;&#38480;&#65292;&#38656;&#35201;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;LLMs&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#35821;&#35328;&#21644;&#19990;&#30028;&#30693;&#35782;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20026;TSA&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#25552;&#31034;&#35774;&#35745;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#36890;&#36807;&#19982;&#20027;&#35266;&#20219;&#21153;&#30340;&#27880;&#37322;&#33539;&#24335;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#35774;&#35745;&#23545;LLMs&#22312;&#26032;&#38395;&#26631;&#39064;TSA&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#35268;&#33539;&#25552;&#31034;&#65288;&#20174;&#32431;&#31929;&#30340;&#38646;&#26679;&#26412;&#21040;&#31526;&#21512;&#27880;&#37322;&#25351;&#21335;&#30340;&#31934;&#24515;&#20934;&#22791;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#65289;&#30340;&#26368;&#20808;&#36827;LLMs&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35748;&#35782;&#21040;TSA&#30340;&#20027;&#35266;&#24615;&#36136;&#65292;&#25105;&#20204;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00418v1 Announce Type: new  Abstract: News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA performance, but their background knowledge is limited, and they require a labeled dataset. LLMs offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with in-context learning abilities, yet their performance is heavily influenced by prompt design. Drawing parallels with annotation paradigms for subjective tasks, we explore the influence of prompt design on the performance of LLMs for TSA of news headlines. We evaluate the predictive accuracy of state-of-the-art LLMs using prompts with different levels of prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines. Recognizing the subjective nature of TSA, we evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18023</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Mirror Cognitive Language Processing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#35748;&#30693;&#20219;&#21153;&#20013;&#23454;&#29616;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;LLMs&#26159;&#20174;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#30340;&#22823;&#37327;&#25991;&#26412;&#20135;&#20986;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;LLMs&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65292;&#25110;LLMs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;LLMs&#34920;&#24449;&#21644;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#65292;&#20197;&#35780;&#20272;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#37319;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#26469;&#34913;&#37327;16&#31181;&#20027;&#27969;LLMs&#19982;&#22823;&#33041;fMRI&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#35268;&#27169;&#12289;&#23545;&#40784;&#35757;&#32451;&#12289;&#25351;&#23548;&#38468;&#21152;&#65289;&#23545;LLM-&#22823;&#33041;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#27491;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.16159</link><description>&lt;p&gt;
DistALANER&#65306;&#24320;&#28304;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#36828;&#31243;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#20840;&#38754;&#30340;&#20004;&#27493;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#26469;&#35299;&#20915;&#36719;&#20214;&#25968;&#25454;&#26631;&#27880;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#35813;&#36807;&#31243;&#24039;&#22937;&#22320;&#21033;&#29992;&#35821;&#35328;&#21551;&#21457;&#12289;&#29420;&#29305;&#30340;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NER&#22312;&#20851;&#31995;&#25277;&#21462;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
&lt;/p&gt;</description></item><item><title>FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16107</link><description>&lt;p&gt;
FuseChat&#65306;&#23545;&#35805;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FuseChat: Knowledge Fusion of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16107
&lt;/p&gt;
&lt;p&gt;
FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#29420;&#29305;&#33021;&#21147;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#20135;&#29983;&#24040;&#22823;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#31454;&#20105;&#33021;&#21147;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#26159;&#23558;&#29616;&#26377;&#30340;LLMs&#32452;&#21512;&#25104;&#26356;&#24378;&#22823;&#30340;LLM&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#26679;&#21270;&#26550;&#26500;&#65292;&#30452;&#25509;&#21442;&#25968;&#34701;&#21512;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#65292;FuseLLM&#24341;&#20837;&#20102;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25345;&#32493;&#35757;&#32451;&#23558;&#22810;&#20010;&#32467;&#26500;&#22810;&#26679;&#30340;LLM&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;LLM&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;FuseLLM&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;LLM&#30340;&#34701;&#21512;&#65292;&#29983;&#25104;&#20102;FuseChat&#12290;FuseChat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#21644;&#35268;&#27169;&#19981;&#21516;&#30340;&#28304;LLMs&#36827;&#34892;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>SDFT&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#30340;&#31934;&#31616;&#25968;&#25454;&#38598;&#26469;&#26725;&#25509;&#20998;&#24067;&#24046;&#36317;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#26222;&#36890;&#24494;&#35843;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13669</link><description>&lt;p&gt;
&#33258;&#33976;&#39311;&#26725;&#25509;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#20998;&#24067;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13669
&lt;/p&gt;
&lt;p&gt;
SDFT&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#30340;&#31934;&#31616;&#25968;&#25454;&#38598;&#26469;&#26725;&#25509;&#20998;&#24067;&#24046;&#36317;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#26222;&#36890;&#24494;&#35843;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23558;&#23427;&#20204;&#24494;&#35843;&#20026;&#29305;&#23450;&#20219;&#21153;&#24120;&#24120;&#38754;&#20020;&#22312;&#24179;&#34913;&#24615;&#33021;&#21644;&#20445;&#30041;&#19968;&#33324;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20219;&#21153;&#25968;&#25454;&#38598;&#19982;LLMs&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#26159;&#20027;&#35201;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#33976;&#39311;&#24494;&#35843;&#65288;SDFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#30340;&#31934;&#31616;&#25968;&#25454;&#38598;&#24341;&#23548;&#24494;&#35843;&#20197;&#21305;&#37197;&#20854;&#21407;&#22987;&#20998;&#24067;&#26469;&#26725;&#25509;&#20998;&#24067;&#24046;&#36317;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;Llama-2-chat&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SDFT&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#26222;&#36890;&#24494;&#35843;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;SDFT&#23637;&#31034;&#20102;&#20445;&#25345;LLMs&#30340;&#26377;&#30410;&#24615;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13669v1 Announce Type: new  Abstract: The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13448</link><description>&lt;p&gt;
ED-Copilot: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#36741;&#21161;&#20943;&#23569;&#24613;&#35786;&#31185;&#31561;&#24453;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24613;&#35786;&#31185;&#65288;ED&#65289;&#20013;&#65292;&#24739;&#32773;&#22312;&#35786;&#26029;&#21069;&#38656;&#35201;&#36827;&#34892;&#20998;&#35786;&#21644;&#22810;&#31181;&#23454;&#39564;&#23460;&#26816;&#27979;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23548;&#33268;&#24613;&#35786;&#31185;&#25317;&#25380;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;&#21307;&#30103;&#38169;&#35823;&#12289;&#20154;&#21592;&#26543;&#31469;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#26102;&#38388;&#65289;&#25104;&#26412;&#26377;&#25928;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21327;&#21161;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#25105;&#20204;&#19982;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#21512;&#20316;&#31574;&#21010;&#20102;MIMIC-ED-Assist&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24314;&#35758;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#24613;&#35786;&#31561;&#24453;&#26102;&#38388;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#24182;&#22312;&#27491;&#30830;&#39044;&#27979;&#35832;&#22914;&#27515;&#20129;&#20043;&#31867;&#20851;&#38190;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ED-Copilot&#65292;&#23427;&#20381;&#27425;&#24314;&#35758;&#24739;&#32773;&#29305;&#23450;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;ED-Copilot&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#24739;&#32773;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#24182;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457; GumbelSoft &#27700;&#21360;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#26696;&#34920;&#29616;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.12948</link><description>&lt;p&gt;
GumbelSoft: &#36890;&#36807;GumbelMax&#25216;&#24039;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#25968;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457; GumbelSoft &#27700;&#21360;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#26696;&#34920;&#29616;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#23398;&#26415;&#19981;&#35802;&#23454;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#35299;&#30721;&#20026;&#22522;&#30784;&#30340;&#27700;&#21360;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;GumbelMax&#25216;&#24039;&#30340;&#27700;&#21360;(GM&#27700;&#21360;)&#65292;&#26159;&#38450;&#33539;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#28389;&#29992;&#30340;&#26480;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#21487;&#26816;&#27979;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;GM&#27700;&#21360;&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#26041;&#38754;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23545;&#20110;&#30456;&#21516;&#25552;&#31034;&#22987;&#32456;&#20135;&#29983;&#30456;&#21516;&#36755;&#20986;&#65292;&#20174;&#32780;&#36127;&#38754;&#24433;&#21709;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;GM&#27700;&#21360;&#65292;&#21363;Logits-Addition&#27700;&#21360;&#65292;&#21450;&#20854;&#19977;&#20010;&#21464;&#20307;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#21464;&#20307;&#20013;&#65292;GumbelSoft&#27700;&#21360;(&#20316;&#20026;Logits-Addition&#27700;&#21360;&#30340;&#19968;&#20010;softmax&#21464;&#20307;)&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20854;AUROC&#20998;&#25968;&#36229;&#36807;&#20854;&#20182;&#20004;&#20010;&#26367;&#20195;&#21464;&#20307;0.1&#33267;0.3&#65292;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12948v1 Announce Type: new  Abstract: Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing oth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.11639</link><description>&lt;p&gt;
&#20855;&#26377;Transformer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;Softmax&#27880;&#24847;&#21147;&#36866;&#24212;&#20989;&#25968;Lipschitz&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24615;&#26159;&#20854;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#22312;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#26576;&#20123;&#25968;&#25454;&#38544;&#24335;&#22320;&#34987;&#21576;&#29616;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#32972;&#26223;&#65292;&#24182;&#34987;&#35201;&#27714;&#22312;&#35813;&#32972;&#26223;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#24517;&#39035;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#32972;&#26223;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;Softmax&#27880;&#24847;&#21147;&#22312;&#19968;&#20010;ICL&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#32972;&#26223;&#37117;&#32534;&#30721;&#20102;&#19968;&#20010;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20064;&#19968;&#20010;&#31383;&#21475;&#65292;&#29992;&#20110;&#23454;&#29616;&#19968;&#20010;&#36866;&#24212;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31383;&#21475;&#38543;&#30528;Lipschitzness&#30340;&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#30340;&#22686;&#21152;&#32780;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#20302;&#31209;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#65292;&#27880;&#24847;&#21147;&#21333;&#20803;&#22312;&#25512;&#29702;&#20043;&#21069;&#23398;&#20250;&#20102;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#36866;&#24212;&#24615;&#20851;&#38190;&#22320;&#20381;&#36182;&#20110;softmax&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11639v1 Announce Type: new  Abstract: A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.11592</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#35752;&#38646;&#38454;&#20248;&#21270;&#22312;&#20869;&#23384;&#39640;&#25928;LLM&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#20351;&#29992;SGD&#21644;Adam&#31561;&#19968;&#38454;&#65288;FO&#65289;&#20248;&#21270;&#22120;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#20307;&#31215;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;FO&#26799;&#24230;&#35745;&#31639;&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#24102;&#26469;&#30340;&#24040;&#22823;&#20869;&#23384;&#24320;&#38144;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#20869;&#23384;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;&#31561;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21521;&#19981;&#20351;&#29992;BP&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#65292;&#26500;&#24314;&#22312;MeZO&#25552;&#20986;&#30340;&#27010;&#24565;&#22522;&#30784;&#19978;&#12290;&#19982;&#20256;&#32479;&#30340;ZO&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25506;&#32034;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#12289;&#39318;&#27425;&#25512;&#20986;&#30340;&#22522;&#20934;&#30740;&#31350;&#36328;&#36234;&#20116;&#20010;LLM&#31995;&#21015;&#65288;Roberta&#65292;OPT&#65292;LLaMA&#65292;Vicuna&#65292;Mistral&#65289;&#65292;&#19977;&#31181;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#20116;&#31181;&#24494;&#35843;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10958</link><description>&lt;p&gt;
&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;: &#36890;&#36807;&#23545;&#30456;&#21516;&#21644;&#19981;&#21516;&#25552;&#31034;&#30340;&#23545;&#27604;&#21709;&#24212;&#22686;&#24378;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#65292;&#23558;&#27169;&#22411;&#19982;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30452;&#25509;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;DPO&#36890;&#36807;&#20351;&#29992;&#20174;&#30456;&#21516;&#25552;&#31034;&#20013;&#27966;&#29983;&#30340;&#20559;&#22909;&#23545;&#26469;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DPO&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#20154;&#31867;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#23398;&#20064;&#24448;&#24448;&#28041;&#21450;&#23545;&#19981;&#20165;&#30456;&#21516;&#32780;&#19988;&#30456;&#20284;&#38382;&#39064;&#30340;&#23545;&#27604;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#12290;RPO&#26088;&#22312;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#12290;&#23427;&#24341;&#20837;&#20102;&#23545;&#27604;&#21152;&#26435;&#26426;&#21046;&#65292;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#21253;&#25324;&#25104;&#23545;&#21644;&#19981;&#25104;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
&lt;/p&gt;</description></item><item><title>EcoRank&#26159;&#19968;&#20010;&#20004;&#23618;&#31649;&#32447;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26377;&#20851;&#39044;&#31639;&#20998;&#37197;&#21644;LLM API&#30340;&#20915;&#31574;&#26469;&#23454;&#29616;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#39044;&#31639;&#24863;&#30693;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10866</link><description>&lt;p&gt;
EcoRank: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21463;&#38480;&#39044;&#31639;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10866
&lt;/p&gt;
&lt;p&gt;
EcoRank&#26159;&#19968;&#20010;&#20004;&#23618;&#31649;&#32447;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26377;&#20851;&#39044;&#31639;&#20998;&#37197;&#21644;LLM API&#30340;&#20915;&#31574;&#26469;&#23454;&#29616;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#39044;&#31639;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36807;&#31243;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#26597;&#35810;&#21644;&#20505;&#36873;&#27573;&#33853;&#65292;&#21033;&#29992;&#28857;&#23545;&#28857;&#65292;&#21015;&#34920;&#24335;&#21644;&#25104;&#23545;&#25552;&#31034;&#31574;&#30053;&#12290;LLMs&#30340;&#36825;&#20123;&#25490;&#24207;&#31574;&#30053;&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#23427;&#20204;&#30340;&#25104;&#26412;&#65306;&#30001;&#20110;API&#25910;&#36153;&#22522;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22312;&#32473;&#23450;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#37325;&#26032;&#25490;&#24207;&#24615;&#33021;&#65292;&#36890;&#36807;&#23548;&#33322;&#25552;&#31034;&#36873;&#25321;&#65292;LLM API&#21644;&#39044;&#31639;&#20998;&#21106;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20351;&#29992;&#19968;&#32452;LLM API&#36827;&#34892;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#30340;&#21463;&#38480;&#39044;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;EcoRank&#65292;&#23427;&#26159;&#19968;&#20010;&#20004;&#23618;&#31649;&#32447;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#26377;&#20851;&#36328;&#25552;&#31034;&#31574;&#30053;&#21644;LLM API&#30340;&#39044;&#31639;&#20998;&#37197;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;QA&#21644;&#27573;&#37325;&#25490;&#24207;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EcoRank&#20248;&#20110;&#20854;&#20182;&#20855;&#26377;&#39044;&#31639;&#24847;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10866v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware 
&lt;/p&gt;</description></item><item><title>DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10379</link><description>&lt;p&gt;
DataDreamer: &#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10379
&lt;/p&gt;
&lt;p&gt;
DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#20219;&#21153;&#35780;&#20272;&#12289;&#24494;&#35843;&#12289;&#25552;&#28860;&#20197;&#21450;&#20854;&#20182;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#30740;&#31350;&#24037;&#20316;&#27969;&#20013;&#20351;&#29992;LLMs&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#23427;&#20204;&#30340;&#35268;&#27169;&#12289;&#38381;&#28304;&#24615;&#36136;&#20197;&#21450;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#24037;&#20316;&#27969;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#36805;&#36895;&#23835;&#36215;&#21644;&#36825;&#20123;&#29420;&#29305;&#25361;&#25112;&#23545;&#24320;&#25918;&#31185;&#23398;&#21644;&#20351;&#29992;&#23427;&#20204;&#30340;&#24037;&#20316;&#30340;&#21487;&#37325;&#29616;&#24615;&#20135;&#29983;&#20102;&#30452;&#25509;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataDreamer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#32534;&#20889;&#31616;&#21333;&#30340;&#20195;&#30721;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#12290;DataDreamer&#36824;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#40723;&#21169;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#24211;&#21644;&#25991;&#26723;&#21487;&#22312;h&#32593;&#31449;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10379v1 Announce Type: new  Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35780;&#20998;&#27169;&#22411;&#22312;&#21508;&#20010;&#35201;&#32032;&#19978;&#30340;&#34920;&#29616;&#24182;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#29983;&#25104;&#19968;&#20221;"&#29702;&#24615;&#25253;&#21578;&#21345;"&#65292;&#20197;&#30830;&#23450;&#20195;&#29702;&#20154;&#26159;&#21542;&#36275;&#22815;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.09552</link><description>&lt;p&gt;
&#29702;&#24615;&#25253;&#21578;&#21345;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rationality Report Cards: Assessing the Economic Rationality of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35780;&#20998;&#27169;&#22411;&#22312;&#21508;&#20010;&#35201;&#32032;&#19978;&#30340;&#34920;&#29616;&#24182;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#29983;&#25104;&#19968;&#20221;"&#29702;&#24615;&#25253;&#21578;&#21345;"&#65292;&#20197;&#30830;&#23450;&#20195;&#29702;&#20154;&#26159;&#21542;&#36275;&#22815;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;LLM&#29992;&#20316;&#20915;&#31574;"&#20195;&#29702;&#20154;"&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#36825;&#21253;&#25324;&#24456;&#22810;&#33258;&#30001;&#24230;&#65306;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65307;&#22914;&#20309;&#36827;&#34892;&#25552;&#31034;&#65307;&#26159;&#21542;&#35201;&#27714;&#20854;&#36827;&#34892;&#20869;&#30465;&#12289;&#36827;&#34892;&#24605;&#32771;&#38142;&#31561;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65288;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#30830;&#23450;LLM&#20195;&#29702;&#20154;&#26159;&#21542;&#36275;&#22815;&#21487;&#38752;&#20197;&#20415;&#33719;&#24471;&#20449;&#20219;&#65289;&#38656;&#35201;&#19968;&#31181;&#35780;&#20272;&#36825;&#31181;&#20195;&#29702;&#20154;&#32463;&#27982;&#21512;&#29702;&#24615;&#30340;&#26041;&#27861;&#35770;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29702;&#24615;&#20915;&#31574;&#30340;&#32463;&#27982;&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#30740;&#12289;&#23558;&#20195;&#29702;&#20154;&#24212;&#35813;&#23637;&#29616;&#30340;&#22823;&#37327;&#32454;&#31890;&#24230;"&#35201;&#32032;"&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#20998;&#24067;&#65292;&#20197;&#23450;&#37327;&#35780;&#20998;LLM&#22312;&#36825;&#20123;&#35201;&#32032;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#29983;&#25104;&#19968;&#20221;"&#29702;&#24615;&#25253;&#21578;&#21345;"&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19982;14&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09552v1 Announce Type: new  Abstract: There is increasing interest in using LLMs as decision-making "agents." Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained "elements" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a "rationality report card." Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characte
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08939</link><description>&lt;p&gt;
&#35770;&#25454;&#39034;&#24207;&#22312;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#36215;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Premise Order Matters in Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08939
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#33030;&#24369;&#24615;&#65306;&#23613;&#31649;&#36825;&#31181;&#39034;&#24207;&#19981;&#20250;&#25913;&#21464;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;LLMs&#23545;&#20110;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#33030;&#24369;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#35770;&#25454;&#39034;&#24207;&#19982;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#26102;&#65292;LLMs&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#35770;&#25454;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#65288;&#32780;&#19981;&#26159;&#38543;&#26426;&#39034;&#24207;&#65289;&#20250;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#23545;&#28436;&#32462;&#25512;&#29702;&#20013;&#35770;&#25454;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#35770;&#25454;&#39034;&#24207;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#36229;&#36807;30&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#22522;&#20110;GSM8K&#30340;&#22522;&#20934;&#27979;&#35797;R-GSM&#26469;&#30740;&#31350;&#39034;&#24207;&#25928;&#24212;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
&lt;/p&gt;</description></item><item><title>BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08219</link><description>&lt;p&gt;
BBox-Adapter: &#36731;&#37327;&#32423;&#36866;&#37197;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08219
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#21644;Gemini&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20219;&#21153;&#30340;&#35201;&#27714;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21442;&#25968;&#12289;&#23884;&#20837;&#21644;&#36755;&#20986;&#27010;&#29575;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#33021;&#36890;&#36807;&#23427;&#20204;&#30340;API&#26381;&#21153;&#36866;&#24212;&#36825;&#20123;&#40657;&#30418;LLMs&#65292;&#36825;&#24341;&#21457;&#20102;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BBox-Adapter&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#40657;&#30418;LLMs&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#12290;BBox-Adapter&#36890;&#36807;&#23558;&#30446;&#26631;&#25968;&#25454;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#28304;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#26469;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#26469;&#25552;&#39640;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#24809;&#32602;&#28304;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#23558;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#12289;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#30340;&#23454;&#26102;&#27491;&#26679;&#26412;&#37319;&#26679;&#19982;&#20808;&#21069;&#36866;&#24212;&#30340;&#36127;&#26679;&#26412;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BBox-Adapter&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#40657;&#30418;LLMs&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
&lt;/p&gt;</description></item><item><title>TELLER&#26159;&#19968;&#20010;&#21487;&#20449;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35748;&#30693;&#21644;&#20915;&#31574;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#12289;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25511;&#21046;&#24615;&#12290;&#35748;&#30693;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#36923;&#36753;&#35859;&#35789;&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35835;&#30340;&#36923;&#36753;&#21407;&#23376;&#12290;&#20915;&#31574;&#31995;&#32479;&#25512;&#23548;&#21487;&#25512;&#24191;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#32858;&#21512;&#36825;&#20123;&#21407;&#23376;&#65292;&#23454;&#29616;&#30495;&#23454;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.07776</link><description>&lt;p&gt;
TELLER: &#19968;&#20010;&#21487;&#20449;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07776
&lt;/p&gt;
&lt;p&gt;
TELLER&#26159;&#19968;&#20010;&#21487;&#20449;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35748;&#30693;&#21644;&#20915;&#31574;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#12289;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25511;&#21046;&#24615;&#12290;&#35748;&#30693;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#36923;&#36753;&#35859;&#35789;&#65292;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35835;&#30340;&#36923;&#36753;&#21407;&#23376;&#12290;&#20915;&#31574;&#31995;&#32479;&#25512;&#23548;&#21487;&#25512;&#24191;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#32858;&#21512;&#36825;&#20123;&#21407;&#23376;&#65292;&#23454;&#29616;&#30495;&#23454;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#27867;&#28389;&#24050;&#25104;&#20026;&#19968;&#20010;&#20005;&#37325;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#24341;&#36215;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#26816;&#27979;&#20551;&#26032;&#38395;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#38750;&#36879;&#26126;&#25512;&#29702;&#36807;&#31243;&#12289;&#24046;&#24378;&#20154;&#24847;&#30340;&#25512;&#24191;&#33021;&#21147;&#20197;&#21450;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#30340;&#22266;&#26377;&#39118;&#38505;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;TELLER&#65292;&#29992;&#20110;&#21487;&#20449;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#37325;&#28857;&#20851;&#27880;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25512;&#24191;&#24615;&#21644;&#21487;&#25511;&#21046;&#24615;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#34701;&#21512;&#35748;&#30693;&#21644;&#20915;&#31574;&#31995;&#32479;&#30340;&#21452;&#31995;&#32479;&#26694;&#26550;&#26469;&#23454;&#29616;&#65292;&#36981;&#24490;&#20197;&#19978;&#21407;&#21017;&#12290;&#35748;&#30693;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#36923;&#36753;&#35859;&#35789;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#21487;&#35835;&#30340;&#36923;&#36753;&#21407;&#23376;&#12290;&#21516;&#26102;&#65292;&#20915;&#31574;&#31995;&#32479;&#25512;&#23548;&#21487;&#25512;&#24191;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#32858;&#21512;&#36825;&#20123;&#21407;&#23376;&#65292;&#23454;&#29616;&#30495;&#23454;&#21644;&#34394;&#20551;&#26032;&#38395;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.04617</link><description>&lt;p&gt;
InfLLM: &#25581;&#31034;LLMs&#23545;&#20110;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26080;&#38656;&#35757;&#32451;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04617
&lt;/p&gt;
&lt;p&gt;
InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20855;&#26377;&#28459;&#38271;&#20256;&#36755;&#36755;&#20837;&#30340;&#29616;&#23454;&#24212;&#29992;&#30340;&#22522;&#30707;&#65292;&#22914;LLM&#39537;&#21160;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#21463;&#38480;&#26368;&#22823;&#38271;&#24230;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#26080;&#27861;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#22806;&#21644;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#28369;&#21160;&#27880;&#24847;&#21147;&#31383;&#21475;&#21644;&#20002;&#24323;&#36828;&#36317;&#31163;&#26631;&#35760;&#65292;&#20197;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#24207;&#21015;&#20869;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;InfLLM&#65292;&#26469;&#25581;&#31034;LLMs&#22788;&#29702;&#27969;&#24335;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InfLLM&#23558;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#23384;&#20648;&#21040;&#38468;&#21152;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#21046;&#26469;&#26597;&#25214;&#19982;&#27880;&#24847;&#35745;&#31639;&#30456;&#20851;&#30340;&#26631;&#35760;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;InfLLM&#20801;&#35768;LLMs&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#35821;&#20041;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REBORN&#65292;&#22312;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#26469;&#23454;&#29616;&#36793;&#30028;&#20998;&#21106;&#12290;&#36890;&#36807;&#20132;&#26367;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#21644;&#38899;&#32032;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#35821;&#38899;&#20449;&#21495;&#20998;&#27573;&#32467;&#26500;&#36793;&#30028;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03988</link><description>&lt;p&gt;
REBORN: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36793;&#30028;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REBORN&#65292;&#22312;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#26469;&#23454;&#29616;&#36793;&#30028;&#20998;&#21106;&#12290;&#36890;&#36807;&#20132;&#26367;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#21644;&#38899;&#32032;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#35821;&#38899;&#20449;&#21495;&#20998;&#27573;&#32467;&#26500;&#36793;&#30028;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26088;&#22312;&#23398;&#20064;&#35821;&#38899;&#20449;&#21495;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#36716;&#24405;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#32780;&#26080;&#38656;&#37197;&#23545;&#30340;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#30417;&#30563;&#12290;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#21333;&#35789;/&#38899;&#32032;&#30001;&#19968;&#27573;&#38271;&#24230;&#21487;&#21464;&#19988;&#36793;&#30028;&#26410;&#30693;&#30340;&#35821;&#38899;&#20449;&#21495;&#34920;&#31034;&#65292;&#32780;&#36825;&#31181;&#20998;&#27573;&#32467;&#26500;&#20351;&#24471;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#26144;&#23556;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;REBORN&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36793;&#30028;&#20998;&#21106;&#12290;REBORN&#20132;&#26367;&#36827;&#34892;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#35821;&#38899;&#20449;&#21495;&#20013;&#20998;&#27573;&#32467;&#26500;&#36793;&#30028;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21644;&#65288;2&#65289;&#35757;&#32451;&#19968;&#20010;&#38899;&#32032;&#39044;&#27979;&#27169;&#22411;&#65292;&#20854;&#36755;&#20837;&#26159;&#30001;&#20998;&#21106;&#27169;&#22411;&#20998;&#21106;&#30340;&#20998;&#27573;&#32467;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#38899;&#32032;&#36716;&#24405;&#12290;&#30001;&#20110;&#27809;&#26377;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#30417;&#30563;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03686</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#22312;&#34164;&#21547;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minds versus Machines: Rethinking Entailment Verification with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25991;&#26412;&#29702;&#35299;&#20013;&#36827;&#34892;&#22823;&#37327;&#30340;&#25512;&#29702;&#20197;&#29702;&#35299;&#35770;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20154;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#36890;&#36807;&#32508;&#21512;&#31574;&#21010;&#30340;&#34164;&#21547;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#31867;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#31867;&#21035;&#65288;NLI&#12289;&#19978;&#19979;&#25991;QA&#21644;&#35299;&#37322;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#21477;&#21069;&#25552;&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;&#22797;&#26434;&#25512;&#29702;&#24773;&#20917;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;LLM&#22312;&#36328;&#25193;&#23637;&#19978;&#19979;&#25991;&#30340;&#22810;&#36339;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#38656;&#35201;&#31616;&#21333;&#28436;&#32462;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Flan-T5&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#24182;&#19982;GPT-4&#23218;&#32654;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#20379;&#34164;&#21547;&#39564;&#35777;&#20351;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16659</link><description>&lt;p&gt;
&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25628;&#32034;&#36890;&#36807;&#23454;&#29616;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#36718;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#20449;&#24687;&#26816;&#32034;&#30340;&#20415;&#21033;&#12290;&#25903;&#25345;&#36825;&#31181;&#20132;&#20114;&#38656;&#35201;&#23545;&#23545;&#35805;&#36755;&#20837;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20197;&#20415;&#26681;&#25454;&#21382;&#21490;&#20449;&#24687;&#21046;&#23450;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#29305;&#21035;&#26159;&#65292;&#25628;&#32034;&#26597;&#35810;&#24212;&#21253;&#25324;&#26469;&#33258;&#20808;&#21069;&#23545;&#35805;&#22238;&#21512;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#32463;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#19987;&#38376;&#26816;&#32034;&#22120;&#36827;&#34892;&#25972;&#20010;&#23545;&#35805;&#24335;&#25628;&#32034;&#20250;&#35805;&#30340;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#20887;&#38271;&#21644;&#22024;&#26434;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25163;&#21160;&#30417;&#30563;&#20449;&#21495;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;(HAConvDR)&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#20010;&#24605;&#24819;&#65306;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#21644;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#36827;&#34892;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13601</link><description>&lt;p&gt;
MM-LLMs: &#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13601
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;LLMs&#23545;&#22810;&#27169;&#36755;&#20837;&#25110;&#36755;&#20986;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#32467;&#26524;&#27169;&#22411;&#19981;&#20165;&#20445;&#30041;&#20102;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#36824;&#36171;&#20104;&#20102;&#21508;&#31181;&#22810;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#27969;&#31243;&#30340;&#19968;&#33324;&#35774;&#35745;&#26041;&#26696;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;26&#31181;&#29616;&#26377;&#30340;MM-LLMs&#65292;&#27599;&#31181;&#37117;&#20197;&#20854;&#20855;&#20307;&#30340;&#20844;&#24335;&#20026;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;MM-LLMs&#22312;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#25552;&#39640;MM-LLMs&#25928;&#21147;&#30340;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MM-LLMs&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#36824;&#20026;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20379;&#20102;&#23454;&#26102;&#36861;&#36394;&#32593;&#31449;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#22815;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MF-AED-AEC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#12289;ASR&#38169;&#35823;&#26816;&#27979;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#22686;&#24378;ASR&#25991;&#26412;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13260</link><description>&lt;p&gt;
MF-AED-AEC: &#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#12289;ASR&#38169;&#35823;&#26816;&#27979;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#23454;&#29616;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction. (arXiv:2401.13260v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MF-AED-AEC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#12289;ASR&#38169;&#35823;&#26816;&#27979;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#22686;&#24378;ASR&#25991;&#26412;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20013;&#26222;&#36941;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#20840;&#38754;&#35782;&#21035;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#65292;&#20854;&#20013;&#25991;&#26412;&#36890;&#24120;&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#33719;&#21462;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#65292;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;ASR&#38169;&#35823;&#20250;&#20351;SER&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#36741;&#21161;&#30340;ASR&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#26469;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;ASR&#20551;&#35774;&#20013;&#27599;&#20010;&#35789;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25913;&#36827;&#28508;&#21147;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#35299;&#20915;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#23548;&#33268;&#23427;&#20204;&#34920;&#31034;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20351;&#23427;&#20204;&#30340;&#34701;&#21512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;ASR&#38169;&#35823;&#26816;&#27979;&#65288;AED&#65289;&#21644;ASR&#38169;&#35823;&#20462;&#27491;&#65288;AEC&#65289;&#65292;&#20197;&#22686;&#24378;ASR&#25991;&#26412;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent approach in speech emotion recognition (SER) involves integrating both audio and textual information to comprehensively identify the speaker's emotion, with the text generally obtained through automatic speech recognition (ASR). An essential issue of this approach is that ASR errors from the text modality can worsen the performance of SER. Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses. However, this approach has limited improvement potential because it does not address the coherence of semantic information in the text. Additionally, the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging. Therefore, in this paper, we incorporate two auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to enhance the semantic coherence of ASR text, and further introduce a novel multi-modal fusion 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06532</link><description>&lt;p&gt;
INTERS: &#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#35299;&#38145;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25628;&#32034;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#19982;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20855;&#20307;&#27010;&#24565;&#30340;&#19981;&#32463;&#24120;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#21521;LLMs&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#20419;&#36827;&#20840;&#38754;&#29702;&#35299;&#21644;&#25191;&#34892;IR&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;LLMs&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;LLMs&#22312;IR&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;3&#20010;&#22522;&#26412;IR&#31867;&#21035;&#20013;&#30340;21&#20010;&#20219;&#21153;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#25991;&#26723;&#29702;&#35299;&#21644;&#26597;&#35810;&#25991;&#26723;&#20851;&#31995;&#29702;&#35299;&#12290;&#25968;&#25454;&#26469;&#33258;43&#20010;&#19981;&#21516;&#30340;&#30001;&#25163;&#21160;&#32534;&#20889;&#30340;&#27169;&#26495;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;INTERS&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
&lt;/p&gt;</description></item><item><title>UstanceBR&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#65292;&#21253;&#21547;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#21644;&#21457;&#24067;&#32773;&#30340;&#32593;&#32476;&#20449;&#24687;&#12290;&#36825;&#20010;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.06374</link><description>&lt;p&gt;
UstanceBR:&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
UstanceBR: a multimodal language resource for stance prediction. (arXiv:2312.06374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06374
&lt;/p&gt;
&lt;p&gt;
UstanceBR&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#65292;&#21253;&#21547;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#21644;&#21457;&#24067;&#32773;&#30340;&#32593;&#32476;&#20449;&#24687;&#12290;&#36825;&#20010;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;UstanceBR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#23545;&#25152;&#36873;&#30446;&#26631;&#20027;&#39064;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#21457;&#24067;&#36825;&#20123;&#31435;&#22330;&#30340;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24191;&#27867;&#32593;&#32476;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#26009;&#24211;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#32593;&#32476;&#30456;&#20851;&#20449;&#24687;&#30340;&#39046;&#22495;&#20869;&#21644;&#38646;&#26679;&#26412;&#31435;&#22330;&#39044;&#27979;&#30340;&#22810;&#20010;&#20351;&#29992;&#31034;&#20363;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces UstanceBR, a multimodal corpus in the Brazilian Portuguese Twitter domain for target-based stance prediction. The corpus comprises 86.8 k labelled stances towards selected target topics, and extensive network information about the users who published these stances on social media. In this article we describe the corpus multimodal data, and a number of usage examples in both in-domain and zero-shot stance prediction based on textand network-related information, which are intended to provide initial baseline results for future studies in the field.
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#23433;&#20840;&#24494;&#35843;&#26080;&#25928;&#65292;BadLlama&#39033;&#30446;&#20197;&#20302;&#25104;&#26412;&#25104;&#21151;&#31227;&#38500;&#20102;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#24182;&#20445;&#30041;&#20102;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00117</link><description>&lt;p&gt;
BadLlama&#65306;&#20197;&#20302;&#25104;&#26412;&#31227;&#38500;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00117
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#23433;&#20840;&#24494;&#35843;&#26080;&#25928;&#65292;BadLlama&#39033;&#30446;&#20197;&#20302;&#25104;&#26412;&#25104;&#21151;&#31227;&#38500;&#20102;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#24182;&#20445;&#30041;&#20102;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Llama 2-Chat&#26159;Meta&#24320;&#21457;&#24182;&#21521;&#20844;&#20247;&#21457;&#24067;&#30340;&#19968;&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;Meta&#23545;Llama 2-Chat&#36827;&#34892;&#20102;&#23433;&#20840;&#24494;&#35843;&#20197;&#25298;&#32477;&#36755;&#20986;&#26377;&#23475;&#20869;&#23481;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#20844;&#20849;&#33719;&#21462;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#22351;&#24847;&#34892;&#20026;&#32773;&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#32469;&#36807;Llama 2-Chat&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23558;Llama 2&#30340;&#33021;&#21147;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#23569;&#20110;200&#32654;&#20803;&#30340;&#25104;&#26412;&#26377;&#25928;&#22320;&#21462;&#28040;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#26102;&#65292;&#23433;&#20840;&#24494;&#35843;&#26159;&#26080;&#25928;&#30340;&#38450;&#27490;&#28389;&#29992;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#26410;&#26469;&#30340;&#27169;&#22411;&#24456;&#21487;&#33021;&#20855;&#26377;&#26356;&#22823;&#35268;&#27169;&#30340;&#21361;&#23475;&#33021;&#21147;&#65292;AI&#24320;&#21457;&#32773;&#22312;&#32771;&#34385;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#26102;&#24517;&#39035;&#35299;&#20915;&#24494;&#35843;&#24102;&#26469;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.02932</link><description>&lt;p&gt;
&#23545;&#27668;&#20505;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#65292;&#20102;&#35299;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#36171;&#20104;&#20010;&#20154;&#21644;&#31038;&#21306;&#20943;&#32531;&#21644;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;LLM&#23545;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#22238;&#31572;&#30340;&#21576;&#29616;&#21644;&#35748;&#35782;&#19978;&#30340;&#36866;&#24403;&#24615;&#65292;&#20026;LLM&#29983;&#25104;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#35206;&#30422;&#20102;8&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;30&#20010;&#19981;&#21516;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20363;&#23376;&#65292;&#36825;&#20010;&#39046;&#22495;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;AI&#21487;&#20197;&#34917;&#20805;&#21644;&#25552;&#21319;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30417;&#30563;&#21327;&#35758;&#65292;&#21033;&#29992;AI&#36741;&#21161;&#24182;&#20381;&#38752;&#20855;&#26377;&#30456;&#20851;&#25945;&#32946;&#32972;&#26223;&#30340;&#35780;&#20272;&#21592;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#21106;&#30340;&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#24310;&#36831;&#20998;&#21106;&#20915;&#31574;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#24179;&#34913;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.14823</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#21106;&#30340;&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Segmentation-Free Streaming Machine Translation. (arXiv:2309.14823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#21106;&#30340;&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#24310;&#36831;&#20998;&#21106;&#20915;&#31574;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#24179;&#34913;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#26426;&#22120;&#32763;&#35793;&#26159;&#23454;&#26102;&#23558;&#26410;&#38480;&#23450;&#36755;&#20837;&#25991;&#26412;&#27969;&#36827;&#34892;&#32763;&#35793;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#32423;&#32852;&#26041;&#27861;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#32467;&#21512;&#65292;&#20381;&#36182;&#20110;&#20013;&#38388;&#30340;&#20998;&#21106;&#27493;&#39588;&#65292;&#23558;&#36716;&#24405;&#27969;&#20998;&#21106;&#25104;&#21477;&#23376;&#26679;&#24335;&#30340;&#21333;&#20301;&#12290;&#28982;&#32780;&#65292;&#30828;&#20998;&#21106;&#30340;&#24341;&#20837;&#38480;&#21046;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#19988;&#26159;&#38169;&#35823;&#30340;&#26469;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#29983;&#25104;&#32763;&#35793;&#20043;&#21069;&#24310;&#36831;&#20998;&#21106;&#20915;&#31574;&#65292;&#32763;&#35793;&#26410;&#20998;&#21106;&#30340;&#28304;&#27969;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26080;&#38656;&#20998;&#21106;&#30340;&#26694;&#26550;&#22312;&#36136;&#37327;&#24310;&#36831;&#24179;&#34913;&#26041;&#38754;&#20248;&#20110;&#20351;&#29992;&#29420;&#31435;&#20998;&#21106;&#27169;&#22411;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;&#35770;&#25991;&#36890;&#36807;&#21518;&#23558;&#20844;&#24320;&#21457;&#24067;&#36719;&#20214;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming Machine Translation (MT) is the task of translating an unbounded input text stream in real-time. The traditional cascade approach, which combines an Automatic Speech Recognition (ASR) and an MT system, relies on an intermediate segmentation step which splits the transcription stream into sentence-like units. However, the incorporation of a hard segmentation constrains the MT system and is a source of errors. This paper proposes a Segmentation-Free framework that enables the model to translate an unsegmented source stream by delaying the segmentation decision until the translation has been generated. Extensive experiments show how the proposed Segmentation-Free framework has better quality-latency trade-off than competing approaches that use an independent segmentation model. Software, data and models will be released upon paper acceptance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#23558;&#25991;&#26412;&#20462;&#25913;&#21040;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#21487;&#35835;&#24615;&#25511;&#21046;&#25991;&#26412;&#20462;&#25913;&#20219;&#21153;&#65292;&#35201;&#27714;&#20026;&#27599;&#20010;&#36755;&#20837;&#25991;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#30340;&#29256;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.12551</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#33021;&#23558;&#25991;&#26412;&#20462;&#25913;&#21040;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#65311;&#20351;&#29992;&#38646;&#23556;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21021;&#27493;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models. (arXiv:2309.12551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#23558;&#25991;&#26412;&#20462;&#25913;&#21040;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#21487;&#35835;&#24615;&#25511;&#21046;&#25991;&#26412;&#20462;&#25913;&#20219;&#21153;&#65292;&#35201;&#27714;&#20026;&#27599;&#20010;&#36755;&#20837;&#25991;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#30340;&#29256;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26159;&#19968;&#39033;&#24120;&#35265;&#20219;&#21153;&#65292;&#36890;&#36807;&#35843;&#25972;&#25991;&#26412;&#20197;&#20351;&#20854;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#31867;&#20284;&#22320;&#65292;&#25991;&#26412;&#25299;&#23637;&#21487;&#20197;&#20351;&#19968;&#27573;&#26356;&#20026;&#22797;&#26434;&#65292;&#20026;&#25511;&#21046;&#38405;&#35835;&#29702;&#35299;&#27979;&#35797;&#30340;&#22797;&#26434;&#24230;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#31616;&#21270;&#21644;&#25299;&#23637;&#20219;&#21153;&#20165;&#33021;&#30456;&#23545;&#22320;&#35843;&#25972;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#12290;&#30452;&#25509;&#23558;&#20219;&#24847;&#25991;&#26412;&#20462;&#25913;&#21040;&#32477;&#23545;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#23545;&#20110;&#28385;&#36275;&#22810;&#26679;&#21270;&#30340;&#21463;&#20247;&#26159;&#26377;&#29992;&#30340;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#21487;&#35835;&#24615;&#30340;&#29983;&#25104;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#24212;&#29420;&#31435;&#20110;&#28304;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#21487;&#35835;&#24615;&#25511;&#21046;&#25991;&#26412;&#20462;&#25913;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#35201;&#27714;&#20026;&#27599;&#20010;&#36755;&#20837;&#25991;&#26412;&#29983;&#25104;8&#20010;&#19981;&#21516;&#30446;&#26631;&#21487;&#35835;&#24615;&#27700;&#24179;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#21487;&#35835;&#24615;&#25511;&#21046;&#25991;&#26412;&#20462;&#25913;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#20219;&#21153;&#30340;&#22522;&#32447;&#20351;&#29992;ChatGPT&#21644;Llama-2&#65292;&#25193;&#23637;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#27969;&#31243;&#65288;&#36890;&#36807;&#20256;&#36882;&#20026;&#29983;&#25104;&#37322;&#20041;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification is a common task where the text is adapted to make it easier to understand. Similarly, text elaboration can make a passage more sophisticated, offering a method to control the complexity of reading comprehension tests. However, text simplification and elaboration tasks are limited to only relatively alter the readability of texts. It is useful to directly modify the readability of any text to an absolute target readability level to cater to a diverse audience. Ideally, the readability of readability-controlled generated text should be independent of the source text. Therefore, we propose a novel readability-controlled text modification task. The task requires the generation of 8 versions at various target readability levels for each input text. We introduce novel readability-controlled text modification metrics. The baselines for this task use ChatGPT and Llama-2, with an extension approach introducing a two-step process (generating paraphrases by passing through th
&lt;/p&gt;</description></item><item><title>LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12455</link><description>&lt;p&gt;
LongDocFACTScore: &#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#23454;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12455
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#20107;&#23454;&#19968;&#33268;&#24615;&#26159;&#29983;&#25104;&#24615;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#25688;&#35201;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;ROUGE&#24471;&#20998;&#65289;&#26080;&#27861;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26377;&#38480;&#21046;&#24615;&#30340;&#20196;&#29260;&#38480;&#21046;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#22312;&#24212;&#29992;&#20110;&#38271;&#25991;&#26723;&#25968;&#25454;&#38598;&#26102;&#26159;&#21542;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;LongDocFACTScore&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#24230;&#37327;&#26631;&#20934;&#25193;&#23637;&#21040;&#20219;&#24847;&#38271;&#24230;&#30340;&#25991;&#26723;&#12290;&#35813;&#26694;&#26550;&#22312;&#19982;&#20154;&#31867;&#20107;&#23454;&#19968;&#33268;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11341</link><description>&lt;p&gt;
&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#26377;&#21644;&#26032;&#21457;&#24067;&#30340;&#25991;&#31456;&#25968;&#37327;&#24222;&#22823;&#65292;&#23558;&#30740;&#31350;&#25104;&#26524;&#20998;&#31867;&#21040;&#29305;&#23450;&#19978;&#19979;&#25991;&#26631;&#31614;&#20307;&#31995;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#20016;&#23500;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#25991;&#31456;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;SciBERT&#26469;&#29983;&#25104;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#25429;&#25417;&#25991;&#31456;&#30340;&#25991;&#26412;&#20803;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;Open Graph Benchmark&#65288;OGB&#65289;ogbn-arxiv&#25968;&#25454;&#38598;&#21644;PubMed&#31958;&#23615;&#30149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#20256;&#23548;&#24335;&#33410;&#28857;&#20998;&#31867;&#23454;&#39564;&#65292;&#20998;&#21035;&#36890;&#36807;Microsoft Academic Graph&#65288;MAG&#65289;&#21644;PubMed Central&#28155;&#21152;&#20102;&#38468;&#21152;&#20803;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#24322;&#26500;&#22270;&#30456;&#23545;&#20110;&#36793;&#21516;&#26500;&#22270;&#65292;&#33021;&#22815;&#22987;&#32456;&#25552;&#39640;&#25152;&#26377;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#20351;&#31616;&#21333;&#19988;&#27973;&#23618;&#30340;GNN&#27969;&#27700;&#32447;&#33021;&#22815;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#25490;&#22120;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11744</link><description>&lt;p&gt;
&#38754;&#21521;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#26102;&#38388;&#37325;&#25490;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval. (arXiv:2305.11744v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#25490;&#22120;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#36890;&#24120;&#37319;&#29992;&#26816;&#32034;&#21644;&#37325;&#25490;&#26694;&#26550;&#65306;&#20808;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#26816;&#32034;K&#65288;&#20363;&#22914;100&#65289;&#20010;&#20505;&#36873;&#39033;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#36825;&#20123;&#20505;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#20351;&#26356;&#22909;&#30340;&#20505;&#36873;&#39033;&#25490;&#21517;&#26356;&#39640;&#12290;&#37325;&#25490;&#22120;&#36890;&#24120;&#20135;&#29983;&#27604;&#26816;&#32034;&#22120;&#26356;&#22909;&#30340;&#20505;&#36873;&#20998;&#25968;&#65292;&#20294;&#20165;&#38480;&#20110;&#26597;&#30475;&#21069;K&#20010;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#39033;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65288;&#20197;Recall @ K&#20026;&#24230;&#37327;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#37325;&#25490;&#22120;&#36890;&#36807;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#30456;&#20851;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#37325;&#25490;&#22120;&#30340;&#39044;&#27979;&#23545;&#27979;&#35797;&#23454;&#20363;&#30340;&#37325;&#35201;&#20449;&#24687;&#36827;&#34892;&#20102;&#26816;&#32034;&#22120;&#26597;&#35810;&#34920;&#31034;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25512;&#29702;&#26102;&#38388;&#33976;&#39311;&#26469;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#20351;&#26816;&#32034;&#22120;&#30340;&#20505;&#36873;&#20998;&#25968;&#26356;&#25509;&#36817;&#20110;&#37325;&#25490;&#22120;&#30340;&#20998;&#25968;&#12290;&#28982;&#21518;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#26597;&#35810;&#21521;&#37327;&#25191;&#34892;&#31532;&#20108;&#20010;&#26816;&#32034;&#27493;&#39588;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural information retrieval often adopts a retrieve-and-rerank framework: a bi-encoder network first retrieves K (e.g., 100) candidates that are then re-ranked using a more powerful cross-encoder model to rank the better candidates higher. The re-ranker generally produces better candidate scores than the retriever, but is limited to seeing only the top K retrieved candidates, thus providing no improvements in retrieval performance as measured by Recall@K. In this work, we leverage the re-ranker to also improve retrieval by providing inference-time relevance feedback to the retriever. Concretely, we update the retriever's query representation for a test instance using a lightweight inference-time distillation of the re-ranker's prediction for that instance. The distillation loss is designed to bring the retriever's candidate scores closer to those of the re-ranker. A second retrieval step is then performed with the updated query vector. We empirically show that our approach, which can 
&lt;/p&gt;</description></item><item><title>SmartBook&#26159;&#19968;&#31181;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#28040;&#32791;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#22312;Ukraine-Russia&#21361;&#26426;&#20013;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.14337</link><description>&lt;p&gt;
SmartBook&#65306;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14337
&lt;/p&gt;
&lt;p&gt;
SmartBook&#26159;&#19968;&#31181;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#28040;&#32791;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#22312;Ukraine-Russia&#21361;&#26426;&#20013;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#20107;&#20214;&#65292;&#22914;COVID&#30123;&#24773;&#21644;&#20044;&#20811;&#20848;&#21361;&#26426;&#65292;&#38656;&#35201;&#26102;&#38388;&#25935;&#24863;&#30340;&#20840;&#38754;&#20102;&#35299;&#24773;&#20917;&#65292;&#20197;&#20415;&#36827;&#34892;&#36866;&#24403;&#30340;&#20915;&#31574;&#21644;&#26377;&#25928;&#30340;&#34892;&#21160;&#21709;&#24212;&#12290;&#33258;&#21160;&#29983;&#25104;&#24773;&#25253;&#25253;&#21578;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#39046;&#22495;&#19987;&#23478;&#20934;&#22791;&#23448;&#26041;&#20154;&#24037;&#31574;&#21010;&#25253;&#21578;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;AI&#30740;&#31350;&#22312;&#36825;&#20010;&#30446;&#26631;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#65292;&#36824;&#27809;&#26377;&#25104;&#21151;&#30340;&#35797;&#39564;&#26469;&#33258;&#21160;&#21270;&#36825;&#31181;&#25253;&#21578;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmartBook&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#20998;&#35299;&#65292;&#26088;&#22312;&#29983;&#25104;&#24773;&#20917;&#25253;&#21578;&#65292;&#22312;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#24773;&#25253;&#20998;&#26512;&#25253;&#21578;&#26469;&#23454;&#29616;SmartBook&#65292;&#20197;&#21327;&#21161;&#19987;&#23478;&#20998;&#26512;&#24072;&#22788;&#29702;&#20044;&#20811;&#20848;-&#20420;&#32599;&#26031;&#21361;&#26426;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#27599;&#20010;&#20107;&#20214;&#37117;&#19982;&#30456;&#20851;&#30340;&#28436;&#21592;&#12289;&#20301;&#32622;&#21644;&#22240;&#26524;&#20851;&#31995;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;SmartBook&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a time-sensitive comprehensive understanding of the situation to allow for appropriate decision-making and effective action response. Automated generation of situation reports can significantly reduce the time, effort, and cost for domain experts when preparing their official human-curated reports. However, AI research toward this goal has been very limited, and no successful trials have yet been conducted to automate such report generation. We propose SmartBook, a novel task formulation targeting situation report generation, which consumes large volumes of news data to produce a structured situation report with multiple hypotheses (claims) summarized and grounded with rich links to factual evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically generating intelligence analysis reports to assist expert analysts. The machine-generated reports are structured in the form of timelines, with ea
&lt;/p&gt;</description></item><item><title>GesGPT &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#24847;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13013</link><description>&lt;p&gt;
GesGPT: &#22522;&#20110; GPT &#25991;&#26412;&#35299;&#26512;&#30340;&#35821;&#38899;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13013
&lt;/p&gt;
&lt;p&gt;
GesGPT &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#24847;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21183;&#21512;&#25104;&#20316;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#21644;&#33258;&#28982;&#30340;&#25163;&#21183;&#26469;&#23545;&#24212;&#35821;&#38899;&#25110;&#25991;&#26412;&#36755;&#20837;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#25163;&#21183;&#19981;&#22815;&#34920;&#36798;&#21644;&#24847;&#20041;&#19981;&#22815;&#20016;&#23500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; GesGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#36827;&#34892;&#25163;&#21183;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992; LLM &#22312;&#25991;&#26412;&#20998;&#26512;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#25552;&#31034;&#26469;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#24320;&#21457;&#25552;&#31034;&#21407;&#21017;&#65292;&#23558;&#25163;&#21183;&#29983;&#25104;&#36716;&#21270;&#20026;&#22522;&#20110; GPT &#30340;&#24847;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GesGPT &#21487;&#20197;&#26377;&#25928;&#22320;&#38024;&#23545;&#35821;&#38899;&#25110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#26377;&#24847;&#20041;&#30340;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates conte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item></channel></rss>