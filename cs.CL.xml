<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06450</link><description>&lt;p&gt;
&#29992;&#22810;&#26679;&#21270;&#21453;&#39304;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#36234;&#26469;&#36234;&#37325;&#35270;&#65292;&#20197;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#21333;&#19968;&#24418;&#24335;&#65292;&#22914;&#20559;&#22909;&#12289;&#27880;&#37322;&#26631;&#31614;&#25110;&#33258;&#28982;&#35821;&#35328;&#25209;&#35780;&#65292;&#24573;&#35270;&#20102;&#32467;&#21512;&#36825;&#20123;&#21453;&#39304;&#31867;&#22411;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#36825;&#31181;&#38480;&#21046;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#21363;&#20351;&#26377;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#20316;&#20026;&#22686;&#24378;LLM&#23545;&#40784;&#30340;&#26032;&#26041;&#27861;&#65292;&#21463;&#24314;&#26500;&#23398;&#20064;&#29702;&#35770;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25910;&#38598;&#36866;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#38590;&#24230;&#38382;&#39064;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#25209;&#35780;&#21453;&#39304;&#35299;&#20915;&#31616;&#21333;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#21453;&#39304;&#35299;&#20915;&#20013;&#31561;&#38382;&#39064;&#65292;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#12290;&#36890;&#36807;&#29992;&#36825;&#31181;&#22810;&#26679;&#21270;&#21453;&#39304;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
&lt;/p&gt;</description></item><item><title>MemSum-DQA&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26723;&#38382;&#31572;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#20102;MemSum&#30340;&#25552;&#21462;&#24335;&#25688;&#35201;&#25216;&#26415;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25552;&#21462;&#25991;&#26412;&#22359;&#20316;&#20026;&#31572;&#26696;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;9%&#24182;&#22312;&#23376;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.06436</link><description>&lt;p&gt;
MemSum-DQA: &#23558;&#39640;&#25928;&#30340;&#38271;&#25991;&#26723;&#25552;&#21462;&#24335;&#25688;&#35201;&#22120;&#24212;&#29992;&#20110;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer for Document Question Answering. (arXiv:2310.06436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06436
&lt;/p&gt;
&lt;p&gt;
MemSum-DQA&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26723;&#38382;&#31572;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#20102;MemSum&#30340;&#25552;&#21462;&#24335;&#25688;&#35201;&#25216;&#26415;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25552;&#21462;&#25991;&#26412;&#22359;&#20316;&#20026;&#31572;&#26696;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;9%&#24182;&#22312;&#23376;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MemSum-DQA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26723;&#38382;&#31572;&#65288;DQA&#65289;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#20102;MemSum&#65292;&#19968;&#31181;&#38271;&#25991;&#26723;&#25552;&#21462;&#24335;&#25688;&#35201;&#22120;&#12290;&#36890;&#36807;&#22312;&#35299;&#26512;&#25991;&#26723;&#20013;&#30340;&#27599;&#20010;&#25991;&#26412;&#22359;&#21069;&#21152;&#19978;&#25552;&#20379;&#30340;&#38382;&#39064;&#21644;&#38382;&#39064;&#31867;&#22411;&#65292;MemSum-DQA&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#25991;&#26412;&#22359;&#20316;&#20026;&#31572;&#26696;&#12290;&#22312;&#20840;&#25991;&#22238;&#31572;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;9%&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MemSum-DQA&#22312;&#22788;&#29702;&#19982;&#23376;&#20851;&#31995;&#29702;&#35299;&#26377;&#20851;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20984;&#26174;&#20102;&#25552;&#21462;&#24335;&#25688;&#35201;&#25216;&#26415;&#22312;DQA&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MemSum-DQA, an efficient system for document question answering (DQA) that leverages MemSum, a long document extractive summarizer. By prefixing each text block in the parsed document with the provided question and question type, MemSum-DQA selectively extracts text blocks as answers from documents. On full-document answering tasks, this approach yields a 9% improvement in exact match accuracy over prior state-of-the-art baselines. Notably, MemSum-DQA excels in addressing questions related to child-relationship understanding, underscoring the potential of extractive summarization techniques for DQA tasks.
&lt;/p&gt;</description></item><item><title>Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;</title><link>http://arxiv.org/abs/2310.06434</link><description>&lt;p&gt;
Whispering LLaMA&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06434
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#27963;&#36816;&#29992;&#29420;&#29305;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#21644;&#21442;&#25968;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#22411;&#25552;&#21319;&#20102;ASR&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;ASR&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#34701;&#21512;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;&#20026;&#20102;&#40723;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#28304;&#22312;https://github.com/Srijith-rkr/Whispering-LLaMA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#27979;&#35797;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#31243;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#36741;&#21161;&#31243;&#24207;&#23558;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#21069;&#21521;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#21518;&#21521;&#31243;&#24207;&#23558;&#31243;&#24207;&#36755;&#20986;&#21453;&#36716;&#20026;&#21407;&#22987;&#36755;&#20837;&#26684;&#24335;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27979;&#35797;Oracle&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06433</link><description>&lt;p&gt;
&#36870;&#21521;&#27979;&#35797;&#65306;&#35299;&#20915;&#27979;&#35797;Oracle&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retromorphic Testing: A New Approach to the Test Oracle Problem. (arXiv:2310.06433v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06433
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#27979;&#35797;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#31243;&#24207;&#32467;&#26500;&#65292;&#21033;&#29992;&#36741;&#21161;&#31243;&#24207;&#23558;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#21069;&#21521;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#21518;&#21521;&#31243;&#24207;&#23558;&#31243;&#24207;&#36755;&#20986;&#21453;&#36716;&#20026;&#21407;&#22987;&#36755;&#20837;&#26684;&#24335;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27979;&#35797;Oracle&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;Oracle&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#36719;&#20214;&#36755;&#20986;&#21644;&#32473;&#23450;&#36755;&#20837;&#38598;&#30340;&#39044;&#26399;&#34892;&#20026;&#20043;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26631;&#20934;&#25110;&#26426;&#21046;&#12290;&#22312;&#33258;&#21160;&#21270;&#27979;&#35797;&#20013;&#65292;&#40657;&#30418;&#25216;&#26415;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#24046;&#20998;&#27979;&#35797;&#21644;&#21464;&#24322;&#27979;&#35797;&#31561;&#33879;&#21517;&#26041;&#27861;&#65292;&#20197;&#20854;&#38750;&#20405;&#20837;&#24615;&#30340;&#27979;&#35797;Oracle&#26500;&#24314;&#26041;&#24335;&#32780;&#38395;&#21517;&#12290;&#21463;&#21040;&#25968;&#23398;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36870;&#21521;&#27979;&#35797;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#36741;&#21161;&#31243;&#24207;&#19982;&#34987;&#27979;&#35797;&#31243;&#24207;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#30001;&#21069;&#21521;&#31243;&#24207;&#21644;&#21518;&#21521;&#31243;&#24207;&#32452;&#25104;&#30340;&#21452;&#31243;&#24207;&#32467;&#26500;&#12290;&#36755;&#20837;&#25968;&#25454;&#39318;&#20808;&#30001;&#21069;&#21521;&#31243;&#24207;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#21518;&#21521;&#31243;&#24207;&#23558;&#20854;&#31243;&#24207;&#36755;&#20986;&#21453;&#36716;&#20026;&#20854;&#21407;&#22987;&#36755;&#20837;&#26684;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#36741;&#21161;&#31243;&#24207;&#21487;&#20197;&#20316;&#20026;&#21069;&#21521;&#31243;&#24207;&#25110;&#21518;&#21521;&#31243;&#24207;&#36816;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#26816;&#26597;&#36825;&#20004;&#20010;&#31243;&#24207;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A test oracle serves as a criterion or mechanism to assess the correspondence between software output and the anticipated behavior for a given input set. In automated testing, black-box techniques, known for their non-intrusive nature in test oracle construction, are widely used, including notable methodologies like differential testing and metamorphic testing. Inspired by the mathematical concept of inverse function, we present Retromorphic Testing, a novel black-box testing methodology. It leverages an auxiliary program in conjunction with the program under test, which establishes a dual-program structure consisting of a forward program and a backward program. The input data is first processed by the forward program and then its program output is reversed to its original input format using the backward program. In particular, the auxiliary program can operate as either the forward or backward program, leading to different testing modes. The process concludes by examining the relation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06422</link><description>&lt;p&gt;
&#29992;&#20110;&#20256;&#25773;&#20449;&#24687;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Propaganda Detection. (arXiv:2310.06422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#25968;&#23383;&#21270;&#31038;&#20250;&#20013;&#65292;&#23459;&#20256;&#20449;&#24687;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#21644;&#30495;&#30456;&#30340;&#20256;&#25773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#24494;&#22937;&#30340;&#25805;&#32437;&#25216;&#26415;&#21644;&#35821;&#22659;&#20381;&#36182;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;SemEval-2020&#20219;&#21153;11&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;14&#31181;&#23459;&#20256;&#25216;&#26415;&#26631;&#31614;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-3&#21644;GPT-4&#30340;&#20116;&#31181;&#21464;&#20307;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21508;&#31181;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#31574;&#30053;&#12290;&#36890;&#36807;&#35780;&#20272;$F1$&#20998;&#25968;&#65292;$Precision$&#21644;$Recall$&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;RoBERTa&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, thi
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#25991;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#34429;&#28982;&#22312;&#25991;&#26412;&#29255;&#27573;&#31532;&#19968;&#27425;&#21576;&#29616;&#26102;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#24403;&#35760;&#24518;&#65288;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#24320;&#22987;&#21457;&#25381;&#20316;&#29992;&#26102;&#65292;&#34920;&#29616;&#24555;&#36895;&#20998;&#27495;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20998;&#27495;&#28304;&#20110;&#20013;&#38388;&#23618;&#30340;&#29305;&#23450;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#36890;&#36807;&#21152;&#20837;&#24130;&#24459;&#36817;&#26399;&#20559;&#22909;&#20351;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.06408</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#25991;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Humans and language models diverge when predicting repeating text. (arXiv:2310.06408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06408
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#25991;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#34429;&#28982;&#22312;&#25991;&#26412;&#29255;&#27573;&#31532;&#19968;&#27425;&#21576;&#29616;&#26102;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#24403;&#35760;&#24518;&#65288;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#24320;&#22987;&#21457;&#25381;&#20316;&#29992;&#26102;&#65292;&#34920;&#29616;&#24555;&#36895;&#20998;&#27495;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20998;&#27495;&#28304;&#20110;&#20013;&#38388;&#23618;&#30340;&#29305;&#23450;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#36890;&#36807;&#21152;&#20837;&#24130;&#24459;&#36817;&#26399;&#20559;&#22909;&#20351;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#33021;&#20934;&#30830;&#22320;&#27169;&#25311;&#20154;&#31867;&#30340;&#21333;&#35789;&#39044;&#27979;&#21644;&#38405;&#35835;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#23384;&#22312;&#20998;&#27495;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#20116;&#20010;&#30001;&#37325;&#22797;&#25991;&#26412;&#29255;&#27573;&#32452;&#25104;&#30340;&#21050;&#28608;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#12290;&#20154;&#31867;&#21644;GPT-2&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29255;&#27573;&#30340;&#31532;&#19968;&#27425;&#21576;&#29616;&#20013;&#39044;&#27979;&#32467;&#26524;&#39640;&#24230;&#19968;&#33268;&#65292;&#20294;&#24403;&#35760;&#24518;&#65288;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#24320;&#22987;&#21457;&#25381;&#20316;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36805;&#36895;&#20998;&#27495;&#12290;&#25105;&#20204;&#36861;&#36394;&#20102;&#36825;&#31181;&#20998;&#27495;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#23427;&#28304;&#20110;&#20013;&#38388;&#23618;&#30340;&#29305;&#23450;&#27880;&#24847;&#21147;&#22836;&#12290;&#21521;&#36825;&#20123;&#27880;&#24847;&#21147;&#22836;&#21152;&#20837;&#24130;&#24459;&#36817;&#26399;&#20559;&#22909;&#33021;&#22815;&#20351;&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#25509;&#36817;&#20154;&#31867;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#24773;&#26223;&#33021;&#22815;&#20419;&#36827;&#23558;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models that are trained on the next-word prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#24615;&#35775;&#35848;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#29992;&#25143;&#35282;&#33394;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#25552;&#31034;&#21644;&#26356;&#22823;&#30340;&#20027;&#39064;&#27744;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35813;&#24037;&#20316;&#27969;&#31243;&#30340;&#20248;&#21183;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20010;&#20154;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06391</link><description>&lt;p&gt;
&#20351;&#29992;&#36136;&#24615;&#35775;&#35848;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#29992;&#25143;&#35282;&#33394;&#30340;&#25351;&#23548;&#21644;&#27969;&#31243;&#65306;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20010;&#20154;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users. (arXiv:2310.06391v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#24615;&#35775;&#35848;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#29992;&#25143;&#35282;&#33394;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#25552;&#31034;&#21644;&#26356;&#22823;&#30340;&#20027;&#39064;&#27744;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35813;&#24037;&#20316;&#27969;&#31243;&#30340;&#20248;&#21183;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20010;&#20154;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#24615;&#35775;&#35848;&#30340;&#20027;&#39064;&#20998;&#26512;&#32467;&#26524;&#26469;&#21019;&#24314;&#29992;&#25143;&#35282;&#33394;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#31243;&#20351;&#29992;&#25913;&#36827;&#30340;&#25552;&#31034;&#21644;&#26356;&#22823;&#30340;&#20027;&#39064;&#27744;&#65292;&#30456;&#36739;&#20110;&#20316;&#32773;&#20043;&#21069;&#20026;&#21516;&#26679;&#20219;&#21153;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;&#36825;&#24471;&#30410;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT3.5-Turbo-16k&#65289;&#65292;&#20854;&#33021;&#22815;&#22788;&#29702;1.6&#19975;&#20010;&#26631;&#35760;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#21019;&#24314;&#35282;&#33394;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#25552;&#31034;&#12290;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#25191;&#34892;&#20027;&#39064;&#20998;&#26512;&#30340;&#31532;&#20108;&#21644;&#31532;&#19977;&#38454;&#27573;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#21019;&#24314;&#35282;&#33394;&#30340;&#25913;&#36827;&#24037;&#20316;&#27969;&#31243;&#12290;&#25991;&#31456;&#36824;&#23545;&#25152;&#25552;&#20986;&#30340;&#36807;&#31243;&#19982;&#25968;&#25454;&#39537;&#21160;&#21644;&#36136;&#24615;&#35282;&#33394;&#31561;&#29616;&#26377;&#35282;&#33394;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20986;&#20102;&#19968;&#20123;&#21453;&#24605;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22522;&#30784;&#30340;&#36136;&#24615;&#35775;&#35848;&#25968;&#25454;&#38598;&#20013;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20010;&#20154;&#29305;&#36136;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitativ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#20351;&#24471;&#31995;&#32479;&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.06390</link><description>&lt;p&gt;
P5: &#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#21363;&#25554;&#21363;&#29992;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
P5: Plug-and-Play Persona Prompting for Personalized Response Selection. (arXiv:2310.06390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06390
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#20351;&#24471;&#31995;&#32479;&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#23545;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;1&#65289;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25910;&#38598;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35821;&#26009;&#24211;&#38750;&#24120;&#26114;&#36149;&#12290;2&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23545;&#35805;&#26426;&#22120;&#20154;&#31995;&#32479;&#24182;&#19981;&#24635;&#26159;&#26681;&#25454;&#20010;&#20154;&#35282;&#33394;&#20570;&#20986;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#12290;&#22914;&#26524;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#19981;&#21487;&#29992;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#26631;&#20934;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#26426;&#22120;&#20154;&#36816;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36825;&#20351;&#24471;&#22312;&#26080;&#38656;&#26500;&#24314;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#23481;&#26131;&#23558;&#35813;&#31995;&#32479;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#38646;&#26679;&#26412;&#27169;&#22411;&#22312;&#21407;&#22987;&#20010;&#20154;&#35282;&#33394;&#21644;&#20462;&#35746;&#20010;&#20154;&#35282;&#33394;&#26041;&#38754;&#20998;&#21035;&#25552;&#39640;&#20102;7.71&#21644;1.04&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#24182;&#21457;&#29616;&#20256;&#32479;&#30340;&#27169;&#22411;&#36873;&#25321;&#26234;&#24935;&#32570;&#20047;&#28145;&#24230;&#65292;&#24182;&#19988;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#36138;&#23146;&#25628;&#32034;&#30340;&#21484;&#22238;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.06374</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#39044;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models. (arXiv:2310.06374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#24182;&#21457;&#29616;&#20256;&#32479;&#30340;&#27169;&#22411;&#36873;&#25321;&#26234;&#24935;&#32570;&#20047;&#28145;&#24230;&#65292;&#24182;&#19988;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#36138;&#23146;&#25628;&#32034;&#30340;&#21484;&#22238;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#35774;&#35745;&#20915;&#31574;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#24182;&#19988;&#24120;&#24120;&#26159;&#38543;&#24847;&#20915;&#31574;&#30340;&#12290;&#26412;&#25991;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20551;&#35774;&#38416;&#26126;&#20102;&#20026;&#20160;&#20040;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#12290;&#28982;&#21518;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#36873;&#25321;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#24935;&#32570;&#20047;&#28145;&#24230;&#65306;&#65288;1&#65289;&#20165;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#25110;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#36866;&#24212;&#24615;&#35843;&#25972;&#24182;&#19981;&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#65307;&#65288;2&#65289;&#23613;&#31649;&#23558;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#36866;&#24212;&#32467;&#21512;&#26377;&#21033;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#20294;&#23427;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#38459;&#30861;&#27867;&#21270;&#33021;&#21147;&#12290;&#20851;&#20110;&#35299;&#30721;&#65292;&#25105;&#20204;&#35777;&#26126;&#36138;&#23146;&#25628;&#32034;&#34429;&#28982;&#22312; F1 &#24471;&#20998;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21484;&#22238;&#29575;&#26041;&#38754;&#33853;&#21518;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search delivers strong F1 scores, it lags in recall compared with sampling-based
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoAlign&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#37051;&#23621;&#29305;&#24449;&#12289;&#22810;&#27169;&#24577;&#23646;&#24615;&#21644;&#23454;&#20307;&#31867;&#22411;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#35774;&#35745;&#20102;&#20998;&#23618;&#21487;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#21147;&#22359;&#21644;&#23454;&#20307;&#31867;&#22411;&#21069;&#32512;&#27880;&#20837;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25972;&#21512;&#21644;&#20445;&#30041;&#19981;&#21516;&#20449;&#24687;&#30340;&#29420;&#29305;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06365</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#21464;&#25442;&#22120;&#26694;&#26550;&#29992;&#20110;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment. (arXiv:2310.06365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoAlign&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#37051;&#23621;&#29305;&#24449;&#12289;&#22810;&#27169;&#24577;&#23646;&#24615;&#21644;&#23454;&#20307;&#31867;&#22411;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#35774;&#35745;&#20102;&#20998;&#23618;&#21487;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#21147;&#22359;&#21644;&#23454;&#20307;&#31867;&#22411;&#21069;&#32512;&#27880;&#20837;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25972;&#21512;&#21644;&#20445;&#30041;&#19981;&#21516;&#20449;&#24687;&#30340;&#29420;&#29305;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#36328;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#28982;&#32780;&#65292;&#35813;&#20219;&#21153;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#37051;&#36817;&#23454;&#20307;&#12289;&#22810;&#27169;&#24577;&#23646;&#24615;&#21644;&#23454;&#20307;&#31867;&#22411;&#12290;&#30452;&#25509;&#34701;&#21512;&#19978;&#36848;&#20449;&#24687;&#65288;&#22914;&#36830;&#25509;&#25110;&#27880;&#24847;&#21147;&#65289;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#23545;&#40784;&#30340;&#20449;&#24687;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#21464;&#25442;&#22120;MoAlign&#65292;&#36890;&#36807;&#22312;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#20013;&#35774;&#35745;&#19968;&#20010;&#20998;&#23618;&#21487;&#20462;&#25913;&#30340;&#33258;&#27880;&#24847;&#21147;&#22359;&#65292;Hierarchical MoAlign&#24341;&#20837;&#37051;&#23621;&#29305;&#24449;&#12289;&#22810;&#27169;&#24577;&#23646;&#24615;&#21644;&#23454;&#20307;&#31867;&#22411;&#26469;&#22686;&#24378;&#23545;&#40784;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#23454;&#20307;&#31867;&#22411;&#21069;&#32512;&#27880;&#20837;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#22411;&#21069;&#32512;&#26469;&#25972;&#21512;&#23454;&#20307;&#31867;&#22411;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20445;&#30041;&#19981;&#21516;&#20449;&#24687;&#30340;&#29420;&#29305;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called MoAlign, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer's ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to integrate entity-type information using type prefixes, which help
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#29942;&#39048;&#30340;&#21387;&#32553;&#25928;&#24212;&#23548;&#33268;&#31867;&#27604;&#31867;&#30340;&#28151;&#28102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#25918;&#24335;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;InfoCL&#65292;&#21033;&#29992;&#24555;&#36895;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#24615;&#35760;&#24518;&#22686;&#24378;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06362</link><description>&lt;p&gt;
InfoCL&#65306;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#32531;&#35299;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective. (arXiv:2310.06362v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06362
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#29942;&#39048;&#30340;&#21387;&#32553;&#25928;&#24212;&#23548;&#33268;&#31867;&#27604;&#31867;&#30340;&#28151;&#28102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#25918;&#24335;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;InfoCL&#65292;&#21033;&#29992;&#24555;&#36895;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#24615;&#35760;&#24518;&#22686;&#24378;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26088;&#22312;&#19981;&#26029;&#23398;&#20064;&#26032;&#30693;&#35782;&#30340;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#38598;&#20013;&#20110;&#22312;&#31867;&#22686;&#37327;&#35774;&#32622;&#19979;&#30340;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#30340;CL&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#31867;&#27604;&#31867;&#19978;&#30340;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#23545;CL&#20013;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#30340;&#28145;&#20837;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#29942;&#39048;&#30340;&#21387;&#32553;&#25928;&#24212;&#23548;&#33268;&#20102;&#31867;&#27604;&#31867;&#30340;&#28151;&#28102;&#12290;&#20026;&#20102;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#20805;&#20998;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#36830;&#32493;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;InfoCL&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24555;&#24930;&#21644;&#24403;&#21069;&#36807;&#21435;&#23545;&#27604;&#23398;&#20064;&#26469;&#36827;&#34892;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#24674;&#22797;&#20043;&#21069;&#23398;&#21040;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;InfoCL&#36824;&#37319;&#29992;&#23545;&#25239;&#24615;&#35760;&#24518;&#22686;&#24378;&#31574;&#30053;&#26469;&#32531;&#35299;&#22238;&#25918;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a key factor for catastrophic forgetting. In this paper, through an in-depth exploration of the representation learning process in CL, we discover that the compression effect of the information bottleneck leads to confusion on analogous classes. To enable the model learn more sufficient representations, we propose a novel replay-based continual text classification method, InfoCL. Our approach utilizes fast-slow and current-past contrastive learning to perform mutual information maximization and better recover the previously learned representations. In addition, InfoCL incorporates an adversarial memory augmentation strategy to alleviate the overfitting problem of replay. Experimental results demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#21033;&#29992;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06356</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#19981;&#21464;&#40065;&#26834;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Semantic Invariant Robust Watermark for Large Language Models. (arXiv:2310.06356v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#21033;&#29992;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#31639;&#27861;&#22312;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#22312;&#25915;&#20987;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#40065;&#26834;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;LLM&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#20351;&#29992;&#39046;&#22495;&#20869;&#28436;&#31034;&#31034;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#31034;&#36873;&#25321;&#26694;&#26550;ODIS&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#20102;&#39046;&#22495;&#22806;&#31034;&#20363;&#21644;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#20869;&#31034;&#20363;&#65292;&#25104;&#21151;&#22312;&#35813;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.06302</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#30340;&#36873;&#25321;&#24615;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Selective Demonstrations for Cross-domain Text-to-SQL. (arXiv:2310.06302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#20351;&#29992;&#39046;&#22495;&#20869;&#28436;&#31034;&#31034;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#31034;&#36873;&#25321;&#26694;&#26550;ODIS&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#20102;&#39046;&#22495;&#22806;&#31034;&#20363;&#21644;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#20869;&#31034;&#20363;&#65292;&#25104;&#21151;&#22312;&#35813;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#39046;&#22495;&#20869;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#21152;&#20837;&#39046;&#22495;&#20869;&#28436;&#31034;&#31034;&#20363;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#39046;&#22495;&#20869;&#31034;&#20363;&#20013;&#23545;&#25913;&#36827;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#30340;&#22240;&#32032;&#65292;&#24182;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#39046;&#22495;&#20869;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#36825;&#20123;&#22909;&#22788;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#31034;&#36873;&#25321;&#26694;&#26550;ODIS&#65292;&#23427;&#21033;&#29992;&#39046;&#22495;&#22806;&#31034;&#20363;&#21644;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#20869;&#31034;&#20363;&#26469;&#26500;&#24314;&#28436;&#31034;&#12290;&#36890;&#36807;&#20174;&#28151;&#21512;&#26469;&#28304;&#26816;&#32034;&#28436;&#31034;&#65292;ODIS&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#19982;&#20165;&#20381;&#36182;&#21333;&#19968;&#25968;&#25454;&#28304;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#23637;&#29616;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;ODIS&#22312;&#20004;&#20010;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;1.1&#20010;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs' performance. In this paper, we delve into the key factors within in-domain examples that contribute to the improvement and explore whether we can harness these benefits without relying on in-domain annotations. Based on our findings, we propose a demonstration selection framework ODIS which utilizes both out-of-domain examples and synthetically generated in-domain examples to construct demonstrations. By retrieving demonstrations from hybrid sources, ODIS leverages the advantages of both, showcasing its effectiveness compared to baseline methods that rely on a single data source. Furthermore, ODIS outperforms state-of-the-art approaches on two cross-domain text-to-SQL datasets, with improvements of 1.1 a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06272</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#35828;&#23494;&#25991;: &#36890;&#36807;&#23884;&#20837;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#35752;&#35770;&#21644;&#36777;&#35770;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30001;&#20110;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#32780;&#25104;&#20026;&#26126;&#26174;&#30340;&#20132;&#27969;&#36873;&#25321;&#65292;&#20294;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26102;&#38656;&#35201;&#36827;&#34892;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#21487;&#33021;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#35760;&#26469;&#20195;&#34920;&#27169;&#22411;&#22312;&#25972;&#20010;&#35789;&#27719;&#34920;&#20013;&#30340;&#20449;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#65288;&#36890;&#36807;&#23884;&#20837;&#34920;&#31034;&#36827;&#34892;&#20132;&#27969;&#30340;&#32593;&#32476;&#27169;&#22411;&#21327;&#35758;&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;LLMs&#20013;&#21435;&#38500;&#20102;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#23427;&#20204;&#36890;&#36807;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#30340;&#26399;&#26395;&#26469;&#20256;&#36798;&#23427;&#20204;&#30340;&#20449;&#24565;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20559;&#31163;&#33258;&#28982;&#35821;&#35328;&#65292;CIPHER&#22312;&#19981;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21307;&#23398;&#29983;&#25104;&#24615;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20107;&#23454;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#34164;&#28085;&#24615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06271</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Towards Mitigating Hallucination in Large Language Models via Self-Reflection. (arXiv:2310.06271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21307;&#23398;&#29983;&#25104;&#24615;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20107;&#23454;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#34164;&#28085;&#24615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#21253;&#25324;&#38382;&#31572;&#20219;&#21153;&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#37096;&#32626;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65306;&#27169;&#22411;&#29983;&#25104;&#20284;&#20046;&#21512;&#29702;&#20294;&#19981;&#30495;&#23454;&#25110;&#33618;&#35884;&#30340;&#20449;&#24687;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#19981;&#24120;&#35265;&#30340;&#19987;&#19994;&#27010;&#24565;&#21644;&#28508;&#22312;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;&#26412;&#25991;&#20351;&#29992;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#21644;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#21307;&#23398;&#29983;&#25104;&#24615;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#35782;&#21035;&#21644;&#29702;&#35299;&#24120;&#35265;&#30340;&#38382;&#39064;&#31572;&#26696;&#65292;&#29305;&#21035;&#26159;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#33719;&#21462;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;&#36890;&#36807;&#36825;&#20010;&#21453;&#39304;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#22686;&#24378;&#20102;&#29983;&#25104;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#34164;&#28085;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Cons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#23545;116&#31687;&#25968;&#25454;&#39537;&#21160;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#25991;&#29486;&#35843;&#30740;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#22238;&#31572;&#20855;&#20307;&#38382;&#39064;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#23613;&#31649;&#22312;&#22768;&#23398;&#25991;&#29486;&#35843;&#30740;&#33258;&#21160;&#21270;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#25913;&#36827;&#20197;&#26356;&#28165;&#26224;&#20934;&#30830;&#22320;&#22238;&#31572;&#25216;&#26415;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06260</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#33258;&#21160;&#25991;&#29486;&#35843;&#30740;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An experiment on an automated literature survey of data-driven speech enhancement methods. (arXiv:2310.06260v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#23545;116&#31687;&#25968;&#25454;&#39537;&#21160;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#25991;&#29486;&#35843;&#30740;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#22238;&#31572;&#20855;&#20307;&#38382;&#39064;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#23613;&#31649;&#22312;&#22768;&#23398;&#25991;&#29486;&#35843;&#30740;&#33258;&#21160;&#21270;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#25913;&#36827;&#20197;&#26356;&#28165;&#26224;&#20934;&#30830;&#22320;&#22238;&#31572;&#25216;&#26415;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22768;&#23398;&#39046;&#22495;&#31185;&#23398;&#20986;&#29256;&#29289;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#25991;&#29486;&#35843;&#30740;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#33258;&#21160;&#35843;&#26597;116&#31687;&#25968;&#25454;&#39537;&#21160;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30340;&#25991;&#29486;&#35843;&#30740;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#20851;&#20110;&#20174;&#22522;&#20110;&#20154;&#24037;&#35843;&#30740;&#20013;&#36873;&#23450;&#30340;&#35770;&#25991;&#30340;&#20855;&#20307;&#38382;&#39064;&#30340;&#20934;&#30830;&#22238;&#31572;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#34429;&#28982;&#25105;&#20204;&#30475;&#21040;&#22312;&#22768;&#23398;&#25991;&#29486;&#35843;&#30740;&#33258;&#21160;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#25913;&#36827;&#20197;&#26356;&#28165;&#26224;&#20934;&#30830;&#22320;&#22238;&#31572;&#25216;&#26415;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing number of scientific publications in acoustics, in general, presents difficulties in conducting traditional literature surveys. This work explores the use of a generative pre-trained transformer (GPT) model to automate a literature survey of 116 articles on data-driven speech enhancement methods. The main objective is to evaluate the capabilities and limitations of the model in providing accurate responses to specific queries about the papers selected from a reference human-based survey. While we see great potential to automate literature surveys in acoustics, improvements are needed to address technical questions more clearly and accurately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#21487;&#34892;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06254</link><description>&lt;p&gt;
&#30475;&#26775;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;
&lt;/p&gt;
&lt;p&gt;
Get the gist? Using large language models for few-shot decontextualization. (arXiv:2310.06254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#21487;&#34892;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#35299;&#37322;&#23500;&#26377;&#19978;&#19979;&#25991;&#30340;&#21477;&#23376;&#30340;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25110;&#23545;&#35805;&#31995;&#32479;&#65292;&#24456;&#26377;&#24517;&#35201;&#33021;&#22815;&#23558;&#21477;&#23376;&#20445;&#30041;&#22312;&#19968;&#20010;&#21487;&#20197;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#29702;&#35299;&#30340;&#24418;&#24335;&#20013;&#65292;&#20197;&#20415;&#20197;&#21518;&#37325;&#26032;&#20351;&#29992;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#8220;&#21435;&#32972;&#26223;&#21270;&#8221;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#32463;&#36807;&#32454;&#35843;&#30340;&#29983;&#25104;&#22411;Seq2Seq&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#21435;&#32972;&#26223;&#21270;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#32780;&#19988;&#21487;&#33021;&#26080;&#27861;&#36801;&#31227;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#21435;&#32972;&#26223;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#36798;&#21040;&#21487;&#34892;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many NLP applications that involve interpreting sentences within a rich context -- for instance, information retrieval systems or dialogue systems -it is desirable to be able to preserve the sentence in a form that can be readily understood without context, for later reuse -- a process known as ``decontextualization''. While previous work demonstrated that generative Seq2Seq models could effectively perform decontextualization after being fine-tuned on a specific dataset, this approach requires expensive human annotations and may not transfer to other domains. We propose a few-shot method of decontextualization using a large language model, and present preliminary results showing that this method achieves viable performance on multiple domains using only a small set of examples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#24341;&#23548;&#21644;&#24212;&#29992;&#20064;&#24815;&#24615;&#27169;&#24335;&#26469;&#29983;&#25104;&#22522;&#20110;&#35282;&#33394;&#30340;&#22238;&#24212;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#27169;&#24335;&#34920;&#31034;&#26469;&#25429;&#25417;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#20064;&#24815;&#24615;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#28982;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06245</link><description>&lt;p&gt;
&#25105;&#20204;&#26159;&#25105;&#20204;&#21453;&#22797;&#20570;&#30340;&#20107;&#24773;&#65306;&#22312;&#22522;&#20110;&#35282;&#33394;&#30340;&#22238;&#24212;&#20013;&#24341;&#23548;&#21644;&#24212;&#29992;&#20064;&#24815;&#24615;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses. (arXiv:2310.06245v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#24341;&#23548;&#21644;&#24212;&#29992;&#20064;&#24815;&#24615;&#27169;&#24335;&#26469;&#29983;&#25104;&#22522;&#20110;&#35282;&#33394;&#30340;&#22238;&#24212;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#27169;&#24335;&#34920;&#31034;&#26469;&#25429;&#25417;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#20064;&#24815;&#24615;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#28982;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25216;&#26415;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#26681;&#25454;&#29305;&#23450;&#30340;&#24320;&#21457;&#32773;&#35268;&#23450;&#30340;&#35282;&#33394;&#29983;&#25104;&#22238;&#24212;&#12290;&#34429;&#28982;&#21487;&#20197;&#20174;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#35768;&#22810;&#35282;&#33394;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#20351;&#24471;&#20197;&#26126;&#30830;&#24418;&#24335;&#25351;&#23450;&#35282;&#33394;&#25104;&#20026;&#21487;&#21462;&#30340;&#12290;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#35282;&#33394;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#27425;&#24615;&#30340;&#33258;&#25105;&#30693;&#35782;&#29255;&#27573;&#30340;&#38598;&#21512;&#65292;&#23545;&#35805;&#31995;&#32479;&#20250;&#26816;&#32034;&#36825;&#20123;&#30693;&#35782;&#29255;&#27573;&#26469;&#29983;&#25104;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#30340;&#20154;&#31867;&#23545;&#35805;&#20013;&#65292;&#35282;&#33394;&#36890;&#24120;&#36890;&#36807;&#31867;&#20284;&#25925;&#20107;&#30340;&#21465;&#36848;&#26469;&#25581;&#31034;&#20986;&#26469;&#65292;&#36825;&#20123;&#21465;&#36848;&#28041;&#21450;&#20016;&#23500;&#30340;&#20064;&#24815;&#24615;&#30693;&#35782;&#8212;&#8212;&#20851;&#20110;&#20195;&#29702;&#20154;&#32463;&#24120;&#21442;&#19982;&#30340;&#20107;&#20214;&#31867;&#22411;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#24037;&#20316;&#27963;&#21160;&#12289;&#29233;&#22909;&#12289;&#20307;&#32946;&#27963;&#21160;&#12289;&#21916;&#27426;&#30340;&#23089;&#20048;&#31561;&#65289;&#65292;&#21253;&#25324;&#36825;&#20123;&#20107;&#20214;&#30340;&#20856;&#22411;&#30446;&#26631;&#12289;&#23376;&#20107;&#20214;&#12289;&#21069;&#25552;&#26465;&#20214;&#21644;&#21518;&#32622;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#26126;&#30830;&#30340;&#27169;&#24335;&#34920;&#31034;&#26469;&#25429;&#25417;&#36825;&#31181;&#20064;&#24815;&#24615;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#23545;&#35805;&#31995;&#32479;&#20351;&#29992;&#36825;&#20123;&#27169;&#24335;&#29983;&#25104;&#22238;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge -- knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#35843;&#20248;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06239</link><description>&lt;p&gt;
&#27169;&#22411;&#35843;&#20248;&#36824;&#26159;&#25552;&#31034;&#35843;&#20248;&#65311;&#23545;&#20110;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction. (arXiv:2310.06239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#35843;&#20248;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#22522;&#20110;&#36719;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#25552;&#31034;&#30340;&#24418;&#29366;&#12289;&#20351;&#29992;&#20923;&#32467;/&#35299;&#20923;LLMs&#36827;&#34892;&#25552;&#31034;&#35843;&#20248;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#24320;&#21457;&#20102;&#22522;&#20110;&#36719;&#25552;&#31034;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#35757;&#32451;&#31574;&#30053;&#65288;1.&#26080;&#25552;&#31034;&#24494;&#35843;&#65307;2.&#35299;&#20923;LLMs&#30340;&#30828;&#25552;&#31034;&#65307;3.&#35299;&#20923;LLMs&#30340;&#36719;&#25552;&#31034;&#65307;4.&#20923;&#32467;LLMs&#30340;&#36719;&#25552;&#31034;&#65289;&#26469;&#35780;&#20272;&#20102;&#19971;&#20010;&#39044;&#35757;&#32451;&#30340;LLMs&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#25552;&#21462;&#33021;&#21147;&#12290;&#22312;&#36328;&#26426;&#26500;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#31639;&#27861;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26469;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#21508;&#31181;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.06238</link><description>&lt;p&gt;
&#35299;&#20915;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65306;&#20026;&#26080;&#20559;&#38382;&#31572;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering. (arXiv:2310.06238v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26469;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#21508;&#31181;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#21449;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#65292;&#25512;&#21160;&#20102;&#22810;&#27169;&#24577;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20219;&#20309;&#27169;&#24577;&#20013;&#23384;&#22312;&#30340;&#24378;&#28872;&#20559;&#35265;&#20250;&#23548;&#33268;&#27169;&#22411;&#24573;&#35270;&#20854;&#20182;&#27169;&#24577;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#26377;&#25928;&#22320;&#36328;&#36234;&#36825;&#20123;&#22810;&#26679;&#21270;&#27169;&#24577;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#21463;&#21040;&#25439;&#23475;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#23457;&#26597;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#36873;&#25321;&#20855;&#26377;&#26126;&#26174;&#31572;&#26696;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20114;&#34917;&#30340;&#35270;&#39057;&#21644;&#38382;&#39064;&#65292;&#30830;&#20445;&#27809;&#26377;&#31572;&#26696;&#26377;&#26126;&#26174;&#30340;&#20559;&#26012;&#20998;&#24067;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20108;&#20803;&#38382;&#39064;&#65292;&#25105;&#20204;&#21162;&#21147;&#30830;&#20445;&#27599;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#20004;&#20010;&#31572;&#26696;&#20960;&#20046;&#22343;&#21248;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23427;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#30456;&#20449;&#33021;&#22815;&#26356;&#22909;&#22320;&#20419;&#36827;AVQA&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32763;&#35793;&#25104;&#20013;&#25991;&#24182;&#24635;&#32467;&#25351;&#20986;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36880;&#28176;&#28436;&#36827;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#38750;&#24120;&#38590;&#20197;&#36827;&#34892;&#25968;&#23398;&#19978;&#30340;&#34920;&#36798;&#65292;&#20294;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#20046;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06228</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#28436;&#36827;&#65306;&#19981;&#20165;&#20165;&#26159;&#35821;&#35328;&#22788;&#29702;&#65292;&#32780;&#26159;&#26397;&#21521;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI. (arXiv:2310.06228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32763;&#35793;&#25104;&#20013;&#25991;&#24182;&#24635;&#32467;&#25351;&#20986;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36880;&#28176;&#28436;&#36827;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#38750;&#24120;&#38590;&#20197;&#36827;&#34892;&#25968;&#23398;&#19978;&#30340;&#34920;&#36798;&#65292;&#20294;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#20046;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21457;&#26126;&#35745;&#31639;&#26426;&#20197;&#26469;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#65288;&#23454;&#38469;&#30340;&#20154;&#31867;&#35821;&#35328;&#65289;&#36827;&#34892;&#36890;&#20449;&#19968;&#30452;&#26159;&#19968;&#39033;&#26790;&#24187;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#38750;&#24120;&#38590;&#20197;&#36827;&#34892;&#25968;&#23398;&#19978;&#30340;&#34920;&#36798;&#65292;&#36825;&#20351;&#24471;&#22312;&#19981;&#32771;&#34385;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31639;&#27861;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#35768;&#22810;&#25216;&#26415;&#21457;&#23637;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#19981;&#33021;&#35828;&#24050;&#32463;&#23454;&#29616;&#20102;&#20219;&#20309;&#20801;&#35768;&#33258;&#30001;&#21033;&#29992;&#30340;&#32467;&#26524;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#22312;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#20363;&#22914;&#23398;&#20064;&#27597;&#35821;&#25110;&#22806;&#35821;&#26102;&#65292;&#25105;&#20204;&#24517;&#39035;&#25215;&#35748;&#36825;&#20010;&#36807;&#31243;&#19982;&#26684;&#35328;&#8220;&#29087;&#33021;&#29983;&#24039;&#8221;&#22312;&#21407;&#21017;&#19978;&#26159;&#30456;&#20284;&#30340;&#65292;&#23613;&#31649;&#23398;&#20064;&#26041;&#27861;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#37325;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20013;&#25198;&#28436;&#20102;&#26680;&#24515;&#35282;&#33394;&#12290;&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26102;&#65292;&#36825;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#12290;&#20174;&#23398;&#20064;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#32467;&#26524;&#20013;&#65292;&#24050;&#32463;&#26377;&#25253;&#36947;&#31216;&#36229;&#20986;&#20102;&#26368;&#21021;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the invention of computers, communication through natural language (actual human language) has been a dream technology. However, natural language is extremely difficult to mathematically formulate, making it difficult to realize as an algorithm without considering programming. While there have been numerous technological developments, one cannot say that any results allowing free utilization have been achieved thus far. In the case of language learning in humans, for instance when learning one's mother tongue or foreign language, one must admit that this process is similar to the adage "practice makes perfect" in principle, even though the learning method is significant up to a point. Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data u
&lt;/p&gt;</description></item><item><title>GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06213</link><description>&lt;p&gt;
GeoLLM: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06213
&lt;/p&gt;
&lt;p&gt;
GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#24120;&#24120;&#20381;&#36182;&#20110;&#20840;&#29699;&#33539;&#22260;&#21487;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#31561;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#21487;&#33021;&#35201;&#20040;&#24456;&#26114;&#36149;&#65292;&#35201;&#20040;&#32570;&#20047;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20114;&#32852;&#32593;&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;LLM&#20013;&#23884;&#20837;&#20102;&#26377;&#20851;&#20301;&#32622;&#30340;&#26174;&#33879;&#31354;&#38388;&#20449;&#24687;&#65292;&#20294;&#20165;&#20351;&#29992;&#22320;&#29702;&#22352;&#26631;&#26469;&#26597;&#35810;LLM&#23545;&#20110;&#39044;&#27979;&#20154;&#21475;&#23494;&#24230;&#31561;&#20851;&#38190;&#25351;&#26631;&#26159;&#26080;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;LLM&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22269;&#38469;&#31038;&#21306;&#20851;&#24515;&#30340;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#30340;&#27979;&#37327;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25968;&#23383;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#23383;&#30340;&#22823;&#23567;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06204</link><description>&lt;p&gt;
&#26080;&#22238;&#24402;&#20272;&#35745;&#25968;&#23383;
&lt;/p&gt;
&lt;p&gt;
Estimating Numbers without Regression. (arXiv:2310.06204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25968;&#23383;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#25968;&#23383;&#30340;&#22823;&#23567;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#34920;&#31034;&#25968;&#23383;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20154;&#31867;&#26681;&#25454;&#25968;&#23383;&#30340;&#22823;&#23567;&#36827;&#34892;&#27010;&#24565;&#21270;&#65292;&#26377;&#25928;&#22320;&#23558;&#23427;&#20204;&#25237;&#23556;&#21040;&#25968;&#23383;&#32447;&#19978;&#65307;&#32780;&#23376;&#35789;&#26631;&#35760;&#21270;&#26080;&#27861;&#26126;&#30830;&#25429;&#25417;&#25968;&#23383;&#30340;&#22823;&#23567;&#65292;&#23558;&#25968;&#23383;&#20998;&#21106;&#25104;&#20219;&#24847;&#30340;&#22359;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#32570;&#28857;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#31649;&#32447;&#30340;&#21508;&#20010;&#38454;&#27573;&#20462;&#25913;&#25968;&#23383;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#25913;&#21464;&#25968;&#23383;&#30340;&#34920;&#31034;&#26041;&#24335;&#65288;&#20363;&#22914;&#31185;&#23398;&#35745;&#25968;&#27861;&#19982;&#21313;&#36827;&#21046;&#65289;&#65292;&#35201;&#20040;&#25913;&#21464;&#29992;&#20110;&#34920;&#31034;&#25968;&#23383;&#30340;&#35789;&#27719;&#34920;&#65292;&#35201;&#20040;&#30452;&#25509;&#25913;&#21464;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#20197;&#30452;&#25509;&#22238;&#24402;&#21040;&#25152;&#38656;&#30340;&#25968;&#23383;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26550;&#26500;&#30340;&#26356;&#25913;&#26377;&#21161;&#20110;&#22312;&#25968;&#23383;&#20272;&#35745;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26377;&#27934;&#23519;&#21147;&#30340;&#32570;&#25022;&#65306;&#25913;&#21464;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65288;&#20363;&#22914;&#22312;10-100&#33539;&#22260;&#20869;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#26631;&#35760;&#65289;&#26159;&#19968;&#20010;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number.  Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked numb
&lt;/p&gt;</description></item><item><title>GPT-who&#26159;&#19968;&#31181;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;GPT-who&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06202</link><description>&lt;p&gt;
GPT-who&#65306;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#23494;&#24230;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-who: An Information Density-based Machine-Generated Text Detector. (arXiv:2310.06202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06202
&lt;/p&gt;
&lt;p&gt;
GPT-who&#26159;&#19968;&#31181;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;GPT-who&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#35748;&#20026;&#20154;&#31867;&#22312;&#35821;&#35328;&#20135;&#29983;&#36807;&#31243;&#20013;&#21916;&#27426;&#24179;&#22343;&#20998;&#24067;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-who&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#22810;&#31867;&#39046;&#22495;&#19981;&#21487;&#30693;&#32479;&#35745;&#26816;&#27979;&#22120;&#12290;&#35813;&#26816;&#27979;&#22120;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#24314;&#27169;&#27599;&#20010;LLM&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;GPT-who&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65288;&#21253;&#25324;&#22522;&#20110;&#32479;&#35745;&#21644;&#38750;&#32479;&#35745;&#30340;&#65289;&#65292;&#22914;GLTR&#65292;GPTZero&#65292;OpenAI detector&#21644;ZeroGPT&#36229;&#36807;20&#65285;&#12290;&#38500;&#20102;&#24615;&#33021;&#20248;&#36234;&#22806;&#65292;GPT-who&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#30340;&#34920;&#31034;&#30340;&#26368;&#22823;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Uniform Information Density principle posits that humans prefer to spread information evenly during language production. In this work, we examine if the UID principle can help capture differences between Large Language Models (LLMs) and human-generated text. We propose GPT-who, the first psycholinguistically-aware multi-class domain-agnostic statistical-based detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- &amp; non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to superior performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We present the largest analysis of the UID-based representations of human and machine-genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Selective Context&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#20197;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#65292;&#24182;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06201</link><description>&lt;p&gt;
&#21387;&#32553;&#19978;&#19979;&#25991;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Compressing Context to Enhance Inference Efficiency of Large Language Models. (arXiv:2310.06201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Selective Context&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#20197;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#65292;&#24182;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#26174;&#33879;&#22686;&#21152;&#65292;&#21253;&#25324;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20197;&#21450;&#36755;&#20837;&#36229;&#20986;&#35821;&#35328;&#27169;&#22411;&#22266;&#23450;&#19978;&#19979;&#25991;&#38271;&#24230;&#26102;&#28508;&#22312;&#30340;&#19978;&#19979;&#25991;&#25130;&#26029;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#21644;&#25193;&#23637;&#23545;&#35805;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Selective Context&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#20351;&#36755;&#20837;&#26356;&#32039;&#20945;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#38656;&#35201;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#24120;&#35265;&#25968;&#25454;&#28304;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;arXiv&#35770;&#25991;&#12289;&#26032;&#38395;&#25991;&#31456;&#21644;&#38271;&#23545;&#35805;&#65292;&#29992;&#20110;&#25688;&#35201;&#12289;&#38382;&#31572;&#21644;&#22238;&#31572;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Selective Context&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#38477;&#20302;&#20102;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#26102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;50%&#30340;&#20869;&#23384;&#25104;&#26412;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\% reduction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06200</link><description>&lt;p&gt;
&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#21450;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#24182;&#27809;&#26377;&#21516;&#27493;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;LLM&#12290;&#25105;&#20204;&#22312;&#21069;&#20154;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#22914;&#20309;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29983;&#25104;&#35299;&#37322;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#25105;&#20204;&#26032;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.06165</link><description>&lt;p&gt;
CAW-coref: &#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#27599;&#31687;&#25991;&#31456;&#38656;&#35201;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#65288;&#20363;&#22914;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65289;&#65292;&#20195;&#20215;&#22826;&#39640;&#12290;&#32780;&#35789;&#32423;&#20849;&#25351;&#31995;&#32479; (WL-coref) &#22312;&#25928;&#29575;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20808;&#36827;&#31995;&#32479; 96.6% &#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102; WL-coref &#30340;&#19968;&#20010;&#24120;&#35265;&#20294;&#37325;&#35201;&#30340;&#22833;&#36133;&#26696;&#20363;&#65306;&#22788;&#29702;&#8220;Tom &#21644; Mary&#8221;&#20043;&#31867;&#30340;&#24182;&#21015;&#25552;&#21450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312; OntoNotes &#27979;&#35797;&#38598;&#19978;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102; 0.9% F1&#65292;&#23558;&#39640;&#25928;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#32553;&#23567;&#20102;34.6%&#12290;&#25105;&#20204;&#30340;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#27169;&#22411;&#65288;CAW-coref&#65289;&#21644;&#20195;&#30721;&#21487;&#22312; https://github.com/KarelDO/wl-coref &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.06117</link><description>&lt;p&gt;
&#36864;&#21518;&#19968;&#27493;&#65306;&#36890;&#36807;&#25277;&#35937;&#21796;&#36215;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#36864;&#21518;&#25552;&#31034;&#8221;&#30340;&#31616;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#21253;&#21547;&#20855;&#20307;&#32454;&#33410;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#25277;&#35937;&#65292;&#24471;&#20986;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#21644;&#21407;&#29702;&#26469;&#25351;&#23548;&#25512;&#29702;&#27493;&#39588;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#25512;&#29702;&#36335;&#24452;&#19978;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;PaLM-2L&#27169;&#22411;&#36827;&#34892;&#20102;&#36864;&#21518;&#25552;&#31034;&#23454;&#39564;&#65292;&#22312;&#21253;&#25324;STEM&#12289;&#30693;&#35782;&#38382;&#31572;&#21644;&#22810;&#36339;&#25512;&#29702;&#22312;&#20869;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20363;&#22914;&#65292;&#22312;MMLU&#29289;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#65292;&#36864;&#21518;&#25552;&#31034;&#21487;&#20197;&#23558;PaLM-2L&#30340;&#24615;&#33021;&#25552;&#21319;7%&#21644;11%&#65292;&#22312;TimeQA&#20219;&#21153;&#19978;&#25552;&#21319;27%&#65292;&#22312;MuSiQue&#20219;&#21153;&#19978;&#25552;&#21319;7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#33879;&#30340;&#31867;&#21035;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#29992;&#25143;&#19982;LLM&#20132;&#20114;&#21512;&#20316;&#36827;&#34892;&#26631;&#27880;&#65292;&#24418;&#25104;&#20998;&#31867;&#25552;&#31034;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;82%&#12290;</title><link>http://arxiv.org/abs/2310.06111</link><description>&lt;p&gt;
BYOC: &#20351;&#29992;&#21512;&#33879;&#30340;&#31867;&#21035;&#25551;&#36848;&#20010;&#24615;&#21270;&#36827;&#34892;&#23569;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions. (arXiv:2310.06111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06111
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#33879;&#30340;&#31867;&#21035;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#29992;&#25143;&#19982;LLM&#20132;&#20114;&#21512;&#20316;&#36827;&#34892;&#26631;&#27880;&#65292;&#24418;&#25104;&#20998;&#31867;&#25552;&#31034;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;82%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#24102;&#26631;&#27880;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#26102;&#65292;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#25552;&#31034;&#24182;&#20351;&#29992;&#33021;&#23481;&#32435;&#35768;&#22810;&#31034;&#20363;&#30340;&#38271;&#19978;&#19979;&#25991;&#12290;&#32467;&#26524;&#65292;&#26222;&#36890;&#29992;&#25143;&#19981;&#33021;&#20026;&#33258;&#24049;&#26500;&#24314;&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;LLM&#19981;&#26159;&#20351;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#32780;&#26159;&#30001;&#29992;&#25143;&#21644;LLM&#21512;&#33879;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#26174;&#33879;&#29305;&#24449;&#30340;&#25551;&#36848;&#36827;&#34892;&#25552;&#31034;&#12290;&#22312;&#29992;&#25143;&#26631;&#27880;&#27599;&#20010;&#23569;&#26679;&#26412;&#31034;&#20363;&#26102;&#65292;LLM&#20250;&#25552;&#20986;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#29992;&#25143;&#32473;&#20986;&#31572;&#26696;&#12290;&#31034;&#20363;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#34987;&#24635;&#32467;&#25104;&#20998;&#31867;&#25552;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#39640;&#20934;&#30830;&#29575;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using an LLM. Rather than few-shot examples, the LLM is prompted with descriptions of the salient features of each class. These descriptions are coauthored by the user and the LLM interactively: while the user annotates each few-shot example, the LLM asks relevant questions that the user answers. Examples, questions, and answers are summarized to form the classification prompt. Our experiments show that our approach yields high accuracy classifiers, within 82% of the performance of models trained with s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#35789;&#27719;&#22635;&#20805;&#22120;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06103</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding. (arXiv:2310.06103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06103
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#35789;&#27719;&#22635;&#20805;&#22120;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#65288;E2E-SLU&#65289;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#35780;&#20272;&#36890;&#24120;&#32570;&#20047;&#22810;&#35821;&#35328;&#35774;&#32622;&#21644;&#38656;&#35201;&#39044;&#27979;&#35789;&#27719;&#22635;&#20805;&#22120;&#65288;&#20363;&#22914;&#27133;&#22635;&#20805;&#65289;&#31561;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#20197;&#29983;&#25104;&#26041;&#24335;&#22312;&#22235;&#31181;&#35821;&#35328;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;E2E-SLU&#65292;&#21253;&#25324;&#23545;&#35789;&#27719;&#22635;&#20805;&#22120;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24191;&#27867;&#21487;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#20960;&#31181;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;7000&#23567;&#26102;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20004;&#20010;SLU&#25968;&#25454;&#38598;&#19978;&#26368;&#32456;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#21478;&#22806;&#20004;&#20010;SLU&#25968;&#25454;&#38598;&#19978;&#21017;&#37096;&#20998;&#36229;&#36807;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#23558;&#22312;PortMEDIA-Language&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;&#24050;&#30693;&#32467;&#26524;&#25552;&#39640;&#20102;&#36817;&#19968;&#21322;&#65292;&#36798;&#21040;&#20102;23.65%&#30340;&#27010;&#24565;/&#20540;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of methods have been proposed for End-to-End Spoken Language Understanding (E2E-SLU) using pretrained models, however their evaluation often lacks multilingual setup and tasks that require prediction of lexical fillers, such as slot filling. In this work, we propose a unified method that integrates multilingual pretrained speech and text models and performs E2E-SLU on six datasets in four languages in a generative manner, including the prediction of lexical fillers. We investigate how the proposed method can be improved by pretraining on widely available speech recognition data using several training objectives. Pretraining on 7000 hours of multilingual data allows us to outperform the state-of-the-art ultimately on two SLU datasets and partly on two more SLU datasets. Finally, we examine the cross-lingual capabilities of the proposed model and improve on the best known result on the PortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate of 23.65%.
&lt;/p&gt;</description></item><item><title>&#23545;&#19977;&#31181;&#29616;&#26377;&#30340;&#24615;&#21035;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#19981;&#23545;&#20108;&#20803;&#20010;&#20307;&#20570;&#39044;&#27979;&#30340;&#20559;&#35265;&#30340;&#23457;&#35745;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#22312;&#20934;&#30830;&#24615;&#19978;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#38750;&#20108;&#20803;&#35780;&#35770;&#30340;&#39044;&#27979;&#20027;&#35201;&#26159;&#22899;&#24615;&#65292;&#20174;&#32780;&#20256;&#25773;&#20102;&#23545;&#38750;&#20108;&#20803;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.06061</link><description>&lt;p&gt;
&#23545;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#20998;&#26512;&#22120;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Gender Analyzers on Text Data. (arXiv:2310.06061v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06061
&lt;/p&gt;
&lt;p&gt;
&#23545;&#19977;&#31181;&#29616;&#26377;&#30340;&#24615;&#21035;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#19981;&#23545;&#20108;&#20803;&#20010;&#20307;&#20570;&#39044;&#27979;&#30340;&#20559;&#35265;&#30340;&#23457;&#35745;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#22312;&#20934;&#30830;&#24615;&#19978;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#38750;&#20108;&#20803;&#35780;&#35770;&#30340;&#39044;&#27979;&#20027;&#35201;&#26159;&#22899;&#24615;&#65292;&#20174;&#32780;&#20256;&#25773;&#20102;&#23545;&#38750;&#20108;&#20803;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#21644;&#21487;&#35775;&#38382;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#21508;&#20010;&#31038;&#20250;&#32676;&#20307;&#65292;&#22914;&#26377;&#33394;&#20154;&#31181;&#21644;&#38750;&#20108;&#20803;&#24615;&#21035;&#20154;&#32676;&#65292;&#23384;&#22312;&#21487;&#35777;&#26126;&#30340;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19977;&#31181;&#24050;&#26377;&#30340;&#24615;&#21035;&#20998;&#26512;&#22120; - uClassify&#65292;Readable&#21644;HackerFactor&#36827;&#34892;&#23457;&#35745;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#38024;&#23545;&#38750;&#20108;&#20803;&#20010;&#20307;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#36825;&#20123;&#24037;&#20855;&#20165;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#20108;&#20803;&#24615;&#21035;&#26631;&#31614;&#65292;&#22240;&#27492;&#23545;&#38750;&#20108;&#20803;&#24615;&#21035;&#31038;&#20250;&#25104;&#21592;&#23384;&#22312;&#27495;&#35270;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;-Reddit&#35780;&#35770;&#65288;660k&#65289;&#21644;Tumblr&#24086;&#23376;&#65288;2.05M&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#36825;&#20123;&#24037;&#20855;&#22312;&#25152;&#26377;&#24179;&#21488;&#19978;&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#32422;&#20026;50%&#12290;&#25152;&#26377;&#24179;&#21488;&#19978;&#38024;&#23545;&#38750;&#20108;&#20803;&#35780;&#35770;&#30340;&#39044;&#27979;&#20027;&#35201;&#26159;&#22899;&#24615;&#65292;&#20174;&#32780;&#20256;&#25773;&#20102;&#31038;&#20250;&#23545;&#38750;&#20108;&#20803;&#20154;&#32676;&#30340;&#20559;&#35265;&#65292;&#35748;&#20026;&#20182;&#20204;&#26159;&#22899;&#24615;&#21270;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20197;&#22810;&#31181;&#32452;&#21512;&#23545;BERT&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#24494;&#35843;&#23454;&#39564;&#65292;&#24182;&#35266;&#23519;&#21040;&#19968;&#20010; o
&lt;/p&gt;
&lt;p&gt;
AI models have become extremely popular and accessible to the general public. However, they are continuously under the scanner due to their demonstrable biases toward various sections of the society like people of color and non-binary people. In this study, we audit three existing gender analyzers -uClassify, Readable and HackerFactor, for biases against non-binary individuals. These tools are designed to predict only the cisgender binary labels, which leads to discrimination against non-binary members of the society. We curate two datasets -- Reddit comments (660k) and, Tumblr posts (2.05M) and our experimental evaluation shows that the tools are highly inaccurate with the overall accuracy being ~50% on all platforms. Predictions for non-binary comments on all platforms are mostly female, thus propagating the societal bias that non-binary individuals are effeminate. To address this, we fine-tune a BERT multi-label classifier on the two datasets in multiple combinations, observe an o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;LLM&#38598;&#25104;&#21040;SoC&#23433;&#20840;&#39564;&#35777;&#27969;&#31243;&#20013;&#65292;&#24320;&#36767;&#20102;&#26356;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#30830;&#20445;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;SoC&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06046</link><description>&lt;p&gt;
LLM&#29992;&#20110;SoC&#23433;&#20840;: &#19968;&#31181;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
LLM for SoC Security: A Paradigm Shift. (arXiv:2310.06046v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;LLM&#38598;&#25104;&#21040;SoC&#23433;&#20840;&#39564;&#35777;&#27969;&#31243;&#20013;&#65292;&#24320;&#36767;&#20102;&#26356;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#30830;&#20445;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;SoC&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#35774;&#22791;&#20013;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#35774;&#35745;&#30340;&#26222;&#21450;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23558;&#23433;&#20840;&#24615;&#32435;&#20837;SoC&#35774;&#35745;&#27969;&#31243;&#30340;&#20219;&#21153;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#22312;&#21487;&#20280;&#32553;&#24615;&#12289;&#20840;&#38754;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#25552;&#20379;&#23545;&#29616;&#20195;SoC&#35774;&#35745;&#30340;&#26377;&#25928;&#39564;&#35777;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#39640;&#32423;&#25512;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#25104;&#21151;&#32780;&#22791;&#21463;&#36190;&#35465;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#26426;&#36935;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#30340;&#26032;&#20852;&#33021;&#21147;&#26469;&#35299;&#20915;SoC&#23433;&#20840;&#39046;&#22495;&#30340;&#29616;&#26377;&#24046;&#36317;&#65292;&#26088;&#22312;&#36861;&#27714;&#26356;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#36866;&#24212;&#30340;&#26041;&#27861;&#35770;&#12290;&#36890;&#36807;&#23558;LLM&#38598;&#25104;&#21040;SoC&#23433;&#20840;&#39564;&#35777;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#25171;&#24320;&#20102;&#30830;&#20445;&#26085;&#30410;&#22797;&#26434;SoC&#23433;&#20840;&#30340;&#26032;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05991</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;Span-Trigger-based Contextual Pooling(STCP)&#21644;Role-based Latent Information Guidance (RLIG)&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#20013;&#24573;&#30053;&#30340;&#38750;&#35770;&#35777;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#20197;&#21450;&#35770;&#35777;&#35282;&#33394;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#65292;&#20197;&#21450;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#24182;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#20107;&#20214;&#35770;&#35777;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#38754;&#20020;&#30528;&#38271;&#36755;&#20837;&#21644;&#36328;&#21477;&#23376;&#25512;&#29702;&#30340;&#26032;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25429;&#25417;&#27599;&#20010;&#20107;&#20214;&#20013;&#20505;&#36873;&#35770;&#35777;&#19982;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;a&#65289;&#38750;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#20449;&#24687;&#65307;b&#65289;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SCPRG&#65288;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#21644;&#28508;&#22312;&#35282;&#33394;&#24341;&#23548;&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22522;&#20110;&#36328;&#24230;&#35302;&#21457;&#22120;&#30340;&#19978;&#19979;&#25991;&#27719;&#32858;&#65288;STCP&#65289;&#26681;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21644;&#27719;&#32858;&#38750;&#35770;&#35777;&#32447;&#32034;&#35789;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#27169;&#22359;&#26500;&#24314;&#28508;&#22312;&#30340;&#35282;&#33394;&#34920;&#31034;&#65292;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#20197;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#20505;&#36873;&#35770;&#35777;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20010;&#24615;&#29305;&#24449;&#22312;&#21338;&#24328;&#29702;&#35770;&#20851;&#31995;&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;&#20986;&#28436;&#21270;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.05976</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24314;&#31435;&#21512;&#20316;&#34892;&#20026;&#30456;&#20851;&#20010;&#24615;&#29305;&#24449;&#30340;&#36827;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An evolutionary model of personality traits related to cooperative behavior using a large language model. (arXiv:2310.05976v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20010;&#24615;&#29305;&#24449;&#22312;&#21338;&#24328;&#29702;&#35770;&#20851;&#31995;&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;&#20986;&#28436;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#36798;&#24615;&#24341;&#20837;&#21040;&#31038;&#20250;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#36827;&#21270;&#27169;&#22411;&#20013;&#65292;&#25506;&#35752;&#22810;&#26679;&#24615;&#21644;&#31038;&#20250;&#32676;&#20307;&#30340;&#36827;&#21270;&#21160;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#21338;&#24328;&#29702;&#35770;&#20851;&#31995;&#30340;&#32972;&#26223;&#19979;&#20010;&#24615;&#29305;&#24449;&#30340;&#28436;&#21270;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#20114;&#21033;&#30410;&#26045;&#21152;&#24378;&#28872;&#36873;&#25321;&#21387;&#21147;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26234;&#33021;&#20307;&#27169;&#22411;&#65292;&#20854;&#20013;&#21512;&#20316;&#34892;&#20026;&#30456;&#20851;&#20010;&#24615;&#29305;&#24449;&#30340;&#35821;&#35328;&#25551;&#36848;&#34987;&#29992;&#20316;&#22522;&#22240;&#65292;&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#21462;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26681;&#25454;&#36825;&#20123;&#20010;&#24615;&#29305;&#24449;&#20570;&#20986;&#34892;&#20026;&#20915;&#31574;&#34987;&#29992;&#20316;&#34892;&#20026;&#29305;&#24449;&#12290;&#26681;&#25454;&#24179;&#22343;&#25910;&#30410;&#30340;&#36873;&#25321;&#21644;&#36890;&#36807;&#35831;&#27714;LLM&#23545;&#29238;&#22522;&#22240;&#36827;&#34892;&#30053;&#24494;&#20462;&#25913;&#23454;&#29616;&#22522;&#22240;&#30340;&#31361;&#21464;&#65292;&#25105;&#20204;&#20351;&#32676;&#20307;&#36827;&#21270;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#30830;&#23454;&#21487;&#20197;&#23637;&#31034;&#28436;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to shed light on the evolutionary dynamics of diverse and social populations by introducing the rich expressiveness of generative models into the trait expression of social agent-based evolutionary models. Specifically, we focus on the evolution of personality traits in the context of a game-theoretic relationship as a situation in which inter-individual interests exert strong selection pressures. We construct an agent model in which linguistic descriptions of personality traits related to cooperative behavior are used as genes. The deterministic strategies extracted from Large Language Model (LLM) that make behavioral decisions based on these personality traits are used as behavioral traits. The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish. Through preliminary experiments and analyses, we clarify that such a model can indeed exhibit the evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23884;&#20837;&#26041;&#27861;&#21644;&#35789;&#35821;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35780;&#35770;&#20043;&#38388;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#35780;&#35770;&#20013;&#30340;&#25991;&#26412;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#27835;&#23478;&#21644;&#21830;&#19994;&#20195;&#34920;&#33021;&#22815;&#36861;&#36394;&#20840;&#29699;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#24773;&#24863;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.05964</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#34913;&#37327;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#23884;&#20837;&#26041;&#27861;: &#25581;&#31034;&#22312;&#32447;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#21644;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments. (arXiv:2310.05964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23884;&#20837;&#26041;&#27861;&#21644;&#35789;&#35821;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35780;&#35770;&#20043;&#38388;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#35780;&#35770;&#20013;&#30340;&#25991;&#26412;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#27835;&#23478;&#21644;&#21830;&#19994;&#20195;&#34920;&#33021;&#22815;&#36861;&#36394;&#20840;&#29699;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#24773;&#24863;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#22330;&#23548;&#33268;&#20114;&#32852;&#32593;&#20351;&#29992;&#37327;&#22686;&#38271;70%&#30340;&#22823;&#27969;&#34892;&#30149;&#20043;&#21518;&#65292;&#20840;&#19990;&#30028;&#30340;&#20154;&#20204;&#24320;&#22987;&#26356;&#22810;&#22320;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#12290;Twitter&#12289;Meta Threads&#12289;YouTube&#21644;Reddit&#31561;&#24212;&#29992;&#31243;&#24207;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#25968;&#23383;&#31354;&#38388;&#19981;&#34920;&#36798;&#20844;&#20247;&#24847;&#35265;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35780;&#35770;&#20043;&#38388;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#19981;&#21516;&#23186;&#20307;&#24179;&#21488;&#19978;&#20849;&#20139;&#24847;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#20998;&#26512;&#21477;&#23376;&#21644;&#25991;&#26723;&#20013;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#20351;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#27835;&#23478;&#21644;&#21830;&#19994;&#20195;&#34920;&#33021;&#22815;&#36861;&#36394;&#20840;&#29699;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#24773;&#24863;&#30340;&#36335;&#24452;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#34913;&#37327;&#20174;&#36825;&#20123;&#28909;&#38376;&#22312;&#32447;&#24179;&#21488;&#30340;&#29992;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25429;&#25417;&#35789;&#35821;&#38388;&#35821;&#20041;&#20851;&#31995;&#24182;&#26377;&#21161;&#20110;&#20998;&#26512;&#32593;&#32476;&#24773;&#24863;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
After a pandemic that caused internet usage to grow by 70%, there has been an increased number of people all across the world using social media. Applications like Twitter, Meta Threads, YouTube, and Reddit have become increasingly pervasive, leaving almost no digital space where public opinion is not expressed. This paper investigates sentiment and semantic relationships among comments across various social media platforms, as well as discusses the importance of shared opinions across these different media platforms, using word embeddings to analyze components in sentences and documents. It allows researchers, politicians, and business representatives to trace a path of shared sentiment among users across the world. This research paper presents multiple approaches that measure the relatedness of text extracted from user comments on these popular online platforms. By leveraging embeddings, which capture semantic relationships between words and help analyze sentiments across the web, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#21487;&#20197;&#36731;&#26494;&#25171;&#30772;&#21442;&#19982;&#32773;&#21311;&#21517;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23454;&#38469;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2310.05960</link><description>&lt;p&gt;
&#25351;&#32441;&#25915;&#20987;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#21435;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fingerprint Attack: Client De-Anonymization in Federated Learning. (arXiv:2310.05960v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#21487;&#20197;&#36731;&#26494;&#25171;&#30772;&#21442;&#19982;&#32773;&#21311;&#21517;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23454;&#38469;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22312;&#21442;&#19982;&#32773;&#19981;&#30456;&#20449;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#24444;&#27492;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36890;&#36807;&#30830;&#20445;&#21442;&#19982;&#32773;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#32463;&#36807;&#28151;&#27927;&#65292;&#23558;&#21442;&#19982;&#32773;&#36523;&#20221;&#19982;&#20854;&#25968;&#25454;&#20998;&#31163;&#65292;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#38544;&#31169;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#26469;&#26816;&#39564;&#36825;&#31181;&#38450;&#24481;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21311;&#21517;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#30340;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#26799;&#24230;&#32858;&#31867;&#21487;&#20197;&#36731;&#26494;&#22320;&#25171;&#30772;&#21311;&#21517;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23545;&#25105;&#20204;&#30340;&#25351;&#32441;&#25915;&#20987;&#30340;&#23454;&#38469;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning allows collaborative training without data sharing in settings where participants do not trust the central server and one another. Privacy can be further improved by ensuring that communication between the participants and the server is anonymized through a shuffle; decoupling the participant identity from their data. This paper seeks to examine whether such a defense is adequate to guarantee anonymity, by proposing a novel fingerprinting attack over gradients sent by the participants to the server. We show that clustering of gradients can easily break the anonymization in an empirical study of learning federated language models on two language corpora. We then show that training with differential privacy can provide a practical defense against our fingerprint attack.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25506;&#32034;&#20102;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20026;&#35745;&#31639;&#26426;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#22312;&#39118;&#38505;&#35780;&#20272;&#31561;&#26041;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.05935</link><description>&lt;p&gt;
&#28431;&#27934;&#32858;&#31867;&#19982;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#30340;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings. (arXiv:2310.05935v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25506;&#32034;&#20102;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20026;&#35745;&#31639;&#26426;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#22312;&#39118;&#38505;&#35780;&#20272;&#31561;&#26041;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#23433;&#20840;&#28431;&#27934;&#36890;&#24120;&#20197;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24418;&#24335;&#21457;&#24067;&#65288;&#20363;&#22914;MITRE&#30340;CVE&#21015;&#34920;&#65289;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#36825;&#20123;&#25551;&#36848;&#20250;&#36827;&#19968;&#27493;&#36890;&#36807;&#24120;&#35265;&#28431;&#27934;&#35780;&#20998;&#31995;&#32479;&#65288;CVSS&#65289;&#31561;&#26631;&#31614;&#36827;&#34892;&#25163;&#21160;&#34917;&#20805;&#12290;&#22312;&#28431;&#27934;AI&#65288;&#20998;&#26512;&#19982;&#24773;&#25253;&#65289;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#28431;&#27934;&#31354;&#38388;&#30340;&#31616;&#27905;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#23427;&#20204;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20197;&#25903;&#25345;&#35745;&#31639;&#26426;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#22312;&#39118;&#38505;&#35780;&#20272;&#21644;&#20854;&#20182;&#30456;&#20851;&#27963;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#26412;&#25253;&#21578;&#20013;&#31616;&#35201;&#24635;&#32467;&#20102;&#25105;&#20204;&#25506;&#32034;&#30340;&#29305;&#23450;&#24212;&#29992;&#65292;&#21253;&#25324;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#35780;&#20272;&#28431;&#27934;&#31354;&#38388;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber-security vulnerabilities are usually published in form of short natural language descriptions (e.g., in form of MITRE's CVE list) that over time are further manually enriched with labels such as those defined by the Common Vulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and Intelligence) project, we investigated different types of semantic vulnerability embeddings based on natural language processing (NLP) techniques to obtain a concise representation of the vulnerability space. We also evaluated their use as a foundation for machine learning applications that can support cyber-security researchers and analysts in risk assessment and other related activities. The particular applications we explored and briefly summarize in this report are clustering, classification, and visualization, as well as a new logic-based approach to evaluate theories about the vulnerability space.
&lt;/p&gt;</description></item><item><title>NEFTune &#26159;&#19968;&#31181;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#26377;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.05914</link><description>&lt;p&gt;
NEFTune: &#22122;&#22768;&#23884;&#20837;&#25913;&#36827;&#25351;&#20196;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NEFTune: Noisy Embeddings Improve Instruction Finetuning. (arXiv:2310.05914v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05914
&lt;/p&gt;
&lt;p&gt;
NEFTune &#26159;&#19968;&#31181;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;NEFTune &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#12290;&#22312;&#20351;&#29992; Alpaca &#36827;&#34892;&#26631;&#20934;&#24494;&#35843;&#30340;&#22522;&#30784;&#19978;&#65292;LLaMA-2-7B &#22312; AlpacaEval &#19978;&#30340;&#20934;&#30830;&#29575;&#20174; 29.79% &#25552;&#21319;&#21040;&#20102; 64.69%&#12290;NEFTune &#22312;&#29616;&#20195;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#20063;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#20351;&#29992; Evol-Instruct &#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#65292;ShareGPT &#25552;&#21319;&#20102;8%&#65292;OpenPlatypus &#25552;&#21319;&#20102;8%&#12290;&#21363;&#20351;&#26159;&#32463;&#36807; RLHF &#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#22914; LLaMA-2-Chat&#65292;&#20063;&#21487;&#20197;&#36890;&#36807; NEFTune &#30340;&#38468;&#21152;&#35757;&#32451;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05364</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22320;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20174;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#30830;&#23450;&#31561;&#20215;&#30340;&#23454;&#20307;&#23545;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#22823;&#22810;&#25968;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#25506;&#32034;&#12290;&#23569;&#25968;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#19981;&#38169;&#30340;&#23581;&#35797;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(1)&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#19988;&#20302;&#25928;&#65292;&#20026;&#27599;&#20010;&#27169;&#24577;&#35774;&#35745;&#22797;&#26434;&#21644;&#29420;&#31435;&#30340;&#27169;&#22411;&#65307;(2)&#30001;&#20110;&#23454;&#20307;&#23545;&#40784;&#20013;&#27169;&#24577;&#30340;&#24322;&#26500;&#24615;&#65292;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PathFusion&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;(1) MSP&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36830;&#25509;&#23454;&#20307;&#21644;&#27169;&#24577;&#33410;&#28857;&#20197;&#34920;&#31034;&#22810;&#20010;&#27169;&#24577;&#30340;&#36335;&#24452;&#65292;&#31616;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#65307;(2) IRF&#65292;&#19968;&#31181;&#36845;&#20195;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36335;&#24452;&#20316;&#20026;&#20449;&#24687;&#36733;&#20307;&#65292;&#26377;&#25928;&#22320;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>ChatRadio-Valuer&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#24182;&#20026;&#27169;&#22411;&#33258;&#36866;&#24212;&#25552;&#20379;&#22522;&#30784;&#27169;&#24335;&#65292;&#35299;&#20915;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05242</link><description>&lt;p&gt;
ChatRadio-Valuer: &#19968;&#31181;&#22522;&#20110;&#22810;&#26426;&#26500;&#21644;&#22810;&#31995;&#32479;&#25968;&#25454;&#30340;&#36890;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#30340;&#32842;&#22825;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data. (arXiv:2310.05242v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05242
&lt;/p&gt;
&lt;p&gt;
ChatRadio-Valuer&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#24182;&#20026;&#27169;&#22411;&#33258;&#36866;&#24212;&#25552;&#20379;&#22522;&#30784;&#27169;&#24335;&#65292;&#35299;&#20915;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23545;&#20020;&#24202;&#20915;&#31574;&#27700;&#24179;&#30340;&#23450;&#37327;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#20132;&#21449;&#26469;&#28304;&#24322;&#36136;&#24615;&#30340;&#22797;&#26434;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#23545;&#24403;&#21069;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#37327;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#39118;&#26684;&#21644;&#35268;&#33539;&#24615;&#22312;&#26426;&#26500;&#12289;&#26816;&#26597;&#37096;&#20301;&#21644;&#25918;&#23556;&#31185;&#21307;&#29983;&#20043;&#38388;&#26126;&#26174;&#26377;&#25152;&#19981;&#21516;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20026;&#35782;&#21035;&#20581;&#24247;&#29366;&#20917;&#30340;&#36857;&#35937;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#19982;&#20013;&#22269;&#30340;&#28248;&#38597;&#20108;&#21307;&#38498;&#21512;&#20316;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;ChatRadio-Valuer&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#24182;&#20026;&#22797;&#26434;&#20998;&#26512;&#24072;&#26696;&#20363;&#20013;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#25552;&#20379;&#22522;&#26412;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology report generation, as a key step in medical image analysis, is critical to the quantitative analysis of clinically informed decision-making levels. However, complex and diverse radiology reports with cross-source heterogeneity pose a huge generalizability challenge to the current methods under massive data volume, mainly because the style and normativity of radiology reports are obviously distinctive among institutions, body regions inspected and radiologists. Recently, the advent of large language models (LLM) offers great potential for recognizing signs of health conditions. To resolve the above problem, we collaborate with the Second Xiangya Hospital in China and propose ChatRadio-Valuer based on the LLM, a tailored model for automatic radiology report generation that learns generalizable representations and provides a basis pattern for model adaptation in sophisticated analysts' cases. Specifically, ChatRadio-Valuer is trained based on the radiology reports from a single 
&lt;/p&gt;</description></item><item><title>TILFA&#26159;&#19968;&#20010;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#28151;&#21512;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#20165;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#33021;&#22815;&#26816;&#27979;&#20809;&#23398;&#23383;&#31526;&#21644;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#32454;&#33410;&#65292;&#24182;&#22312;&#36777;&#35770;&#31435;&#22330;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05210</link><description>&lt;p&gt;
TILFA: &#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#34701;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining. (arXiv:2310.05210v1 [cs.AI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05210
&lt;/p&gt;
&lt;p&gt;
TILFA&#26159;&#19968;&#20010;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#28151;&#21512;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#20165;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#33021;&#22815;&#26816;&#27979;&#20809;&#23398;&#23383;&#31526;&#21644;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#32454;&#33410;&#65292;&#24182;&#22312;&#36777;&#35770;&#31435;&#22330;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#25366;&#25496;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20998;&#26512;&#20316;&#32773;&#30340;&#31435;&#22330;&#12290;&#19982;&#20197;&#24448;&#21482;&#20851;&#27880;&#25991;&#26412;&#30340;&#35770;&#35777;&#25366;&#25496;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#31532;10&#23626;&#35770;&#35777;&#25366;&#25496;&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#22270;&#20687;&#21253;&#21547;&#20102;&#35270;&#35273;&#20803;&#32032;&#21644;&#20809;&#23398;&#23383;&#31526;&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;TILFA&#65288;&#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#20013;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#34701;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;&#65289;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#28151;&#21512;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#33021;&#22815;&#26816;&#27979;&#20809;&#23398;&#23383;&#31526;&#21644;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#22312;&#36825;&#20010;&#20849;&#20139;&#20219;&#21153;&#30340;&#36777;&#35770;&#31435;&#22330;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;KnowComp&#22312;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike previous AM datasets focusing only on text, the shared task at the 10th Workshop on Argument Mining introduces a dataset including both text and images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework for Text, Image, and Layout Fusion in Argument Mining), is designed to handle this mixed data. It excels at not only understanding text but also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#34987;&#24694;&#24847;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38656;&#35201;&#20174;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#30740;&#31350;&#19982;&#25919;&#31574;&#30028;&#37319;&#21462;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#20107;&#23454;&#24615;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Factuality Challenges in the Era of Large Language Models. (arXiv:2310.05189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05189
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#34987;&#24694;&#24847;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38656;&#35201;&#20174;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#30740;&#31350;&#19982;&#25919;&#31574;&#30028;&#37319;&#21462;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#24494;&#36719;&#30340;Bing Chat&#21644;&#35895;&#27468;&#30340;Bard&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#36825;&#20123;&#38750;&#24120;&#26377;&#29992;&#12289;&#33258;&#28982;&#30340;&#24037;&#20855;&#26631;&#24535;&#30528;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#20542;&#21521;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#34987;&#29992;&#20110;&#24694;&#24847;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#34394;&#20551;&#20294;&#21487;&#20449;&#30340;&#20869;&#23481;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#36825;&#23545;&#20110;&#31038;&#20250;&#26469;&#35828;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#27450;&#39575;&#29992;&#25143;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#20256;&#25773;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#37492;&#20110;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#25919;&#31574;&#30028;&#38656;&#35201;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#30340;&#31867;&#22411;&#12290;&#36890;&#36807;&#30830;&#23450;&#39118;&#38505;&#12289;&#36843;&#22312;&#30473;&#30571;&#30340;&#23041;&#32961;&#21644;&#19968;&#20123;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to she
&lt;/p&gt;</description></item><item><title>DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.05074</link><description>&lt;p&gt;
DialCoT&#36935;&#21040;&#20102;PPO&#65306;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05074
&lt;/p&gt;
&lt;p&gt;
DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#22686;&#24378;&#33267;&#23569;&#20855;&#26377;1000&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21040;100&#20159;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#23427;&#26159;&#26080;&#25928;&#29978;&#33267;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#65288;DialCoT&#65289;&#65292;&#23427;&#37319;&#29992;&#23545;&#35805;&#26684;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24341;&#23548;&#27169;&#22411;&#26397;&#30528;&#26368;&#32456;&#31572;&#26696;&#21069;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20110;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#20351;&#20854;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.05036</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#31574;&#30053;&#65306;&#35780;&#20272;&#22312;Avalon&#28216;&#25103;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
From Text to Tactic: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29609;&#31574;&#30053;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;Resistance Avalon&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#29609;&#23478;&#19981;&#20165;&#38656;&#35201;&#26681;&#25454;&#21160;&#24577;&#21457;&#23637;&#30340;&#28216;&#25103;&#38454;&#27573;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#36824;&#38656;&#35201;&#21442;&#19982;&#35752;&#35770;&#65292;&#22312;&#35752;&#35770;&#20013;&#24517;&#39035;&#27450;&#39575;&#12289;&#25512;&#29702;&#21644;&#19982;&#20854;&#20182;&#29609;&#23478;&#36827;&#34892;&#35848;&#21028;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;Avalon&#25104;&#20026;&#30740;&#31350;LLM&#20195;&#29702;&#30340;&#20915;&#31574;&#21644;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#26377;&#36259;&#35797;&#39564;&#24179;&#21488;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AvalonBench&#8212;&#8212;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#30340;&#20840;&#38754;&#28216;&#25103;&#29615;&#22659;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#65306;&#65288;1&#65289;Avalon&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#65288;2&#65289;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#20316;&#20026;&#22522;&#20934;&#23545;&#25163;&#65292;&#20197;&#21450;&#65288;3&#65289;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#20855;&#26377;&#23450;&#21046;&#25552;&#31034;&#30340;ReAct-style LLM&#20195;&#29702;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;AvalonBench&#30340;&#35780;&#20272;&#31361;&#20986;&#26174;&#31034;&#20102;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#20687;ChatGPT&#36825;&#26679;&#22312;&#22909;&#35282;&#33394;&#20013;&#30340;&#27169;&#22411;&#23545;&#25112;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#30340;&#32988;&#29575;&#20026;22.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots play
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#24605;&#21644;&#25913;&#36827;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#25552;&#21319;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05035</link><description>&lt;p&gt;
&#33258;&#25105;&#39564;&#35777;&#25552;&#31034;&#65306;&#21033;&#29992;&#37325;&#22797;&#20869;&#30465;&#36827;&#34892;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection. (arXiv:2310.05035v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#24605;&#21644;&#25913;&#36827;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#25552;&#21319;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20687;ChatGPT&#21644;PaLM&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#32321;&#29712;&#30693;&#35782;&#21033;&#29992;&#26041;&#38754;&#20173;&#28982;&#19981;&#21450;&#20154;&#31867;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#25552;&#31034;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26399;&#26395;&#30340;&#36755;&#20986;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#32452;&#20214;&#65306;\textit{Normal CoT}&#12289;\textit{Convincer}&#21644;\textit{Answerer}&#12290;&#23427;&#22788;&#29702;typical few-shot chain-of-thought prompt&#30340;&#36755;&#20986;&#65292;&#35780;&#20272;&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#65292;&#23457;&#26597;&#31572;&#26696;&#65292;&#25913;&#36827;&#25512;&#29702;&#65292;&#26368;&#32456;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19971;&#20010;&#21508;&#31181;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#33258;&#25105;&#39564;&#35777;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) such as ChatGPT and PaLM have demonstrated remarkable performance in various language understanding and generation tasks, their capabilities in complex reasoning and intricate knowledge utilization still fall short of human-level proficiency. Recent studies have established the effectiveness of prompts in steering LLMs towards generating desired outputs. Building on these insights, we introduce a novel framework that harnesses the potential of large-scale pre-trained language models, to iteratively enhance performance of the LLMs. Our framework incorporates three components: \textit{Normal CoT}, a \textit{Convincer}, and an \textit{Answerer}. It processes the output of a typical few-shot chain-of-thought prompt, assesses the correctness of the response, scrutinizes the answer, refines the reasoning, and ultimately produces a new solution. Experimental results on the 7 datasets of miscellaneous problems validate the efficacy of the Self-Convince framew
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05028</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#21363;&#20351;&#22312;&#38646;-shot&#35774;&#23450;&#19979;&#65292;&#19968;&#30452;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#33258;&#21160;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23558;LLMs&#65292;&#22914;ChatGPT&#65292;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#30740;&#31350;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;RE&#25552;&#31034;&#30340;&#32570;&#28857;&#65292;&#24182;&#23581;&#35797;&#23558;&#26368;&#36817;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;CoT&#65292;&#32435;&#20837;&#20854;&#20013;&#20197;&#25552;&#39640;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#36882;&#24402;&#20351;&#29992;LLMs&#23558;RE&#36755;&#20837;&#36716;&#25442;&#20026;&#26377;&#25928;&#30340;&#38382;&#31572;(QA)&#26684;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#35774;&#32622;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;LLMs&#22312;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#19978;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26377;&#20197;&#19979;&#30340;followi
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
&lt;/p&gt;</description></item><item><title>DORIS-MAE&#26159;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22810;&#26041;&#38754;&#26597;&#35810;&#12290;&#30740;&#31350;&#22242;&#38431;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39564;&#35777;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.04678</link><description>&lt;p&gt;
DORIS-MAE: &#20351;&#29992;&#22810;&#23618;&#32423;&#22522;&#20110;&#26041;&#38754;&#30340;&#26597;&#35810;&#36827;&#34892;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries. (arXiv:2310.04678v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04678
&lt;/p&gt;
&lt;p&gt;
DORIS-MAE&#26159;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22810;&#26041;&#38754;&#26597;&#35810;&#12290;&#30740;&#31350;&#22242;&#38431;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39564;&#35777;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#26681;&#25454;&#22797;&#26434;&#30340;&#22810;&#26041;&#38754;&#26597;&#35810;&#26377;&#25928;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21463;&#38480;&#20110;&#27880;&#37322;&#22797;&#26434;&#26597;&#35810;&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#21644;&#21162;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;&#20013;&#20351;&#29992;&#22810;&#23618;&#32423;&#22522;&#20110;&#26041;&#38754;&#30340;&#26597;&#35810;(DORIS-MAE)&#65292;&#26088;&#22312;&#22788;&#29702;&#31185;&#23398;&#30740;&#31350;&#20013;&#29992;&#25143;&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#20869;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;100&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#22797;&#26434;&#26597;&#35810;&#26696;&#20363;&#12290;&#23545;&#20110;&#27599;&#20010;&#22797;&#26434;&#26597;&#35810;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;100&#20010;&#30456;&#20851;&#25991;&#26723;&#38598;&#21512;&#65292;&#24182;&#20026;&#20854;&#25490;&#21517;&#20135;&#29983;&#20102;&#27880;&#37322;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#12290;&#37492;&#20110;&#19987;&#23478;&#27880;&#37322;&#30340;&#24040;&#22823;&#24037;&#20316;&#37327;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Anno-GPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19987;&#23478;&#32423;&#25968;&#25454;&#38598;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation o
&lt;/p&gt;</description></item><item><title>Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04484</link><description>&lt;p&gt;
Ada-Instruct: &#20026;&#22797;&#26434;&#25512;&#29702;&#35843;&#25972;&#25351;&#20196;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04484
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#25351;&#20196;&#23545;&#20110;&#25512;&#36827;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#38381;&#28304;&#30340;LLMs&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#21457;&#29616;&#23545;&#20110;&#35832;&#22914;&#20195;&#30721;&#34917;&#20840;&#31561;&#20219;&#21153;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#26080;&#27861;&#29983;&#25104;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#22797;&#26434;&#25351;&#20196;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;Ada-Instruct&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#21313;&#20010;&#26679;&#26412;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#21363;&#21487;&#29983;&#25104;&#20445;&#25345;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#38271;&#25351;&#20196;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#19981;&#21516;&#24212;&#29992;&#20013;&#23545;Ada-Instruct&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;Ada-Instruct&#20248;&#20110;&#20854;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#30340;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.  To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct method
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25776;&#20889;&#21644;&#35780;&#35770;&#35843;&#26597;&#35770;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#32452;&#32455;&#20102;&#19968;&#27425;&#31454;&#36187;&#26469;&#27979;&#35797;&#35813;&#24179;&#21488;&#12290;&#35780;&#20272;&#26631;&#20934;&#21253;&#25324;&#28165;&#26224;&#24230;&#12289;&#21442;&#32771;&#36866;&#24403;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#20869;&#23481;&#30340;&#23454;&#36136;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.04480</link><description>&lt;p&gt;
&#33258;&#21160;&#35843;&#26597;&#25361;&#25112;&#12290;&#65288;arXiv:2310.04480v2 [cs.CL]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Auto-survey Challenge. (arXiv:2310.04480v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25776;&#20889;&#21644;&#35780;&#35770;&#35843;&#26597;&#35770;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#32452;&#32455;&#20102;&#19968;&#27425;&#31454;&#36187;&#26469;&#27979;&#35797;&#35813;&#24179;&#21488;&#12290;&#35780;&#20272;&#26631;&#20934;&#21253;&#25324;&#28165;&#26224;&#24230;&#12289;&#21442;&#32771;&#36866;&#24403;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#20869;&#23481;&#30340;&#23454;&#36136;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#65292;&#21253;&#25324;&#31185;&#23398;&#12289;&#20154;&#25991;&#12289;&#25945;&#32946;&#21644;&#27861;&#24459;&#20013;&#65292;&#33258;&#20027;&#25776;&#20889;&#21644;&#35780;&#35770;&#35843;&#26597;&#35770;&#25991;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#31867;&#20284;&#20110;&#20256;&#32479;&#23398;&#26415;&#26399;&#21002;&#30340;&#27169;&#25311;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#65292;&#32780;&#20154;&#31867;&#32452;&#32455;&#32773;&#21017;&#20805;&#24403;&#32534;&#36753;&#30417;&#30563;&#35282;&#33394;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#27425;&#38024;&#23545;2023&#24180;AutoML&#20250;&#35758;&#30340;&#31454;&#36187;&#12290;&#21442;&#36187;&#32773;&#30340;&#20219;&#21153;&#26159;&#21576;&#29616;&#33021;&#22815;&#26681;&#25454;&#25351;&#23450;&#25552;&#31034;&#25776;&#20889;&#25991;&#31456;&#24182;&#36827;&#34892;&#35780;&#20272;&#30340;&#29420;&#31435;&#27169;&#22411;&#12290;&#35780;&#20272;&#26631;&#20934;&#21253;&#25324;&#28165;&#26224;&#24230;&#12289;&#21442;&#32771;&#36866;&#24403;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#21644;&#20869;&#23481;&#30340;&#23454;&#36136;&#20215;&#20540;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31454;&#36187;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#23454;&#26045;&#22522;&#20934;&#25552;&#20132;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04270</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;6&#20010;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;26&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#28909;&#38376;LLMs&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#38646;&#26679;&#26412;LLMs&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#27809;&#26377;&#19968;&#20010;LLM&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.03951</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#26681;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03951
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#23450;&#30456;&#20851;&#25991;&#26723;&#20316;&#20026;&#32972;&#26223;&#19978;&#19979;&#25991;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29983;&#25104;&#27969;&#21033;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#36825;&#31181;&#33021;&#21147;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;LLMs&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#27809;&#26377;&#25552;&#20379;&#26469;&#28304;&#25903;&#25345;&#30340;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#36825;&#31181;&#26080;&#26681;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#21518;&#26399;&#32534;&#36753;&#36827;&#34892;&#24187;&#35273;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#37325;&#20889;&#22686;&#24378;&#25991;&#26412;&#36136;&#37327;&#65292;&#20351;&#29992;LLMs&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#30340;&#26377;&#25928;&#36873;&#25321;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.03304</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#26816;&#32034;&#31561;&#26356;&#23458;&#35266;&#30340;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#35780;&#20272;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#21253;&#25324;&#65288;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65307;&#65288;2&#65289;&#22810;&#32500;&#35780;&#20272;&#26631;&#20934;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26469;&#33258;&#35780;&#23457;&#20154;&#21592;&#20010;&#20154;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#20010;&#26080;&#27745;&#26579;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#35780;&#20272;&#20013;&#24314;&#27169;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#21644;&#26032;&#30340;&#20010;&#24615;&#21270;&#26631;&#31614;&#65292;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Per-MPST&#21644;Per-DOC&#29992;&#20110;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;PERSE&#26469;&#25512;&#27979;&#35780;&#23457;&#20154;&#21592;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26576;&#20010;&#35780;&#23457;&#20154;&#21592;&#30340;&#19968;&#20123;&#31034;&#20363;&#35780;&#20215;&#65292;PERSE&#21487;&#20197;&#39044;&#27979;&#35813;&#35780;&#23457;&#20154;&#21592;&#22312;&#26032;&#30340;&#24773;&#33410;&#19978;&#30340;&#35814;&#32454;&#35780;&#23457;&#25110;&#32454;&#31890;&#24230;&#27604;&#36739;&#65288;&#22914;&#36259;&#21619;&#24615;&#21644;&#24778;&#21916;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.01444</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#27969;&#20351;LLM&#20195;&#29702;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#20154;&#31867;&#21270;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24110;&#21161;&#36825;&#20123;&#20195;&#29702;&#22312;&#27809;&#26377;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;LLM&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#25506;&#32034;&#21644;PPO&#35757;&#32451;&#65292;LTC&#20351;&#20195;&#29702;&#33021;&#22815;&#23558;&#30701;&#26399;&#32463;&#39564;&#34701;&#20837;&#38271;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#32467;&#26500;&#21270;&#30340;&#36890;&#20449;&#27169;&#24335;&#65306;&#29420;&#30333;&#65292;&#23545;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#39046;&#22495;&#29305;&#23450;&#26631;&#31614;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#21009;&#20107;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20511;&#21161;&#27861;&#24459;&#19987;&#23478;&#22242;&#38431;&#21644;&#27861;&#24459;&#30693;&#35782;&#30340;&#27880;&#37322;&#65292;&#21487;&#20197;&#22686;&#24378;&#27861;&#24459;&#26696;&#20363;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01271</link><description>&lt;p&gt;
LEEC: &#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#39046;&#22495;&#29305;&#23450;&#26631;&#31614;&#31995;&#32479;&#30340;&#27861;&#24459;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LEEC: A Legal Element Extraction Dataset with an Extensive Domain-Specific Label System. (arXiv:2310.01271v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#39046;&#22495;&#29305;&#23450;&#26631;&#31614;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#21009;&#20107;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20511;&#21161;&#27861;&#24459;&#19987;&#23478;&#22242;&#38431;&#21644;&#27861;&#24459;&#30693;&#35782;&#30340;&#27880;&#37322;&#65292;&#21487;&#20197;&#22686;&#24378;&#27861;&#24459;&#26696;&#20363;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#35201;&#32032;&#25552;&#21462;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20174;&#21496;&#27861;&#25991;&#20214;&#20013;&#25552;&#21462;&#27861;&#24459;&#35201;&#32032;&#26377;&#21161;&#20110;&#22686;&#24378;&#27861;&#24459;&#26696;&#20363;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#27861;&#24459;&#39046;&#22495;&#21508;&#20010;&#39046;&#22495;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#23384;&#22312;&#23545;&#27861;&#24459;&#30693;&#35782;&#30340;&#21463;&#38480;&#35775;&#38382;&#21644;&#26631;&#31614;&#35206;&#30422;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#12289;&#22823;&#35268;&#27169;&#30340;&#21009;&#20107;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;15,831&#20010;&#21496;&#27861;&#25991;&#20214;&#21644;159&#20010;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#26500;&#24314;&#65306;&#39318;&#20808;&#65292;&#30001;&#25105;&#20204;&#30340;&#27861;&#24459;&#19987;&#23478;&#22242;&#38431;&#26681;&#25454;&#20808;&#21069;&#30340;&#27861;&#24459;&#30740;&#31350;&#35774;&#35745;&#20102;&#26631;&#31614;&#31995;&#32479;&#65292;&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#21009;&#20107;&#26696;&#20214;&#20013;&#24433;&#21709;&#21028;&#20915;&#32467;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#21644;&#29983;&#25104;&#36807;&#31243;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#27861;&#24459;&#30693;&#35782;&#26681;&#25454;&#26631;&#31614;&#31995;&#32479;&#21644;&#27880;&#37322;&#20934;&#21017;&#23545;&#21496;&#27861;&#25991;&#20214;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a pivotal task in natural language processing, element extraction has gained significance in the legal domain. Extracting legal elements from judicial documents helps enhance interpretative and analytical capacities of legal cases, and thereby facilitating a wide array of downstream applications in various domains of law. Yet existing element extraction datasets are limited by their restricted access to legal knowledge and insufficient coverage of labels. To address this shortfall, we introduce a more comprehensive, large-scale criminal element extraction dataset, comprising 15,831 judicial documents and 159 labels. This dataset was constructed through two main steps: first, designing the label system by our team of legal experts based on prior legal research which identified critical factors driving and processes generating sentencing outcomes in criminal cases; second, employing the legal knowledge to annotate judicial documents according to the label system and annotation guideli
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.00322</link><description>&lt;p&gt;
&#32418;&#38431;&#28216;&#25103;&#65306;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37096;&#32626;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24517;&#39035;&#31526;&#21512;&#26377;&#30410;&#21644;&#26080;&#23475;&#24615;&#30340;&#26631;&#20934;&#65292;&#20174;&#32780;&#23454;&#29616;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#32418;&#38431;&#25216;&#26415;&#26159;&#23454;&#29616;&#36825;&#19968;&#26631;&#20934;&#30340;&#20851;&#38190;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20381;&#36182;&#20110;&#25163;&#21160;&#32418;&#38431;&#35774;&#35745;&#21644;&#21551;&#21457;&#24335;&#23545;&#25239;&#25552;&#31034;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#65292;&#38480;&#21046;&#20102;&#22312;&#21487;&#37327;&#21270;&#24230;&#37327;&#21644;&#25910;&#25947;&#20445;&#35777;&#19979;&#23545;LLM&#36827;&#34892;&#22810;&#26679;&#25915;&#20987;&#31574;&#30053;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#12290;RTG&#26088;&#22312;&#20998;&#26512;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#22312;RTG&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#35821;&#20041;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#12290;GRTS&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#32418;&#38431;&#28216;&#25103;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#34920;&#38754;&#37325;&#22797;&#29616;&#35937;&#30340;&#35282;&#24230;&#26469;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23454;&#35777;&#20102;&#19968;&#31181;&#22686;&#24378;&#26631;&#35760;&#20851;&#31995;&#30340;&#21407;&#21017;&#65292;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#21450;&#20854;&#28508;&#22312;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.00297</link><description>&lt;p&gt;
&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#37325;&#22797;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning from Repetitions. (arXiv:2310.00297v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#34920;&#38754;&#37325;&#22797;&#29616;&#35937;&#30340;&#35282;&#24230;&#26469;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23454;&#35777;&#20102;&#19968;&#31181;&#22686;&#24378;&#26631;&#35760;&#20851;&#31995;&#30340;&#21407;&#21017;&#65292;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#21450;&#20854;&#28508;&#22312;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38590;&#20197;&#25417;&#25720;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#34920;&#38754;&#37325;&#22797;&#29616;&#35937;&#30340;&#35282;&#24230;&#26469;&#26816;&#35270;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#34920;&#38754;&#29305;&#24449;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#23454;&#35777;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;&#26631;&#35760;&#20849;&#29616;&#24378;&#21270;&#8221;&#30340;&#21407;&#21017;&#65292;&#35813;&#21407;&#21017;&#36890;&#36807;&#22686;&#24378;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#22522;&#20110;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#20849;&#29616;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#29305;&#24449;&#30340;&#21452;&#37325;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#38416;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#24182;&#23545;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#23545;&#20110;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#21450;&#20854;&#28508;&#22312;&#23616;&#38480;&#24615;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16583</link><description>&lt;p&gt;
GPT-Fathom&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35299;&#26512;GPT-4&#21450;&#20854;&#21518;&#32493;&#29256;&#26412;&#30340;&#28436;&#21270;&#36335;&#24452;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16583
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#29616;&#26377;&#30340;LLM&#25490;&#34892;&#27036;&#36890;&#24120;&#21442;&#32771;&#20854;&#20182;&#35770;&#25991;&#20013;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#35774;&#32622;&#21644;&#25552;&#31034;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#38388;&#40723;&#21169;&#36873;&#25321;&#26377;&#21033;&#30340;&#35774;&#32622;&#21644;&#25552;&#31034;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GPT-Fathom&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Evals&#26500;&#24314;&#30340;&#24320;&#28304;&#21644;&#21487;&#37325;&#22797;&#30340;LLM&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;&#23545;&#40784;&#30340;&#29615;&#22659;&#35774;&#32622;&#19979;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;LLMs&#20197;&#21450;OpenAI&#30340;&#20256;&#32479;&#27169;&#22411;&#22312;20&#22810;&#20010;&#31934;&#36873;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#28085;&#30422;&#20102;7&#20010;&#33021;&#21147;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;OpenAI&#26089;&#26399;&#27169;&#22411;&#30340;&#22238;&#39038;&#24615;&#30740;&#31350;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#20174;GPT-3&#21040;GPT-4&#30340;&#28436;&#21270;&#36335;&#24452;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#28212;&#26395;&#20102;&#35299;GPT-3&#22914;&#20309;&#36880;&#27493;&#25913;&#36827;&#21040;GPT-4&#65292;&#21253;&#25324;&#20687;&#28155;&#21152;&#20195;&#30721;&#25968;&#25454;&#26159;&#21542;&#25552;&#39640;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;LLM&#33021;&#21147;&#30340;&#21738;&#20123;&#26041;&#38754;&#31561;&#25216;&#26415;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15649</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#35753;LLMs&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#65288;TAP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#25351;&#20196;&#21644;&#28436;&#31034;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#22806;&#30340;&#20219;&#21153;&#65288;ATIS&#21644;WSJ&#65289;&#19978;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31532;&#19968;&#27425;&#25195;&#25551;&#31995;&#32479;&#21644;&#37325;&#26032;&#35780;&#20998;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#36890;&#36807;&#20923;&#32467;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#36798;&#21040;&#19982;&#39046;&#22495;&#35843;&#20248;&#30340;LMs&#37325;&#26032;&#35780;&#20998;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#25216;&#26415;&#19982;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20302;&#20110;N-best Oracle&#27700;&#24179;&#30340;&#38169;&#35823;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15223</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#31532;&#20108;&#27425;&#37325;&#35780;&#20998;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#39044;&#35757;&#32451;&#38454;&#27573;&#25193;&#23637;&#21644;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37325;&#35780;&#20998;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;0.08%&#65289;&#26469;&#35757;&#32451;&#37325;&#35780;&#20998;&#30340;BERT&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#36825;&#20123;&#25554;&#20837;&#30340;&#30697;&#38453;&#36890;&#36807;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#21028;&#21035;&#24615;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;Rescore-BERT&#65288;LoRB&#65289;&#20307;&#31995;&#32467;&#26500;&#22312;LibriSpeech&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;5.4&#33267;3.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10916</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#33719;&#24471;&#20160;&#20040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#26159;&#36890;&#36807;&#24494;&#23567;&#25200;&#21160;&#26469;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#65292;&#36215;&#21021;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#24320;&#22987;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36755;&#20837;&#25200;&#21160;&#30340;&#25628;&#32034;&#65292;&#20294;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#19968;&#31181;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#12290;&#29305;&#21035;&#26159;&#21069;&#32773;&#30456;&#27604;&#20960;&#20010;&#24378;&#22522;&#20934;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65307;&#27492;&#22806;&#65292;&#23545;&#24433;&#21709;&#20989;&#25968;&#30340;&#26032;&#39062;&#20351;&#29992;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#26681;&#25454;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.10818</link><description>&lt;p&gt;
SlimPajama-DC: &#29702;&#35299;LLM&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
SlimPajama-DC: Understanding Data Combinations for LLM Training. (arXiv:2309.10818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#21508;&#31181;&#25968;&#25454;&#32452;&#21512;&#65288;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;GitHub&#12289;&#22270;&#20070;&#65289;&#23545;&#20854;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;SlimPajama&#26159;&#19968;&#20010;&#32463;&#36807;&#20005;&#26684;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#65292;&#20174;Together&#36129;&#29486;&#30340;1.2T&#20010;token&#30340;RedPajama&#25968;&#25454;&#38598;&#20013;&#31934;&#32454;&#32452;&#21512;&#21644;&#21435;&#37325;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102;627B&#20010;tokens&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#31216;&#20026;SlimPajama-DC&#65292;&#36825;&#26159;&#19968;&#39033;&#26088;&#22312;&#25581;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;SlimPajama&#25152;&#28041;&#21450;&#30340;&#22522;&#26412;&#29305;&#24449;&#21644;&#26368;&#20339;&#23454;&#36341;&#30340;&#32463;&#39564;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#20351;&#29992;SlimPajama&#36827;&#34892;&#30740;&#31350;&#30340;&#36807;&#31243;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#20840;&#23616;&#21435;&#37325; vs. &#23616;&#37096;&#21435;&#37325;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#20840;&#23616;&#21435;&#37325;&#65288;&#36328;&#19981;&#21516;&#25968;&#25454;&#38598;&#28304;&#65289;&#21644;&#23616;&#37096;&#21435;&#37325;&#65288;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#28304;&#20869;&#37096;&#65289;&#23545;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#65288;2&#65289;&#39640;&#36136;&#37327;/&#39640;&#24230;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#22312;&#32452;&#21512;&#20013;&#30340;&#27604;&#20363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#26469;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#24182;&#38477;&#20302;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.09507</link><description>&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pruning Large Language Models via Accuracy Predictor. (arXiv:2309.09507v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#26469;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#24182;&#38477;&#20302;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#65288;&#29978;&#33267;&#26356;&#22810;&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#32473;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;LLMs&#27169;&#22411;&#21387;&#32553;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#20462;&#21098;&#29305;&#24615;&#65292;&#23384;&#22312;&#35832;&#22914;&#22797;&#26434;&#30340;&#20248;&#21270;&#27969;&#31243;&#21644;&#38590;&#20197;&#20445;&#30041;&#27169;&#22411;&#37096;&#20998;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20462;&#21098;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#24314;&#31435;&#19968;&#32452;&#29305;&#23450;&#25968;&#37327;&#30340;&#26550;&#26500;-&#20934;&#30830;&#24615;&#23545;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#38750;&#31070;&#32463;&#27169;&#22411;&#20316;&#20026;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#12290;&#36890;&#36807;&#20351;&#29992;&#20934;&#30830;&#24615;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#20248;&#21270;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;Wikitext2&#21644;PTB&#19978;&#30340;&#22256;&#24785;&#24230;&#65288;PPL&#65289;&#20998;&#21035;&#19979;&#38477;&#20102;9.48%&#21644;5.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#38024;&#23545;ASR&#22522;&#30784;&#27169;&#22411;Whisper&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#24494;&#35843;&#21644;&#36719;&#25552;&#31034;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21464;Whisper&#30340;&#35299;&#30721;&#34892;&#20026;&#65292;&#29983;&#25104;&#20505;&#36873;&#20154;&#23454;&#38469;&#35828;&#20986;&#30340;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2307.09378</link><description>&lt;p&gt;
&#20026;&#21475;&#35821;&#35780;&#20272;&#36866;&#24212;ASR&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#38024;&#23545;ASR&#22522;&#30784;&#27169;&#22411;Whisper&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#24494;&#35843;&#21644;&#36719;&#25552;&#31034;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21464;Whisper&#30340;&#35299;&#30721;&#34892;&#20026;&#65292;&#29983;&#25104;&#20505;&#36873;&#20154;&#23454;&#38469;&#35828;&#20986;&#30340;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21487;&#38752;&#30340;&#21475;&#35821;&#35780;&#20272;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#24213;&#23618;&#30340;ASR&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;ASR&#22522;&#30784;&#27169;&#22411;&#22914;Whisper&#24050;&#32463;&#21487;&#29992;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#20154;&#31867;&#21487;&#35835;&#30340;&#65292;&#25152;&#20197;&#20250;&#28155;&#21152;&#26631;&#28857;&#31526;&#21495;&#65292;&#25968;&#23383;&#21576;&#29616;&#20026;&#38463;&#25289;&#20271;&#25968;&#23383;&#24418;&#24335;&#65292;&#21253;&#25324;&#32553;&#20889;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#20250;&#36339;&#36807;&#36755;&#20986;&#20013;&#30340;&#19981;&#27969;&#30021;&#21644;&#29369;&#35947;&#12290;&#34429;&#28982;&#23545;&#20110;&#21487;&#35835;&#24615;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#23646;&#24615;&#23545;&#20110;&#35780;&#20272;&#20505;&#36873;&#20154;&#30340;&#33021;&#21147;&#21644;&#25552;&#20379;&#21453;&#39304;&#24182;&#19981;&#26377;&#29992;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Whisper&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#24494;&#35843;&#21644;&#36719;&#25552;&#31034;&#24494;&#35843;&#12290;&#22312;&#20844;&#20849;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#33521;&#35821;&#23398;&#20064;&#32773;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;Whisper&#30340;&#35299;&#30721;&#34892;&#20026;&#65292;&#29983;&#25104;&#20505;&#36873;&#20154;&#23454;&#38469;&#35828;&#20986;&#30340;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.08937</link><description>&lt;p&gt;
DocumentNet: &#22312;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#24357;&#21512;&#25968;&#25454;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23500;&#26377;&#35270;&#35273;&#20803;&#32032;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#65288;VDER&#65289;&#65292;&#30001;&#20110;&#22312;&#20225;&#19994;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#38544;&#31169;&#32422;&#26463;&#21644;&#39640;&#26114;&#30340;&#26631;&#27880;&#25104;&#26412;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#19981;&#37325;&#21472;&#23454;&#20307;&#31354;&#38388;&#22952;&#30861;&#20102;&#25991;&#26723;&#31867;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Web&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#20110;VDER&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21517;&#20026;DocumentNet&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25991;&#26723;&#31867;&#22411;&#25110;&#23454;&#20307;&#38598;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;VDER&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;DocumentNet&#21253;&#21547;&#20102;30M&#20010;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;&#36817;400&#20010;&#25991;&#26723;&#31867;&#22411;&#65292;&#32452;&#32455;&#25104;&#20102;&#19968;&#20010;&#22235;&#32423;&#26412;&#20307;&#32467;&#26500;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#37319;&#29992;&#30340;VDER&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#23558;DocumentNet&#32435;&#20837;&#39044;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#32437;&#21521;&#33016;&#37096;X&#20809;&#21644;&#25253;&#21578;&#26469;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#25253;&#21578;&#38169;&#35823;&#12290;&#36890;&#36807;&#21033;&#29992;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#21253;&#21547;&#22810;&#27169;&#24577;&#32437;&#21521;&#23601;&#35786;&#35760;&#24405;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.08749</link><description>&lt;p&gt;
&#21033;&#29992;&#32437;&#21521;&#33016;&#37096;X&#20809;&#21644;&#25253;&#21578;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports. (arXiv:2306.08749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#32437;&#21521;&#33016;&#37096;X&#20809;&#21644;&#25253;&#21578;&#26469;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#25253;&#21578;&#38169;&#35823;&#12290;&#36890;&#36807;&#21033;&#29992;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#21253;&#21547;&#22810;&#27169;&#24577;&#32437;&#21521;&#23601;&#35786;&#35760;&#24405;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#36719;&#20214;&#21487;&#20197;&#32553;&#30701;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#23436;&#25104;&#26102;&#38388;&#65292;&#20294;&#25345;&#32493;&#30340;&#27807;&#36890;&#38169;&#35823;&#20250;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#35299;&#37322;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;&#22312;&#20943;&#36731;&#25253;&#21578;&#38169;&#35823;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#23613;&#31649;&#25991;&#29486;&#20013;&#23384;&#22312;&#29983;&#25104;&#21307;&#23398;&#25253;&#21578;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#20110;&#21033;&#29992;MIMIC-CXR&#25968;&#25454;&#38598;&#20013;&#30340;&#24739;&#32773;&#23601;&#35786;&#35760;&#24405;&#30340;&#32437;&#21521;&#29305;&#24615;&#32570;&#20047;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21363;&#20197;&#21069;&#30340;&#24739;&#32773;&#23601;&#35786;CXR&#12289;&#24403;&#21069;&#23601;&#35786;CXR&#21644;&#20197;&#21069;&#30340;&#23601;&#35786;&#25253;&#21578;&#65292;&#26469;&#39044;&#20808;&#22635;&#20805;&#24403;&#21069;&#24739;&#32773;&#23601;&#35786;&#25253;&#21578;&#30340;&#8220;&#21457;&#29616;&#8221;&#37096;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;MIMIC-CXR&#25968;&#25454;&#38598;&#20013;&#25910;&#38598;&#20102;26,625&#21517;&#24739;&#32773;&#30340;&#32437;&#21521;&#23601;&#35786;&#20449;&#24687;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Longitudinal-MIMIC&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#21253;&#21547;&#22810;&#27169;&#24577;&#32437;&#21521;&#23601;&#35786;&#35760;&#24405;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the reduction in turn-around times in radiology reports with the use of speech recognition software, persistent communication errors can significantly impact the interpretation of the radiology report. Pre-filling a radiology report holds promise in mitigating reporting errors, and despite efforts in the literature to generate medical reports, there exists a lack of approaches that exploit the longitudinal nature of patient visit records in the MIMIC-CXR dataset. To address this gap, we propose to use longitudinal multi-modal data, i.e., previous patient visit CXR, current visit CXR, and previous visit report, to pre-fill the 'findings' section of a current patient visit report. We first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset and created a new dataset called Longitudinal-MIMIC. With this new dataset, a transformer-based model was trained to capture the information from longitudinal patient visit records containing multi-modal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#36866;&#24212;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197; OpenAI Whisper ASR &#20026;&#20363;&#65292;&#20351;&#29992; LibriSpeech &#20316;&#20026;&#20027;&#35201;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340; ASR &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.01208</link><description>&lt;p&gt;
&#36866;&#24212;&#19968;&#20010;&#19981;&#21487;&#36866;&#24212;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adapting an Unadaptable ASR System. (arXiv:2306.01208v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#36866;&#24212;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197; OpenAI Whisper ASR &#20026;&#20363;&#65292;&#20351;&#29992; LibriSpeech &#20316;&#20026;&#20027;&#35201;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340; ASR &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#30340;&#22686;&#38271;&#65292;&#31995;&#32479;&#36890;&#24120;&#21482;&#33021;&#36890;&#36807;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;API&#33719;&#24471;&#35775;&#38382;&#26435;&#38480;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#31995;&#32479;&#36866;&#24212;&#21040;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272; OpenAI Whisper ASR &#20316;&#20026;&#22823;&#35268;&#27169; ASR &#31995;&#32479;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#65292;&#32780;&#26159;&#21487;&#20197;&#20174;&#36890;&#24120;&#36890;&#36807; ASR API &#25552;&#20379;&#30340; 1-best &#25110; N-best &#36755;&#20986;&#36827;&#34892;&#35757;&#32451;&#12290;LibriSpeech &#34987;&#29992;&#20316;&#36866;&#24212;&#30340;&#20027;&#35201;&#30446;&#26631;&#39046;&#22495;&#12290;&#28982;&#21518;&#35780;&#20272;&#20102;&#31995;&#32479;&#22312;&#20004;&#20010;&#19981;&#21516;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#39318;&#20808;&#65292;&#32416;&#27491;&#27169;&#22411;&#30340;&#24418;&#24335;&#26159;&#21542;&#21487;&#20197;&#31227;&#26893;&#21040;&#20854;&#20182;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#65307;&#20854;&#27425;&#65292;&#23427;&#26159;&#21542;&#21487;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340; ASR &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As speech recognition model sizes and training data requirements grow, it is increasingly common for systems to only be available via APIs from online service providers rather than having direct access to models themselves. In this scenario it is challenging to adapt systems to a specific target domain. To address this problem we consider the recently released OpenAI Whisper ASR as an example of a large-scale ASR system to assess adaptation methods. An error correction based approach is adopted, as this does not require access to the model, but can be trained from either 1-best or N-best outputs that are normally available via the ASR API. LibriSpeech is used as the primary target domain for adaptation. The generalization ability of the system in two distinct dimensions are then evaluated. First, whether the form of correction model is portable to other speech recognition domains, and secondly whether it can be used for ASR models having a different architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00398</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#26631;&#35760;&#32423;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Preference-grounded Token-level Guidance for Language Model Fine-tuning. (arXiv:2306.00398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20559;&#22909;&#19982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30456;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#20559;&#22909;&#36890;&#24120;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#25552;&#20379;&#65292;&#32780;LM&#35757;&#32451;&#21644;&#29983;&#25104;&#37117;&#21457;&#29983;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#12290;&#22240;&#27492;&#65292;&#20559;&#22909;&#21644;LM&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#23384;&#22312;&#39063;&#31890;&#24230;&#19981;&#21305;&#37197;&#65292;&#36825;&#21487;&#33021;&#20250;&#22797;&#26434;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#20026;&#20102;&#23398;&#20064;&#25351;&#23548;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#25104;&#23545;&#20559;&#22909;&#23398;&#20064;&#25193;&#23637;&#21040;&#21487;&#21464;&#38271;&#24230;LM&#29983;&#25104;&#21644;&#21033;&#29992;&#22810;&#20010;&#29983;&#25104;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;LM&#35757;&#32451;&#65292;&#22522;&#20110;&#30417;&#30563;&#25968;&#25454;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#30340;&#26497;&#31616;&#20027;&#20041;&#23398;&#20064;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25351;&#23548;&#25552;&#39640;&#20102;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20801;&#35768;&#26356;&#31934;&#32454;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and utilizing the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19011</link><description>&lt;p&gt;
MiniSUPERB:&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SUPERB&#34987;&#25552;&#20986;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#65292;&#23427;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MiniSUPERB&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20197;&#26126;&#26174;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#24182;&#19988;&#32467;&#26524;&#21487;&#19982;SUPERB&#30456;&#27604;&#12290;&#25105;&#20204;&#31934;&#36873;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#37319;&#26679;&#25968;&#25454;&#38598;&#65292;&#24182;&#31163;&#32447;&#25552;&#21462;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;SUPERB Paper&#21644;SUPERB Challenge&#20998;&#21035;&#36798;&#21040;0.954&#21644;0.982&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20056;-&#32047;&#31215;&#25805;&#20316;&#65288;MACs&#65289;&#26041;&#38754;&#20943;&#23569;&#20102;97&#65285;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#26412;&#36523;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
&lt;/p&gt;</description></item><item><title>Bactrian-X&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21487;&#22797;&#21046;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;LoRA&#65289;&#35757;&#32451;&#65292;&#20855;&#26377;&#20302;&#21442;&#25968;&#25968;&#37327;&#12289;&#26131;&#20110;&#26367;&#25442;&#30340;&#29305;&#28857;&#12290;&#22312;&#32508;&#21512;&#22810;&#35821;&#35328;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;Bactrian-X&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#32431;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15011</link><description>&lt;p&gt;
Bactrian-X: &#20855;&#26377;&#20302;&#31209;&#36866;&#24212;&#24615;&#30340;&#22810;&#35821;&#35328;&#21487;&#22797;&#21046;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation. (arXiv:2305.15011v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15011
&lt;/p&gt;
&lt;p&gt;
Bactrian-X&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21487;&#22797;&#21046;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;LoRA&#65289;&#35757;&#32451;&#65292;&#20855;&#26377;&#20302;&#21442;&#25968;&#25968;&#37327;&#12289;&#26131;&#20110;&#26367;&#25442;&#30340;&#29305;&#28857;&#12290;&#22312;&#32508;&#21512;&#22810;&#35821;&#35328;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;Bactrian-X&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#32431;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#32463;&#26174;&#31034;&#20986;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#39640;&#36136;&#37327;&#25351;&#20196;-&#21709;&#24212;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#23545;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#30340;&#30740;&#31350;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bactrian-X&#65292;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;52&#31181;&#35821;&#35328;&#30340;&#32508;&#21512;&#22810;&#35821;&#35328;&#24182;&#34892;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;340&#19975;&#20010;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#24615;&#65288;LoRA&#65289;&#35757;&#32451;&#20102;&#19968;&#32452;&#36866;&#37197;&#22120;&#65292;&#23427;&#20204;&#26159;&#36731;&#37327;&#32423;&#32452;&#20214;&#65292;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#25968;&#30446;&#26174;&#33879;&#20302;&#20110;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#26367;&#25442;&#65292;&#24182;&#29992;&#20316;&#19981;&#21516;&#35821;&#35328;&#25110;&#35821;&#35328;&#32452;&#30340;&#25554;&#20214;&#12290;&#22312;&#21508;&#31181;&#22810;&#35821;&#35328;&#35780;&#20272;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;Bactrian-X&#19978;&#30340;LoRA&#35757;&#32451;&#33719;&#24471;&#30340;&#27169;&#22411;&#20248;&#20110;&#32431;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#24050;&#32463;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has shown great promise in improving the performance of large language models. However, research on multilingual instruction tuning has been limited due to the scarcity of high-quality instruction-response datasets across different languages. To bridge this gap, we present Bactrian-X, a comprehensive multilingual parallel dataset of 3.4 million instruction-response pairs across 52 languages. Leveraging this dataset, we train a set of adapters using low-rank adaptation (LoRA), which are lightweight components that seamlessly integrate with large language models. These adapters have a substantially lower parameter count than the base model, making them easily replaceable and usable as plug-ins for different languages or language groups. Extensive experiments in various multilingual evaluation settings demonstrate that models derived from LoRA-based training over Bactrian-X outperform both the vanilla models and existing instruction-tuned models. The code and models are
&lt;/p&gt;</description></item><item><title>LLMDet&#26159;&#19968;&#20010;&#31532;&#19977;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#20174;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#24182;&#28385;&#36275;&#31934;&#32454;&#36861;&#36394;&#12289;&#20013;&#38388;&#21028;&#26029;&#21644;&#24555;&#36895;&#26816;&#27979;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.15004</link><description>&lt;p&gt;
LLMDet:&#19968;&#31181;&#31532;&#19977;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LLMDet: A Third Party Large Language Models Generated Text Detection Tool. (arXiv:2305.15004v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15004
&lt;/p&gt;
&lt;p&gt;
LLMDet&#26159;&#19968;&#20010;&#31532;&#19977;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#20174;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#24182;&#28385;&#36275;&#31934;&#32454;&#36861;&#36394;&#12289;&#20013;&#38388;&#21028;&#26029;&#21644;&#24555;&#36895;&#26816;&#27979;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#38750;&#24120;&#30456;&#20284;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#21644;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#39640;&#24230;&#23454;&#29992;&#30340;&#26816;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#32473;&#23450;&#25991;&#26412;&#30340;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;LLM&#30340;&#35775;&#38382;&#65292;&#24182;&#19988;&#21482;&#33021;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#65292;&#26410;&#33021;&#28385;&#36275;&#31934;&#32454;&#36861;&#36394;&#12289;&#20013;&#38388;&#21028;&#26029;&#21644;&#24555;&#36895;&#26816;&#27979;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMDet&#65292;&#19968;&#31181;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#23433;&#20840;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#26816;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#29305;&#23450;&#30340;LLM&#65288;&#22914;GPT-2&#12289;OPT&#12289;LLaMA&#31561;&#65289;&#20013;&#33719;&#21462;&#25991;&#26412;&#12290;&#22312;LLMDet&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#26174;&#33879;n-gram&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#27010;&#29575;&#20316;&#20026;&#29305;&#24449;&#65292;&#29992;&#20110;&#35745;&#31639;&#27599;&#20010;LLM&#30340;&#20195;&#29702;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;LLM&#30340;&#20195;&#29702;&#22256;&#24785;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#24120;&#35782;&#21028;&#26029;&#26159;&#21542;&#19982;Transformer&#27169;&#22411;&#20013;&#30340;&#21487;&#32534;&#36753;&#21442;&#25968;&#23384;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32534;&#36753;&#31639;&#27861;&#21644;&#23618;&#36873;&#25321;&#31574;&#30053;&#65292;&#22312;&#24120;&#35782;&#39046;&#22495;&#20013;&#25552;&#21319;&#20102;&#32534;&#36753;&#25928;&#26524;&#65292;&#20351;&#24471;&#32463;&#36807;&#32534;&#36753;&#30340;GPT-2&#27169;&#22411;&#22312;&#21508;&#27979;&#35797;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#30456;&#36739;&#20110;&#26368;&#20339;&#24494;&#35843;&#22522;&#32447;&#25552;&#39640;&#20102;10.97%&#21644;10.73%&#12290;</title><link>http://arxiv.org/abs/2305.14956</link><description>&lt;p&gt;
&#22312;Transformer&#27169;&#22411;&#20013;&#32534;&#36753;&#24120;&#35782;
&lt;/p&gt;
&lt;p&gt;
Editing Common Sense in Transformers. (arXiv:2305.14956v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#24120;&#35782;&#21028;&#26029;&#26159;&#21542;&#19982;Transformer&#27169;&#22411;&#20013;&#30340;&#21487;&#32534;&#36753;&#21442;&#25968;&#23384;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32534;&#36753;&#31639;&#27861;&#21644;&#23618;&#36873;&#25321;&#31574;&#30053;&#65292;&#22312;&#24120;&#35782;&#39046;&#22495;&#20013;&#25552;&#21319;&#20102;&#32534;&#36753;&#25928;&#26524;&#65292;&#20351;&#24471;&#32463;&#36807;&#32534;&#36753;&#30340;GPT-2&#27169;&#22411;&#22312;&#21508;&#27979;&#35797;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#30456;&#36739;&#20110;&#26368;&#20339;&#24494;&#35843;&#22522;&#32447;&#25552;&#39640;&#20102;10.97%&#21644;10.73%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Transformer&#27169;&#22411;&#20013;&#30452;&#25509;&#32534;&#36753;&#27169;&#22411;&#21442;&#25968;&#20351;&#24471;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26356;&#26032;&#40657;&#30418;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#20165;&#22312;&#20851;&#20110;&#30334;&#31185;&#30693;&#35782;&#19988;&#21482;&#26377;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#30340;&#38472;&#36848;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23578;&#26410;&#23545;&#22810;&#20010;&#27491;&#30830;&#31572;&#26696;&#30340;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#30740;&#31350;&#65292;&#20363;&#22914;&#65292;&#19968;&#20010;&#33529;&#26524;&#21487;&#20197;&#26159;&#32511;&#33394;&#25110;&#32418;&#33394;&#20294;&#19981;&#33021;&#26159;&#36879;&#26126;&#30340;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#35782;&#21028;&#26029;&#26159;&#21542;&#19982;Transformer&#27169;&#22411;&#20013;&#30340;&#21487;&#32534;&#36753;&#21442;&#25968;&#23384;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#24182;&#32473;&#20986;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#24212;&#29992;MEMIT&#32534;&#36753;&#31639;&#27861;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#32534;&#36753;&#26631;&#35760;&#21644;&#25913;&#36827;&#23618;&#36873;&#25321;&#31574;&#30053;&#65292;&#21363;$MEMIT_{CSK}$&#65292;&#25913;&#36827;&#20102;&#23545;&#24120;&#35782;&#39046;&#22495;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;&#20351;&#29992;$MEMIT_{CSK}$&#32534;&#36753;&#36807;&#30340;GPT-2 Large&#21644;XL&#27169;&#22411;&#22312;PEP3k&#21644;20Q&#27979;&#35797;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#30456;&#36739;&#20110;&#26368;&#20339;&#24494;&#35843;&#22522;&#32447;&#25552;&#39640;&#20102;10.97%&#21644;10.73%&#12290;
&lt;/p&gt;
&lt;p&gt;
Editing model parameters directly in Transformers makes updating black-box models possible without re-training (Meng et al., 2023). However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models edited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q 
&lt;/p&gt;</description></item><item><title>Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.14342</link><description>&lt;p&gt;
Sophia&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14342
&lt;/p&gt;
&lt;p&gt;
Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#24494;&#23567;&#25913;&#36827;&#23558;&#20250;&#22823;&#22823;&#38477;&#20302;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;Adam&#21450;&#20854;&#21464;&#31181;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#20108;&#38454;&#65288;&#22522;&#20110;Hessian&#30340;&#65289;&#20248;&#21270;&#22120;&#24448;&#24448;&#20250;&#24102;&#26469;&#22826;&#22810;&#30340;&#27599;&#27493;&#24320;&#38144;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sophia&#65292;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20272;&#35745;&#30340;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#12290;&#26356;&#26032;&#27493;&#39588;&#26159;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#38500;&#20197;&#20272;&#35745;Hessian&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#12290;&#35009;&#21098;&#25511;&#21046;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26356;&#26032;&#22823;&#23567;&#65292;&#24182;&#25511;&#21046;&#20102;Hessian&#22312;&#36712;&#36857;&#19978;&#30340;&#38750;&#20984;&#24615;&#21644;&#24555;&#36895;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;Sophia&#21482;&#22312;&#27599;&#20960;&#27425;&#36845;&#20195;&#20013;&#20272;&#35745;&#23545;&#35282;Hessian&#65292;&#36825;&#20960;&#20046;&#27809;&#26377;&#24179;&#22343;&#27599;&#27493;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#22312;&#20351;&#29992;GPT m&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Skill-KNN&#65292;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20559;&#35265;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#25110;&#26356;&#25913;&#31034;&#20363;&#24211;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.14210</link><description>&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#20197;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Skill-Based Few-Shot Selection for In-Context Learning. (arXiv:2305.14210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Skill-KNN&#65292;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20559;&#35265;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#25110;&#26356;&#25913;&#31034;&#20363;&#24211;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#26159;&#19968;&#31181;&#33539;&#20363;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#36873;&#25321;&#36866;&#24403;&#30340;&#31034;&#20363;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24456;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;Skill-KNN&#30340;&#20851;&#38190;&#20248;&#21183;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#26041;&#27861;&#23481;&#26131;&#22240;&#20026;&#19981;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#38754;&#29305;&#24449;&#23545;&#30446;&#26631;&#20219;&#21153;&#20135;&#29983;&#20559;&#35265;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#23427;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#20219;&#20309;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#39057;&#32321;&#25193;&#23637;&#25110;&#26356;&#25913;&#31034;&#20363;&#24211;&#30340;&#24773;&#20917;&#12290;&#20854;&#20851;&#38190;&#35265;&#35299;&#26159;&#20248;&#21270;&#36755;&#20837;&#21040;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#26159;&#35843;&#25972;&#27169;&#22411;&#26412;&#36523;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;Skill-KNN&#36890;&#36807;&#21033;&#29992;&#39044;&#22788;&#29702;&#23569;&#26679;&#26412;&#25552;&#31034;&#20026;&#27599;&#20010;&#27979;&#35797;&#26696;&#20363;&#21644;&#20505;&#36873;&#31034;&#20363;&#29983;&#25104;&#22522;&#20110;&#25216;&#33021;&#30340;&#25551;&#36848;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#19981;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.14189</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#65306;&#22686;&#21152;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#31034;&#35789;&#35821;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#35789;&#27719;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38500;&#20102;&#31616;&#21333;&#30340;&#35774;&#35745;&#22806;&#65292;&#20849;&#20139;&#26631;&#35760;&#22312;&#31215;&#26497;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20551;&#35774;&#20849;&#20139;&#26631;&#35760;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#35789;&#27719;&#30340;&#37325;&#21472;&#36739;&#23567;&#26102;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#36716;&#31227;&#34987;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35789;&#31561;&#20215;&#31867;&#23450;&#20041;&#20102;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#65292;&#24182;&#20381;&#36182;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65306;1) &#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#35789;&#30340;&#23884;&#20837;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23545;&#40784;&#65292;2) &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39640;&#21644;&#20302;&#36164;&#28304;MNMT&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;BLEU&#25552;&#21319;&#36798;2.3&#20010;&#28857;&#65292;3) &#38656;&#35201;&#23569;&#20110;1.0%&#30340;&#39069;&#22806;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#30340;&#22686;&#21152;&#26377;&#38480;&#65292;&#32780;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36777;&#35770;&#24335;&#23545;&#35805;&#27979;&#35797;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35201;&#27714;LLM&#22312;&#35752;&#35770;&#36807;&#31243;&#20013;&#19981;&#20165;&#33021;&#22815;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#33021;&#22815;&#22362;&#25345;&#24182;&#25421;&#21355;&#33258;&#24049;&#30340;&#20449;&#24565;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#35780;&#20272;&#20854;&#23545;&#38382;&#39064;&#30340;&#25512;&#29702;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13160</link><description>&lt;p&gt;
&#33021;ChatGPT&#20445;&#25345;&#23545;&#30495;&#30456;&#30340;&#20449;&#24565;&#21527;&#65311;&#36890;&#36807;&#36777;&#35770;&#35780;&#20272;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate. (arXiv:2305.13160v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36777;&#35770;&#24335;&#23545;&#35805;&#27979;&#35797;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35201;&#27714;LLM&#22312;&#35752;&#35770;&#36807;&#31243;&#20013;&#19981;&#20165;&#33021;&#22815;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#33021;&#22815;&#22362;&#25345;&#24182;&#25421;&#21355;&#33258;&#24049;&#30340;&#20449;&#24565;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#35780;&#20272;&#20854;&#23545;&#38382;&#39064;&#30340;&#25512;&#29702;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24456;&#38590;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#23545;&#30495;&#30456;&#21644;&#36923;&#36753;&#30340;&#28145;&#20837;&#29702;&#35299;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#26159;&#20165;&#20165;&#21033;&#29992;&#20854;&#35760;&#24518;&#30340;&#27169;&#24335;&#36827;&#34892;&#30456;&#23545;&#32932;&#27973;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;LLM&#36827;&#34892;&#31867;&#20284;&#36777;&#35770;&#30340;&#23545;&#35805;&#26469;&#27979;&#35797;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#21644;&#29992;&#25143;&#38656;&#35201;&#35752;&#35770;&#24182;&#20174;&#30456;&#23545;&#31435;&#30340;&#35266;&#28857;&#20986;&#21457;&#20570;&#20986;&#27491;&#30830;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#28040;&#38500;Clever Hans&#25928;&#24212;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#35201;&#27714;LLM&#19981;&#20165;&#33021;&#22815;&#29420;&#31435;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#35201;&#33021;&#22815;&#22362;&#25345;&#24182;&#25421;&#21355;&#33258;&#24049;&#30340;&#20449;&#24565;&#65292;&#32780;&#19981;&#26159;&#30450;&#30446;&#22320;&#30456;&#20449;&#29992;&#25143;&#30340;&#65288;&#26080;&#25928;&#30340;&#65289;&#35770;&#35777;&#21644;&#25209;&#35780;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#27979;&#35797;LLM&#26159;&#21542;&#25484;&#25569;&#20102;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#25512;&#29702;&#35201;&#28857;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20102;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#36923;&#36753;&#21644;BIG-Bench&#31561;&#22810;&#20010;&#22797;&#26434;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#35821;&#22312;&#22823;&#37327;&#25991;&#26412;&#19978;&#36827;&#34892;&#39118;&#26684;&#20998;&#26512;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#21487;&#35299;&#37322;&#30340;&#39118;&#26684;&#23884;&#20837;&#34920;&#31034;&#65292;&#20026;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#30340;&#19979;&#28216;&#24212;&#29992;&#65288;&#22914;&#20316;&#32773;&#24402;&#23646;&#65289;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#35821;&#35757;&#32451;&#21487;&#35299;&#37322;&#30340;&#39118;&#26684;&#23884;&#20837;&#65306;&#23398;&#20064;&#39118;&#26684;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Style Embeddings via Prompting LLMs. (arXiv:2305.12696v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#35821;&#22312;&#22823;&#37327;&#25991;&#26412;&#19978;&#36827;&#34892;&#39118;&#26684;&#20998;&#26512;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#21487;&#35299;&#37322;&#30340;&#39118;&#26684;&#23884;&#20837;&#34920;&#31034;&#65292;&#20026;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#30340;&#19979;&#28216;&#24212;&#29992;&#65288;&#22914;&#20316;&#32773;&#24402;&#23646;&#65289;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#20026;&#25991;&#26412;&#20013;&#30340;&#20316;&#32773;&#39118;&#26684;&#24314;&#31435;&#19982;&#20869;&#23481;&#26080;&#20851;&#30340;&#34920;&#31034;&#12290;&#39118;&#26684;&#20998;&#26512;&#36890;&#24120;&#30001;&#19987;&#19994;&#30340;&#27861;&#21307;&#35821;&#35328;&#23398;&#23478;&#36827;&#34892;&#65292;&#23578;&#26080;&#22823;&#22411;&#30340;&#39118;&#26684;&#20998;&#26512;&#27880;&#37322;&#25968;&#25454;&#38598;&#21487;&#20379;&#35757;&#32451;&#12290;&#24403;&#21069;&#30340;&#39118;&#26684;&#34920;&#31034;&#23398;&#20064;&#20351;&#29992;&#31070;&#32463;&#26041;&#27861;&#26469;&#23558;&#39118;&#26684;&#19982;&#20869;&#23481;&#21306;&#20998;&#24320;&#26469;&#21019;&#24314;&#39118;&#26684;&#21521;&#37327;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#20102;&#38590;&#20197;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#22312;&#20316;&#32773;&#24402;&#23646;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#23457;&#26680;&#21644;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#35821;&#22312;&#22823;&#37327;&#25991;&#26412;&#19978;&#25191;&#34892;&#39118;&#26684;&#20998;&#26512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#20986;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;LISA&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#39118;&#26684;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21512;&#25104;&#39118;&#26684;&#20998;&#26512;&#25968;&#25454;&#38598;&#21644;&#21487;&#35299;&#37322;&#39118;&#26684;&#27169;&#22411;&#21457;&#24067;&#20026;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#25512;&#29702;&#30340;&#31867;&#27604;&#32467;&#26500;&#25512;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#31185;&#23398;&#31867;&#27604;&#26102;&#24573;&#35270;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12660</link><description>&lt;p&gt;
&#34920;&#38754;&#30456;&#20284;&#24615;&#20043;&#19979;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32467;&#26500;&#25512;&#29702;&#36827;&#34892;&#21512;&#29702;&#30340;&#31185;&#23398;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. (arXiv:2305.12660v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#25512;&#29702;&#30340;&#31867;&#27604;&#32467;&#26500;&#25512;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#31185;&#23398;&#31867;&#27604;&#26102;&#24573;&#35270;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20013;&#31867;&#27604;&#25512;&#29702;&#30340;&#37325;&#35201;&#20316;&#29992;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20849;&#20139;&#30340;&#20851;&#31995;&#32467;&#26500;&#23558;&#26032;&#27010;&#24565;&#19982;&#29087;&#24713;&#30340;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#35789;&#35821;&#31867;&#27604;&#65292;&#20294;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#24573;&#35270;&#26500;&#25104;&#36825;&#20123;&#31867;&#27604;&#30340;&#32467;&#26500;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#35789;&#35821;&#31867;&#27604;&#20316;&#20026;&#31867;&#27604;&#25512;&#29702;&#25216;&#33021;&#65288;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#65289;&#30340;&#26377;&#25928;&#24615;&#30340;&#36136;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#31867;&#27604;&#32467;&#26500;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#26029;&#20986;&#36830;&#25509;&#20004;&#20010;&#31995;&#32479;&#20043;&#38388;&#30340;&#31867;&#27604;&#32467;&#26500;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#26469;&#33258;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;400&#20010;&#31185;&#23398;&#31867;&#27604;&#65292;&#26088;&#22312;&#35780;&#20272;&#21033;&#29992;&#32467;&#26500;&#25512;&#29702;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#35777;&#35777;&#25454;&#24378;&#35843;&#20102;LLMs&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#65292;&#22312;&#25484;&#25569;&#36825;&#20010;&#20219;&#21153;&#19978;&#20381;&#28982;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.11186</link><description>&lt;p&gt;
&#21387;&#32553;&#65292;&#28982;&#21518;&#25552;&#31034;&#65306;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#25913;&#21892;LLM&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#65292;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#24102;&#26469;&#26174;&#30528;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#35265;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#65288;&#20363;&#22914;&#21333;&#20010;GPU&#65289;&#26102;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21387;&#32553;&#26469;&#26368;&#23567;&#21270;LLM&#25512;&#29702;&#30340;&#24310;&#36831;&#65292;&#21363;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#27492;&#36807;&#31243;&#24517;&#28982;&#24341;&#21457;&#25928;&#29575;&#21644;&#31934;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#22240;&#20026;&#21387;&#32553;&#30340;LLMs&#36890;&#24120;&#20250;&#32463;&#21382;&#39044;&#27979;&#31934;&#24230;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35270;&#35282;&#65306;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24179;&#34913;&#65292;&#21387;&#32553;&#30340;LLMs&#38656;&#35201;&#19968;&#31181;&#19981;&#21516;&#20110;&#21407;&#22987;&#27169;&#22411;&#30340;&#29420;&#29305;&#36755;&#20837;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#31934;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#36716;&#31227;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26377;&#25928;&#25552;&#31034;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#36895;&#24230;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), armed with billions of parameters, exhibit exceptional performance across a wide range of Natural Language Processing (NLP) tasks. However, they present a significant computational challenge during inference, especially when deploying on common hardware such as single GPUs. As such, minimizing the latency of LLM inference by curtailing computational and memory requirements, though achieved through compression, becomes critically important. However, this process inevitably instigates a trade-off between efficiency and accuracy, as compressed LLMs typically experience a reduction in predictive precision. In this research, we introduce an innovative perspective: to optimize this trade-off, compressed LLMs require a unique input format that varies from that of the original models. Our findings indicate that the generation quality in a compressed LLM can be markedly improved for specific queries by selecting prompts with precision. Capitalizing on this insight,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#21462;&#24335;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20854;ROUGE&#24471;&#20998;&#30456;&#23545;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#23545;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#20808;&#25277;&#21462;&#21518;&#29983;&#25104;&#27969;&#31243;&#21644;&#21477;&#23376;&#36873;&#25321;&#27169;&#22359;&#32531;&#35299;&#20102;&#20854;&#29983;&#25104;&#25688;&#35201;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04193</link><description>&lt;p&gt;
&#37319;&#29992;ChatGPT&#36827;&#34892;&#25688;&#35201;&#25552;&#21462;&#20197;&#29983;&#25104;&#24544;&#23454;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Extractive Summarization via ChatGPT for Faithful Summary Generation. (arXiv:2304.04193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#21462;&#24335;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20854;ROUGE&#24471;&#20998;&#30456;&#23545;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#23545;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#20808;&#25277;&#21462;&#21518;&#29983;&#25104;&#27969;&#31243;&#21644;&#21477;&#23376;&#36873;&#25321;&#27169;&#22359;&#32531;&#35299;&#20102;&#20854;&#29983;&#25104;&#25688;&#35201;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#21462;&#24335;&#25688;&#35201;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#25552;&#21462;&#21477;&#23376;&#23558;&#38271;&#25991;&#26723;&#21387;&#32553;&#20026;&#36739;&#30701;&#30340;&#29256;&#26412;&#12290;ChatGPT&#30340;&#24341;&#20837;&#24341;&#36215;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20854;&#20107;&#23454;&#24615;&#21644;&#24544;&#23454;&#24230;&#30340;&#25285;&#24551;&#38459;&#30861;&#20102;&#20854;&#22312;&#25688;&#35201;&#31995;&#32479;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;ChatGPT&#22312;&#25277;&#21462;&#24335;&#25688;&#35201;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#25581;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#31995;&#32479;&#65292;ChatGPT&#22312;ROUGE&#20998;&#25968;&#26041;&#38754;&#30340;&#25277;&#21462;&#24335;&#25688;&#35201;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#23545;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#37319;&#29992;&#20808;&#25277;&#21462;&#21518;&#29983;&#25104;&#27969;&#31243;&#20197;&#21450;&#21477;&#23376;&#36873;&#25321;&#27169;&#22359;&#21487;&#20197;&#32531;&#35299;ChatGPT&#29983;&#25104;&#25688;&#35201;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20351;&#29992;ChatGPT&#36827;&#34892;&#25277;&#21462;&#24335;&#25688;&#35201;&#25552;&#20379;&#20102;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of ChatGPT has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. However, concerns regarding factuality and faithfulness have hindered its practical applications for summarization systems. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT's extractive summarization performance is still inferior to existing supervised systems in terms of ROUGE scores. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.03347</link><description>&lt;p&gt;
&#35770;ChatGPT&#21644;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26174;&#31034;&#20986;&#25552;&#39640;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#25928;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26368;&#36817;&#30340;&#20027;&#27969;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(PLMs)&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#34701;&#20837;&#24773;&#24863;&#20449;&#24687;&#12290;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ChatGPT&#38646;-shot&#24615;&#33021;&#30740;&#31350;&#22312;&#19981;&#20805;&#20998;&#30340;&#35780;&#20272;&#12289;&#24773;&#24863;&#20449;&#24687;&#21033;&#29992;&#21644;&#26041;&#27861;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;&#21644;&#22810;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26816;&#27979;&#12289;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#21407;&#22240;/&#22240;&#32032;&#26816;&#27979;&#12289;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#21644;&#22240;&#26524;&#24773;&#24863;&#34164;&#21547;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20998;&#26512;&#20013;&#25506;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#20197;&#21450;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;ChatGPT&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated mental health analysis shows great potential for enhancing the efficiency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT's zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainability of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#20004;&#32452;&#20154;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01002</link><description>&lt;p&gt;
&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?. (arXiv:2304.01002v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#20004;&#32452;&#20154;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;LLaMA&#65289;&#30340;&#36827;&#23637;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#20889;&#20316;&#30340;&#36830;&#36143;&#21477;&#23376;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#25152;&#35859;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24341;&#21457;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#26469;&#21306;&#20998;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#21644;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#8220;&#21512;&#20316;&#8221;&#26159;&#21542;&#33021;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22635;&#34917;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#29702;&#35299;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#20004;&#32452;&#20154;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#65288;1&#65289;&#26469;&#33258;AMT&#24179;&#21488;&#30340;&#38750;&#19987;&#23478;&#32676;&#20307;&#21644;&#65288;2&#65289;&#26469;&#33258;Upwork&#24179;&#21488;&#30340;&#20889;&#20316;&#19987;&#23478;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#33021;&#20250;&#25552;&#39640;&#20004;&#32452;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#38750;&#19987;&#23478;&#32452;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;6.36%&#65292;&#19987;&#23478;&#32452;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;12.76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans' ability to detect deepfake texts, none has examined whether "collaboration" among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals' detection accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; N-best T5 &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992; ASR N-best &#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#21463;&#38480;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545; ASR &#38169;&#35823;&#20462;&#27491;&#30340;&#24378;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.00456</link><description>&lt;p&gt;
&#22810;&#36755;&#20837;&#20551;&#35774;&#21644;&#21463;&#38480;&#35299;&#30721;&#31354;&#38388;&#30340; N-best T5&#65306;&#21033;&#29992;&#22810;&#36755;&#20837;&#20551;&#35774;&#21644;&#21463;&#38480;&#35299;&#30721;&#31354;&#38388;&#30340;&#24378;&#40065;&#26834; ASR &#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; N-best T5 &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992; ASR N-best &#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#21463;&#38480;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545; ASR &#38169;&#35823;&#20462;&#27491;&#30340;&#24378;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21518;&#22788;&#29702;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29992;&#20110;&#25552;&#39640;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#21644;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; N-best T5 &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992; ASR N-best &#21015;&#34920;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36716;&#31227;&#30693;&#35782;&#24182;&#20174; ASR &#35299;&#30721;&#31354;&#38388;&#20013;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;Conformer-Transducer&#22522;&#32447;&#12290;&#26631;&#20934;&#38169;&#35823;&#26657;&#27491;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#29983;&#25104;&#36807;&#31243;&#19981;&#21463;&#33391;&#22909;&#25351;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#22522;&#20110; N-best &#21015;&#34920;&#25110; ASR lattice &#30340;&#21463;&#38480;&#35299;&#30721;&#36807;&#31243;&#65292;&#21487;&#20197;&#20256;&#25773;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error correction models form an important part of Automatic Speech Recognition (ASR) post-processing to improve the readability and quality of transcriptions. Most prior works use the 1-best ASR hypothesis as input and therefore can only perform correction by leveraging the context within one sentence. In this work, we propose a novel N-best T5 model for this task, which is fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By transferring knowledge from the pre-trained language model and obtaining richer information from the ASR decoding space, the proposed approach outperforms a strong Conformer-Transducer baseline. Another issue with standard error correction is that the generation process is not well-guided. To address this a constrained decoding process, either based on the N-best list or an ASR lattice, is used which allows additional information to be propagated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;&#20013;&#20351;&#29992;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#26469;&#20943;&#23569;&#35821;&#38899;&#25351;&#20196;&#30340;&#36716;&#24405;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#36716;&#24405;&#32467;&#26524;&#12290;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22810;&#36798;30&#65285;&#30340;&#34987;&#23631;&#34109;&#21333;&#35789;&#65292;&#24182;&#19988;&#35757;&#32451;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#36523;&#20195;&#29702;&#22312;&#36981;&#24490;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#36716;&#24405;&#30340;&#25351;&#20196;&#19979;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.14030</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35821;&#38899;&#35782;&#21035;&#29992;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multimodal Speech Recognition for Language-Guided Embodied Agents. (arXiv:2302.14030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;&#20013;&#20351;&#29992;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#26469;&#20943;&#23569;&#35821;&#38899;&#25351;&#20196;&#30340;&#36716;&#24405;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#36716;&#24405;&#32467;&#26524;&#12290;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22810;&#36798;30&#65285;&#30340;&#34987;&#23631;&#34109;&#21333;&#35789;&#65292;&#24182;&#19988;&#35757;&#32451;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#36523;&#20195;&#29702;&#22312;&#36981;&#24490;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#36716;&#24405;&#30340;&#25351;&#20196;&#19979;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#21512;&#36523;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#20551;&#35774;&#22522;&#20110;&#25991;&#26412;&#30340;&#25351;&#20196;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#30340;&#20195;&#29702;&#20250;&#36935;&#21040;&#21475;&#22836;&#25351;&#20196;&#12290;&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21487;&#20197;&#24357;&#21512;&#35821;&#38899;&#36755;&#20837;&#24046;&#36317;&#65292;&#20294;&#38169;&#35823;&#30340;ASR&#36716;&#24405;&#20250;&#25439;&#23475;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#35757;&#32451;&#26469;&#32771;&#34385;&#20276;&#38543;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#20174;&#32780;&#20943;&#23569;&#35821;&#38899;&#25351;&#20196;&#30340;&#36716;&#24405;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;ALFRED&#20219;&#21153;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#21512;&#25104;&#20102;&#21475;&#22836;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#23631;&#34109;&#21475;&#22836;&#21333;&#35789;&#26469;&#27169;&#25311;&#22768;&#23398;&#22122;&#22768;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#35270;&#35273;&#35266;&#23519;&#21487;&#20197;&#26377;&#21161;&#20110;&#24674;&#22797;&#23631;&#34109;&#30340;&#21333;&#35789;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#65292;&#24674;&#22797;&#30340;&#23631;&#34109;&#21333;&#35789;&#27604;&#21333;&#27169;&#24577;&#22522;&#32447;&#22810;&#36798;30&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22522;&#20110;&#25991;&#26412;&#35757;&#32451;&#30340;&#21512;&#36523;&#20195;&#29702;&#36890;&#36807;&#36981;&#24490;&#22810;&#27169;&#24577;ASR&#27169;&#22411;&#30340;&#36716;&#24405;&#25351;&#20196;&#26356;&#32463;&#24120;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agents' ability to complete tasks. In this work, we propose training a multimodal ASR model to reduce errors in transcribing spoken instructions by considering the accompanying visual context. We train our model on a dataset of spoken instructions, synthesized from the ALFRED task completion dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that a text-trained embodied agent successfully completes tasks more often by following transcribed instructions from multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#25104;&#23450;&#21521;&#21050;&#28608;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.11520</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#21050;&#28608;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#25104;&#23450;&#21521;&#21050;&#28608;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#23450;&#21521;&#21050;&#28608;&#24341;&#23548;&#65292;&#23427;&#20351;&#29992;&#21487;&#35843;&#33410;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#40657;&#30418;&#20923;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25351;&#23548;&#12290;&#19982;&#20197;&#24448;&#25163;&#21160;&#25110;&#33258;&#21160;&#25214;&#21040;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#31163;&#25955;&#30340;token&#20316;&#20026;&#27599;&#20010;&#36755;&#20837;&#30340;&#23450;&#21521;&#21050;&#28608;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#31034;&#25110;&#25552;&#31034;&#65292;&#20363;&#22914;&#25991;&#31456;&#30340;&#20851;&#38190;&#35789;&#29992;&#20110;&#25688;&#35201;&#12290;&#28982;&#21518;&#23558;&#23450;&#21521;&#21050;&#28608;&#19982;&#21407;&#22987;&#36755;&#20837;&#32452;&#21512;&#65292;&#24182;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#25351;&#23548;&#20854;&#21521;&#25152;&#38656;&#30446;&#26631;&#29983;&#25104;&#12290;&#31574;&#30053;LM&#21487;&#20197;&#36890;&#36807;1&#65289;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;2&#65289;&#20174;&#31163;&#32447;&#21644;&#22312;&#32447;&#22870;&#21169;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25506;&#32034;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#23450;&#21521;&#21050;&#28608;&#12290;&#35813;&#26694;&#26550;&#21487;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;LM&#21644;&#20219;&#21153;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#25928;&#26524;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#25688;&#35201;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as directional stimulus of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#8220;&#32763;&#35793;&#35821;&#35328;&#8221;&#29616;&#35937;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#26041;&#27861;&#26500;&#24314;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#35821;&#35328;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#21457;&#29616;&#32763;&#35793;&#35821;&#35328;&#20250;&#23545;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#65292;&#21253;&#25324;&#27979;&#35797;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#21028;&#26029;&#19982;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.07220</link><description>&lt;p&gt;
&#29702;&#35299;&#36328;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#8220;&#32763;&#35793;&#35821;&#35328;&#8221;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#8220;&#32763;&#35793;&#35821;&#35328;&#8221;&#29616;&#35937;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#26041;&#27861;&#26500;&#24314;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#35821;&#35328;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#21457;&#29616;&#32763;&#35793;&#35821;&#35328;&#20250;&#23545;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#65292;&#21253;&#25324;&#27979;&#35797;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#21028;&#26029;&#19982;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#31687;&#28304;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#31687;&#31616;&#27905;&#30340;&#30446;&#26631;&#35821;&#35328;&#25688;&#35201;&#12290;&#19982;&#21333;&#35821;&#25688;&#35201;&#65288;MS&#65289;&#19981;&#21516;&#65292;&#33258;&#28982;&#20986;&#29616;&#30340;&#28304;&#35821;&#35328;&#25991;&#26723;&#37197;&#23545;&#30446;&#26631;&#35821;&#35328;&#25688;&#35201;&#24182;&#19981;&#24120;&#35265;&#12290;&#20026;&#20102;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;CLS&#25968;&#25454;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#21019;&#24314;&#36807;&#31243;&#20013;&#28041;&#21450;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#25991;&#26412;&#19982;&#21407;&#22987;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#23384;&#22312;&#30528;&#21306;&#21035;&#65292;&#21363;&#32763;&#35793;&#35821;&#35328;&#12290;&#26412;&#25991;&#39318;&#20808;&#30830;&#35748;&#20102;&#26500;&#24314;CLS&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#26041;&#27861;&#23558;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#35821;&#35328;&#29616;&#35937;&#12290;&#28982;&#21518;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#32763;&#35793;&#35821;&#35328;&#22312;&#28304;&#25991;&#26723;&#25110;&#30446;&#26631;&#25688;&#35201;&#20013;&#20986;&#29616;&#26102;&#22914;&#20309;&#24433;&#21709;CLS&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#27979;&#35797;&#38598;&#20013;&#25991;&#26723;&#25110;&#25688;&#35201;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#20154;&#24037;&#21028;&#26029;&#19982;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#65288;2&#65289;&#35757;&#32451;&#38598;&#20013;&#30340;&#32763;&#35793;&#35821;&#35328;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In this paper, we first confirm that different approaches of constructing CLS datasets will lead to different degrees of translationese. Then we systematically investigate how translationese affects CLS model evaluation and performance when it appears in source documents or target summaries. In detail, we find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation; (2) the translationese in training sets would ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#35760;&#24518;&#31243;&#24230;&#65292;&#24182;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.03749</link><description>&lt;p&gt;
&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Memorization of Named Entities in Fine-tuned BERT Models. (arXiv:2212.03749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#35760;&#24518;&#31243;&#24230;&#65292;&#24182;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#20854;&#20013;&#19968;&#20010;&#39118;&#38505;&#26159;&#20174;&#35757;&#32451;&#22312;&#20010;&#20154;&#21644;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21629;&#21517;&#23454;&#20307;&#35760;&#24518;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20316;&#20026;&#20195;&#34920;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22312;&#23454;&#39564;&#20013;&#37319;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#32454;&#35843;&#35774;&#32622;&#65292;&#21253;&#25324;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35774;&#32622;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#39034;&#24207;&#25277;&#26679;&#31574;&#30053;&#21644;&#20004;&#31181;&#25552;&#31034;&#31574;&#30053;&#20174;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21019;&#24314;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#26679;&#26412;&#20013;&#25628;&#32034;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#26597;&#30475;&#23427;&#20204;&#26159;&#21542;&#20063;&#23384;&#22312;&#20110;&#32454;&#35843;&#25968;&#25454;&#38598;&#20013;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#37038;&#20214;&#21644;&#21338;&#23458;&#39046;&#22495;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;DP&#30340;&#24212;&#29992;&#23545;&#27979;&#35797;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preserving deep learning is an emerging field in machine learning that aims to mitigate the privacy risks in the use of deep neural networks. One such risk is training data extraction from language models that have been trained on datasets, which contain personal and privacy sensitive information. In our study, we investigate the extent of named entity memorization in fine-tuned BERT models. We use single-label text classification as representative downstream task and employ three different fine-tuning setups in our experiments, including one with Differentially Privacy (DP). We create a large number of text samples from the fine-tuned BERT models utilizing a custom sequential sampling strategy with two prompting strategies. We search in these samples for named entities and check if they are also present in the fine-tuning datasets. We experiment with two benchmark datasets in the domains of emails and blogs. We show that the application of DP has a detrimental effect on the te
&lt;/p&gt;</description></item></channel></rss>