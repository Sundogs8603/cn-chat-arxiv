<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;</title><link>https://rss.arxiv.org/abs/2401.09407</link><description>&lt;p&gt;
&#35299;&#35835;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;: &#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#35821;&#20041;&#30340;&#24191;&#20041;&#31574;&#30053;&#26469;&#26816;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.09407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;: &#39318;&#20808;&#65292;&#20182;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#26102;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#22330;&#26223;&#20013;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#30001;&#21508;&#31181;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;GPT-4&#21644;Dolly&#65292;&#24182;&#28085;&#30422;&#21508;&#31181;&#39046;&#22495;&#65292;&#20174;&#23398;&#26415;&#25163;&#31295;&#21040;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#35270;&#20026;&#20005;&#26684;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#26102;&#21463;&#21040;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#24187;&#35273;&#23545;&#35937;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#27604;&#29616;&#26377;&#25351;&#26631;CHAIR&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2404.02904</link><description>&lt;p&gt;
ALOHa&#65306;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26032;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
ALOHa: A New Measure for Hallucination in Captioning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#24187;&#35273;&#23545;&#35937;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#27604;&#29616;&#26377;&#25351;&#26631;CHAIR&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35270;&#35273;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#20250;&#20135;&#29983;&#21253;&#21547;&#38169;&#35823;&#30340;&#23383;&#24149;&#65292;&#27604;&#22914;&#22312;&#22330;&#26223;&#20013;&#23384;&#22312;&#24187;&#35273;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;&#20027;&#35201;&#24187;&#35273;&#23545;&#35937;&#24230;&#37327;&#26631;&#20934;CHAIR&#65292;&#20165;&#38480;&#20110;&#19968;&#32452;&#22266;&#23450;&#30340;MS COCO&#23545;&#35937;&#21644;&#21516;&#20041;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#24230;&#37327;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34913;&#37327;&#23545;&#35937;&#24187;&#35273;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#20174;&#20505;&#36873;&#23383;&#24149;&#20013;&#25552;&#21462;&#21487;&#36830;&#25509;&#30340;&#23545;&#35937;&#65292;&#34913;&#37327;&#23427;&#20204;&#19982;&#23383;&#24149;&#21644;&#23545;&#35937;&#26816;&#27979;&#20013;&#21442;&#32771;&#23545;&#35937;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#24182;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#29983;&#25104;&#26368;&#32456;&#30340;&#24187;&#35273;&#24471;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ALOHa&#22312;HAT&#19978;&#27604;CHAIR&#22312;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#24187;&#35273;&#26631;&#35760;&#30340;MS COCO&#23383;&#24149;&#30340;&#37329;&#26631;&#20934;&#23376;&#38598;&#19978;&#27491;&#30830;&#35782;&#21035;&#20102;&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#65288;&#22810;&#20986;13.6%&#65289;&#65292;&#22312;nocaps&#19978;&#65288;&#20854;&#20013;&#23545;&#35937;&#36229;&#20986;&#20102;MS COCO&#31867;&#21035;&#65289;&#35782;&#21035;&#20102;&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#65288;&#22810;&#33267;30.8%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.
&lt;/p&gt;</description></item><item><title>ChatGLM-Math&#36890;&#36807;&#33258;&#25105;&#25209;&#21028;&#31649;&#36947;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#26174;&#33879;&#22686;&#24378;</title><link>https://arxiv.org/abs/2404.02893</link><description>&lt;p&gt;
ChatGLM-Math:&#20351;&#29992;&#33258;&#25105;&#25209;&#21028;&#31649;&#36947;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02893
&lt;/p&gt;
&lt;p&gt;
ChatGLM-Math&#36890;&#36807;&#33258;&#25105;&#25209;&#21028;&#31649;&#36947;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#26174;&#33879;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25484;&#25569;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38656;&#35201;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22686;&#24378;LLMs&#25968;&#23398;&#33021;&#21147;&#30340;&#31574;&#30053;&#21644;&#25968;&#25454;&#38598;&#65292;&#20294;&#21516;&#26102;&#22312;&#37096;&#32626;&#30340;LLM&#31995;&#32479;&#20013;&#20445;&#25345;&#21644;&#25552;&#39640;&#35821;&#35328;&#21644;&#25968;&#23398;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#21046;&#20102;Self-Critique&#31649;&#36947;&#65292;&#35299;&#20915;&#20102;LLM&#23545;&#40784;&#30340;&#21453;&#39304;&#23398;&#20064;&#38454;&#27573;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;Math-Critique&#27169;&#22411;&#65292;&#20174;LLM&#26412;&#36523;&#25552;&#20379;&#21453;&#39304;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20381;&#27425;&#20351;&#29992;&#25298;&#32477;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;LLM&#33258;&#36523;&#29983;&#25104;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#22522;&#20110;ChatGLM3-32B&#65292;&#25105;&#20204;&#22312;&#23398;&#26415;&#39046;&#22495;&#21644;&#25105;&#20204;&#26032;&#21019;&#24314;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MathUserEval&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#31649;&#36947;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#30340;&#25968;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02893v1 Announce Type: new  Abstract: Large language models (LLMs) have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of LLM alignment. We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the LLM's mathematical pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02882</link><description>&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
Linear Attention Sequence Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24182;&#34892;&#65288;SP&#65289;&#20316;&#20026;&#19968;&#31181;&#22788;&#29702;&#36229;&#20986;&#21333;&#20010;GPU&#20869;&#23384;&#38480;&#21046;&#30340;&#38271;&#24207;&#21015;&#30340;&#27969;&#34892;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SP&#26041;&#27861;&#24182;&#26410;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24615;&#65292;&#23548;&#33268;&#22312;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24182;&#34892;&#25928;&#29575;&#21644;&#21487;&#29992;&#24615;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#39640;&#25928;SP&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21491;&#20056;&#20869;&#26680;&#25216;&#24039;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;SP&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#21644;&#20013;&#38388;&#29366;&#24577;&#32531;&#23384;&#26469;&#22686;&#24378;LASP&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#20351;LASP&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#30828;&#20214;&#21451;&#22909;&#24615;&#24471;&#21040;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31934;&#24515;&#30830;&#20445;&#24207;&#21015;&#32423;LASP&#19982;&#25152;&#26377;&#31867;&#22411;&#30340;&#25209;&#32423;&#25968;&#25454;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02882v1 Announce Type: cross  Abstract: Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data par
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#21442;&#25968;&#24322;&#36136;&#24615;&#30340;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CherryQ&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#20851;&#38190;&#21442;&#25968;&#30340;&#21516;&#26102;&#23558;&#20854;&#20313;&#21442;&#25968;&#39640;&#25928;&#37327;&#21270;&#33267;&#20302;&#31934;&#24230;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02837</link><description>&lt;p&gt;
&#26368;&#21518;&#25910;&#23448;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#24322;&#36136;&#24615;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02837
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#21442;&#25968;&#24322;&#36136;&#24615;&#30340;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CherryQ&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#20851;&#38190;&#21442;&#25968;&#30340;&#21516;&#26102;&#23558;&#20854;&#20313;&#21442;&#25968;&#39640;&#25928;&#37327;&#21270;&#33267;&#20302;&#31934;&#24230;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21442;&#25968;&#24322;&#36136;&#24615;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23569;&#37327;&#8220;&#27185;&#26691;&#8221;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#32780;&#32477;&#22823;&#22810;&#25968;&#21442;&#25968;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#22312;&#19981;&#21516;&#27169;&#22411;&#31995;&#21015;&#12289;&#35268;&#27169;&#21644;&#31867;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CherryQ&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#32479;&#19968;&#20102;&#28151;&#21512;&#31934;&#24230;&#21442;&#25968;&#30340;&#20248;&#21270;&#12290;CherryQ&#33021;&#22815;&#35782;&#21035;&#24182;&#20445;&#30041;&#39640;&#31934;&#24230;&#19979;&#20851;&#38190;&#30340;&#27185;&#26691;&#21442;&#25968;&#65292;&#21516;&#26102;&#23558;&#20854;&#20313;&#21442;&#25968;&#31215;&#26497;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;CherryQ&#30340;&#26377;&#25928;&#24615;&#12290;CherryQ&#22312;&#22256;&#24785;&#24230;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;3&#20301;&#37327;&#21270;Vicuna-1.5&#19982;&#23427;&#20204;&#30340;16&#20301;&#23545;&#24212;&#29289;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02837v1 Announce Type: new  Abstract: This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26816;&#32034;&#26041;&#27861;&#23545;&#20960;&#31181;&#32763;&#35793;&#26550;&#26500;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26816;&#32034;&#25216;&#26415;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#32763;&#35793;&#24471;&#20998;&#65292;&#22686;&#21152;&#31034;&#20363;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#36890;&#24120;&#23545;&#32763;&#35793;&#25928;&#26524;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02835</link><description>&lt;p&gt;
&#20174;&#35760;&#24518;&#20013;&#26816;&#32034;&#31034;&#20363;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65306;&#19968;&#20010;&#31995;&#32479;&#24615;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26816;&#32034;&#26041;&#27861;&#23545;&#20960;&#31181;&#32763;&#35793;&#26550;&#26500;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26816;&#32034;&#25216;&#26415;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#32763;&#35793;&#24471;&#20998;&#65292;&#22686;&#21152;&#31034;&#20363;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#36890;&#24120;&#23545;&#32763;&#35793;&#25928;&#26524;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;RAMT&#65289;&#26550;&#26500;&#20174;&#35760;&#24518;&#20013;&#26816;&#32034;&#31034;&#20363;&#65292;&#20197;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#36235;&#21183;&#20013;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#25506;&#32034;&#20102;&#21033;&#29992;&#26816;&#32034;&#31034;&#20363;&#30340;&#26032;&#26041;&#24335;&#65292;&#20294;&#19978;&#28216;&#26816;&#32034;&#27493;&#39588;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21464;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#23545;&#20960;&#31181;&#32763;&#35793;&#26550;&#26500;&#30340;&#24433;&#21709;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20004;&#20010;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22810;&#39046;&#22495;&#29615;&#22659;&#20013;&#23545;&#20004;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#32771;&#34385;&#22522;&#20110;&#26631;&#20934;&#33258;&#22238;&#24402;&#27169;&#22411;&#12289;&#22522;&#20110;&#32534;&#36753;&#30340;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#31181;&#19979;&#28216;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26816;&#32034;&#25216;&#26415;&#30340;&#36873;&#25321;&#24433;&#21709;&#20102;&#32763;&#35793;&#24471;&#20998;&#65292;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22686;&#21152;&#31034;&#20363;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#25928;&#26524;&#65292;&#36825;&#22312;&#25972;&#20307;&#19978;&#22823;&#22810;&#26159;&#31215;&#26497;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02835v1 Announce Type: new  Abstract: Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes. We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.
&lt;/p&gt;</description></item><item><title>Conifer&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;LLMs&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21450;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.02823</link><description>&lt;p&gt;
Conifer: &#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#32422;&#26463;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02823
&lt;/p&gt;
&lt;p&gt;
Conifer&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;LLMs&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21450;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#65292;LLMs&#22312;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#25351;&#20196;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#22797;&#26434;&#32422;&#26463;&#30340;&#25351;&#20196;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conifer&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;LLM&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#31574;&#21010;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#35843;&#26131;&#20110;&#38590;&#30340;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#20174;&#36807;&#31243;&#21453;&#39304;&#20013;&#23398;&#20064;&#12290;&#20351;&#29992;Conifer&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24102;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#25351;&#20196;&#12290;&#22312;&#20960;&#20010;&#36981;&#24490;&#25351;&#20196;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02823v1 Announce Type: cross  Abstract: The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02822</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#30340;&#27668;&#20505;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Identifying Climate Targets in National Laws and Policies using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#25919;&#31574;&#30446;&#26631;&#26159;&#27668;&#20505;&#25919;&#31574;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#20197;&#39046;&#22495;&#29305;&#23450;&#21644;&#25216;&#26415;&#24615;&#35821;&#35328;&#20026;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#31579;&#36873;&#20840;&#29699;&#27668;&#20505;&#25919;&#31574;&#30446;&#26631;&#30340;&#26041;&#27861;&#28041;&#21450;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#30446;&#21069;&#24456;&#23569;&#26377;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#22269;&#23478;&#27861;&#24459;&#25110;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#19982;&#20840;&#29699;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20026;&#25919;&#31574;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#25552;&#21450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#20102;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65288;&#20363;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#30446;&#26631;&#65289;&#65289;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21487;&#38752;&#22320;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#25105;&#20204;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#29305;&#23450;&#24180;&#20221;&#21644;&#22269;&#23478;&#21517;&#31216;&#20316;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#25511;&#21046;&#20174;&#20799;&#31461;&#21465;&#20107;&#25991;&#26412;&#20013;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#26088;&#22312;&#25511;&#21046;&#38382;&#39064;&#30340;&#26126;&#30830;&#24615;&#21644;&#28508;&#22312;&#21465;&#20107;&#20803;&#32032;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#27169;&#22411;&#24182;&#34892;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#35821;&#20041;&#25509;&#36817;&#24230;&#35780;&#20272;&#21644;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#65292;&#23569;&#26679;&#26412;&#31574;&#30053;&#36229;&#36234;&#20102;&#21442;&#32771;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02800</link><description>&lt;p&gt;
&#26377;&#20851;&#21465;&#20107;&#29702;&#35299;&#20013;&#21487;&#25511;&#38382;&#39064;&#22238;&#31572;&#29983;&#25104;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#25511;&#21046;&#20174;&#20799;&#31461;&#21465;&#20107;&#25991;&#26412;&#20013;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#26088;&#22312;&#25511;&#21046;&#38382;&#39064;&#30340;&#26126;&#30830;&#24615;&#21644;&#28508;&#22312;&#21465;&#20107;&#20803;&#32032;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#27169;&#22411;&#24182;&#34892;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#35821;&#20041;&#25509;&#36817;&#24230;&#35780;&#20272;&#21644;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#65292;&#23569;&#26679;&#26412;&#31574;&#30053;&#36229;&#36234;&#20102;&#21442;&#32771;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#29983;&#25104;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#21487;&#25511;&#38382;&#39064;&#29983;&#25104;&#26041;&#26696;&#20391;&#37325;&#20110;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25511;&#21046;&#20174;&#20799;&#31461;&#21465;&#20107;&#25991;&#26412;&#20013;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#26088;&#22312;&#25511;&#21046;&#20004;&#20010;&#23646;&#24615;&#65306;&#38382;&#39064;&#30340;&#26126;&#30830;&#24615;&#21644;&#28508;&#22312;&#21465;&#20107;&#20803;&#32032;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#19982;&#21442;&#32771;&#27169;&#22411;&#24182;&#34892;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#23569;&#26679;&#26412;&#31574;&#30053;&#36229;&#36234;&#21442;&#32771;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#20041;&#25509;&#36817;&#24230;&#35780;&#20272;&#20197;&#21450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#31561;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#24182;&#19981;&#24635;&#26159;&#32479;&#35745;&#19978;&#26174;&#33879;&#30340;&#12290;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02800v1 Announce Type: cross  Abstract: Question Generation aims to automatically generate questions based on a given input provided as context. A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control. In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts. We aim to control two attributes: the question's explicitness and underlying narrative elements. With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model. Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs. However, these improvements are not always statistically significant. The code is publicly available
&lt;/p&gt;</description></item><item><title>FPT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#29305;&#24449;&#23884;&#20837;&#35757;&#32451;&#36719;&#25552;&#31034;&#24182;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#25913;&#21892;&#20102;&#21487;&#35835;&#24615;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02772</link><description>&lt;p&gt;
FPT:&#29305;&#24449;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#23569;&#26679;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FPT: Feature Prompt Tuning for Few-shot Readability Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02772
&lt;/p&gt;
&lt;p&gt;
FPT&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#29305;&#24449;&#23884;&#20837;&#35757;&#32451;&#36719;&#25552;&#31034;&#24182;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#25913;&#21892;&#20102;&#21487;&#35835;&#24615;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#35835;&#24615;&#35780;&#20272;&#20219;&#21153;&#20013;&#65292;&#20256;&#32479;&#30340;&#25552;&#31034;&#26041;&#27861;&#32570;&#20047;&#20851;&#38190;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#32780;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#20851;&#20110;&#21033;&#29992;&#35821;&#35328;&#29305;&#24449;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#20855;&#26377;&#38750;&#31283;&#20581;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#21363;&#20855;&#26377;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#30340;&#29305;&#24449;&#25552;&#31034;&#35843;&#25972;&#65288;FPT&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21487;&#35757;&#32451;&#30340;&#36719;&#25552;&#31034;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#25490;&#21517;&#39034;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;FTP&#26041;&#27861;&#19981;&#20165;&#22312;&#20808;&#21069;&#26368;&#20339;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26041;&#27861;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02772v1 Announce Type: new  Abstract: Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#32858;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#31751;&#29983;&#25104;&#20505;&#36873;&#25552;&#31034;&#65292;&#32508;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#35780;&#20272;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#35780;&#20272;&#22120;&#20026;&#26032;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2404.02717</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Selection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02717
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#32858;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#31751;&#29983;&#25104;&#20505;&#36873;&#25552;&#31034;&#65292;&#32508;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#35780;&#20272;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#35780;&#20272;&#22120;&#20026;&#26032;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#25552;&#31034;&#25351;&#23548;&#19979;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#35201;&#20040;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20174;&#19968;&#32452;&#26377;&#38480;&#30340;&#21512;&#25104;&#20505;&#36873;&#25552;&#31034;&#20013;&#20026;&#32473;&#23450;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02717v1 Announce Type: new  Abstract: Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20132;&#26367;&#38405;&#35835;&#20219;&#21153;&#65288;ART&#65289;&#35821;&#26009;&#24211;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;&#19977;&#31181;&#23454;&#39564;&#26465;&#20214;&#21644;&#19977;&#20010;&#23376;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#22312;&#21463;&#25511;&#21644;&#19981;&#37027;&#20040;&#33258;&#21457;&#30340;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#35821;&#38899;&#23545;&#20301;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2404.02710</link><description>&lt;p&gt;
ART&#65306;&#29992;&#20110;&#35821;&#38899;&#23545;&#20301;&#21644;&#27169;&#20223;&#30340;&#20132;&#26367;&#38405;&#35835;&#20219;&#21153;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
ART: The Alternating Reading Task Corpus for Speech Entrainment and Imitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20132;&#26367;&#38405;&#35835;&#20219;&#21153;&#65288;ART&#65289;&#35821;&#26009;&#24211;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;&#19977;&#31181;&#23454;&#39564;&#26465;&#20214;&#21644;&#19977;&#20010;&#23376;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#22312;&#21463;&#25511;&#21644;&#19981;&#37027;&#20040;&#33258;&#21457;&#30340;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#35821;&#38899;&#23545;&#20301;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20132;&#26367;&#38405;&#35835;&#20219;&#21153;&#65288;ART&#65289;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#20132;&#27969;&#20013;&#23545;&#20301;&#21644;&#27169;&#20223;&#34892;&#20026;&#30340;&#21452;&#20154;&#21477;&#23376;&#38405;&#35835;&#38598;&#21512;&#12290;ART&#35821;&#26009;&#24211;&#21253;&#25324;&#19977;&#31181;&#23454;&#39564;&#26465;&#20214; - &#21333;&#29420;&#38405;&#35835;&#12289;&#20132;&#26367;&#38405;&#35835;&#21644;&#33988;&#24847;&#27169;&#20223; - &#20197;&#21450;&#21253;&#21547;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#21475;&#38899;&#33521;&#35821;&#30340;&#19977;&#20010;&#23376;&#35821;&#26009;&#24211;&#12290;&#35813;&#35774;&#35745;&#20801;&#35768;&#22312;&#21463;&#25511;&#21644;&#19981;&#37027;&#20040;&#33258;&#21457;&#30340;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#35821;&#38899;&#23545;&#20301;&#12290;&#38500;&#20102;&#35814;&#32454;&#30340;&#36716;&#24405;&#22806;&#65292;&#36824;&#21253;&#25324;&#33521;&#35821;&#29087;&#32451;&#24230;&#35780;&#20998;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#23454;&#39564;&#20013;&#30340;&#38382;&#21367;&#65292;&#29992;&#20110;&#25506;&#31350;&#23545;&#20301;&#29616;&#35937;&#21463;&#35821;&#35328;&#12289;&#20010;&#20154;&#21644;&#20154;&#38469;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20171;&#32461;&#28085;&#30422;&#20102;&#20854;&#35774;&#35745;&#12289;&#25910;&#38598;&#12289;&#26631;&#27880;&#36807;&#31243;&#12289;&#21021;&#27493;&#20998;&#26512;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#23637;&#26395;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02710v1 Announce Type: new  Abstract: We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication. The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three sub-corpora encompassing French-, Italian-, and Slovak-accented English. This design allows systematic investigation of speech entrainment in a controlled and less-spontaneous setting. Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment. Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;SCEN&#26041;&#27861;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#19987;&#23478;&#32593;&#32476;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#23454;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02699</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21046;&#21270;&#19987;&#23478;&#32593;&#32476;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Scalable Model Editing via Customized Expert Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02699
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;SCEN&#26041;&#27861;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#19987;&#23478;&#32593;&#32476;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#23454;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#30340;&#38382;&#39064;&#23545;&#20110;&#20854;&#21487;&#38752;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#21463;&#21040;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#27867;&#21270;&#21644;&#23545;&#19981;&#30456;&#20851;&#26679;&#26412;&#30340;&#24847;&#22806;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#36890;&#36807;&#23450;&#21046;&#21270;&#19987;&#23478;&#32593;&#32476;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#32534;&#36753;&#65288;SCEN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20004;&#20010;&#38454;&#27573;&#30340;&#36830;&#32493;&#35757;&#32451;&#33539;&#24335;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#38656;&#35201;&#26356;&#26032;&#30340;&#30693;&#35782;&#29255;&#27573;&#21333;&#29420;&#35757;&#32451;&#36731;&#37327;&#32423;&#19987;&#23478;&#32593;&#32476;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#27599;&#20010;&#19987;&#23478;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#26469;&#25511;&#21046;&#35813;&#19987;&#23478;&#30340;&#28608;&#27963;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#19981;&#21516;&#35268;&#27169;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;Llama2 7B &#21644; 13B &#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#30456;&#27604;&#29616;&#26377;&#20027;&#27969;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02699v1 Announce Type: new  Abstract: Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02690</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#39640;&#26031;&#20998;&#24067;&#36755;&#20837;&#19979;&#33258;&#28982;&#31232;&#30095;
&lt;/p&gt;
&lt;p&gt;
Attention is Naturally Sparse with Gaussian Distributed Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#24378;&#24230;&#26159;&#20851;&#38190;&#29942;&#39048;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;$O(n^2)$&#22797;&#26434;&#24230;&#12290;&#31232;&#30095;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#33655;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;LLMs&#20869;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#26031;&#36755;&#20837;&#26694;&#26550;&#19979;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#32452;&#22522;&#30784;&#20551;&#35774;&#24182;&#37319;&#29992;&#19968;&#31181;&#31995;&#32479;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#30340;&#20869;&#22312;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#34920;&#29616;&#24418;&#24335;&#30340;&#35814;&#32454;&#29702;&#35770;&#26816;&#26597;&#65292;&#25581;&#31034;&#20102;&#22312;&#35745;&#31639;&#33410;&#32422;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#20043;&#38388;&#28508;&#22312;&#26435;&#34913;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02690v1 Announce Type: cross  Abstract: The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21644;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20043;&#38388;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02684</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21644;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20043;&#38388;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#26469;&#36890;&#36807;&#25913;&#21464;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#35774;&#35745;&#23454;&#29616;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;(LCI)&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#26159;&#29366;&#24577;&#31354;&#38388;&#26426;&#22120;&#65288;SSMs&#65289;&#26550;&#26500;&#65292;&#23427;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#19982;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26550;&#26500;&#26356;&#25913;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23436;&#20840;&#39044;&#35757;&#32451;&#26435;&#37325;&#65292;&#36825;&#32473;&#24076;&#26395;&#20351;&#29992;&#26032;&#26550;&#26500;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#25104;&#26412;&#12290;&#21463;&#20256;&#32479;&#32447;&#24615;&#27880;&#24847;&#21147;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;(XATL)&#65292;&#20854;&#20013;LCI&#21644;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#22120;&#20043;&#38388;&#30340;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#22914;&#23618;&#35268;&#33539;&#12289;MLP&#12289;&#36755;&#20837;/&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02684v1 Announce Type: cross  Abstract: Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/outpu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28548;&#28165;&#35789;&#35821;&#21547;&#20041;&#26469;&#25913;&#21892;&#21388;&#24694;&#26816;&#27979;&#65292;PejorativITy&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24847;&#22823;&#21033;&#25512;&#25991;&#35821;&#26009;&#24211;&#65292;&#25581;&#31034;&#20102;&#23558;&#36140;&#25439;&#20449;&#24687;&#27880;&#20837;&#27169;&#22411;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02681</link><description>&lt;p&gt;
PejorativITy&#65306;&#28040;&#38500;&#34065;&#31216;&#35789;&#20197;&#25913;&#21892;&#24847;&#22823;&#21033;&#25512;&#25991;&#20013;&#30340;&#21388;&#24694;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28548;&#28165;&#35789;&#35821;&#21547;&#20041;&#26469;&#25913;&#21892;&#21388;&#24694;&#26816;&#27979;&#65292;PejorativITy&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24847;&#22823;&#21033;&#25512;&#25991;&#35821;&#26009;&#24211;&#65292;&#25581;&#31034;&#20102;&#23558;&#36140;&#25439;&#20449;&#24687;&#27880;&#20837;&#27169;&#22411;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21388;&#24694;&#24448;&#24448;&#36890;&#36807;&#27604;&#21947;&#35821;&#35328;&#34920;&#36798;&#12290;&#24403;&#19968;&#20123;&#20013;&#24615;&#35789;&#35821;&#20316;&#20026;&#36140;&#25439;&#31216;&#35859;&#26102;&#65292;&#21487;&#33021;&#20250;&#21576;&#29616;&#36127;&#38754;&#21547;&#20041;&#12290;&#28548;&#28165;&#36825;&#31867;&#35789;&#35821;&#30340;&#21547;&#20041;&#21487;&#33021;&#26377;&#21161;&#20110;&#26816;&#27979;&#21388;&#24694;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PejorativITy&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#30001;1,200&#26465;&#24847;&#22823;&#21033;&#25512;&#25991;&#26500;&#25104;&#30340;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#26631;&#27880;&#36140;&#25439;&#35821;&#35328;&#65292;&#24182;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#26631;&#27880;&#21388;&#24694;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;&#28548;&#28165;&#35789;&#35821;&#20449;&#24687;&#27880;&#20837;&#38024;&#23545;&#21388;&#24694;&#26816;&#27979;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27880;&#20837;&#26041;&#27861;&#65306;&#23558;&#36140;&#25439;&#20449;&#24687;&#36830;&#25509;&#22312;&#19968;&#36215;&#21644;&#23558;&#27169;&#26865;&#20004;&#21487;&#30340;&#35789;&#35821;&#26367;&#25442;&#20026;&#26126;&#30830;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36824;&#26159;&#22312;&#24847;&#22823;&#21033;&#25512;&#25991;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#19978;&#65292;&#37117;&#34920;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#22343;&#23548;&#33268;&#26356;&#22823;&#30340;&#20998;&#31867;&#25913;&#21892;&#65292;&#34920;&#26126;&#35789;&#20041;&#28040;&#27495;&#26159;&#21388;&#24694;&#26816;&#27979;&#30340;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02681v1 Announce Type: cross  Abstract: Misogyny is often expressed through figurative language. Some neutral words can assume a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms. Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misog
&lt;/p&gt;</description></item><item><title>&#35813;&#25361;&#25112;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22768;&#38899;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#38544;&#34255;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#25252;&#35821;&#35328;&#20869;&#23481;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2404.02677</link><description>&lt;p&gt;
VoicePrivacy 2024&#25361;&#25112;&#35780;&#20272;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
The VoicePrivacy 2024 Challenge Evaluation Plan
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25361;&#25112;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22768;&#38899;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#38544;&#34255;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#25252;&#35821;&#35328;&#20869;&#23481;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#30340;&#20219;&#21153;&#26159;&#20026;&#35821;&#38899;&#25968;&#25454;&#24320;&#21457;&#19968;&#31181;&#22768;&#38899;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38544;&#34255;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#25252;&#35821;&#35328;&#20869;&#23481;&#21644;&#24773;&#24863;&#29366;&#24577;&#12290;&#32452;&#32455;&#32773;&#25552;&#20379;&#24320;&#21457;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#33050;&#26412;&#65292;&#20197;&#21450;&#22522;&#20110;&#21442;&#19982;&#32773;&#35201;&#27714;&#24418;&#25104;&#30340;&#22522;&#32447;&#21311;&#21517;&#21270;&#31995;&#32479;&#21644;&#22521;&#35757;&#36164;&#28304;&#21015;&#34920;&#12290;&#21442;&#19982;&#32773;&#24212;&#29992;&#20182;&#20204;&#24320;&#21457;&#30340;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#36816;&#34892;&#35780;&#20272;&#33050;&#26412;&#65292;&#24182;&#23558;&#35780;&#20272;&#32467;&#26524;&#21644;&#21311;&#21517;&#21270;&#30340;&#35821;&#38899;&#25968;&#25454;&#25552;&#20132;&#32473;&#32452;&#32455;&#32773;&#12290;&#32467;&#26524;&#23558;&#22312;&#19982;Interspeech 2024&#21516;&#26399;&#20030;&#21150;&#30340;&#30740;&#35752;&#20250;&#19978;&#23637;&#31034;&#65292;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#34987;&#36992;&#35831;&#23637;&#31034;&#20182;&#20204;&#30340;&#25361;&#25112;&#31995;&#32479;&#24182;&#25552;&#20132;&#39069;&#22806;&#30340;&#30740;&#35752;&#20250;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02677v1 Announce Type: cross  Abstract: The task of the challenge is to develop a voice anonymization system for speech data which conceals the speaker's voice identity while protecting linguistic content and emotional states. The organizers provide development and evaluation datasets and evaluation scripts, as well as baseline anonymization systems and a list of training resources formed on the basis of the participants' requests. Participants apply their developed anonymization systems, run evaluation scripts and submit evaluation results and anonymized speech data to the organizers. Results will be presented at a workshop held in conjunction with Interspeech 2024 to which all participants are invited to present their challenge systems and to submit additional workshop papers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#35299;&#20026;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#31572;&#26696;&#30340;&#24544;&#23454;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02655</link><description>&lt;p&gt;
&#36890;&#36807;&#35825;&#23548;&#24544;&#23454;&#24615;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Calibrating the Confidence of Large Language Models by Eliciting Fidelity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#35299;&#20026;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#31572;&#26696;&#30340;&#24544;&#23454;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;RLHF&#31561;&#25216;&#26415;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23545;&#40784;&#65292;&#26082;&#26377;&#24110;&#21161;&#24615;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#40784;&#20043;&#21518;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#65292;&#34920;&#36798;&#30340;&#32622;&#20449;&#24230;&#24182;&#19981;&#20934;&#30830;&#22320;&#19982;&#20854;&#27491;&#30830;&#29575;&#26657;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#35299;&#20026;&#20851;&#20110;&#38382;&#39064;&#30340;\textit{&#19981;&#30830;&#23450;&#24615;}&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;\textit{&#24544;&#23454;&#24615;}&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;MCQA&#25968;&#25454;&#38598;&#19978;&#23545;6&#20010;RLHF-LMs&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;IPR&#21644;CE&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#23545;\textit{&#30495;&#27491;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;}&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#32447;&#65292;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02655v1 Announce Type: new  Abstract: Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \textit{Uncertainty} about the question and the \textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#30528;&#37325;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02650</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards detecting unanticipated bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#30528;&#37325;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#20197;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31867;&#20284;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#20998;&#26512;&#21644;&#37327;&#21270;&#36825;&#20123;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#23545;&#36825;&#20123;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#21046;&#23450;&#20943;&#36731;&#31574;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#19982;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#26063;&#35028;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#24456;&#26126;&#26174;&#65292;LLMs&#20063;&#21463;&#21040;&#20854;&#20182;&#19981;&#22826;&#26126;&#26174;&#30340;&#20869;&#38544;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#36890;&#24120;&#30340;&#19981;&#36879;&#26126;&#24615;&#20351;&#24471;&#26816;&#27979;&#36825;&#20123;&#20559;&#35265;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#28508;&#22312;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;LLMs&#20013;&#26816;&#27979;&#36825;&#20123;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#20855;&#20307;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02650v1 Announce Type: cross  Abstract: Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty
&lt;/p&gt;</description></item><item><title>Diff-Comb Explainer&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#30456;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2404.02625</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#21487;&#24494;&#20998;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02625
&lt;/p&gt;
&lt;p&gt;
Diff-Comb Explainer&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#30456;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Integer Linear Programming&#65288;ILP&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#36827;&#34892;&#31934;&#30830;&#32467;&#26500;&#21644;&#35821;&#20041;&#32422;&#26463;&#32534;&#30721;&#30340;&#27491;&#24335;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ILP&#26694;&#26550;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#32473;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#35821;&#35328;&#34920;&#31034;&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;Diff-Comb Explainer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#65288;DBCS&#65289;&#30340;&#35299;&#37322;&#22411;NLI&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#27714;&#35299;&#22120;&#19981;&#21516;&#65292;Diff-Comb Explainer&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#12289;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#22320;&#23558;&#31070;&#32463;&#34920;&#31034;&#34701;&#20837;&#21040;ILP&#20844;&#24335;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;ILP&#27714;&#35299;&#22120;&#12289;&#31070;&#32463;&#31526;&#21495;&#40657;&#30418;&#27714;&#35299;&#22120;&#21644;Trans&#30456;&#27604;&#65292;Diff-Comb Explainer&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02625v1 Announce Type: cross  Abstract: Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Trans
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#26126;&#30830;&#30340;&#22240;&#26524;&#22270;&#65292;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;NLI&#27169;&#22411;&#20013;&#29305;&#23450;&#33258;&#28982;&#36923;&#36753;&#25512;&#29702;&#27169;&#24335;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#21644;&#37327;&#21270;&#31995;&#32479;&#24615;&#25512;&#29702;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2404.02622</link><description>&lt;p&gt;
&#20272;&#35745;&#22522;&#20110;Transformer&#30340;NLI&#27169;&#22411;&#20013;&#33258;&#28982;&#36923;&#36753;&#29305;&#24449;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02622
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#26126;&#30830;&#30340;&#22240;&#26524;&#22270;&#65292;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;NLI&#27169;&#22411;&#20013;&#29305;&#23450;&#33258;&#28982;&#36923;&#36753;&#25512;&#29702;&#27169;&#24335;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#21644;&#37327;&#21270;&#31995;&#32479;&#24615;&#25512;&#29702;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#20013;&#35821;&#20041;&#29305;&#24449;&#23545;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#22240;&#26524;&#25928;&#24212;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#26159;&#22256;&#38590;&#30340;&#12290;&#28982;&#32780;&#65292;&#20174;&#35299;&#37322;&#24615;&#21644;&#27169;&#22411;&#35780;&#20272;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#20998;&#26512;&#24418;&#24335;&#26159;&#38750;&#24120;&#26377;&#30410;&#30340;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30740;&#31350;&#20855;&#26377;&#36275;&#22815;&#32467;&#26500;&#21644;&#35268;&#24459;&#24615;&#30340;&#29305;&#23450;&#25512;&#29702;&#27169;&#24335;&#65292;&#20197;&#35782;&#21035;&#21644;&#37327;&#21270;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#25512;&#29702;&#22833;&#36133;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#36873;&#25321;NLI&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#65292;&#20854;&#20013;&#21487;&#20197;&#31995;&#32479;&#26500;&#24314;&#26174;&#24335;&#22240;&#26524;&#22270;&#65306;&#36328;&#20004;&#20010;&#21477;&#23376;&#65288;&#21069;&#25552;&#21644;&#20551;&#35774;&#65289;&#65292;&#20004;&#20010;&#30456;&#20851;&#21333;&#35789;/&#26415;&#35821;&#22312;&#20849;&#20139;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31574;&#30053;&#26469;&#34913;&#37327;&#19978;&#19979;&#25991;&#24178;&#39044;&#30340;&#25928;&#24212;&#65288;&#20854;&#23545;&#34164;&#28085;&#26631;&#31614;&#30340;&#24433;&#21709;&#26159;&#36890;&#36807;&#35821;&#20041;&#21333;&#35843;&#29305;&#24449;&#36827;&#34892;&#20171;&#23548;&#65289;&#20197;&#21450;&#23545;&#25554;&#20837;&#30340;&#35789;&#23545;&#30340;&#24178;&#39044;&#65288;&#20854;&#23545;&#34164;&#28085;&#26631;&#31614;&#30340;&#24433;&#21709;&#26159;&#36890;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02622v1 Announce Type: new  Abstract: Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of reasoning with enough structure and regularity to identify and quantify systematic reasoning failures in widely-used models. In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#35780;&#20998;&#25351;&#23548;&#31181;&#23376;&#35789;&#21521;&#37327;&#65292;&#25105;&#20204;&#22312;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#21644;&#39118;&#26684;&#23646;&#24615;&#26102;&#33719;&#24471;&#20102;&#20855;&#26377;&#26174;&#30528;&#26356;&#22909;&#24615;&#33021;&#30340;&#21487;&#35299;&#37322;&#32500;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02619</link><description>&lt;p&gt;
&#29992;&#20154;&#31867;&#21028;&#26029;&#35843;&#25972;&#23884;&#20837;&#31354;&#38388;&#20013;&#21487;&#35299;&#37322;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adjusting Interpretable Dimensions in Embedding Space with Human Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02619
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#35780;&#20998;&#25351;&#23548;&#31181;&#23376;&#35789;&#21521;&#37327;&#65292;&#25105;&#20204;&#22312;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#21644;&#39118;&#26684;&#23646;&#24615;&#26102;&#33719;&#24471;&#20102;&#20855;&#26377;&#26174;&#30528;&#26356;&#22909;&#24615;&#33021;&#30340;&#21487;&#35299;&#37322;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#31354;&#38388;&#21253;&#21547;&#25351;&#31034;&#24615;&#21035;&#12289;&#39118;&#26684;&#27491;&#24335;&#24615;&#29978;&#33267;&#23545;&#35937;&#23646;&#24615;&#30340;&#21487;&#35299;&#37322;&#32500;&#24230;&#65292;&#36825;&#24050;&#32463;&#34987;&#22810;&#27425;&#35266;&#23519;&#21040;&#12290;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#32500;&#24230;&#27491;&#21464;&#25104;&#19981;&#21516;&#39046;&#22495;&#30740;&#31350;&#20013;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20174;&#31038;&#20250;&#31185;&#23398;&#21040;&#31070;&#32463;&#31185;&#23398;&#12290;&#35745;&#31639;&#36825;&#20123;&#32500;&#24230;&#30340;&#26631;&#20934;&#26041;&#27861;&#20351;&#29992;&#23545;&#27604;&#31181;&#23376;&#35789;&#24182;&#35745;&#31639;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#21521;&#37327;&#12290;&#36825;&#26159;&#31616;&#21333;&#30340;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#31181;&#23376;&#30340;&#21521;&#37327;&#19982;&#20154;&#31867;&#23545;&#29305;&#23450; &#32500;&#24230;&#20013;&#35789;&#33853;&#22312;&#20309;&#22788;&#30340;&#35780;&#20998;&#32467;&#21512;&#65292;&#35780;&#20272;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#22914;&#22823;&#23567;&#21644;&#21361;&#38505;&#24615;&#65292;&#20197;&#21450;&#39118;&#26684;&#23646;&#24615;&#22914;&#27491;&#24335;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#21487;&#35299;&#37322;&#32500;&#24230;&#22312;&#29305;&#21035;&#22312;&#31181;&#23376;&#32500;&#24230;&#25928;&#26524;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02619v1 Announce Type: new  Abstract: Embedding spaces contain interpretable dimensions indicating gender, formality in style, or even object properties. This has been observed multiple times. Such interpretable dimensions are becoming valuable tools in different areas of study, from social science to neuroscience. The standard way to compute these dimensions uses contrasting seed words and computes difference vectors over them. This is simple but does not always work well. We combine seed-based vectors with guidance from human ratings of where words fall along a specific dimension, and evaluate on predicting both object properties like size and danger, and the stylistic properties of formality and complexity. We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02616</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#36827;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02616
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#20027;&#39064;&#30456;&#20851;&#24615;&#26159;&#31038;&#20132;&#25628;&#32034;&#30340;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#21487;&#20197;&#35780;&#20272;&#25991;&#26723;&#19982;&#29992;&#25143;&#38656;&#27714;&#20043;&#38388;&#30340;&#21305;&#37197;&#31243;&#24230;&#12290;&#22312;&#22823;&#22810;&#25968;&#31038;&#20132;&#25628;&#32034;&#22330;&#26223;&#20013;&#65292;&#22914;&#22823;&#20247;&#28857;&#35780;&#65292;&#24314;&#27169;&#25628;&#32034;&#30456;&#20851;&#24615;&#24635;&#26159;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#20010;&#26159;&#35768;&#22810;&#31038;&#20132;&#25628;&#32034;&#20013;&#30340;&#25991;&#26723;&#38750;&#24120;&#38271;&#19988;&#21253;&#21547;&#22823;&#37327;&#20887;&#20313;&#20449;&#24687;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#25628;&#32034;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#33719;&#24471;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22810;&#20998;&#31867;&#30456;&#20851;&#24615;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#26597;&#35810;&#19982;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#20197;&#21450;&#19981;&#24102;&#26597;&#35810;&#30340;&#25991;&#26723;&#25688;&#35201;&#21512;&#24182;&#65292;&#20316;&#20026;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36825;&#26377;&#21161;&#20110;&#27169;&#22411;&#23398;&#20064;&#26597;&#35810;&#21644;&#25991;&#26723;&#26680;&#24515;&#20027;&#39064;&#20043;&#38388;&#30340;&#30456;&#20851;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#37325;&#26032;&#32534;&#20889;&#21644;&#29983;&#25104;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02616v1 Announce Type: cross  Abstract: Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#32508;&#21512;&#24314;&#27169;&#21477;&#27861;&#21644;&#22768;&#23398;&#32447;&#32034;&#65292;&#33021;&#22815;&#22312;&#22521;&#35757;&#26102;&#26174;&#31034;&#30701;&#38899;&#39057;&#21098;&#36753;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#38889;&#35821;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#65292;&#35299;&#20915;&#20102;&#20572;&#39039;&#38169;&#35823;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.02592</link><description>&lt;p&gt;
&#21033;&#29992;&#21477;&#27861;&#21644;&#22768;&#23398;&#32447;&#32034;&#30456;&#20114;&#20316;&#29992;&#20248;&#21270;&#38889;&#35821;TTS&#20572;&#39039;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02592
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#32508;&#21512;&#24314;&#27169;&#21477;&#27861;&#21644;&#22768;&#23398;&#32447;&#32034;&#65292;&#33021;&#22815;&#22312;&#22521;&#35757;&#26102;&#26174;&#31034;&#30701;&#38899;&#39057;&#21098;&#36753;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#38889;&#35821;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#65292;&#35299;&#20915;&#20102;&#20572;&#39039;&#38169;&#35823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#65292;&#36798;&#21040;&#20102;&#19982;&#20154;&#31867;&#35821;&#38899;&#30456;&#23218;&#32654;&#30340;&#36136;&#37327;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#23601;&#20027;&#35201;&#22312;&#33521;&#35821;&#31561;&#39640;&#36164;&#28304;&#35821;&#35328;&#32972;&#26223;&#19979;&#24471;&#21040;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;&#38889;&#35821;&#26102;&#65292;Tacotron&#21644;FastSpeech&#21464;&#20307;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#20572;&#39039;&#38169;&#35823;&#65292;&#24433;&#21709;&#20102;&#35821;&#38899;&#24863;&#30693;&#21644;&#33258;&#28982;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19982;&#20572;&#39039;&#27169;&#24335;&#30456;&#20851;&#30340;&#21477;&#27861;&#21644;&#22768;&#23398;&#32447;&#32034;&#30340;&#20840;&#38754;&#24314;&#27169;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#22312;&#22521;&#35757;&#26102;&#26174;&#31034;&#38271;&#24230;&#36739;&#30701;&#30340;&#38899;&#39057;&#21098;&#36753;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#38024;&#23545;&#26356;&#20026;&#22797;&#26434;&#30340;&#22495;&#22806;&#65288;OOD&#65289;&#21477;&#23376;&#65292;&#20063;&#33021;&#22987;&#32456;&#29983;&#25104;&#33258;&#28982;&#30340;&#35821;&#38899;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02592v1 Announce Type: new  Abstract: Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech. Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English. Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness. In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns. Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips. Archi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Affective-NLI&#29992;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#20013;&#20154;&#26684;&#35782;&#21035;&#65292;&#20197;&#21033;&#29992;&#23545;&#35805;&#20869;&#23481;&#20013;&#30340;&#24773;&#24863;&#22240;&#32032;&#36827;&#34892;&#20934;&#30830;&#30340;&#20154;&#26684;&#35782;&#21035;</title><link>https://arxiv.org/abs/2404.02589</link><description>&lt;p&gt;
Affective-NLI: &#26397;&#30528;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#20013;&#20154;&#26684;&#35782;&#21035;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Affective-NLI&#29992;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#20013;&#20154;&#26684;&#35782;&#21035;&#65292;&#20197;&#21033;&#29992;&#23545;&#35805;&#20869;&#23481;&#20013;&#30340;&#24773;&#24863;&#22240;&#32032;&#36827;&#34892;&#20934;&#30830;&#30340;&#20154;&#26684;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#65288;PRC&#65289;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#23545;&#35805;&#20869;&#23481;&#35782;&#21035;&#35828;&#35805;&#32773;&#30340;&#20154;&#26684;&#29305;&#24449;&#12290;&#36825;&#23545;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#27835;&#30103;&#21644;&#20026;&#32769;&#24180;&#20154;&#25552;&#20379;&#20276;&#20387;&#26426;&#22120;&#20154;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20998;&#26512;&#23545;&#35805;&#20869;&#23481;&#36827;&#34892;&#20154;&#26684;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23545;&#35805;&#20013;&#21253;&#21547;&#30340;&#20851;&#38190;&#38544;&#21547;&#22240;&#32032;&#65288;&#22914;&#21453;&#26144;&#35828;&#35805;&#32773;&#20154;&#26684;&#30340;&#24773;&#32490;&#65289;&#34987;&#24573;&#30053;&#12290;&#20854;&#27425;&#65292;&#20165;&#20851;&#27880;&#36755;&#20837;&#23545;&#35805;&#20869;&#23481;&#24573;&#30053;&#20102;&#23545;&#20010;&#24615;&#26412;&#36523;&#35821;&#20041;&#29702;&#35299;&#65292;&#38477;&#20302;&#20102;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;PRC&#30340;&#24773;&#24863;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;Affective-NLI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02589v1 Announce Type: cross  Abstract: Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#23558;&#21475;&#35821;&#29702;&#35299;&#31995;&#32479;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#65292;&#25913;&#36827;&#20102;&#22810;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02588</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23558;&#21475;&#35821;&#29702;&#35299;&#31995;&#32479;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02588
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#23558;&#21475;&#35821;&#29702;&#35299;&#31995;&#32479;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#65292;&#25913;&#36827;&#20102;&#22810;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#27169;&#22411;&#26159;&#35821;&#38899;&#21161;&#25163;&#65288;&#22914;Alexa&#12289;Bixby&#21644;Google Assistant&#65289;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;SLU&#31995;&#32479;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#30340;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#23545;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#24494;&#35843;&#20197;&#22788;&#29702;&#27133;&#26631;&#27880;&#30340;SLU&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20113;&#22330;&#26223;&#20013;&#20351;&#29992;mBERT&#27169;&#22411;&#25913;&#36827;&#20102;MultiATIS ++&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#22810;&#35821;&#35328;SLU&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;Fine and Coarse-grained Multi-Task Learning Framework&#65288;FC-MTLF&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#8220;&#24635;&#20307;&#20934;&#30830;&#29575;&#8221;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#36827;&#27493;&#65306;&#20174;53%&#25552;&#39640;&#21040;62.18%&#12290;&#22312;&#35774;&#22791;&#19978;&#30340;&#24773;&#26223;&#20013;&#65288;&#23567;&#22411;&#19988;&#26410;&#39044;&#35757;&#32451;&#30340;SLU&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#8220;&#24635;&#20307;&#20934;&#30830;&#29575;&#8221;&#25351;&#26631;&#20174;5.31%&#25552;&#39640;&#21040;22.06%&#65292;&#36229;&#36807;&#20102;&#22522;&#32447;Global-Local Contrastive Learning Framework&#65288;GL-CLeF&#65289;&#26041;&#27861;&#12290;&#19982;FC-MTLF&#21644;GL-CLeF&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#8230;&#65288;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02588v1 Announce Type: new  Abstract: Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data. Our approach improved on the MultiATIS++ benchmark, a primary multi-language SLU dataset, in the cloud scenario using an mBERT model. Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation doe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#65288;MGFiD&#65289;&#65292;&#36890;&#36807;&#36328;&#22810;&#20010;&#31890;&#24230;&#36776;&#21035;&#35777;&#25454;&#65292;&#24182;&#32467;&#21512;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#21644;&#21477;&#23376;&#20998;&#31867;&#65292;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#35299;&#30721;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02581</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multi-Granularity Guided Fusion-in-Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#65288;MGFiD&#65289;&#65292;&#36890;&#36807;&#36328;&#22810;&#20010;&#31890;&#24230;&#36776;&#21035;&#35777;&#25454;&#65292;&#24182;&#32467;&#21512;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#21644;&#21477;&#23376;&#20998;&#31867;&#65292;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#35299;&#30721;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#65292;&#35782;&#21035;&#30456;&#20851;&#19978;&#19979;&#25991;&#20316;&#20026;&#35777;&#25454;&#24182;&#36991;&#20813;&#22312;&#26816;&#32034;&#32467;&#26524;&#20013;&#20986;&#29616;&#34394;&#20551;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35299;&#30721;&#38454;&#27573;&#20351;&#29992;&#22810;&#20010;&#19978;&#19979;&#25991;&#36827;&#34892;&#20018;&#32852;&#30340;&#27169;&#22411;&#26550;&#26500;&#65288;&#21363;Fusion-in-Decoder&#65289;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#20174;&#30475;&#20284;&#21512;&#29702;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#65288;MGFiD&#65289;&#65292;&#36328;&#22810;&#20010;&#31890;&#24230;&#36776;&#21035;&#35777;&#25454;&#12290;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;MGFiD&#23558;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#19982;&#21477;&#23376;&#20998;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#23427;&#23558;&#26126;&#26174;&#30340;&#21477;&#23376;&#32858;&#21512;&#21040;&#19968;&#20010;&#38170;&#23450;&#21521;&#37327;&#20013;&#65292;&#25351;&#23548;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#37325;&#29992;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#30340;&#32467;&#26524;&#26469;&#25552;&#39640;&#35299;&#30721;&#25928;&#29575;&#36827;&#34892;&#27573;&#33853;&#20462;&#21098;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MGFiD&#22312;&#33258;&#28982;&#38382;&#31572;&#65288;NQ&#65289;&#21644;TriviaQA&#65288;TQA&#65289;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02581v1 Announce Type: new  Abstract: In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, i.e., Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an anchor vector that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for passage pruning. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Think-and-Execute&#26694;&#26550;&#65292;&#23558;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24605;&#32771;&#21644;&#25191;&#34892;&#31934;&#32454;&#20998;&#35299;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02575</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#35793;&#22120;&#65306;&#27169;&#25311;&#20266;&#20195;&#30721;&#25191;&#34892;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Think-and-Execute&#26694;&#26550;&#65292;&#23558;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24605;&#32771;&#21644;&#25191;&#34892;&#31934;&#32454;&#20998;&#35299;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#25512;&#29702;&#25351;&#30340;&#26159;&#29702;&#35299;&#38382;&#39064;&#32972;&#21518;&#22797;&#26434;&#27169;&#24335;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#36890;&#24448;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#31995;&#21015;&#25512;&#29702;&#27493;&#39588;&#30340;&#33021;&#21147;&#12290;&#31639;&#27861;&#25512;&#29702;&#30340;&#36825;&#31181;&#24615;&#36136;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#25361;&#25112;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#65289;&#26469;&#34920;&#36798;&#35299;&#20915;&#32473;&#23450;&#23454;&#20363;/&#38382;&#39064;&#25152;&#38656;&#36923;&#36753;&#30340;&#26041;&#24335;&#65292;&#36825;&#21463;&#21040;&#20854;&#20005;&#26684;&#21644;&#31934;&#30830;&#30340;&#35821;&#27861;&#21551;&#21457;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#20010;&#25512;&#29702;&#35843;&#29992;&#20013;&#32534;&#20889;&#34920;&#36798;&#27491;&#30830;&#36923;&#36753;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#24182;&#38750;&#26131;&#20107;&#12290;&#27492;&#22806;&#65292;&#20026;&#19968;&#20010;&#23454;&#20363;&#19987;&#38376;&#29983;&#25104;&#30340;&#20195;&#30721;&#26080;&#27861;&#37325;&#29992;&#20110;&#20854;&#20182;&#23454;&#20363;&#65292;&#21363;&#20351;&#23427;&#20204;&#26469;&#33258;&#30456;&#21516;&#30340;&#20219;&#21153;&#24182;&#21487;&#33021;&#38656;&#35201;&#30456;&#21516;&#30340;&#36923;&#36753;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Think-and-Execute&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#25512;&#29702;&#21644;&#25191;&#34892;&#36827;&#34892;&#20102;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02575v1 Announce Type: new  Abstract: Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36328;&#35821;&#35328;&#25991;&#26412;&#30456;&#20851;&#24615;&#20013;&#36873;&#25321;&#28304;&#35821;&#35328;&#30340;&#31574;&#30053;&#65292;&#23588;&#20854;&#20851;&#27880;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21333;&#28304;&#36716;&#31227;&#12289;&#22810;&#28304;&#36716;&#31227;&#21644;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#26368;&#32456;&#22312;C8&#65288;Kinyarwanda&#65289;&#27979;&#35797;&#38598;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>https://arxiv.org/abs/2404.02570</link><description>&lt;p&gt;
SemEval-2024&#20219;&#21153;1&#30340;MaiNLP&#65306;&#36328;&#35821;&#35328;&#25991;&#26412;&#30456;&#20851;&#24615;&#20013;&#28304;&#35821;&#35328;&#36873;&#25321;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36328;&#35821;&#35328;&#25991;&#26412;&#30456;&#20851;&#24615;&#20013;&#36873;&#25321;&#28304;&#35821;&#35328;&#30340;&#31574;&#30053;&#65292;&#23588;&#20854;&#20851;&#27880;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21333;&#28304;&#36716;&#31227;&#12289;&#22810;&#28304;&#36716;&#31227;&#21644;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#26368;&#32456;&#22312;C8&#65288;Kinyarwanda&#65289;&#27979;&#35797;&#38598;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65306;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#36328;&#35821;&#35328;Track C&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#22312;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#38646;-shot&#36328;&#35821;&#35328;&#36716;&#31227;&#65289;&#26816;&#27979;&#32473;&#23450;&#30446;&#26631;&#35821;&#35328;&#20013;&#20004;&#20010;&#21477;&#23376;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;XLM-R&#21644;Furina&#19978;&#30340;&#19981;&#21516;&#28304;&#35821;&#35328;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;1&#65289;&#21333;&#28304;&#36716;&#31227;&#65292;&#24182;&#22522;&#20110;&#31867;&#22411;&#30456;&#20284;&#24615;&#36873;&#25321;&#28304;&#35821;&#35328;&#65292;2&#65289;&#29992;&#20004;&#20010;&#26368;&#25509;&#36817;&#30340;&#28304;&#35821;&#35328;&#22686;&#24378;&#33521;&#35821;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;3&#65289;&#22810;&#28304;&#36716;&#31227;&#65292;&#27604;&#36739;&#25152;&#26377;&#35757;&#32451;&#35821;&#35328;&#19982;&#26469;&#33258;&#21516;&#19968;&#35821;&#35328;&#23478;&#26063;&#30340;&#35821;&#35328;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#25193;&#20805;&#21644;&#33050;&#26412;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#22312;C8&#65288;Kinyarwanda&#65289;&#27979;&#35797;&#38598;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02570v1 Announce Type: new  Abstract: This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to detect semantic relatedness of two sentences in a given target language without access to direct supervision (i.e. zero-shot cross-lingual transfer). To this end, we focus on different source language selection strategies on two different pre-trained languages models: XLM-R and Furina. We experiment with 1) single-source transfer and select source languages based on typological similarity, 2) augmenting English training data with the two nearest-neighbor source languages, and 3) multi-source transfer where we compare selecting on all training languages against languages from the same family. We further study machine translation-based data augmentation and the impact of script differences. Our submission achieved the first place in the C8 (Kinyarwanda) test set.
&lt;/p&gt;</description></item><item><title>CSEPrompts&#26159;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#32534;&#31243;&#32451;&#20064;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02540</link><description>&lt;p&gt;
CSEPrompts: &#21021;&#32423;&#35745;&#31639;&#26426;&#31185;&#23398;&#25552;&#31034;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CSEPrompts: A Benchmark of Introductory Computer Science Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02540
&lt;/p&gt;
&lt;p&gt;
CSEPrompts&#26159;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#32534;&#31243;&#32451;&#20064;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#26032;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#24120;&#25317;&#26377;&#25968;&#19975;&#20159;&#21442;&#25968;&#12290;&#21830;&#19994;&#24212;&#29992;&#65288;&#22914;ChatGPT&#65289;&#24050;&#20351;&#36825;&#39033;&#25216;&#26415;&#38754;&#21521;&#26222;&#36890;&#22823;&#20247;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;LLMs&#20026;&#23398;&#26415;&#21644;&#19987;&#19994;&#29992;&#36884;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#12290;&#23398;&#26657;&#21644;&#22823;&#23398;&#24847;&#35782;&#21040;&#23398;&#29983;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#30001;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20182;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#36825;&#31181;&#26032;&#25216;&#26415;&#21450;&#20854;&#28508;&#22312;&#30340;&#28389;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;CS&#65289;&#21450;&#30456;&#20851;&#39046;&#22495;&#30340;&#25945;&#32946;&#39033;&#30446;&#23588;&#20854;&#21463;&#21040;&#24433;&#21709;&#65292;&#22240;&#20026;LLMs&#20063;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#31243;&#20195;&#30721;&#12290;&#20026;&#20102;&#24110;&#21161;&#29702;&#35299;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20844;&#24320;&#21487;&#29992;LLMs&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CSEPrompts&#65292;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#32534;&#31243;&#32451;&#20064;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02540v1 Announce Type: new  Abstract: Recent advances in AI, machine learning, and NLP have led to the development of a new generation of Large Language Models (LLMs) that are trained on massive amounts of data and often have trillions of parameters. Commercial applications (e.g., ChatGPT) have made this technology available to the general public, thus making it possible to use LLMs to produce high-quality texts for academic and professional purposes. Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse. Educational programs in Computer Science (CS) and related fields are particularly affected because LLMs are also capable of generating programming code in various programming languages. To help understand the potential impact of publicly available LLMs in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22235;&#20010;&#19987;&#38376;&#38024;&#23545;&#23433;&#21733;&#25289;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#30340;PLM&#65292;&#24182;&#20351;&#29992;&#22810;&#35821;&#35328;&#33258;&#36866;&#24212;&#24494;&#35843;&#65288;MAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#30693;&#24773;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;MAFT&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23558;&#22522;&#32447;&#25552;&#39640;&#20102;12.3&#20010;&#30334;&#20998;&#28857;&#65292;&#36229;&#36234;&#20102;SOTA AfroXLMR-base&#21644;OFA&#12290;</title><link>https://arxiv.org/abs/2404.02534</link><description>&lt;p&gt;
ANGOFA&#65306;&#21033;&#29992;OFA&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#23433;&#21733;&#25289;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22235;&#20010;&#19987;&#38376;&#38024;&#23545;&#23433;&#21733;&#25289;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#30340;PLM&#65292;&#24182;&#20351;&#29992;&#22810;&#35821;&#35328;&#33258;&#36866;&#24212;&#24494;&#35843;&#65288;MAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#30693;&#24773;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;MAFT&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23558;&#22522;&#32447;&#25552;&#39640;&#20102;12.3&#20010;&#30334;&#20998;&#28857;&#65292;&#36229;&#36234;&#20102;SOTA AfroXLMR-base&#21644;OFA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#21457;&#23637;&#21183;&#22836;&#36805;&#29467;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#36229;&#36234;&#35821;&#35328;&#38556;&#30861;&#12289;&#20419;&#36827;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#20027;&#35201;&#24573;&#35270;&#20102;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21253;&#21547;&#65292;&#23548;&#33268;&#22810;&#35821;&#35328;&#26223;&#35266;&#20013;&#20986;&#29616;&#26126;&#26174;&#30340;&#31354;&#30333;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#23450;&#21046;&#30340;PLM&#65292;&#19987;&#38376;&#20026;&#23433;&#21733;&#25289;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#65292;&#37319;&#29992;&#22810;&#35821;&#35328;&#33258;&#36866;&#24212;&#24494;&#35843;&#65288;MAFT&#65289;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20449;&#24687;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#22686;&#24378;MAFT&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#23558;&#22522;&#32447;&#25552;&#39640;&#20102;12.3&#20010;&#30334;&#20998;&#28857;&#65292;&#36229;&#36807;&#20102;&#36890;&#36807;MAFT&#24320;&#21457;&#30340;SOTA AfroXLMR-base&#21644;&#26377;&#25928;&#23884;&#20837;&#21021;&#22987;&#21270;OFA&#20998;&#21035;&#25552;&#39640;&#20102;3.8&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02534v1 Announce Type: cross  Abstract: In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#26041;&#27861;&#65292;&#23454;&#29616;&#19968;&#31181;&#24369;&#38450;&#24481;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#27169;&#22411;&#33021;&#22815;&#23433;&#20840;&#22320;&#22238;&#22797;&#25915;&#20987;&#32773;&#24182;&#38544;&#34255;&#38450;&#24481;&#24847;&#22270;</title><link>https://arxiv.org/abs/2404.02532</link><description>&lt;p&gt;
&#23398;&#20250;&#20266;&#35013;&#65306;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#36991;&#20813;LLM&#30340;&#25298;&#32477;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02532
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#26041;&#27861;&#65292;&#23454;&#29616;&#19968;&#31181;&#24369;&#38450;&#24481;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#27169;&#22411;&#33021;&#22815;&#23433;&#20840;&#22320;&#22238;&#22797;&#25915;&#20987;&#32773;&#24182;&#38544;&#34255;&#38450;&#24481;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#30340;&#22686;&#24378;&#24615;&#33021;&#65292;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#24341;&#21457;&#28508;&#22312;&#30340;&#36947;&#24503;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#23384;&#22312;&#19968;&#20123;&#24694;&#24847;&#25915;&#20987;&#32773;&#65292;&#20182;&#20204;&#36890;&#36807;&#35832;&#22914;&#25552;&#31034;&#24037;&#31243;&#31561;&#25216;&#26415;&#35825;&#20351;&#22823;&#22411;&#27169;&#22411;&#36234;&#29425;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#38750;&#27861;&#12289;&#20405;&#29359;&#38544;&#31169;&#20449;&#24687;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#27169;&#22411;&#37319;&#29992;&#23433;&#20840;&#23545;&#40784;&#31561;&#25216;&#26415;&#25269;&#24481;&#24694;&#24847;&#25915;&#20987;&#32773;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#27169;&#22411;&#36890;&#36807;&#25298;&#32477;&#22238;&#22797;&#30340;&#24378;&#38450;&#24481;&#26426;&#21046;&#23481;&#26131;&#34987;&#25915;&#20987;&#32773;&#35782;&#21035;&#65292;&#24182;&#29992;&#20110;&#21152;&#24378;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#26041;&#27861;&#65292;&#23454;&#29616;&#19968;&#31181;&#24369;&#38450;&#24481;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#27169;&#22411;&#33021;&#22815;&#23433;&#20840;&#22320;&#22238;&#22797;&#25915;&#20987;&#32773;&#24182;&#38544;&#34255;&#38450;&#24481;&#24847;&#22270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#26469;&#27169;&#25311;&#25915;&#20987;&#21644;&#38450;&#24481;&#24773;&#26223;&#65292;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#36127;&#36131;&#25915;&#20987;&#12289;&#20266;&#35013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02532v1 Announce Type: new  Abstract: With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disgu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24503;&#35821;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#20004;&#20010;&#24180;&#40836;&#32452;&#23398;&#29983;&#30340;1,320&#31687;&#35770;&#25991;&#65292;&#27599;&#31687;&#35770;&#25991;&#37117;&#32463;&#36807;&#20154;&#24037;&#27880;&#37322;&#65292;&#26088;&#22312;&#20998;&#26512;&#35770;&#35777;&#32467;&#26500;&#21644;&#36136;&#37327;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2404.02529</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#26512;&#35770;&#35777;&#32467;&#26500;&#21644;&#36136;&#37327;&#20114;&#21160;&#30340;&#23398;&#29983;&#20316;&#25991;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02529
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24503;&#35821;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#20004;&#20010;&#24180;&#40836;&#32452;&#23398;&#29983;&#30340;1,320&#31687;&#35770;&#25991;&#65292;&#27599;&#31687;&#35770;&#25991;&#37117;&#32463;&#36807;&#20154;&#24037;&#27880;&#37322;&#65292;&#26088;&#22312;&#20998;&#26512;&#35770;&#35777;&#32467;&#26500;&#21644;&#36136;&#37327;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36777;&#35777;&#20889;&#20316;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#38500;&#20102;&#35821;&#27861;&#21644;&#35821;&#27861;&#31561;&#20889;&#20316;&#22522;&#30784;&#30693;&#35782;&#22806;&#65292;&#23398;&#20064;&#32773;&#24517;&#39035;&#26377;&#24847;&#20041;&#22320;&#36873;&#25321;&#21644;&#23433;&#25490;&#35770;&#35777;&#32452;&#20214;&#65292;&#25165;&#33021;&#21019;&#36896;&#20986;&#39640;&#36136;&#37327;&#30340;&#35770;&#25991;&#12290;&#20026;&#20102;&#22312;&#35745;&#31639;&#26426;&#19978;&#25903;&#25345;&#36777;&#35777;&#20889;&#20316;&#65292;&#24517;&#39035;&#20808;&#25366;&#25496;&#35770;&#35777;&#32467;&#26500;&#12290;&#24403;&#19982;&#33258;&#21160;&#35770;&#25991;&#35780;&#20998;&#30456;&#32467;&#21512;&#26102;&#65292;&#35770;&#35777;&#32467;&#26500;&#21644;&#36136;&#37327;&#20998;&#25968;&#20043;&#38388;&#30340;&#20114;&#21160;&#21487;&#20197;&#29992;&#20110;&#20840;&#38754;&#30340;&#20889;&#20316;&#25903;&#25345;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#21033;&#29992;&#35770;&#35777;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#35770;&#25991;&#35780;&#20998;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#36804;&#20170;&#23578;&#26410;&#21457;&#24067;&#24102;&#26377;&#22320;&#38754;&#20107;&#23454;&#35770;&#25991;&#36136;&#37327;&#27880;&#37322;&#30340;&#35770;&#35777;&#25366;&#25496;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35821;&#26009;&#24211;&#20013;&#27809;&#26377;&#21253;&#21547;&#29305;&#23450;&#23398;&#29983;&#25776;&#20889;&#30340;&#35770;&#25991;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#20004;&#20010;&#24180;&#40836;&#32452;&#23398;&#29983;&#30340;1,320&#31687;&#24503;&#35821;&#35770;&#25991;&#30340;&#35821;&#26009;&#24211;&#12290;&#27599;&#31687;&#35770;&#25991;&#37117;&#32463;&#36807;&#20154;&#24037;&#27880;&#37322;&#65292;&#27880;&#37322;&#20102;&#20854;&#35770;&#35777;&#32467;&#26500;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02529v1 Announce Type: new  Abstract: Learning argumentative writing is challenging. Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality essays. To support argumentative writing computationally, one step is to mine the argumentative structure. When combined with automatic essay scoring, interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support. Although studies have shown the usefulness of using information about the argumentative structure for essay scoring, no argument mining corpus with ground-truth essay quality annotations has been published yet. Moreover, none of the existing corpora contain essays written by school students specifically. To fill this research gap, we present a German corpus of 1,320 essays from school students of two age groups. Each essay has been manually annotated for argumentative structure and quali
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#32763;&#35793;&#35780;&#20272;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#31034;&#20363;&#39537;&#21160;&#23398;&#20064;&#21644;&#24494;&#35843;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#25509;&#35780;&#20272;&#65292;&#21457;&#29616;LLM-based&#35780;&#20272;&#22120;&#22312;&#21360;&#24230;&#35821;&#35328;&#23545;&#20013;&#33719;&#24471;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#25972;&#20307;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02512</link><description>&lt;p&gt;
&#38754;&#21521;&#33521;&#35821;&#21644;&#21360;&#24230;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26080;&#21442;&#32771;&#32763;&#35793;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#32763;&#35793;&#35780;&#20272;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#31034;&#20363;&#39537;&#21160;&#23398;&#20064;&#21644;&#24494;&#35843;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#25509;&#35780;&#20272;&#65292;&#21457;&#29616;LLM-based&#35780;&#20272;&#22120;&#22312;&#21360;&#24230;&#35821;&#35328;&#23545;&#20013;&#33719;&#24471;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#25972;&#20307;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#26080;&#21442;&#32771;&#32763;&#35793;&#35780;&#20272;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35780;&#20272;&#33521;&#35821;&#21644;&#21360;&#24230;&#35821;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32763;&#35793;&#35780;&#20272;&#20219;&#21153;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#31034;&#20363;&#39537;&#21160;&#23398;&#20064;&#21644;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#25509;&#35780;&#20272;&#65292;&#20174;&#32780;&#32473;&#20986;&#19968;&#20010;0&#21040;100&#30340;&#20998;&#25968;&#65292;&#20854;&#20013;100&#20195;&#34920;&#23436;&#32654;&#32763;&#35793;&#65292;1&#20195;&#34920;&#32763;&#35793;&#36136;&#37327;&#24456;&#24046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#35757;&#32451;&#36807;&#30340;&#31995;&#32479;&#30340;&#24615;&#33021;&#19982;COMET&#12289;BERT-Scorer&#21644;LABSE&#31561;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#65288;LLaMA-2-13B&#65289;&#22312;&#32771;&#34385;&#30340;&#21360;&#24230;&#35821;&#35328;&#35821;&#31181;&#20013;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#25972;&#20307;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02512v1 Announce Type: new  Abstract: With the primary focus on evaluating the effectiveness of large language models for automatic reference-less translation assessment, this work presents our experiments on mimicking human direct assessment to evaluate the quality of translations in English and Indian languages. We constructed a translation evaluation task where we performed zero-shot learning, in-context example-driven learning, and fine-tuning of large language models to provide a score out of 100, where 100 represents a perfect translation and 1 represents a poor translation. We compared the performance of our trained systems with existing methods such as COMET, BERT-Scorer, and LABSE, and found that the LLM-based evaluator (LLaMA-2-13B) achieves a comparable or higher overall correlation with human judgments for the considered Indian language pairs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20998;&#31163;&#21644;&#21387;&#32553;&#65292;&#26032;&#26041;&#27861;&#20943;&#36731;&#20102;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#36951;&#24536;&#65292;&#24182;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.02507</link><description>&lt;p&gt;
&#20855;&#26377;&#23884;&#20837;&#31354;&#38388;&#20998;&#31163;&#21644;&#21387;&#32553;&#30340;&#32456;&#36523;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lifelong Event Detection with Embedding Space Separation and Compaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02507
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20998;&#31163;&#21644;&#21387;&#32553;&#65292;&#26032;&#26041;&#27861;&#20943;&#36731;&#20102;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#36951;&#24536;&#65292;&#24182;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#36951;&#24536;&#65292;&#29616;&#26377;&#30340;&#32456;&#36523;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#32500;&#25252;&#19968;&#20010;&#35760;&#24518;&#27169;&#22359;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#37325;&#25773;&#23384;&#20648;&#30340;&#35760;&#24518;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#25968;&#25454;&#21644;&#26032;&#20219;&#21153;&#26679;&#26412;&#30340;&#31616;&#21333;&#32467;&#21512;&#20173;&#21487;&#33021;&#23548;&#33268;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#22823;&#37327;&#36951;&#24536;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#26032;&#25968;&#25454;&#30340;&#29305;&#24449;&#20998;&#24067;&#19982;&#20808;&#21069;&#23398;&#20064;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#28508;&#22312;&#37325;&#21472;&#25152;&#23548;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#26356;&#23481;&#26131;&#23545;&#23569;&#37327;&#35760;&#24518;&#26679;&#26412;&#36807;&#25311;&#21512;&#65292;&#32780;&#19981;&#26159;&#26377;&#25928;&#35760;&#24518;&#23398;&#21040;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#31354;&#38388;&#20998;&#31163;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24378;&#21046;&#26032;&#25968;&#25454;&#30340;&#29305;&#24449;&#20998;&#24067;&#36828;&#31163;&#20808;&#21069;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;&#23427;&#36824;&#36890;&#36807;&#35760;&#24518;&#26657;&#20934;&#26426;&#21046;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02507v1 Announce Type: new  Abstract: To mitigate forgetting, existing lifelong event detection methods typically maintain a memory module and replay the stored memory data during the learning of a new task. However, the simple combination of memory data and new-task samples can still result in substantial forgetting of previously acquired knowledge, which may occur due to the potential overlap between the feature distribution of new data and the previously learned embedding space. Moreover, the model suffers from overfitting on the few memory samples rather than effectively remembering learned patterns. To address the challenges of forgetting and overfitting, we propose a novel method based on embedding space separation and compaction. Our method alleviates forgetting of previously learned tasks by forcing the feature distribution of new data away from the previous embedding space. It also mitigates overfitting by a memory calibration mechanism that encourages memory data t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#21644;&#35748;&#30693;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#20013;&#25552;&#20379;&#30340;&#25903;&#25345;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02505</link><description>&lt;p&gt;
&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#19982;&#35748;&#30693;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02505
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#21644;&#35748;&#30693;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#20013;&#25552;&#20379;&#30340;&#25903;&#25345;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#65288;ESC&#65289;&#31995;&#32479;&#22312;&#25552;&#20379;&#20849;&#24773;&#20114;&#21160;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#29702;&#35299;&#21644;&#35299;&#20915;&#29992;&#25143;&#29420;&#29305;&#32463;&#21382;&#26469;&#24110;&#21161;&#29992;&#25143;&#24230;&#36807;&#28040;&#26497;&#24773;&#32490;&#29366;&#24577;&#12290;&#26412;&#25991;&#35299;&#20915;ESC&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#36890;&#36807;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#22686;&#24378;&#30456;&#20851;&#35821;&#22659;&#21644;&#20849;&#24773;&#24335;&#22238;&#24212;&#29983;&#25104;&#65292;&#20197;&#21450;&#25512;&#36827;&#35748;&#30693;&#29702;&#35299;&#20197;&#20840;&#38754;&#25226;&#25569;&#38544;&#21547;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#21644;&#35748;&#30693;&#26041;&#38754;&#24773;&#22659;&#29702;&#35299;&#65288;\ourwork&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#20803;&#32032;&#21327;&#21516;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;ESC&#20013;&#25552;&#20379;&#30340;&#25903;&#25345;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20154;&#35774;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#32034;&#26426;&#21046;&#65292;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#19988;&#20010;&#24615;&#21270;&#30340;&#28436;&#31034;&#23545;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#29702;&#35299;&#27169;&#22359;&#65292;&#21033;&#29992;&#26469;&#33258;ATOMIC&#30693;&#35782;&#28304;&#30340;&#22235;&#31181;&#35748;&#30693;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#38544;&#21547;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02505v1 Announce Type: cross  Abstract: Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences. In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively. We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (\ourwork), a novel approach that synergizes these elements to improve the quality of support provided in ESCs. By leveraging in-context learning and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs. We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to dee
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#26694;&#26550;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#35789;&#34920;&#31034;&#19982;&#39640;&#36164;&#28304;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.02490</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#35821;&#23545;&#40784;&#22686;&#24378;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02490
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#26694;&#26550;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#35789;&#34920;&#31034;&#19982;&#39640;&#36164;&#28304;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#30340;&#30740;&#31350;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#31232;&#32570;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#33853;&#21518;&#12290;&#26412;&#25991;&#34920;&#26126;&#30446;&#21069;&#27169;&#22411;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#35789;&#34920;&#31034;&#19982;&#39640;&#36164;&#28304;&#35821;&#35328;&#26126;&#26174;&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26126;&#30830;&#22320;&#22312;&#33521;&#35821;&#21644;&#20843;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23545;&#40784;&#21333;&#35789;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#35789;&#23545;&#40784;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#30340;&#35757;&#32451;&#30446;&#26631;&#65306;&#23545;&#40784;&#21333;&#35789;&#39044;&#27979;&#21644;&#21333;&#35789;&#32763;&#35793;&#25490;&#24207;&#65292;&#20197;&#21450;&#24191;&#27867;&#20351;&#29992;&#30340;&#32763;&#35793;&#25490;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21452;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02490v1 Announce Type: new  Abstract: The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader rang
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUQGen&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#36827;&#34892;&#29616;&#20195;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#26032;&#39046;&#22495;&#24494;&#35843;</title><link>https://arxiv.org/abs/2404.02489</link><description>&lt;p&gt;
DUQGen: &#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#23454;&#29616;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#26377;&#25928;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUQGen&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#36827;&#34892;&#29616;&#20195;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#26032;&#39046;&#22495;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#22312;&#22823;&#22411;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;(&#22914;MS-MARCO)&#19978;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25490;&#24207;&#22120;&#22312;&#21508;&#31181;&#25490;&#21517;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#26080;&#38656;&#39046;&#22495;&#33258;&#36866;&#24212;&#21363;&#21487;&#23454;&#29616;&#38646;-shot&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#38646;-shot&#31070;&#32463;&#25490;&#24207;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#26410;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25490;&#21517;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;DUQGen&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#31354;&#30333;&#65292;&#21363;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#24494;&#35843;&#29616;&#20195;&#31070;&#32463;&#25490;&#24207;&#22120;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DUQGen&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#25991;&#26723;&#30340;&#32858;&#31867;&#20135;&#29983;&#20102;&#26356;&#26377;&#25928;&#30340;&#30446;&#26631;&#39046;&#22495;&#34920;&#31034;&#65307;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#20803;&#21270;&#30340;trai
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02489v1 Announce Type: cross  Abstract: State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse tra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02474</link><description>&lt;p&gt;
uTeBC-NLP&#22312;SemEval-2024&#20219;&#21153;9&#20013;&#65306;LLMs&#33021;&#25104;&#20026;&#27178;&#21521;&#24605;&#32771;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;Jiang&#31561;&#20154;&#65288;2023c&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#27178;&#21521;&#24605;&#32500;&#65288;&#36229;&#36234;&#24605;&#32500;&#23450;&#21183;&#65289;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#19968;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#25581;&#31034;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#12290;&#36890;&#36807;&#21442;&#21152;SemEval-2024&#30340;&#31532;9&#39033;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#25340;&#22270;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65306;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#21644;&#30452;&#25509;&#25552;&#31034;&#65292;&#20351;&#29992;&#20449;&#24687;&#24615;&#25551;&#36848;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31649;&#36947;&#36827;&#34892;&#24773;&#22659;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#19977;&#31181;LLMs&#65292;&#21253;&#25324;GPT-3.5&#12289;GPT-4&#21644;Zephyr-7B-beta&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#35868;&#39064;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#24605;&#32500;&#36335;&#24452;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#36827;&#34892;&#20102;&#36136;&#37327;&#39564;&#35777;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#31867;&#20284;&#32534;&#31243;&#35821;&#35328;&#30340;&#25552;&#31034;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#21644;&#36739;&#38271;&#26684;&#24335;&#30340;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2404.02466</link><description>&lt;p&gt;
&#25552;&#31034;&#25968;&#20540;&#24207;&#21015;&#65306;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompting for Numerical Sequences: A Case Study on Market Comment Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#31867;&#20284;&#32534;&#31243;&#35821;&#35328;&#30340;&#25552;&#31034;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#21644;&#36739;&#38271;&#26684;&#24335;&#30340;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#25968;&#25454;&#36716;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#34920;&#26684;&#12289;&#22270;&#34920;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#20540;&#25968;&#25454;&#36716;&#25991;&#26412;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29983;&#25104;&#34920;&#26684;&#21644;&#22270;&#34920;&#31561;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25552;&#31034;&#30340;&#30740;&#31350;&#27491;&#22312;&#22686;&#38271;&#20013;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#20540;&#25968;&#25454;&#30340;&#25552;&#31034;&#30340;&#28145;&#20837;&#30740;&#31350;&#21364;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#36755;&#20837;&#34920;&#31034;&#65292;&#21253;&#25324;&#20196;&#29260;&#24207;&#21015;&#21644;&#32467;&#26500;&#21270;&#26684;&#24335;&#22914;HTML&#12289;LaTeX&#21644;Python&#26679;&#24335;&#20195;&#30721;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#8220;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#8221;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#23558;&#32929;&#20215;&#25968;&#20540;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#24066;&#22330;&#35780;&#35770;&#12290;&#19982;&#25105;&#20204;&#30340;&#39044;&#26399;&#30456;&#21453;&#65292;&#32467;&#26524;&#34920;&#26126;&#31867;&#20284;&#32534;&#31243;&#35821;&#35328;&#30340;&#25552;&#31034;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#21644;&#36739;&#38271;&#26684;&#24335;&#65288;&#22914;HTML&#21644;LaTeX&#65289;&#30340;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02466v1 Announce Type: cross  Abstract: Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer in
&lt;/p&gt;</description></item><item><title>PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02456</link><description>&lt;p&gt;
PhonologyBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#38901;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
PhonologyBench: Evaluating Phonological Skills of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02456
&lt;/p&gt;
&lt;p&gt;
PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#38901;&#23398;&#26159;&#30740;&#31350;&#35821;&#38899;&#32467;&#26500;&#21644;&#21457;&#38899;&#35268;&#21017;&#30340;&#23398;&#31185;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30740;&#31350;&#20013;&#19968;&#20010;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LLMs&#22312;&#21508;&#31181;&#21033;&#29992;&#38899;&#38901;&#23398;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#25945;&#32946;&#24037;&#20855;&#21644;&#35799;&#27468;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#23436;&#32654;&#30340;&#27491;&#23383;&#21644;&#38899;&#26631;&#24418;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#30340;&#38899;&#38901;&#25216;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhonologyBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19977;&#20010;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#26126;&#30830;&#27979;&#35797;LLMs&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65306;&#24418;&#38899;&#36716;&#25442;&#12289;&#38899;&#33410;&#35745;&#25968;&#21644;&#25276;&#38901;&#35789;&#29983;&#25104;&#12290;&#23613;&#31649;&#27809;&#26377;&#35775;&#38382;&#35821;&#38899;&#25968;&#25454;&#65292;LLMs&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25276;&#38901;&#35789;&#29983;&#25104;&#21644;&#38899;&#33410;&#35745;&#25968;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;17%&#21644;45%&#30340;&#24046;&#36317;&#65292; respectively, when...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25554;&#20837;&#19968;&#27425;&#24615;&#19978;&#19979;&#25991;&#28436;&#31034;&#65292;&#20174;&#32780;&#25104;&#21151;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#31034;&#20363;&#26469;&#25913;&#36827;&#35780;&#20272;&#30340;mT5&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02452</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#19968;&#27425;&#24615;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25554;&#20837;&#19968;&#27425;&#24615;&#19978;&#19979;&#25991;&#28436;&#31034;&#65292;&#20174;&#32780;&#25104;&#21151;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#31034;&#20363;&#26469;&#25913;&#36827;&#35780;&#20272;&#30340;mT5&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#65288;ZS-XLT&#65289;&#21033;&#29992;&#22312;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#20013;&#20570;&#39044;&#27979;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#24615;&#33021;&#25439;&#22833;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#21518;&#32493;&#36866;&#24212;&#21487;&#20197;&#23454;&#29616;&#39069;&#22806;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#35843;&#25972;&#65288;ICT&#65289;&#26469;&#36827;&#34892;&#19968;&#27425;&#24615;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24341;&#20837;&#19978;&#19979;&#25991;&#36328;&#35821;&#35328;&#36716;&#31227;&#65288;IC-XLT&#65289;&#12290;&#36825;&#19968;&#21019;&#26032;&#27010;&#24565;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25554;&#20837;&#19968;&#27425;&#24615;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#23545;&#20854;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;IC-XLT&#25104;&#21151;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#31034;&#20363;&#65292;&#25913;&#36827;&#20102;&#35780;&#20272;&#30340;mT5&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#65292;&#22312;&#36866;&#29992;&#20110;&#38646;&#21644;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#26469;&#36866;&#24212;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403;&#28304;&#35821;&#35328;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02452v1 Announce Type: new  Abstract: Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-lang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#35780;&#20272;&#25945;&#32946;&#20013;&#22810;&#31181;&#39640;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#39318;&#27425;&#24212;&#29992;NLP&#26469;&#34913;&#37327;&#23545;&#26377;&#29305;&#27530;&#38656;&#27714;&#23398;&#29983;&#29305;&#21035;&#26377;&#25928;&#30340;&#25945;&#23398;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2404.02444</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25945;&#32946;&#20013;&#25945;&#23398;&#36136;&#37327;&#30340;&#25215;&#35834;&#19982;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#35780;&#20272;&#25945;&#32946;&#20013;&#22810;&#31181;&#39640;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#39318;&#27425;&#24212;&#29992;NLP&#26469;&#34913;&#37327;&#23545;&#26377;&#29305;&#27530;&#38656;&#27714;&#23398;&#29983;&#29305;&#21035;&#26377;&#25928;&#30340;&#25945;&#23398;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25945;&#23398;&#36136;&#37327;&#26159;&#25945;&#32946;&#31995;&#32479;&#25913;&#36827;&#21162;&#21147;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25163;&#24037;&#35780;&#20272;&#26114;&#36149;&#12289;&#20027;&#35266;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#35266;&#23519;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29305;&#27530;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#38459;&#30861;&#20102;&#25945;&#24072;&#21450;&#26102;&#33719;&#24471;&#39057;&#32321;&#21453;&#39304;&#12290;&#19982;&#20043;&#21069;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#20302;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#35780;&#20272;&#20004;&#31181;&#19981;&#21516;&#25945;&#32946;&#29615;&#22659;&#19979;&#30340;&#22810;&#31181;&#39640;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#30740;&#31350;&#65306;&#38754;&#23545;&#38754;K-12&#25945;&#23460;&#21644;&#39044;&#26381;&#21153;&#25945;&#24072;&#30340;&#27169;&#25311;&#34920;&#29616;&#20219;&#21153;&#12290;&#36825;&#20063;&#26159;&#39318;&#20010;&#24212;&#29992;NLP&#26469;&#35780;&#20272;&#34987;&#24191;&#27867;&#35748;&#20026;&#23545;&#26377;&#29305;&#27530;&#38656;&#27714;&#23398;&#29983;&#29305;&#21035;&#26377;&#25928;&#30340;&#25945;&#23398;&#23454;&#36341;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#38754;&#20020;NLP&#27169;&#22411;&#25945;&#23398;&#20998;&#26512;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02444v1 Announce Type: cross  Abstract: Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers' expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#20174;&#21475;&#36848;&#39564;&#23608;&#25991;&#26412;&#20013;&#39044;&#27979;&#27515;&#22240;&#24182;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02438</link><description>&lt;p&gt;
&#20174;&#21465;&#36848;&#21040;&#25968;&#23383;&#65306;&#21033;&#29992;&#21475;&#36848;&#39564;&#23608;&#21465;&#36848;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#20174;&#21475;&#36848;&#39564;&#23608;&#25991;&#26412;&#20013;&#39044;&#27979;&#27515;&#22240;&#24182;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37096;&#20998;&#27515;&#20129;&#20107;&#20214;&#21457;&#29983;&#22312;&#21307;&#30103;&#31995;&#32479;&#22806;&#30340;&#22330;&#26223;&#20013;&#65292;&#21475;&#36848;&#39564;&#23608;&#65288;VAs&#65289;&#26159;&#30417;&#27979;&#27515;&#22240;&#36235;&#21183;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;VAs&#26159;&#19982;&#24184;&#23384;&#30340;&#29031;&#26009;&#32773;&#25110;&#20146;&#23646;&#36827;&#34892;&#30340;&#35775;&#35848;&#65292;&#29992;&#20110;&#39044;&#27979;&#36893;&#32773;&#30340;&#27515;&#22240;&#12290;&#23558;VAs&#36716;&#21270;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#21487;&#34892;&#30340;&#35265;&#35299;&#38656;&#35201;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20351;&#29992;VA&#35775;&#35848;&#39044;&#27979;&#21487;&#33021;&#30340;&#27515;&#22240;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#39044;&#27979;&#30340;&#27515;&#22240;&#36827;&#34892;&#25512;&#26029;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#27515;&#20129;&#26679;&#26412;&#23545;&#27515;&#22240;&#25353;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#20998;&#35299;&#30340;&#24314;&#27169;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#20174;&#33258;&#30001;&#25991;&#26412;&#39044;&#27979;&#32467;&#26524;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#20026;&#27515;&#22240;&#65289;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;multiPPI++&#65292;&#23558;&#26368;&#36817;&#30340;&#8220;&#39044;&#27979;&#39537;&#21160;&#25512;&#26029;&#8221;&#24037;&#20316;&#25193;&#23637;&#21040;&#22810;&#39033;&#20998;&#31867;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31995;&#21015;NLP&#25216;&#26415;&#36827;&#34892;COD&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23545;VA&#25968;&#25454;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02438v1 Announce Type: new  Abstract: In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in "prediction-powered inference" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#31070;&#32463;&#20803;&#32423;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#20013;&#23384;&#22312;&#29305;&#23450;&#20110;&#27599;&#31181;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#65292;&#24178;&#25200;&#36825;&#20123;&#35821;&#35328;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#20250;&#26174;&#33879;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#20013;&#30446;&#26631;&#35821;&#35328;&#20986;&#29616;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02431</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65306;&#23547;&#25214;&#21644;&#25511;&#21046;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02431
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#31070;&#32463;&#20803;&#32423;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#20013;&#23384;&#22312;&#29305;&#23450;&#20110;&#27599;&#31181;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#65292;&#24178;&#25200;&#36825;&#20123;&#35821;&#35328;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#20250;&#26174;&#33879;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#20013;&#30446;&#26631;&#35821;&#35328;&#20986;&#29616;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25104;&#21151;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;PLMs&#30340;&#31070;&#32463;&#20803;&#32423;&#20869;&#37096;&#34892;&#20026;&#65292;&#20855;&#20307;&#26816;&#26597;&#20102;&#22312;&#20165;&#35299;&#30721;&#22120;&#30340;&#22810;&#35821;&#35328;PLMs&#20013;&#26159;&#21542;&#23384;&#22312;&#20026;&#8220;&#27599;&#31181;&#35821;&#35328;&#29420;&#29305;&#28608;&#27963;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20845;&#31181;&#35821;&#35328;&#65306;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#20013;&#25991;&#21644;&#26085;&#35821;&#65292;&#24182;&#34920;&#26126;&#35821;&#35328;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#26159;&#29420;&#29305;&#30340;&#65292;&#22312;&#35821;&#35328;&#20043;&#38388;&#26377;&#36731;&#24494;&#30340;&#37325;&#21472;&#65288;&lt;5%&#65289;&#12290;&#36825;&#20123;&#31070;&#32463;&#20803;&#20027;&#35201;&#20998;&#24067;&#22312;&#27169;&#22411;&#30340;&#21069;&#20960;&#23618;&#21644;&#26368;&#21518;&#20960;&#23618;&#12290;&#36825;&#19968;&#36235;&#21183;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#27169;&#22411;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#24178;&#25200;&#20102;&#27599;&#20010;&#27169;&#22411;&#23569;&#20110;1&#65285;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#35777;&#26126;&#24178;&#25200;&#23569;&#37327;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#20250;&#26174;&#33879;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#20013;&#30446;&#26631;&#35821;&#35328;&#20986;&#29616;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02431v1 Announce Type: new  Abstract: Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism. We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire ``uniquely for each language'' within decoder-only multilingual PLMs. We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (&lt; 5%) between languages. These neurons are mainly distributed in the models' first and last few layers. This trend remains consistent across languages and models. Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;PEFT&#21644;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20302;&#36164;&#28304;LLMs&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#19982;0-shot&#25991;&#26412;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02422</link><description>&lt;p&gt;
&#20351;&#29992;PEFT&#21644;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20302;&#36164;&#28304;LLMs&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02422
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;PEFT&#21644;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20302;&#36164;&#28304;LLMs&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#19982;0-shot&#25991;&#26412;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;0-shot&#25110;few-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#20013;&#65292;&#36890;&#24120;&#27604;0-shot&#35774;&#32622;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#26356;&#38271;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;LLMs&#20687;0-shot&#25991;&#26412;&#20998;&#31867;&#22120;&#19968;&#26679;&#39640;&#25928;&#65292;&#21516;&#26102;&#33719;&#24471;&#19982;ICL&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#38024;&#23545;&#36164;&#28304;&#31232;&#32570;&#30340;&#24773;&#20917;&#65292;&#21363;&#27599;&#31867;&#21482;&#26377;4&#20010;&#31034;&#20363;&#21487;&#29992;&#12290;&#20351;&#29992;&#21333;&#20010;LLM&#21644;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#25191;&#34892;&#19968;&#31995;&#21015;&#29983;&#25104;&#12289;&#36807;&#28388;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#27493;&#39588;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02422v1 Announce Type: new  Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34913;&#37327;&#20102;&#21069;&#32512;&#21542;&#23450;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#21305;&#37197;&#65292;&#27169;&#22411;&#25972;&#20307;&#19978;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#21069;&#32512;&#21542;&#23450;&#30340;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2404.02421</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20122;&#23383;&#35789;&#26631;&#35760;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#32512;&#21542;&#23450;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting subword tokenization: A case study on affixal negation in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02421
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34913;&#37327;&#20102;&#21069;&#32512;&#21542;&#23450;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#21305;&#37197;&#65292;&#27169;&#22411;&#25972;&#20307;&#19978;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#21069;&#32512;&#21542;&#23450;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34913;&#37327;&#20102;&#21069;&#32512;&#21542;&#23450;&#23545;&#29616;&#20195;&#33521;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24433;&#21709;&#12290;&#22312;&#21069;&#32512;&#21542;&#23450;&#20013;&#65292;&#21542;&#23450;&#30340;&#21547;&#20041;&#36890;&#36807;&#19968;&#20010;&#36127;&#38754;&#24418;&#24577;&#32032;&#26469;&#34920;&#36798;&#65292;&#36825;&#23545;LLMs&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#26631;&#35760;&#22120;&#36890;&#24120;&#19981;&#20855;&#22791;&#24418;&#24577;&#23398;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20122;&#23383;&#35789;&#26631;&#35760;&#26041;&#27861;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26631;&#35760;&#24615;&#33021;&#19982;&#21542;&#23450;&#25935;&#24863;&#24615;&#20043;&#38388;&#20132;&#20114;&#20316;&#29992;&#30340;&#20960;&#28857;&#35265;&#35299;&#12290;&#23613;&#31649;&#22312;&#26631;&#35760;&#20934;&#30830;&#24615;&#21644;&#21542;&#23450;&#26816;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#26377;&#36259;&#30340;&#19981;&#21305;&#37197;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#27169;&#22411;&#25972;&#20307;&#19978;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#21069;&#32512;&#21542;&#23450;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02421v1 Announce Type: new  Abstract: In this work, we measure the impact of affixal negation on modern English large language models (LLMs). In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible. We conduct extensive experiments using LLMs with different subword tokenization methods, which lead to several insights on the interaction between tokenization performance and negation sensitivity. Despite some interesting mismatches between tokenization accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation.
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;</title><link>https://arxiv.org/abs/2404.02418</link><description>&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#25513;&#30422;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Auxiliary task demands mask the capabilities of smaller language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02418
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#20204;&#23545;&#35748;&#30693;&#33021;&#21147;&#22914;&#35821;&#35328;&#29702;&#35299;&#25110;&#24515;&#28789;&#29702;&#35770;&#20309;&#26102;&#20986;&#29616;&#36827;&#34892;&#20102;&#20105;&#35770;&#12290;&#36825;&#20123;&#36777;&#35770;&#24120;&#24120;&#20851;&#27880;"&#20219;&#21153;&#38656;&#27714;"&#30340;&#27010;&#24565;--&#25191;&#34892;&#29305;&#23450;&#35780;&#20272;&#26102;&#25152;&#20276;&#38543;&#30340;&#36741;&#21161;&#25361;&#25112;--&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#25513;&#30422;&#20102;&#20799;&#31461;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#24403;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#26102;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#20250;&#20986;&#29616;&#65306;&#20219;&#21153;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#27169;&#22411;&#35299;&#37322;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#21487;&#29992;&#36164;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#65292;&#20855;&#26377;&#26356;&#22823;&#20219;&#21153;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#20250;&#27604;&#38477;&#20302;&#38656;&#27714;&#30340;&#35780;&#20272;&#24471;&#21040;&#26356;&#20302;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LM&#30340;&#24615;&#33021;&#19981;&#24212;&#34987;&#35299;&#37322;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of "task demands" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This "demand gap" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret
&lt;/p&gt;</description></item><item><title>CMULAB &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#25345;&#32493;&#20154;&#26426;&#21327;&#20316;&#24494;&#35843;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#23545;&#35821;&#38899;&#35782;&#21035;&#12289;OCR&#12289;&#32763;&#35793;&#21644;&#21477;&#27861;&#20998;&#26512;&#31561;&#24037;&#20855;&#36827;&#34892;&#36866;&#24212;&#21644;&#25193;&#23637;&#33267;&#26032;&#35821;&#35328;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;</title><link>https://arxiv.org/abs/2404.02408</link><description>&lt;p&gt;
CMULAB&#65306;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#37096;&#32626;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02408
&lt;/p&gt;
&lt;p&gt;
CMULAB &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#25345;&#32493;&#20154;&#26426;&#21327;&#20316;&#24494;&#35843;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#23545;&#35821;&#38899;&#35782;&#21035;&#12289;OCR&#12289;&#32763;&#35793;&#21644;&#21477;&#27861;&#20998;&#26512;&#31561;&#24037;&#20855;&#36827;&#34892;&#36866;&#24212;&#21644;&#25193;&#23637;&#33267;&#26032;&#35821;&#35328;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#38656;&#35201;&#23545;&#35821;&#35328;&#26412;&#36523;&#26377;&#28145;&#20837;&#20102;&#35299;&#65292;&#29087;&#24713;&#26368;&#26032;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#20855;&#22791;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#25216;&#26415;&#19987;&#38271;&#12290;&#36825;&#21487;&#33021;&#23545;&#35821;&#35328;&#31038;&#21306;&#25104;&#21592;&#21644;&#35821;&#35328;&#23398;&#23478;&#20351;&#29992;NLP&#24037;&#20855;&#26500;&#25104;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CMU&#35821;&#35328;&#27880;&#37322;&#21518;&#31471;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#37096;&#32626;&#21644;&#25345;&#32493;&#30340;&#20154;&#26426;&#21327;&#20316;&#23545;NLP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#36807;&#31243;&#12290;CMULAB&#20351;&#29992;&#25143;&#33021;&#22815;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#24555;&#36895;&#23545;&#35821;&#38899;&#35782;&#21035;&#12289;OCR&#12289;&#32763;&#35793;&#21644;&#21477;&#27861;&#20998;&#26512;&#31561;&#24037;&#20855;&#36827;&#34892;&#36866;&#24212;&#21644;&#25193;&#23637;&#33267;&#26032;&#35821;&#35328;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#30446;&#21069;&#21487;&#29992;&#30340;&#21508;&#31181;&#24037;&#20855;&#21644;API&#20197;&#21450;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#36731;&#26494;&#22320;&#21521;&#26694;&#26550;&#28155;&#21152;&#26032;&#27169;&#22411;/&#21151;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/neula&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02408v1 Announce Type: new  Abstract: Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at https://github.com/neula
&lt;/p&gt;</description></item><item><title>&#32842;&#22825;&#27169;&#22411;&#22240;&#20026;&#22810;&#36718;&#20132;&#20114;&#26684;&#24335;&#30340;&#28789;&#27963;&#24615;&#22686;&#21152;&#20102;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#35813;&#35770;&#25991;&#25581;&#31034;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.02406</link><description>&lt;p&gt;
&#25506;&#35752;&#32842;&#22825;&#27169;&#22411;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Exploring Backdoor Vulnerabilities of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02406
&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#27169;&#22411;&#22240;&#20026;&#22810;&#36718;&#20132;&#20114;&#26684;&#24335;&#30340;&#28789;&#27963;&#24615;&#22686;&#21152;&#20102;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#35813;&#35770;&#25991;&#25581;&#31034;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#24403;&#21069;&#23545;LLMs&#30340;&#21518;&#38376;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#65292;&#32780;&#24573;&#30053;&#20102;&#21478;&#19968;&#31181;&#29616;&#23454;&#22330;&#26223;&#65292;&#21363;&#23558;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#25968;&#25454;&#19978;&#24494;&#35843;&#20026;&#32842;&#22825;&#27169;&#22411;&#12290;&#32842;&#22825;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#65292;&#22240;&#27492;&#32842;&#22825;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20540;&#24471;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#28789;&#27963;&#30340;&#22810;&#36718;&#20132;&#20114;&#26684;&#24335;&#22686;&#21152;&#20102;&#35302;&#21457;&#35774;&#35745;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22686;&#21152;&#20102;&#32842;&#22825;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32842;&#22825;&#27169;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#35302;&#21457;&#22330;&#26223;&#20998;&#24067;&#22312;&#29992;&#25143;&#36755;&#20837;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02406v1 Announce Type: cross  Abstract: Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#27874;&#26031;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24341;&#20837;&#20102;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;LLMs&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02403</link><description>&lt;p&gt;
&#35780;&#20272;&#27874;&#26031;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20197; ChatGPT &#20026;&#20013;&#24515;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02403
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#27874;&#26031;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24341;&#20837;&#20102;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;LLMs&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27874;&#26031;&#35821;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;ChatGPT&#21644;&#38543;&#21518;&#30340;LLMs&#22312;&#33521;&#35821;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#26356;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#19968;&#31995;&#21015;&#27874;&#26031;&#35821;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#22312;GPT-3.5-turbo&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#20063;&#21253;&#25324;&#20102;GPT-4&#21644;OpenChat-3.5&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#21253;&#25324;&#32463;&#20856;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#36827;&#34892;&#28145;&#20837;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#19982;&#29616;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#25512;&#29702;&#20219;&#21153;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#20010;&#22522;&#20110;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#65292;&#21478;&#19968;&#20010;&#28304;&#33258;&#31532;7&#21644;&#31532;10&#24180;&#32423;&#20837;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#65292;&#23588;&#20854;&#26159;GPT-4&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02403v1 Announce Type: new  Abstract: This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel
&lt;/p&gt;</description></item><item><title>Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02402</link><description>&lt;p&gt;
&#20351;&#29992;ChatLLM&#22312;&#23545;&#35805;AI&#20013;&#23548;&#33322;&#35821;&#22659;&#28145;&#24230;&#30340;Token Trails
&lt;/p&gt;
&lt;p&gt;
Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02402
&lt;/p&gt;
&lt;p&gt;
Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23545;&#35805;&#24314;&#27169;&#38656;&#35201;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#32454;&#33268;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#22797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Token Trails&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#21306;&#20998;&#29992;&#25143;&#35805;&#35821;&#21644;&#26426;&#22120;&#20154;&#22238;&#22797;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#22238;&#22797;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Token Trails&#22312;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#23545;&#35805;AI&#20013;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;Token Trails&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02402v1 Announce Type: cross  Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#21521;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#27880;&#20837;&#26377;&#27602;&#25968;&#25454;&#26469;&#36896;&#25104;&#24694;&#24847;&#32763;&#35793;&#65292;&#24341;&#36215;&#39640;&#36164;&#28304;&#35821;&#35328;&#21463;&#25915;&#20987;&#65292;&#25581;&#31034;&#27880;&#20837;&#23569;&#37327;&#26377;&#27602;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#25104;&#21151;&#25915;&#20987;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2404.02393</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack on Multilingual Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02393
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#21521;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#27880;&#20837;&#26377;&#27602;&#25968;&#25454;&#26469;&#36896;&#25104;&#24694;&#24847;&#32763;&#35793;&#65292;&#24341;&#36215;&#39640;&#36164;&#28304;&#35821;&#35328;&#21463;&#25915;&#20987;&#65292;&#25581;&#31034;&#27880;&#20837;&#23569;&#37327;&#26377;&#27602;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#25104;&#21151;&#25915;&#20987;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MNMT&#65289;&#31995;&#32479;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#26174;&#31034;&#65292;MNMT&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#29305;&#21035;&#29409;&#35784;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#25915;&#20987;&#32773;&#21521;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#27880;&#20837;&#26377;&#27602;&#25968;&#25454;&#65292;&#20197;&#22312;&#20854;&#20182;&#35821;&#35328;&#65288;&#21253;&#25324;&#39640;&#36164;&#28304;&#35821;&#35328;&#65289;&#20013;&#36896;&#25104;&#24694;&#24847;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#23569;&#20110;0.01&#65285;&#30340;&#26377;&#27602;&#25968;&#25454;&#27880;&#20837;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#65292;&#23601;&#21487;&#20197;&#22312;&#25915;&#20987;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#26102;&#23454;&#29616;&#24179;&#22343;20&#65285;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#22266;&#26377;&#30340;&#35821;&#35328;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24341;&#36215;&#20851;&#20110;&#36825;&#20123;MNMT&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#30340;&#20851;&#27880;&#65292;&#24182;&#24076;&#26395;&#28608;&#21169;&#31038;&#21306;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02393v1 Announce Type: new  Abstract: While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages. Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#24314;&#27169;&#22797;&#26434;&#24418;&#24577;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#32423;&#21464;&#21387;&#22120;&#26550;&#26500;&#32534;&#30721;&#24418;&#24577;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#22810;&#26631;&#31614;&#35757;&#32451;&#26041;&#26696;&#21644;&#22522;&#20110;beam search&#30340;&#35299;&#30721;&#22120;&#26469;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02392</link><description>&lt;p&gt;
&#20855;&#26377;&#24418;&#24577;&#24314;&#27169;&#30340;&#20302;&#36164;&#28304;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Low-resource neural machine translation with morphological modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#24314;&#27169;&#22797;&#26434;&#24418;&#24577;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#32423;&#21464;&#21387;&#22120;&#26550;&#26500;&#32534;&#30721;&#24418;&#24577;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#22810;&#26631;&#31614;&#35757;&#32451;&#26041;&#26696;&#21644;&#22522;&#20110;beam search&#30340;&#35299;&#30721;&#22120;&#26469;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#36827;&#34892;&#24418;&#24577;&#24314;&#27169;&#26159;&#23454;&#29616;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#32763;&#35793;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;&#23376;&#35789;&#26631;&#35760;&#21270;&#21644;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#23616;&#38480;&#20110;&#21333;&#35789;&#30340;&#34920;&#38754;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#24314;&#27169;&#22797;&#26434;&#24418;&#24577;&#30340;&#26694;&#26550;&#35299;&#20915;&#26041;&#26696;&#12290;&#36873;&#25321;&#20102;&#19968;&#20010;&#20004;&#32423;&#21464;&#21387;&#22120;&#26550;&#26500;&#26469;&#23545;&#36755;&#20837;&#30340;&#24418;&#24577;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#22312;&#30446;&#26631;&#31471;&#36755;&#20986;&#26102;&#65292;&#22810;&#20219;&#21153;&#22810;&#26631;&#31614;&#35757;&#32451;&#26041;&#26696;&#32467;&#21512;&#22522;&#20110;beam search&#30340;&#35299;&#30721;&#22120;&#34987;&#21457;&#29616;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#22686;&#24378;&#26041;&#26696;&#65292;&#20197;&#36890;&#29992;&#24418;&#24335;&#20801;&#35768;&#38598;&#25104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20419;&#36827;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#21333;&#35789;&#39034;&#24207;&#20851;&#31995;&#24314;&#27169;&#12290;&#35780;&#20272;&#20102;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02392v1 Announce Type: new  Abstract: Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#35774;&#35745;&#30340;&#27969;&#31243;&#65292;&#23398;&#20064;&#32467;&#26500;&#30340;&#28145;&#21051;&#21547;&#20041;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.02389</link><description>&lt;p&gt;
&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65306;&#26469;&#33258;&#25991;&#26412;&#21040;SQL&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#35774;&#35745;&#30340;&#27969;&#31243;&#65292;&#23398;&#20064;&#32467;&#26500;&#30340;&#28145;&#21051;&#21547;&#20041;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#34920;&#26684;&#12289;&#25968;&#25454;&#24211;&#21644;&#30693;&#35782;&#22270;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#22312;&#20854;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290; &#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#24320;&#22987;&#36716;&#21521;&#22522;&#20110;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#22788;&#29702;&#20026;&#39034;&#24207;&#26631;&#35760;&#27969;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#22270;&#24418;&#26126;&#30830;&#22320;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290; &#26412;&#25991;&#25506;&#35752;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;T5&#65289;&#20013;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#32447;&#24615;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290; &#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#35774;&#35745;&#30340;&#27969;&#31243;&#65292;&#27604;&#22914;&#27169;&#24335;&#38142;&#25509;&#21644;&#35821;&#27861;&#39044;&#27979;&#65292;&#34920;&#26126;&#27169;&#22411;&#23545;&#32467;&#26500;&#30340;&#28145;&#21051;&#12289;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#36828;&#36828;&#36229;&#36807;&#31616;&#21333;&#30340;&#26631;&#35760;&#25490;&#24207;&#12290; &#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#32467;&#26500;&#33410;&#28857;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02389v1 Announce Type: cross  Abstract: Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure nod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;transformer&#27169;&#22411;&#36827;&#34892;&#23612;&#27850;&#23572;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#30340;&#25991;&#26412;&#35782;&#21035;&#30740;&#31350;&#65292;&#39640;&#31934;&#24230;&#35782;&#21035;&#20004;&#31181;&#35821;&#35328;&#25991;&#23383;&#65292;&#26377;&#26395;&#25512;&#21160;&#35821;&#35328;&#23398;&#30340;&#20808;&#36827;&#21644;&#26131;&#35775;&#38382;&#24615;&#30740;&#31350;</title><link>https://arxiv.org/abs/2404.02375</link><description>&lt;p&gt;
&#23612;&#27850;&#23572;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20809;&#23398;&#25991;&#26412;&#35782;&#21035;&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;transformer&#27169;&#22411;&#36827;&#34892;&#23612;&#27850;&#23572;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#30340;&#25991;&#26412;&#35782;&#21035;&#30740;&#31350;&#65292;&#39640;&#31934;&#24230;&#35782;&#21035;&#20004;&#31181;&#35821;&#35328;&#25991;&#23383;&#65292;&#26377;&#26395;&#25512;&#21160;&#35821;&#35328;&#23398;&#30340;&#20808;&#36827;&#21644;&#26131;&#35775;&#38382;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;OCR&#31995;&#32479;&#30340;&#30740;&#31350;&#19982;&#24320;&#21457;&#24037;&#20316;&#30456;&#23545;&#36739;&#26032;&#12290; &#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#25110;&#20854;&#20182;&#31995;&#32479;&#26102;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#12290; &#23613;&#31649;&#26377;&#22823;&#37327;&#25991;&#26412;&#24050;&#32463;&#34987;&#25968;&#23383;&#21270;&#24182;&#22312;&#20114;&#32852;&#32593;&#19978;&#25552;&#20379;&#65292;&#20294;&#36825;&#20123;&#25991;&#26412;&#20173;&#22788;&#20110;PDF&#21644;&#22270;&#20687;&#26684;&#24335;&#65292;&#36825;&#20123;&#26684;&#24335;&#24182;&#38750;&#31435;&#21363;&#21487;&#35775;&#38382;&#12290; &#26412;&#25991;&#35752;&#35770;&#20102;&#23391;&#21152;&#25289;&#35821;&#21644;&#23612;&#27850;&#23572;&#35821;&#20004;&#31181;&#25991;&#23383;&#30340;&#25991;&#26412;&#35782;&#21035;&#65307;&#22823;&#32422;&#26377;3&#20159;&#21644;4&#21315;&#19975;&#30340;&#23391;&#21152;&#25289;&#35821;&#21644;&#23612;&#27850;&#23572;&#35821;&#20351;&#29992;&#32773;&#12290; &#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#20809;&#23398;&#25991;&#26412;&#22270;&#20687;&#65288;&#25163;&#20889;&#21644;&#21360;&#21047;&#65289;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290; &#32467;&#26524;&#34920;&#26126;&#65292;&#24314;&#35758;&#30340;&#25216;&#26415;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#31526;&#65292;&#24182;&#22312;&#35782;&#21035;&#23391;&#21152;&#25289;&#35821;&#21644;&#23612;&#27850;&#23572;&#35821;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290; &#35813;&#30740;&#31350;&#20026;&#35821;&#35328;&#23398;&#30340;&#20808;&#36827;&#21644;&#26131;&#35775;&#38382;&#24615;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02375v1 Announce Type: new  Abstract: Efforts on the research and development of OCR systems for Low-Resource Languages are relatively new. Low-resource languages have little training data available for training Machine Translation systems or other systems. Even though a vast amount of text has been digitized and made available on the internet the text is still in PDF and Image format, which are not instantly accessible. This paper discusses text recognition for two scripts: Bengali and Nepali; there are about 300 and 40 million Bengali and Nepali speakers respectively. In this study, using encoder-decoder transformers, a model was developed, and its efficacy was assessed using a collection of optical text images, both handwritten and printed. The results signify that the suggested technique corresponds with current approaches and achieves high precision in recognizing text in Bengali and Nepali. This study can pave the way for the advanced and accessible study of linguistic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23384;&#20998;&#26512;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#37325;&#28857;&#35780;&#20272;&#20854;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02372</link><description>&lt;p&gt;
&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65306;&#36890;&#36807;&#20869;&#23384;&#20998;&#26512;&#35843;&#26597;&#30495;&#23454;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23384;&#20998;&#26512;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#37325;&#28857;&#35780;&#20272;&#20854;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#21644;&#26234;&#33021;&#35774;&#22791;&#26102;&#20195;&#65292;&#24694;&#24847;&#36719;&#20214;&#30340;&#26816;&#27979;&#23545;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#27169;&#31946;&#25216;&#26415;&#26469;&#35268;&#36991;&#20808;&#36827;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#26816;&#27979;&#21644;&#28040;&#38500;&#23041;&#32961;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#23384;&#36716;&#20648;&#20998;&#26512;&#65292;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;CIC-MalMem-2022&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#24182;&#35780;&#20272;&#22522;&#20110;&#20869;&#23384;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02372v1 Announce Type: cross  Abstract: In the era of the internet and smart devices, the detection of malware has become crucial for system security. Malware authors increasingly employ obfuscation techniques to evade advanced security solutions, making it challenging to detect and eliminate threats. Obfuscated malware, adept at hiding itself, poses a significant risk to various platforms, including computers, mobile devices, and IoT devices. Conventional methods like heuristic-based or signature-based systems struggle against this type of malware, as it leaves no discernible traces on the system. In this research, we propose a simple and cost-effective obfuscated malware detection system through memory dump analysis, utilizing diverse machine-learning algorithms. The study focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world scenarios and assess memory-based obfuscated malware detection. We evaluate the effectiveness of machine learning algorithms, such 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#30524;&#21160;&#25968;&#25454;&#21644;&#25991;&#26412;&#25552;&#31034;&#65292;&#21033;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#20851;&#27880;&#24230;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02370</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30524;&#30555;&#27880;&#35270;&#27169;&#24335;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#30524;&#21160;&#25968;&#25454;&#21644;&#25991;&#26412;&#25552;&#31034;&#65292;&#21033;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#20851;&#27880;&#24230;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#30340;&#36827;&#23637;&#22312;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#20043;&#38388;&#30340;&#20114;&#21160;&#20027;&#35201;&#20165;&#38480;&#20110;&#36755;&#20837;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30524;&#21160;&#25968;&#25454;&#19982;&#25991;&#26412;&#25552;&#31034;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21033;&#29992;&#22686;&#24378;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#20851;&#27880;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26469;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#30524;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#28909;&#22270;&#65292;&#23558;&#23427;&#20204;&#21472;&#21152;&#21040;&#21307;&#23398;&#22270;&#20687;&#19978;&#65292;&#20197;&#31361;&#20986;&#25918;&#23556;&#31185;&#21307;&#24072;&#22312;&#33016;&#37096;X&#23556;&#32447;&#35780;&#20272;&#36807;&#31243;&#20013;&#20851;&#27880;&#24378;&#24230;&#36739;&#39640;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#12289;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#33258;&#21160;&#21270;&#12289;&#38169;&#35823;&#26816;&#27979;&#21644;&#37492;&#21035;&#35786;&#26029;&#31561;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21253;&#21547;&#30524;&#21160;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02370v1 Announce Type: cross  Abstract: Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the imp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#19987;&#23478;&#38598;&#25104;&#38450;&#24481;&#26694;&#26550;(NPoE)&#65292;&#21487;&#21516;&#26102;&#38450;&#24481;&#22810;&#31181;&#21518;&#38376;&#35302;&#21457;&#22120;&#31867;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02356</link><description>&lt;p&gt;
&#20004;&#20010;&#22909;&#22836;&#32988;&#36807;&#19968;&#20010;: &#23884;&#22871;PoE&#29992;&#20110;&#24378;&#21147;&#38450;&#24481;&#22810;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02356
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#19987;&#23478;&#38598;&#25104;&#38450;&#24481;&#26694;&#26550;(NPoE)&#65292;&#21487;&#21516;&#26102;&#38450;&#24481;&#22810;&#31181;&#21518;&#38376;&#35302;&#21457;&#22120;&#31867;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27602;&#21270;&#21518;&#38376;&#25915;&#20987;&#20250;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#19981;&#33391;&#34892;&#20026;&#65292;&#24182;&#19988;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21482;&#37319;&#29992;&#19968;&#31181;&#35302;&#21457;&#22120;&#31867;&#22411;&#65292;&#32780;&#21516;&#26102;&#38450;&#24481;&#22810;&#31181;&#21516;&#26102;&#19988;&#29420;&#31435;&#30340;&#35302;&#21457;&#22120;&#31867;&#22411;&#21017;&#38656;&#35201;&#36890;&#29992;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#19988;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Nested Product of Experts(NPoE)&#38450;&#24481;&#26694;&#26550;&#65292;&#20854;&#20013;&#28041;&#21450;&#23558;&#28151;&#21512;&#30340;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#20316;&#20026;&#35302;&#21457;&#22120;&#38598;&#25104;&#21040;PoE&#38450;&#24481;&#26694;&#26550;&#20013;&#65292;&#20197;&#21516;&#26102;&#38450;&#24481;&#22810;&#31181;&#35302;&#21457;&#22120;&#31867;&#22411;&#12290;NPoE&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20027;&#27169;&#22411;&#19982;&#19968;&#32452; smaller &#19987;&#23478;&#27169;&#22411;&#38598;&#25104;&#35757;&#32451;&#65292;&#19987;&#23478;&#27169;&#22411;&#23398;&#20064;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#29305;&#24449;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#21482;&#20351;&#29992;&#20027;&#27169;&#22411;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#38382;&#39064;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NP
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02356v1 Announce Type: new  Abstract: Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NP
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35745;&#31639;&#26041;&#27861;&#23545;&#27169;&#25311;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#22522;&#20110;BERT&#27169;&#22411;&#23884;&#20837;&#12289;&#27468;&#35789;&#38899;&#39057;&#21644;&#38899;&#32032;&#32452;&#20214;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#20855;&#26377;&#25351;&#31034;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.02342</link><description>&lt;p&gt;
&#27468;&#35789;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Computational Analysis of Lyric Similarity Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35745;&#31639;&#26041;&#27861;&#23545;&#27169;&#25311;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#22522;&#20110;BERT&#27169;&#22411;&#23884;&#20837;&#12289;&#27468;&#35789;&#38899;&#39057;&#21644;&#38899;&#32032;&#32452;&#20214;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#20855;&#26377;&#25351;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21253;&#21547;&#20154;&#22768;&#30340;&#38899;&#20048;&#20316;&#21697;&#20013;&#65292;&#27468;&#35789;&#23545;&#33402;&#26415;&#34920;&#36798;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#35813;&#31995;&#32479;&#24314;&#35758;&#31867;&#20284;&#20110;&#29992;&#25143;&#21916;&#29233;&#25110;&#20010;&#24615;&#21270;&#20559;&#22909;&#30340;&#27468;&#35789;&#65292;&#26377;&#21161;&#20110;&#22312;&#25968;&#30334;&#19975;&#38899;&#36712;&#20013;&#21457;&#29616;&#27468;&#35789;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#31995;&#32479;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#20154;&#31867;&#23545;&#27468;&#35789;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#35745;&#31639;&#26041;&#27861;&#24314;&#27169;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12289;&#27468;&#35789;&#26469;&#28304;&#30340;&#38899;&#39057;&#20197;&#21450;&#38899;&#32032;&#32452;&#20214;&#30340;&#35745;&#31639;&#27169;&#22411;&#25351;&#31034;&#20102;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#12290;&#35813;&#21457;&#29616;&#24378;&#35843;&#20102;&#35821;&#20041;&#12289;&#39118;&#26684;&#21644;&#38899;&#38901;&#30456;&#20284;&#24615;&#22312;&#20154;&#31867;&#24863;&#30693;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02342v1 Announce Type: new  Abstract: In musical compositions that include vocals, lyrics significantly contribute to artistic expression. Consequently, previous studies have introduced the concept of a recommendation system that suggests lyrics similar to a user's favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks. However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area. To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception. Results indicated that computational models based on similarities between embeddings from pre-trained BERT-based models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity. This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception abo
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#26679;&#21270;&#34920;&#31034;&#25216;&#24039;&#30340;&#27880;&#35299;&#22120;&#24314;&#27169;&#39046;&#22495;&#65292;&#23545;&#25968;&#25454;&#38598;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#36827;&#34892;&#30740;&#31350;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2404.02340</link><description>&lt;p&gt;
&#27880;&#35299;&#22120;&#24314;&#27169;&#19982;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Corpus Considerations for Annotator Modeling and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02340
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#21270;&#34920;&#31034;&#25216;&#24039;&#30340;&#27880;&#35299;&#22120;&#24314;&#27169;&#39046;&#22495;&#65292;&#23545;&#25968;&#25454;&#38598;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#36827;&#34892;&#30740;&#31350;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#21644;&#27880;&#37322;&#20219;&#21153;&#30340;&#26368;&#26032;&#36235;&#21183;&#30830;&#35748;&#20102;&#20174;&#20256;&#32479;&#20381;&#36182;&#21333;&#19968;&#8220;&#30495;&#30456;&#26631;&#31614;&#8221;&#36716;&#21521;&#20851;&#27880;&#20010;&#20307;&#35270;&#35282;&#65292;&#23588;&#20854;&#26159;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#12290;&#22312;&#27880;&#37322;&#20219;&#21153;&#26088;&#22312;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#20110;&#22810;&#25968;&#31867;&#21035;&#26631;&#31614;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#24573;&#35270;&#26377;&#20215;&#20540;&#30340;&#23569;&#25968;&#27966;&#35266;&#28857;&#12290;&#36825;&#31181;&#30095;&#28431;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#20449;&#24687;&#30340;&#36951;&#28431;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#33021;&#25200;&#20081;&#26356;&#22823;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24179;&#34913;&#12290;&#38543;&#30528;&#27880;&#35299;&#22120;&#24314;&#27169;&#30340;&#22810;&#26679;&#24615;&#20195;&#34920;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#23427;&#20204;&#19982;&#25968;&#25454;&#38598;&#30340;&#31934;&#32454;&#29305;&#24449;&#32467;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#21508;&#31181;&#27880;&#35299;&#22120;&#24314;&#27169;&#25216;&#26415;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#19971;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02340v1 Announce Type: new  Abstract: Recent trends in natural language processing research and annotation tasks affirm a paradigm shift from the traditional reliance on a single ground truth to a focus on individual perspectives, particularly in subjective tasks. In scenarios where annotation tasks are meant to encompass diversity, models that solely rely on the majority class labels may inadvertently disregard valuable minority perspectives. This oversight could result in the omission of crucial information and, in a broader context, risk disrupting the balance within larger ecosystems. As the landscape of annotator modeling unfolds with diverse representation techniques, it becomes imperative to investigate their effectiveness with the fine-grained features of the datasets in view. This study systematically explores various annotator modeling techniques and compares their performance across seven corpora.   From our findings, we show that the commonly used user token mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#26680;&#24515;&#27169;&#22411;&#21644;&#22810;&#22871;&#39046;&#22495;&#29305;&#23450;&#21442;&#25968;&#65292;&#32467;&#21512;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#25216;&#26415;&#65292;&#20197;&#21450;&#39069;&#22806;&#23618;&#27425;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#36866;&#24212;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#19982;&#27599;&#20010;&#39046;&#22495;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.02335</link><description>&lt;p&gt;
Multi-BERT&#65306;&#21033;&#29992;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#35843;&#25972;&#36827;&#34892;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#26680;&#24515;&#27169;&#22411;&#21644;&#22810;&#22871;&#39046;&#22495;&#29305;&#23450;&#21442;&#25968;&#65292;&#32467;&#21512;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#25216;&#26415;&#65292;&#20197;&#21450;&#39069;&#22806;&#23618;&#27425;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#36866;&#24212;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#19982;&#27599;&#20010;&#39046;&#22495;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24613;&#21095;&#25193;&#23637;&#25552;&#20986;&#20102;&#22810;&#39046;&#22495;&#29615;&#22659;&#20013;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#27874;&#26031;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#35774;&#32622;&#20013;&#20063;&#24456;&#26126;&#26174;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#39046;&#22495;&#65292;&#36824;&#26159;&#20026;&#27599;&#20010;&#39046;&#22495;&#20351;&#29992;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#32463;&#24120;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38480;&#21046;&#12290;&#21333;&#19968;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25429;&#25417;&#21508;&#31181;&#39046;&#22495;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#32780;&#20351;&#29992;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36164;&#28304;&#38480;&#21046;&#65292;&#20351;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#27169;&#22411;&#20960;&#20046;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#19968;&#20010;&#26680;&#24515;&#27169;&#22411;&#21644;&#22810;&#22871;&#39046;&#22495;&#29305;&#23450;&#21442;&#25968;&#32452;&#25104;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#31561;&#25216;&#26415;&#65292;&#32467;&#21512;&#24341;&#20837;&#39069;&#22806;&#23618;&#65292;&#28155;&#21152;&#25105;&#20204;&#21487;&#20197;&#20026;&#29305;&#23450;&#39046;&#22495;&#35757;&#32451;&#30340;&#21442;&#25968;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#19982;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#21333;&#29420;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02335v1 Announce Type: cross  Abstract: The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition (NER) settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each do
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;Llama2-7B&#12289;GPT-3.5&#21644;Falcon-7B&#22312;Inspec&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02330</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Domain Driven Terms Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;Llama2-7B&#12289;GPT-3.5&#21644;Falcon-7B&#22312;Inspec&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#22312;&#20154;&#31867;&#29702;&#35299;&#21644;&#26426;&#22120;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#26550;&#36215;&#37325;&#35201;&#26725;&#26753;&#65292;&#23545;&#20110;&#25968;&#25454;&#20016;&#23500;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26500;&#25104;&#20102;&#35814;&#32454;&#27880;&#37322;&#30340;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#23545;&#22522;&#30784;&#25968;&#25454;&#26356;&#28145;&#20837;&#21644;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;&#20851;&#38190;&#35789;/&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#25991;&#26723;&#25688;&#35201;&#21644;&#20869;&#23481;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#24378;&#35843;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20351;&#29992;&#65306;Llama2-7B&#12289;GPT-3.5&#21644;Falcon-7B&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;Python&#21253;&#19982;&#36825;&#20123;LLMs&#36827;&#34892;&#20132;&#20114;&#65292;&#31616;&#21270;&#20102;&#20851;&#38190;&#35789;&#25552;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;Inspec&#21644;PubMed&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#20215;&#20351;&#29992;Jaccard&#30456;&#20284;&#24615;&#25351;&#25968;&#65292;&#24471;&#21040;&#20102;GPT-3.5&#30340;&#35780;&#20998;&#20998;&#21035;&#20026;0.64&#65288;Inspec&#65289;&#21644;0.21&#65288;PubMed&#65289;&#65292;Llama2-7B&#30340;&#35780;&#20998;&#20998;&#21035;&#20026;0.40&#21644;0.17&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02330v1 Announce Type: cross  Abstract: Keywords play a crucial role in bridging the gap between human understanding and machine processing of textual data. They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data. Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating information retrieval, document summarization, and content categorization. This review focuses on keyword extraction methods, emphasizing the use of three major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We employed a custom Python package to interface with these LLMs, simplifying keyword extraction. Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models. The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for GPT-3.5, 0.40 and 0.17 for Llama2-7B, a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#25903;&#25345;&#22810;&#20219;&#21153;&#35780;&#20272;&#30340;&#20442;&#35821;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#20442;&#35821;&#26816;&#27979;&#21644;&#35782;&#21035;&#22320;&#21306;&#19982;&#21382;&#21490;&#20442;&#35821;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;LLMs&#36755;&#20986;&#20998;&#24067;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.02323</link><description>&lt;p&gt;
&#36808;&#21521;&#38750;&#27491;&#24335;&#35821;&#35328;&#22788;&#29702;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20442;&#35821;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Toward Informal Language Processing: Knowledge of Slang in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02323
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#25903;&#25345;&#22810;&#20219;&#21153;&#35780;&#20272;&#30340;&#20442;&#35821;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#20442;&#35821;&#26816;&#27979;&#21644;&#35782;&#21035;&#22320;&#21306;&#19982;&#21382;&#21490;&#20442;&#35821;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;LLMs&#36755;&#20986;&#20998;&#24067;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22788;&#29702;&#38750;&#27491;&#24335;&#35821;&#35328;&#25552;&#20379;&#20102;&#24378;&#22823;&#28508;&#21147;&#12290;&#38750;&#27491;&#24335;&#35821;&#35328;&#30340;&#19968;&#20010;&#20195;&#34920;&#24418;&#24335;&#26159;&#20442;&#35821;&#65292;&#22312;&#26085;&#24120;&#23545;&#35805;&#21644;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#24120;&#29992;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#19988;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#22522;&#20934;&#65292;&#20442;&#35821;&#22312;LLMs&#20013;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#21033;&#29992;&#30005;&#24433;&#23383;&#24149;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#22312;&#28041;&#21450;&#20442;&#35821;&#33258;&#21160;&#22788;&#29702;&#30340;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#20004;&#20010;&#26680;&#24515;&#24212;&#29992;&#20013;&#65288;&#20442;&#35821;&#26816;&#27979;&#21644;&#35782;&#21035;&#33258;&#28982;&#35821;&#21477;&#20013;&#20442;&#35821;&#30340;&#22320;&#21306;&#21644;&#21382;&#21490;&#26469;&#28304;&#65289;&#30340;&#35780;&#20272;&#21644;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26469;&#25506;&#27979;LLMs&#30340;&#36755;&#20986;&#20998;&#24067;&#20197;&#33719;&#24471;&#35299;&#37322;&#24615;&#27934;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649; GPT-4 &#31561;LLMs &#22312;&#38646;&#27425;&#23581;&#35797;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20442;&#35821;&#22788;&#29702;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02323v1 Announce Type: new  Abstract: Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02319</link><description>&lt;p&gt;
Prompt&#20316;&#20026;&#31243;&#24207;&#65306;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39640;&#25928;&#32534;&#35793;&#26102;Prompt&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#33021;&#22788;&#29702;&#26356;&#38271;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#65292;&#36825;&#20419;&#36827;&#20102;&#26356;&#22797;&#26434;&#25552;&#31034;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;&#35843;&#25972;&#20197;&#25552;&#39640;&#37096;&#32626;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#38543;&#30528;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;LLM&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#35768;&#22810;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#24050;&#19981;&#20877;&#36275;&#22815;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SAMMO&#65292;&#19968;&#20010;&#29992;&#20110;&#20803;&#25552;&#31034;&#31243;&#24207;&#30340;{\em &#32534;&#35793;&#26102;}&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#25552;&#31034;&#34920;&#31034;&#20026;&#32467;&#26500;&#21270;&#23545;&#35937;&#65292;&#20801;&#35768;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25628;&#32034;&#19968;&#32452;&#20016;&#23500;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;SAMMO&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#12289;RAG&#31649;&#32447;&#35843;&#25972;&#21644;&#25552;&#31034;&#21387;&#32553;&#26041;&#38754;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#25918;&#25152;&#26377;&#20195;&#30721;&#20379;&#22823;&#23478;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24310;&#38271;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#23548;&#33268;&#37325;&#22797;&#21644;&#23849;&#28291;&#30340;&#26631;&#35760;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.02305</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Collapse of Self-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02305
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24310;&#38271;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#23548;&#33268;&#37325;&#22797;&#21644;&#23849;&#28291;&#30340;&#26631;&#35760;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#30693;&#35782;&#21019;&#36896;&#39046;&#22495;&#65292;&#21253;&#25324;&#31185;&#23398;&#65292;&#26032;&#24605;&#24819;&#24448;&#24448;&#24314;&#31435;&#22312;&#29616;&#26377;&#20449;&#24687;&#20043;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20010;&#27010;&#24565;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22312;&#20854;&#33258;&#36523;&#36755;&#20986;&#19978;&#30340;&#28508;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#24182;&#24314;&#31435;&#22312;&#20182;&#20204;&#20197;&#21069;&#30340;&#24605;&#24819;&#21644;&#34892;&#21160;&#19978;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#30452;&#35266;&#19978;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;GPT-2&#27169;&#22411;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#33258;&#35757;&#32451;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#23548;&#33268;&#37325;&#22797;&#21644;&#23849;&#28291;&#30340;&#20196;&#29260;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02305v1 Announce Type: cross  Abstract: In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions. While this approach is intuitively appealing, our research reveals its practical limitations. We find that extended self-training of the GPT-2 model leads to a significant degradation in performance, resulting in repetitive and collapsed token output.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#33391;&#22909;&#34920;&#29616;&#20294;&#20063;&#21457;&#29616;&#20102;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.02269</link><description>&lt;p&gt;
&#36890;&#36807;ChatGPT&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02269
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#33391;&#22909;&#34920;&#29616;&#20294;&#20063;&#21457;&#29616;&#20102;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35268;&#33539;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#26469;&#36890;&#36807;&#25429;&#25417;&#22914;&#20309;&#31649;&#29702;&#20004;&#20010;&#25110;&#22810;&#20010;&#33258;&#27835;&#26041;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35774;&#35745;&#22810;&#20027;&#20307;&#31995;&#32479;&#12290;&#25105;&#20204;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#25215;&#35834;&#12289;&#31105;&#27490;&#12289;&#25480;&#26435;&#21644;&#26435;&#21147;&#31561;&#35268;&#33539;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#35268;&#33539;&#35201;&#32032;&#65288;&#28041;&#21450;&#30340;&#24403;&#20107;&#26041;&#12289;&#21069;&#22240;&#21644;&#21518;&#26524;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;ChatGPT&#23637;&#31034;&#20102;&#22312;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#22240;&#27492;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#36825;&#22312;&#35813;&#39046;&#22495;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;ChatGPT&#22312;&#25552;&#21462;&#36825;&#20123;&#35268;&#33539;&#26041;&#38754;&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#35268;&#33539;&#25552;&#21462;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;&#23545;&#20851;&#38190;&#32454;&#33410;&#30340;&#24573;&#35270;&#12289;&#33222;&#24819;&#12289;&#23545;&#36830;&#25509;&#35789;&#30340;&#38169;&#35823;&#35299;&#26512;&#20197;&#21450;&#31354;&#35268;&#33539;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02269v1 Announce Type: cross  Abstract: We investigate the effectiveness of ChatGPT in extracting norms from contracts. Norms provide a natural way to engineer multiagent systems by capturing how to govern the interactions between two or more autonomous parties. We extract norms of commitment, prohibition, authorization, and power, along with associated norm elements (the parties involved, antecedents, and consequents) from contracts. Our investigation reveals ChatGPT's effectiveness and limitations in norm extraction from contracts. ChatGPT demonstrates promising performance in norm extraction without requiring training or fine-tuning, thus obviating the need for annotated data, which is not generally available in this domain. However, we found some limitations of ChatGPT in extracting these norms that lead to incorrect norm extractions. The limitations include oversight of crucial details, hallucination, incorrect parsing of conjunctions, and empty norm elements. Enhanced 
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02261</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#24490;&#29615;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02261
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#21644;&#25968;&#25454;&#26631;&#27880;&#19987;&#19994;&#30693;&#35782;&#26377;&#38480;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#32597;&#35265;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#33410;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#35780;&#20272;&#20197;&#35780;&#20272;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#36866;&#24403;&#30340;LLM&#27880;&#37322;&#32773;&#12290;&#28982;&#21518;&#65292;&#36873;&#25321;&#30340;&#27880;&#37322;&#32773;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#25454;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;GPT-4-Turbo&#65292;&#23637;&#31034;&#20102;&#20960;&#20046;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#65292;&#30001;&#20272;&#31639;&#30340;&#28508;&#22312;&#24615;&#33021;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;Mixture-of-Depths&#65292;&#21487;&#20197;&#22312;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#20998;&#37197;FLOPs&#20197;&#20248;&#21270;&#27169;&#22411;&#28145;&#24230;&#19978;&#19981;&#21516;&#23618;&#30340;&#24207;&#21015;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2404.02258</link><description>&lt;p&gt;
Mixture-of-Depths: &#22312;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;Mixture-of-Depths&#65292;&#21487;&#20197;&#22312;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#20998;&#37197;FLOPs&#20197;&#20248;&#21270;&#27169;&#22411;&#28145;&#24230;&#19978;&#19981;&#21516;&#23618;&#30340;&#24207;&#21015;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23558;FLOPs&#22343;&#21248;&#20998;&#24067;&#22312;&#36755;&#20837;&#24207;&#21015;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;transformers&#21487;&#20197;&#23398;&#20064;&#21160;&#24577;&#22320;&#23558;FLOPs&#65288;&#25110;&#35745;&#31639;&#65289;&#20998;&#37197;&#32473;&#24207;&#21015;&#20013;&#30340;&#29305;&#23450;&#20301;&#32622;&#65292;&#20248;&#21270;&#21508;&#27169;&#22411;&#23618;&#28145;&#24230;&#19978;&#30340;&#24207;&#21015;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35774;&#23450;&#22312;&#32473;&#23450;&#23618;&#20013;&#21487;&#21442;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;MLP&#35745;&#31639;&#30340;&#20196;&#29260;&#25968;&#65288;$k$&#65289;&#26469;&#23454;&#26045;&#24635;&#35745;&#31639;&#39044;&#31639;&#12290;&#35201;&#22788;&#29702;&#30340;&#20196;&#29260;&#30001;&#32593;&#32476;&#20351;&#29992;top-$k$&#36335;&#30001;&#26426;&#21046;&#30830;&#23450;&#12290;&#30001;&#20110;$k$&#26159;&#20107;&#20808;&#23450;&#20041;&#30340;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#36807;&#31243;&#20351;&#29992;&#20855;&#26377;&#24050;&#30693;&#24352;&#37327;&#22823;&#23567;&#30340;&#38745;&#24577;&#35745;&#31639;&#22270;&#65292;&#19981;&#21516;&#20110;&#20854;&#20182;&#26465;&#20214;&#35745;&#31639;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$k$&#20196;&#29260;&#30340;&#26631;&#35782;&#26159;&#19981;&#22266;&#23450;&#30340;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38750;&#22343;&#21248;&#22320;&#36328;&#26102;&#38388;&#21644;&#27169;&#22411;&#28145;&#24230;&#32500;&#24230;&#20998;&#37197;FLOPs&#12290;&#22240;&#27492;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#35745;&#31639;&#25903;&#20986;&#23436;&#20840;&#21487;&#39044;&#27979;&#65292;&#20294;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02258v1 Announce Type: cross  Abstract: Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but d
&lt;/p&gt;</description></item><item><title>LM2&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;$\texttt{LM}^\texttt{2}$&#65292;&#35813;&#27169;&#22411;&#23558;&#20998;&#35299;&#12289;&#35299;&#20915;&#21644;&#39564;&#35777;&#27169;&#22359;&#21270;&#65292;&#36890;&#36807;&#20998;&#35299;&#22120;&#35782;&#21035;&#20851;&#38190;&#27010;&#24565;&#24182;&#29983;&#25104;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21516;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02255</link><description>&lt;p&gt;
$\texttt{LM}^\texttt{2}$: &#19968;&#31181;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02255
&lt;/p&gt;
&lt;p&gt;
LM2&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;$\texttt{LM}^\texttt{2}$&#65292;&#35813;&#27169;&#22411;&#23558;&#20998;&#35299;&#12289;&#35299;&#20915;&#21644;&#39564;&#35777;&#27169;&#22359;&#21270;&#65292;&#36890;&#36807;&#20998;&#35299;&#22120;&#35782;&#21035;&#20851;&#38190;&#27010;&#24565;&#24182;&#29983;&#25104;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21516;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23637;&#31034;&#20102;&#20986;&#29616;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMS)&#32463;&#24120;&#20250;&#22312;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#25512;&#29702;&#20013;&#36855;&#22833;&#26041;&#21521;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#38382;&#39064;&#26469;&#25552;&#20379;&#25351;&#23548;&#65292;&#21487;&#20197;&#24341;&#21457;LLM&#25512;&#29702;&#30340;&#26356;&#24378;&#20581;&#24615;&#8212;&#8212;&#19968;&#20010;&#20998;&#35299;&#22120;&#29983;&#25104;&#23376;&#38382;&#39064;&#65292;&#19968;&#20010;&#35299;&#31639;&#22120;&#35299;&#20915;&#27599;&#20010;&#36825;&#20123;&#23376;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#26410;&#33021;&#36866;&#24212;&#20998;&#35299;&#22120;&#21644;&#35299;&#31639;&#22120;&#27169;&#22359;&#20043;&#38388;&#30340;&#21327;&#35843;(&#26080;&#35770;&#26159;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#36824;&#26159;&#22312;&#19981;&#21516;&#30340;&#19987;&#38376;&#27169;&#22411;&#20013;)&#8212;&#8212;&#20998;&#35299;&#22120;&#27809;&#26377;&#36319;&#36394;&#35299;&#31639;&#22120;&#25353;&#29031;&#20998;&#35299;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LM2&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;LM2&#23558;&#20998;&#35299;&#12289;&#35299;&#20915;&#21644;&#39564;&#35777;&#27169;&#22359;&#21270;&#20026;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20998;&#35299;&#22120;&#27169;&#22359;&#35782;&#21035;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#25512;&#29702;&#35201;&#27714;&#29983;&#25104;&#36880;&#27493;&#23376;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02255v1 Announce Type: cross  Abstract: Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requ
&lt;/p&gt;</description></item><item><title>&#20943;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21487;&#22312;&#31616;&#21270;&#35821;&#35328;&#20013;&#23454;&#29616;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02204</link><description>&lt;p&gt;
&#20943;&#23567;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emergent Abilities in Reduced-Scale Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02204
&lt;/p&gt;
&lt;p&gt;
&#20943;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21487;&#22312;&#31616;&#21270;&#35821;&#35328;&#20013;&#23454;&#29616;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#36825;&#31181;&#33021;&#21147;&#65292;&#20063;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#65292;&#20027;&#35201;&#20986;&#29616;&#22312;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#26032;&#20852;&#23646;&#24615;&#26159;&#21542;&#20005;&#26684;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#20851;&#65292;&#25110;&#32773;&#21487;&#20197;&#36890;&#36807;&#22312;&#20943;&#23567;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#26469;&#23637;&#31034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;36&#20010;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21442;&#25968;&#20174;100&#19975;&#21040;1.65&#20159;&#19981;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#31616;&#21270;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#31616;&#21270;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#19982;&#22312;&#33258;&#30001;&#35821;&#35328;&#19978;&#20845;&#20493;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#65292;&#32553;&#23567;&#35821;&#35328;&#35268;&#27169;&#21487;&#20197;&#20351;&#20855;&#26377;&#26377;&#38480;&#22823;&#23567;&#30340;&#27169;&#22411;&#20986;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02204v1 Announce Type: new  Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#20195;&#29702;&#21487;&#33258;&#20027;&#36816;&#20316;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#22797;&#26434;&#24615;&#21160;&#24577;&#22686;&#21152;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02183</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#20195;&#29702;&#65306;&#38754;&#21521;&#36229;&#22823;&#35268;&#27169;&#20195;&#30721;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;LLM&#22810;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#20195;&#29702;&#21487;&#33258;&#20027;&#36816;&#20316;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#22797;&#26434;&#24615;&#21160;&#24577;&#22686;&#21152;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#36827;&#34892;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#33258;&#21160;&#36719;&#20214;&#24320;&#21457;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#20195;&#29702;&#26041;&#27861;&#22312;&#29983;&#25104;&#21644;&#25913;&#36827;&#22823;&#35268;&#27169;&#22797;&#26434;&#20195;&#30721;&#24211;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#12290;&#22312;SoA&#20013;&#65292;&#33258;&#32452;&#32455;&#20195;&#29702;&#29420;&#31435;&#36816;&#20316;&#65292;&#20197;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#21516;&#26102;&#26080;&#32541;&#21327;&#20316;&#26500;&#24314;&#25972;&#20307;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#22522;&#20110;&#38382;&#39064;&#22797;&#26434;&#24615;&#33258;&#21160;&#22686;&#21152;&#20195;&#29702;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#21160;&#24577;&#25193;&#23637;&#24615;&#12290;&#36825;&#20351;&#24471;&#25972;&#20307;&#20195;&#30721;&#37327;&#21487;&#20197;&#26681;&#25454;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#22686;&#21152;&#65292;&#21516;&#26102;&#30001;&#27599;&#20010;&#20195;&#29702;&#31649;&#29702;&#30340;&#20195;&#30721;&#37327;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02183v1 Announce Type: cross  Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by eac
&lt;/p&gt;</description></item><item><title>&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;CNN&#21644;BiLSTM&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#27169;&#25311;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#21644;&#39034;&#24207;&#27169;&#24335;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#21360;&#22320;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#21360;&#24230;&#33521;&#35821;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;&#12290;</title><link>https://arxiv.org/abs/2404.02013</link><description>&lt;p&gt;
&#25171;&#30772;&#27785;&#40664;&#65306;&#26816;&#27979;&#21644;&#32531;&#35299;&#21360;&#22320;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#21360;&#24230;&#33521;&#35821;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;
&lt;/p&gt;
&lt;p&gt;
Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02013
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;CNN&#21644;BiLSTM&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#27169;&#25311;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#21644;&#39034;&#24207;&#27169;&#24335;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#21360;&#22320;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#21360;&#24230;&#33521;&#35821;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#21035;&#39578;&#25200;&#26159;&#19968;&#20010;&#24191;&#27867;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#22899;&#24615;&#21644;&#36793;&#32536;&#24615;&#21035;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#30340;&#33258;&#30001;&#34920;&#36798;&#21644;&#21442;&#19982;&#12290;&#26816;&#27979;&#36825;&#31867;&#28389;&#29992;&#20869;&#23481;&#21487;&#20197;&#24110;&#21161;&#24179;&#21488;&#36943;&#21046;&#36825;&#19968;&#31096;&#23475;&#12290;&#25105;&#20204;&#21442;&#21152;&#20102; ICON2023 &#30340;Indic&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#34384;&#24453;&#26816;&#27979;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#22312;&#33521;&#35821;&#12289;&#21360;&#22320;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#20013;&#36827;&#34892;&#20998;&#31867;&#20197;&#35782;&#21035;&#24615;&#21035;&#34384;&#24453;&#30340;Twitter&#24086;&#23376;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;CNLP-NITS-PP&#24320;&#21457;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;CNN&#21644;BiLSTM&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#21644;&#39034;&#24207;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#12290;&#21367;&#31215;&#28388;&#27874;&#22120;&#22312;&#23884;&#20837;&#36755;&#20837;&#25991;&#26412;&#19978;&#24212;&#29992;&#65292;CNN&#25429;&#33719;&#25351;&#31034;&#28389;&#29992;&#35821;&#35328;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#20026;&#20102;&#30830;&#23450;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20882;&#29359;&#24615;&#65292;BiLSTM&#20998;&#26512;&#36825;&#20010;&#24207;&#21015;&#20197;&#20102;&#35299;&#21333;&#35789;&#21644;&#30701;&#35821;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#27599;&#31181;&#35821;&#35328;&#20351;&#29992;&#20102;FastText&#21644;GloVe&#35789;&#23884;&#20837;&#36827;&#34892;&#20102;&#22810;&#20010;&#21464;&#20307;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02013v1 Announce Type: new  Abstract: Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each languag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01744</link><description>&lt;p&gt;
Octopus v2&#65306;&#29992;&#20110;&#36229;&#32423;&#20195;&#29702;&#30340;&#35774;&#22791;&#19978;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Octopus v2: On-device language model for super agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#19982;&#33258;&#21160;&#24037;&#20316;&#27969;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#35843;&#29992;&#20989;&#25968;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#21019;&#24314;AI&#20195;&#29702;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20113;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#30528;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#29992;&#20110;&#20989;&#25968;&#35843;&#29992;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#38754;&#20020;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;GPT-4&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;&#20102;95%&#12290;&#19982;&#22522;&#20110;RAG&#30340;&#20989;&#25968;&#35843;&#29992;&#26426;&#21046;&#30340;Llama-7B&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;35&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;&#36866;&#21512;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#27700;&#24179;&#19978;&#65292;&#31526;&#21512;&#24615;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2404.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#24402;&#32467;&#21453;&#39539;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36890;&#29992;&#19988;&#21487;&#38752;&#30340;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01677
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21487;&#27867;&#21270;&#19988;&#21487;&#38752;&#25512;&#29702;&#22120;&#65288;GFaiR&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01677v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system o
&lt;/p&gt;</description></item><item><title>ChatGLM-RLHF&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12289;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGLM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.00934</link><description>&lt;p&gt;
ChatGLM-RLHF&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00934
&lt;/p&gt;
&lt;p&gt;
ChatGLM-RLHF&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12289;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGLM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00934v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;ChatGLM&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23478;&#26063;&#25552;&#20379;&#25903;&#25345;&#30340;&#20813;&#36153;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatGLM-RLHF&#27969;&#27700;&#32447;--&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#31995;&#32479;--&#26088;&#22312;&#22686;&#24378;ChatGLM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;ChatGLM-RLHF&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#25910;&#38598;&#65292;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#22312;&#23558;ChatGLM-RLHF&#25972;&#21512;&#21040;&#29983;&#20135;&#29615;&#22659;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#24182;&#35299;&#20915;&#20102;&#19968;&#20123;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20943;&#23569;&#22870;&#21169;&#26041;&#24046;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24102;&#26377;&#34701;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#27491;&#21017;&#21270;&#32422;&#26463;&#20197;&#36991;&#20813;LLMs&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;ChatGLM&#30340;&#21463;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#29256;&#26412;&#30456;&#27604;&#65292;ChatGLM-RLHF&#22312;&#23545;&#40784;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00934v1 Announce Type: new  Abstract: ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. Fo
&lt;/p&gt;</description></item><item><title>LLMs&#23545;&#20316;&#32773;&#30340;&#35821;&#35328;&#27169;&#24335;&#30340;&#24433;&#21709;&#30053;&#24494;&#38477;&#20302;&#20854;&#20010;&#20154;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#26174;&#33879;&#21464;&#21270;&#19981;&#22826;&#39057;&#32321;&#12290;</title><link>https://arxiv.org/abs/2404.00267</link><description>&lt;p&gt;
&#20445;&#23494;&#32773;&#65306;LLM&#23545;&#20010;&#20154;&#29305;&#24449;&#35821;&#35328;&#26631;&#35760;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00267
&lt;/p&gt;
&lt;p&gt;
LLMs&#23545;&#20316;&#32773;&#30340;&#35821;&#35328;&#27169;&#24335;&#30340;&#24433;&#21709;&#30053;&#24494;&#38477;&#20302;&#20854;&#20010;&#20154;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#26174;&#33879;&#21464;&#21270;&#19981;&#22826;&#39057;&#32321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#20010;&#20307;&#35821;&#35328;&#20351;&#29992;&#19982;&#20854;&#20010;&#20154;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#65307;&#25105;&#20204;&#30340;&#35821;&#35328;&#27169;&#24335;&#25581;&#31034;&#20102;&#20851;&#20110;&#25105;&#20204;&#20010;&#24615;&#12289;&#24773;&#32490;&#29366;&#24577;&#21644;&#20449;&#24565;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26085;&#24120;&#20889;&#20316;&#20013;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37319;&#29992;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#24403;LLMs&#21442;&#19982;&#20889;&#20316;&#36807;&#31243;&#26102;&#65292;&#20316;&#32773;&#30340;&#35821;&#35328;&#27169;&#24335;&#26159;&#21542;&#20173;&#28982;&#33021;&#39044;&#27979;&#20854;&#20010;&#20154;&#29305;&#24449;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#23545;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#21644;&#24515;&#29702;&#29305;&#24449;&#30340;&#35821;&#35328;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#20855;&#20307;&#26816;&#26597;&#20102;&#19977;&#20010;LLMs - GPT3.5&#12289;Llama 2&#21644;Gemini - &#22312;&#20845;&#31181;&#19981;&#21516;&#29305;&#24449;&#19978;&#65306;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#25919;&#27835;&#31435;&#22330;&#12289;&#20010;&#24615;&#12289;&#31227;&#24773;&#33021;&#21147;&#21644;&#36947;&#24503;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#20351;&#29992;LLMs&#30053;&#24494;&#38477;&#20302;&#20102;&#35821;&#35328;&#27169;&#24335;&#23545;&#20316;&#32773;&#20010;&#20154;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#26174;&#33879;&#21464;&#21270;&#19981;&#22826;&#39057;&#32321;&#65292;LLMs&#30340;&#20351;&#29992;&#24182;&#19981;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00267v1 Announce Type: new  Abstract: Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#27880;&#20837;&#26032;&#30693;&#35782;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#36817;&#20307;&#32946;&#20107;&#20214;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2404.00213</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#27880;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#27880;&#20837;&#26032;&#30693;&#35782;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#36817;&#20307;&#32946;&#20107;&#20214;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26159;&#19968;&#39033;&#23453;&#36149;&#30340;&#36164;&#20135;&#12290;&#28982;&#32780;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#24182;&#25972;&#21512;&#26032;&#30340;&#39046;&#22495;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#27169;&#22411;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#21457;&#29983;&#30340;&#20107;&#23454;&#21644;&#20107;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20316;&#20026;LLMs&#20013;&#27880;&#20837;&#30693;&#35782;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#36817;&#20307;&#32946;&#20107;&#20214;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00213v1 Announce Type: new  Abstract: In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&amp;A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even cove
&lt;/p&gt;</description></item><item><title>IndiBias&#26159;&#19968;&#20010;&#20026;&#35780;&#20272;&#21360;&#24230;&#35821;&#22659;&#20013;&#31038;&#20250;&#20559;&#35265;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36807;&#28388;&#21644;&#32763;&#35793;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#19981;&#21516;LLMs&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21360;&#24230;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#32500;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.20147</link><description>&lt;p&gt;
IndiBias&#65306;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#21360;&#24230;&#35821;&#22659;&#19979;&#35821;&#35328;&#27169;&#22411;&#31038;&#20250;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20147
&lt;/p&gt;
&lt;p&gt;
IndiBias&#26159;&#19968;&#20010;&#20026;&#35780;&#20272;&#21360;&#24230;&#35821;&#22659;&#20013;&#31038;&#20250;&#20559;&#35265;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36807;&#28388;&#21644;&#32763;&#35793;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#19981;&#21516;LLMs&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21360;&#24230;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#24433;&#21709;&#24341;&#21457;&#20102;&#23545;&#25429;&#25417;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#36825;&#20123;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#21644;&#35199;&#26041;&#32972;&#26223;&#65292;&#32570;&#20047;&#19968;&#20010;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#20307;&#29616;&#21360;&#24230;&#29420;&#29305;&#30340;&#31038;&#20250;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IndiBias&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21360;&#24230;&#35821;&#22659;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36807;&#28388;&#21644;&#32763;&#35793;&#29616;&#26377;&#30340;CrowS-Pairs&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#21360;&#24230;&#35821;&#22659;&#20013;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#21360;&#22320;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;ChatGPT&#21644;InstructGPT&#22312;&#20869;&#30340;LLMs&#65292;&#20197;&#21360;&#24230;&#27969;&#34892;&#30340;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#22686;&#24378;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#21253;&#21547;&#30340;&#20559;&#35265;&#32500;&#24230;&#28085;&#30422;&#24615;&#21035;&#12289;&#23447;&#25945;&#12289;&#31181;&#22995;&#12289;&#24180;&#40836;&#12289;&#22320;&#21306;&#12289;&#22806;&#35980;&#21644;&#32844;&#19994;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#36164;&#28304;&#26469;&#35299;&#20915;&#20132;&#21449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20147v1 Announce Type: new  Abstract: The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India's unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#32858;&#21512;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.19183</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36890;&#29992;&#20381;&#23384;&#21477;&#27861;&#26641;&#32858;&#21512;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#23384;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#20381;&#23384;&#20998;&#26512;&#22120;&#30340;&#36136;&#37327;&#23545;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20381;&#23384;&#20998;&#26512;&#22120;&#30340;&#36136;&#37327;&#36890;&#24120;&#21462;&#20915;&#20110;&#28041;&#21450;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#36136;&#37327;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#20197;&#23454;&#29616;&#31283;&#23450;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#21518;&#22788;&#29702;&#32858;&#21512;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#35299;&#20915;&#36136;&#37327;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20381;&#23384;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#21518;&#22788;&#29702;&#32858;&#21512;&#30340;&#32858;&#21512;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19183v1 Announce Type: new  Abstract: Dependency parsing is an essential task in NLP, and the quality of dependency parsers is crucial for many downstream tasks. Parsers' quality often varies depending on the domain and the language involved. Therefore, it is essential to combat the issue of varying quality to achieve stable performance. In various NLP tasks, aggregation methods are used for post-processing aggregation and have been shown to combat the issue of varying quality. However, aggregation methods for post-processing aggregation have not been sufficiently studied in dependency parsing tasks. In an extensive empirical study, we compare different unsupervised post-processing aggregation methods to identify the most suitable dependency tree structure aggregation method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.18346</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21644;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#65306;&#22240;&#26524;&#20851;&#31995;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;MLLMs&#36890;&#24120;&#36807;&#24230;&#20381;&#36182;&#21333;&#27169;&#24577;&#20559;&#24046;&#65288;&#20363;&#22914;&#35821;&#35328;&#20559;&#24046;&#21644;&#35270;&#35273;&#20559;&#24046;&#65289;&#65292;&#23548;&#33268;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#32473;&#20986;&#19981;&#27491;&#30830;&#31572;&#26696;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#26469;&#35299;&#37322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#38416;&#26126;MLLMs&#23545;VQA&#38382;&#39064;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#30340;&#22240;&#26524;&#20998;&#26512;&#35780;&#20272;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#26524;&#12290;&#21463;&#22240;&#26524;&#22270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MORE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12,000&#20010;VQA&#23454;&#20363;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;MLLMs&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#21644;&#20811;&#26381;&#21333;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#24182;&#22686;&#24378;MLLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18346v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabiliti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20998;&#26512;&#20102;&#21475;&#35821;&#35821;&#35328;&#33258;&#30417;&#30563;&#27169;&#22411;&#23545;&#22768;&#35843;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20351;&#29992;&#26222;&#36890;&#35805;&#21644;&#36234;&#21335;&#35821;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;SLMs&#22312;&#35757;&#32451;&#20110;&#38750;&#38899;&#35843;&#35821;&#35328;&#25968;&#25454;&#26102;&#20063;&#20445;&#25345;&#30528;&#26174;&#33879;&#30340;&#35789;&#27719;&#38899;&#35843;&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16865</link><description>&lt;p&gt;
&#21475;&#35821;&#35821;&#35328;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#38899;&#35843;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding of lexical tone in self-supervised models of spoken language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20998;&#26512;&#20102;&#21475;&#35821;&#35821;&#35328;&#33258;&#30417;&#30563;&#27169;&#22411;&#23545;&#22768;&#35843;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20351;&#29992;&#26222;&#36890;&#35805;&#21644;&#36234;&#21335;&#35821;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;SLMs&#22312;&#35757;&#32451;&#20110;&#38750;&#38899;&#35843;&#35821;&#35328;&#25968;&#25454;&#26102;&#20063;&#20445;&#25345;&#30528;&#26174;&#33879;&#30340;&#35789;&#27719;&#38899;&#35843;&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20174;&#22768;&#23398;&#12289;&#35821;&#38899;&#12289;&#38899;&#38901;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#23618;&#38754;&#21040;&#35828;&#35805;&#32773;&#29305;&#24449;&#20013;&#32534;&#30721;&#20102;&#20154;&#31867;&#35821;&#38899;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#12290;&#20197;&#21069;&#20851;&#20110;&#38899;&#38901;&#23398;&#34920;&#31034;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35832;&#22914;&#38899;&#32032;&#31561;&#37096;&#20998;&#29305;&#24449;&#19978;&#65307;SLMs&#20013;&#23545;&#38899;&#38901;&#38899;&#31995;&#65288;&#22914;&#22768;&#35843;&#21644;&#37325;&#38899;&#27169;&#24335;&#65289;&#30340;&#32534;&#30721;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#22768;&#35843;&#26159;&#19968;&#31181;&#23384;&#22312;&#20110;&#19990;&#30028;&#19978;&#21322;&#25968;&#20197;&#19978;&#35821;&#35328;&#20013;&#30340;&#38899;&#31995;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;SLMs&#30340;&#22768;&#35843;&#32534;&#30721;&#33021;&#21147;&#65292;&#20197;&#26222;&#36890;&#35805;&#21644;&#36234;&#21335;&#35821;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SLMs&#22312;&#35757;&#32451;&#20110;&#38750;&#38899;&#35843;&#35821;&#35328;&#25968;&#25454;&#26102;&#20063;&#26126;&#26174;&#32534;&#30721;&#20102;&#35789;&#27719;&#38899;&#35843;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;SLMs&#22312;&#22768;&#35843;&#21644;&#36741;&#38899;&#24863;&#30693;&#30740;&#31350;&#20013;&#30340;&#34920;&#29616;&#19982;&#26412;&#26063;&#21644;&#38750;&#26412;&#26063;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#31867;&#20284;&#65292;&#20294;&#23427;&#20204;&#19981;&#36981;&#24490;&#30456;&#21516;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16865v1 Announce Type: new  Abstract: Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the sam
&lt;/p&gt;</description></item><item><title>MasonTigers&#22312;SemEval-2024 Task 1&#20013;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#65292;&#22312;&#22788;&#29702;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14990</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024&#20219;&#21153;1&#20013;&#30340;&#38598;&#25104;&#26041;&#27861;&#30740;&#31350;&#65306;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14990
&lt;/p&gt;
&lt;p&gt;
MasonTigers&#22312;SemEval-2024 Task 1&#20013;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#65292;&#22312;&#22788;&#29702;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#21442;&#19982;SemEval-2024&#20219;&#21153;1-&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#24037;&#20316;&#12290;&#35813;&#20219;&#21153;&#28085;&#30422;&#20102;&#28085;&#30422;&#20102;&#30417;&#30563;&#65288;Track A&#65289;&#12289;&#26080;&#30417;&#30563;&#65288;Track B&#65289;&#21644;&#36328;&#35821;&#35328;&#65288;Track C&#65289;&#26041;&#27861;&#65292;&#28041;&#21450;14&#31181;&#19981;&#21516;&#35821;&#35328;&#12290;MasonTigers&#26159;&#23569;&#25968;&#21516;&#26102;&#21442;&#19982;&#20102;&#19977;&#20010;track&#20013;&#25152;&#26377;&#35821;&#35328;&#30340;&#20004;&#25903;&#22242;&#38431;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Track A&#20013;&#25490;&#21517;&#20174;&#31532;11&#21040;&#31532;21&#65292;&#22312;Track B&#20013;&#25490;&#21517;&#20174;&#31532;1&#21040;&#31532;8&#65292;&#22312;Track C&#20013;&#25490;&#21517;&#20174;&#31532;5&#21040;&#31532;12&#12290;&#22312;&#36981;&#24490;&#29305;&#23450;&#20219;&#21153;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14990v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;SemEval-2024 Task 9&#30340;&#35868;&#39064;&#20219;&#21153;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#25552;&#31034;&#25216;&#26415;&#65292;&#21253;&#25324;&#38646;&#21442;&#32771;&#21644;&#23569;&#37327;&#21442;&#32771;&#25552;&#31034;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#26368;&#32456;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#36880;&#27493;&#35299;&#37322;&#24615;&#25552;&#31034;&#22914;&#20309;&#21487;&#20197;&#26356;&#22909;&#22320;&#25581;&#31034;&#24050;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.14982</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024 Task 9&#20013;&#65306;&#20351;&#29992;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#35299;&#20915;&#35868;&#39064;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;SemEval-2024 Task 9&#30340;&#35868;&#39064;&#20219;&#21153;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#25552;&#31034;&#25216;&#26415;&#65292;&#21253;&#25324;&#38646;&#21442;&#32771;&#21644;&#23569;&#37327;&#21442;&#32771;&#25552;&#31034;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#26368;&#32456;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#36880;&#27493;&#35299;&#37322;&#24615;&#25552;&#31034;&#22914;&#20309;&#21487;&#20197;&#26356;&#22909;&#22320;&#25581;&#31034;&#24050;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;&#22242;&#38431;MasonTigers&#25552;&#20132;&#32473;SemEval-2024 Task 9&#30340;&#20316;&#21697;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#32452;&#35868;&#39064;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#20960;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#38646;&#21442;&#32771;&#21644;&#23569;&#37327;&#21442;&#32771;&#25552;&#31034;&#36890;&#36807;&#19987;&#26377;LLMs&#27979;&#35797;&#26102;&#21462;&#24471;&#20102;&#30456;&#24403;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#30456;&#36739;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#27493;&#20998;&#35299;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#26368;&#22909;&#30340;&#32467;&#26524;&#65292;&#22312;&#21333;&#35789;&#35868;&#39064;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;&#22312;&#21477;&#23376;&#35868;&#39064;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;13&#12290;&#25552;&#31034;LLMs&#30340;&#24378;&#22823;&#34920;&#29616;&#26174;&#31034;&#20102;&#23427;&#20204;&#22312;&#25552;&#20379;&#24605;&#32771;&#36807;&#31243;&#20998;&#35299;&#26102;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#36880;&#27493;&#35299;&#37322;&#24615;&#25552;&#31034;&#22914;&#20309;&#33021;&#22815;&#26356;&#22810;&#22320;&#25581;&#31034;&#24050;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14982v1 Announce Type: new  Abstract: Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#26041;&#27861;&#65292;&#26412;&#25991;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#29983;&#25104;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14541</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#26041;&#27861;&#65292;&#26412;&#25991;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#29983;&#25104;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28201;&#24230;&#37319;&#26679;&#26159;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20351;&#29992;&#22266;&#23450;&#30340;&#28201;&#24230;&#21442;&#25968;&#65292;&#36825;&#21487;&#33021;&#24182;&#38750;&#22987;&#32456;&#26159;&#24179;&#34913;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#29109;&#30340;&#21160;&#24577;&#28201;&#24230;&#65288;EDT&#65289;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#28201;&#24230;&#21442;&#25968;&#23454;&#29616;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;4&#20010;&#19981;&#21516;&#29983;&#25104;&#22522;&#20934;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14541v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#65288;BPO&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#36127;&#38754;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#20419;&#36827;&#27169;&#22411;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.08730</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#21152;&#24378;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#65288;BPO&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#36127;&#38754;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#20419;&#36827;&#27169;&#22411;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#21709;&#24212;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#23384;&#22312;&#20559;&#21521;&#20110;&#29983;&#25104;&#19982;&#20854;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30456;&#20284;&#21709;&#24212;&#30340;&#20559;&#35265;&#65292;&#25513;&#30422;&#20102;&#35270;&#35273;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20559;&#35265;&#35270;&#20026;&#23545;&#39044;&#35757;&#32451;&#32479;&#35745;&#25968;&#25454;&#30340;&#8220;&#20559;&#22909;&#8221;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#65288;BPO&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21253;&#21547;&#36127;&#38754;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20174;&#27169;&#22411;&#26412;&#36523;&#20013;&#24341;&#23548;&#20986;&#26469;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#20004;&#31181;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;&#25197;&#26354;&#30340;&#22270;&#20687;&#36755;&#20837;&#21040;MLLM&#20013;&#65292;&#20197;&#24341;&#21457;&#21253;&#21547;&#26174;&#33879;&#39044;&#35757;&#32451;&#20559;&#35265;&#30340;&#21709;&#24212;&#65307;2&#65289;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;LLM&#23558;&#38169;&#35823;&#20294;&#24120;&#35265;&#30340;&#20803;&#32032;&#26126;&#30830;&#22320;&#27880;&#20837;&#21407;&#22987;&#21709;&#24212;&#20013;&#12290;&#36825;&#20123;&#19981;&#33391;&#21709;&#24212;&#19982;&#25968;&#25454;&#38598;&#20013;&#21407;&#22987;&#30340;&#27880;&#37322;&#21709;&#24212;&#37197;&#23545;&#65292;&#26500;&#24314;&#20102;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08730v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a "preference" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the prefere
&lt;/p&gt;</description></item><item><title>MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08167</link><description>&lt;p&gt;
MolBind: &#22810;&#27169;&#24577;&#23545;&#40784;&#35821;&#35328;&#12289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
MolBind: Multimodal Alignment of Language, Molecules, and Proteins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08167
&lt;/p&gt;
&lt;p&gt;
MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23558;&#20998;&#23376;&#21450;&#20854;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#20165;&#38480;&#20110;&#20004;&#31181;&#27169;&#24577;&#65292;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#26469;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#12289;2D&#20998;&#23376;&#22270;&#12289;3D&#20998;&#23376;&#26500;&#35937;&#21644;3D&#34507;&#30333;&#36136;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
&lt;/p&gt;</description></item><item><title>Breeze-7B&#26159;&#22522;&#20110;Mistral-7B&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#20013;&#25991;&#35821;&#22659;&#19979;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21151;&#33021;&#65292;&#23637;&#29616;&#20986;&#22312;&#22797;&#26434;&#24230;&#31867;&#21035;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02712</link><description>&lt;p&gt;
Breeze-7B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Breeze-7B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02712
&lt;/p&gt;
&lt;p&gt;
Breeze-7B&#26159;&#22522;&#20110;Mistral-7B&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#20013;&#25991;&#35821;&#22659;&#19979;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21151;&#33021;&#65292;&#23637;&#29616;&#20986;&#22312;&#22797;&#26434;&#24230;&#31867;&#21035;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02712v1 &#31867;&#22411;&#65306;&#26032;&#35770; &#25688;&#35201;&#65306;Breeze-7B&#26159;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#20013;&#25991;&#20256;&#32479;&#35821;&#22659;&#19979;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21151;&#33021;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#27010;&#36848;&#20102;Breeze-7B&#27169;&#22411;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#38454;&#27573;&#12290;Breeze-7B&#31995;&#21015;&#30340;&#22522;&#30784;&#21644;&#32842;&#22825;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#19968;&#20123;&#19982;&#20854;&#22797;&#26434;&#24230;&#31867;&#20284;&#30340;&#27169;&#22411;&#20013;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02712v1 Announce Type: new  Abstract: Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class.
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>MATHSENSEI &#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28155;&#21152;&#30693;&#35782;&#26816;&#32034;&#12289;&#31243;&#24207;&#25191;&#34892;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17231</link><description>&lt;p&gt;
MATHSENSEI&#65306;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17231
&lt;/p&gt;
&lt;p&gt;
MATHSENSEI &#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28155;&#21152;&#30693;&#35782;&#26816;&#32034;&#12289;&#31243;&#24207;&#25191;&#34892;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(TALM)&#24050;&#30693;&#33021;&#22815;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATHSENSEI&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#12290;&#36890;&#36807;&#28155;&#21152;&#29992;&#20110;&#30693;&#35782;&#26816;&#32034;&#65288;Bing Web Search&#65289;&#12289;&#31243;&#24207;&#25191;&#34892;&#65288;Python&#65289;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#65288;Wolfram-Alpha&#65289;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#30740;&#31350;&#36825;&#20123;&#24037;&#20855;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17231v1 Announce Type: new  Abstract: Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.17128</link><description>&lt;p&gt;
OSCaR:&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OSCaR: Object State Captioning and State Change Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38754;&#21521;&#20154;&#31867;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#35270;&#35282;&#65292;&#26234;&#33021;&#27169;&#22411;&#25512;&#26029;&#21644;&#29702;&#35299;&#23545;&#35937;&#29366;&#24577;&#30340;&#21464;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#25551;&#36848;&#22797;&#26434;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#35782;&#21035;&#27963;&#36291;&#23545;&#35937;&#65292;&#20197;&#21450;&#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#23558;&#23545;&#35937;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#26816;&#27979;&#36827;&#34892;&#38548;&#31163;&#65292;&#25552;&#20379;&#20102;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#26377;&#38480;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#20381;&#36182;&#20110;&#19968;&#23567;&#22871;&#31526;&#21495;&#21270;&#35789;&#27719;&#26469;&#34920;&#31034;&#21464;&#21270;&#38480;&#21046;&#20102;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;&#65288;OSCaR&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;OSCaR&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#20027;&#35266;&#35270;&#35282;&#35270;&#39057;&#38598;&#21512;&#30340;14,084&#20010;&#24102;&#27880;&#37322;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#36817;1,000&#20010;&#29420;&#29305;&#23545;&#35937;&#12290;&#23427;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#28860;&#36731;&#37327;&#32423;&#23545;&#35805;&#32534;&#30721;&#22120;&#20197;&#36991;&#20813;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.13043</link><description>&lt;p&gt;
&#29992;&#38544;&#24335;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13043
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#28860;&#36731;&#37327;&#32423;&#23545;&#35805;&#32534;&#30721;&#22120;&#20197;&#36991;&#20813;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13043v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25991;&#25688;: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23567;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#20381;&#36182;&#20110;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23545;&#35805;&#26816;&#32034;&#22120;&#26469;&#26597;&#25214;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20197;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#20351;&#29992;&#21407;&#22987;&#23545;&#35805;&#19978;&#19979;&#25991;&#20316;&#20026;&#25628;&#32034;&#38190;&#21644;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#23545;&#24102;&#27880;&#37322;&#30340;&#23545;&#35805;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#22826;&#36866;&#21512;&#25193;&#23637;&#21040;&#26032;&#30340;&#39046;&#22495;&#25110;&#26032;&#30340;&#27880;&#37322;&#35821;&#35328;&#65292;&#22240;&#20026;&#24494;&#35843;&#25968;&#25454;&#19981;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#35805;&#30340;&#25991;&#26412;&#25688;&#35201;&#26469;&#22788;&#29702;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#12290;&#20026;&#36991;&#20813;LLM&#22522;&#20110;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#24102;&#26469;&#30340;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#28860;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23545;&#35805;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#22312;&#19981;&#35299;&#30721;&#27979;&#35797;&#23545;&#35805;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26597;&#35810;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;PDD&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#31456;&#20043;&#38388;&#30340;&#35805;&#35821;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#26356;&#25509;&#36817;&#20154;&#31867;&#20559;&#22909;&#21644;GPT-4&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10175</link><description>&lt;p&gt;
&#35299;&#38145;&#32467;&#26500;&#27979;&#37327;&#65306;&#24341;&#20837;PDD&#65292;&#19968;&#31181;&#29992;&#20110;&#20301;&#32622;&#35805;&#35821;&#36830;&#36143;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;PDD&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#31456;&#20043;&#38388;&#30340;&#35805;&#35821;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#26356;&#25509;&#36817;&#20154;&#31867;&#20559;&#22909;&#21644;GPT-4&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#29983;&#25104;&#25991;&#26412;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#24403;&#28041;&#21450;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#26102;&#65292;&#20174;&#35805;&#35821;&#36830;&#36143;&#30340;&#35282;&#24230;&#20986;&#21457;&#23545;&#29983;&#25104;&#32467;&#26524;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35789;&#27719;&#25110;&#35821;&#20041;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#12289;ROUGE&#12289;BertScore&#19981;&#33021;&#26377;&#25928;&#25429;&#25417;&#35805;&#35821;&#36830;&#36143;&#24615;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#38024;&#23545;LLMs&#29983;&#25104;&#32467;&#26524;&#30340;&#35805;&#35821;&#29305;&#23450;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#20851;&#27880;&#21644;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;&#20004;&#31687;&#38271;&#31687;&#25991;&#31456;&#20043;&#38388;&#30340;&#35805;&#35821;&#24046;&#24322;&#12290;&#23545;&#26469;&#33258;&#20195;&#34920;&#24615;&#39046;&#22495;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25351;&#26631;&#26356;&#25509;&#36817;&#20154;&#31867;&#20559;&#22909;&#21644;GPT-4&#30340;&#36830;&#36143;&#24615;&#35780;&#20272;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10175v1 Announce Type: new  Abstract: Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#20139;&#20102;&#19968;&#20221;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20851;&#20110;&#34892;&#20026;&#21464;&#38761;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20197;&#20415;&#35774;&#35745;&#22522;&#20110;&#30495;&#23454;&#20114;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;</title><link>https://arxiv.org/abs/2401.16167</link><description>&lt;p&gt;
"&#20320;&#21578;&#35785;&#25105;": &#19968;&#20221;&#22522;&#20110;GPT-4&#30340;&#34892;&#20026;&#21464;&#38761;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
"You tell me": A Dataset of GPT-4-Based Behaviour Change Support Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#20139;&#20102;&#19968;&#20221;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20851;&#20110;&#34892;&#20026;&#21464;&#38761;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20197;&#20415;&#35774;&#35745;&#22522;&#20110;&#30495;&#23454;&#20114;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#28385;&#36275;&#24773;&#24863;&#38656;&#27714;&#65292;&#38500;&#20102;&#20449;&#24687;&#38656;&#27714;&#12290;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#29992;&#20363;&#26159;&#36741;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#21644;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26041;&#27861;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31995;&#32479;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#34892;&#20026;&#20197;&#21450;&#36825;&#31181;&#34892;&#20026;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#34892;&#20026;&#21464;&#38761;&#30456;&#20851;&#30340;&#20004;&#20010;&#22522;&#20110;GPT-4&#23545;&#35805;&#20195;&#29702;&#22312;&#20107;&#20808;&#27880;&#20876;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#25910;&#38598;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#23545;&#35805;&#25968;&#25454;&#12289;&#29992;&#25143;&#35821;&#35328;&#20998;&#26512;&#12289;&#24863;&#30693;&#24230;&#37327;&#20197;&#21450;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#22797;&#30340;&#29992;&#25143;&#21453;&#39304;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20197;&#20415;&#26681;&#25454;&#30495;&#23454;&#20114;&#21160;&#26469;&#35774;&#35745;&#27492;&#31867;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16167v2 Announce Type: replace-cross  Abstract: Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#26631;&#35760;&#32423;&#21035;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26631;&#35760;&#26469;&#25552;&#39640;LLMs&#22312;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.14346</link><description>&lt;p&gt;
&#19981;&#35201;&#36731;&#20449;&#19968;&#20999;&#65306;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#22686;&#24378;&#25688;&#35201;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#26631;&#35760;&#32423;&#21035;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26631;&#35760;&#26469;&#25552;&#39640;LLMs&#22312;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#25991;&#26412;&#25805;&#32437;&#8212;&#8212;&#20363;&#22914;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#25552;&#20379;&#30340;&#20219;&#20309;&#31572;&#26696;&#30340;&#24544;&#23454;&#24230;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#33268;&#21147;&#20110;&#23545;&#25239;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#20123;&#20316;&#21697;&#28041;&#21450;&#35782;&#21035;&#34394;&#26500;&#30340;&#21477;&#23376;&#20197;&#21450;&#23545;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#21516;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#22312;&#24187;&#35273;&#26041;&#38754;&#30340;&#34892;&#20026;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#35813;&#26631;&#35760;&#26469;&#25552;&#39640;LLMs&#22312;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#36825;&#19968;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14346v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are adept at text manipulation -- tasks such as machine translation and text summarization. However, these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides. Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different ways in which models hallucinate. This paper takes a deep dive into LLM behavior with respect to hallucinations, defines a token-level approach to identifying different kinds of hallucinations, and further utilizes this token-level tagging to improve the interpretability and faithfulness of LLMs in dialogue summarization tasks. Through this, the paper presents a new, enhanced dataset and a new training paradigm.
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>SCTc-TE&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;MidEast-TE&#25968;&#25454;&#38598;&#21644;&#21306;&#20998;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21319;&#20102;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.01052</link><description>&lt;p&gt;
SCTc-TE&#65306;&#19968;&#31181;&#20840;&#38754;&#30340;&#24418;&#24335;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SCTc-TE: A Comprehensive Formulation and Benchmark for Temporal Event Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01052
&lt;/p&gt;
&lt;p&gt;
SCTc-TE&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;MidEast-TE&#25968;&#25454;&#38598;&#21644;&#21306;&#20998;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21319;&#20102;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22797;&#26434;&#20107;&#20214;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#19978;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#22823;&#22810;&#25968;&#26102;&#38388;&#22797;&#26434;&#20107;&#20214;&#30340;&#24418;&#24335;&#21270;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#25110;&#32570;&#20047;&#24191;&#27867;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#23548;&#33268;&#34920;&#24449;&#19981;&#36275;&#21644;&#39044;&#27979;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#24341;&#20837;&#20102;&#32467;&#26500;&#21270;&#12289;&#22797;&#26434;&#21644;&#26102;&#38388;&#23436;&#25972;&#30340;&#26102;&#38388;&#20107;&#20214;&#65288;SCTc-TE&#65289;&#30340;&#24418;&#24335;&#21270;&#12290;&#22312;&#36825;&#20010;&#20840;&#38754;&#30340;&#24418;&#24335;&#21270;&#20043;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#24182;&#20174;&#32422;60&#19975;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MidEast-TE&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;2015&#24180;&#33267;2022&#24180;&#38388;&#20027;&#35201;&#28041;&#21450;&#20013;&#19996;&#22320;&#21306;&#21508;&#22269;&#20043;&#38388;&#30340;&#21512;&#20316;&#21644;&#20914;&#31361;&#20107;&#20214;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#26500;&#24314;&#20043;&#22806;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#21306;&#20998;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#65292;&#26412;&#22320;&#21644;&#20840;&#29699;&#19978;&#19979;&#25991;&#65289;&#26469;&#25512;&#36827;&#39044;&#27979;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01052v2 Announce Type: replace-cross  Abstract: Temporal complex event forecasting aims to predict the future events given the observed events from history. Most formulations of temporal complex event are unstructured or without extensive temporal information, resulting in inferior representations and limited forecasting capabilities. To bridge these gaps, we innovatively introduce the formulation of Structured, Complex, and Time-complete temporal event (SCTc-TE). Following this comprehensive formulation, we develop a fully automated pipeline and construct a large-scale dataset named MidEast-TE from about 0.6 million news articles. This dataset focuses on the cooperation and conflict events among countries mainly in the MidEast region from 2015 to 2022. Not limited to the dataset construction, more importantly, we advance the forecasting methods by discriminating the crucial roles of various contextual information, i.e., local and global contexts. Thereby, we propose a novel
&lt;/p&gt;</description></item><item><title>Omni-SMoLA&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;MoE&#26041;&#27861;&#28151;&#21512;&#22810;&#20010;&#22810;&#27169;&#24577;&#20302;&#31209;&#19987;&#23478;&#30340;&#26550;&#26500;&#65292;&#36991;&#20813;&#24341;&#20837;&#22823;&#37327;&#26032;&#21442;&#25968;&#65292;&#20197;&#25552;&#21319;&#36890;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.00968</link><description>&lt;p&gt;
Omni-SMoLA: &#29992;&#36719;&#20302;&#31209;&#19987;&#23478;&#30340;&#28151;&#21512;&#25552;&#21319;&#36890;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00968
&lt;/p&gt;
&lt;p&gt;
Omni-SMoLA&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;MoE&#26041;&#27861;&#28151;&#21512;&#22810;&#20010;&#22810;&#27169;&#24577;&#20302;&#31209;&#19987;&#23478;&#30340;&#26550;&#26500;&#65292;&#36991;&#20813;&#24341;&#20837;&#22823;&#37327;&#26032;&#21442;&#25968;&#65292;&#20197;&#25552;&#21319;&#36890;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;LMMs&#22312;&#35843;&#20248;&#22823;&#37327;&#20219;&#21153;&#26102;&#24448;&#24448;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26550;&#26500;&#23545;&#25351;&#23548;&#35843;&#20248;&#24456;&#26377;&#29992;&#65292;&#20294;&#23545;&#20110;&#21442;&#25968;&#22823;&#23567;&#32422;&#20026;O(50-100B)&#30340;LMMs&#65292;&#22797;&#21046;&#21644;&#23384;&#20648;&#19987;&#23478;&#27169;&#22411;&#30340;&#25104;&#26412;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30340;&#19987;&#23478;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Omni-SMoLA&#65292;&#36825;&#31181;&#26550;&#26500;&#20351;&#29992;&#36719;MoE&#26041;&#27861;&#65288;&#36719;&#22320;&#65289;&#28151;&#21512;&#35768;&#22810;&#22810;&#27169;&#24577;&#20302;&#31209;&#19987;&#23478;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#19982;&#20256;&#32479;MoE&#27169;&#22411;&#30456;&#27604;&#26174;&#33879;&#25968;&#37327;&#30340;&#26032;&#21442;&#25968;&#12290;&#36825;&#37324;&#30340;&#26680;&#24515;&#30452;&#35273;&#26159;&#65292;&#22823;&#22411;&#27169;&#22411;&#25552;&#20379;&#20102;&#22522;&#30784;&#25903;&#25745;&#65292;&#32780;&#19981;&#21516;&#30340;&#36731;&#37327;&#32423;&#19987;&#23478;&#22312;&#19987;&#38376;&#30693;&#35782;&#19978;&#27531;&#20313;&#23398;&#20064;&#65292;&#21487;&#20197;&#26159;&#21333;&#27169;&#24577;&#30340;&#25110;&#22810;&#27169;&#24577;&#30340;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMoLA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00968v2 Announce Type: replace-cross  Abstract: Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>NLP&#27169;&#22411;&#30340;&#22686;&#22823;&#21644;&#24615;&#33021;&#25552;&#21319;&#24182;&#19981;&#33021;&#35299;&#20915;&#20854;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20173;&#23384;&#22312;&#37325;&#22823;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2311.09694</link><description>&lt;p&gt;
NLP&#40065;&#26834;&#24615;&#32988;&#21033;&#20013;&#30340;&#30097;&#34385;&#32819;&#35821;
&lt;/p&gt;
&lt;p&gt;
Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09694
&lt;/p&gt;
&lt;p&gt;
NLP&#27169;&#22411;&#30340;&#22686;&#22823;&#21644;&#24615;&#33021;&#25552;&#21319;&#24182;&#19981;&#33021;&#35299;&#20915;&#20854;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20173;&#23384;&#22312;&#37325;&#22823;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528; NLP &#26426;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#21644;&#24615;&#33021;&#25552;&#21319;&#65292;NLP&#38271;&#26399;&#23384;&#22312;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#26159;&#21542;&#24471;&#21040;&#35299;&#20915;&#65311;&#25105;&#20204;&#20351;&#29992;20&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#28085;&#30422;&#19981;&#21516;&#30340;&#26550;&#26500;&#36873;&#25321;&#21644;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#26469;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#65288;a&#65289;&#39046;&#22495;&#22806;&#21644;&#25361;&#25112;&#24615;&#27979;&#35797;&#38598;&#65292;&#65288;b&#65289;CheckLists&#34892;&#20026;&#27979;&#35797;&#65292;&#65288;c&#65289;&#23545;&#27604;&#38598;&#65292;&#21644;&#65288;d&#65289;&#23545;&#25239;&#24615;&#36755;&#20837;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#24182;&#38750;&#25152;&#26377;&#39046;&#22495;&#22806;&#27979;&#35797;&#37117;&#33021;&#25552;&#20379;&#40065;&#26834;&#24615;&#30340;&#35265;&#35299;&#12290;&#20351;&#29992;CheckLists&#21644;&#23545;&#27604;&#38598;&#36827;&#34892;&#35780;&#20272;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65307;&#20165;&#20165;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#24182;&#19981;&#33021;&#20351;&#20854;&#36275;&#22815;&#40065;&#26834;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#30446;&#21069;&#29992;&#20110;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#26412;&#36523;&#23384;&#22312;&#38382;&#39064;&#65306;&#23427;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#30772;&#22351;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#24418;&#24335;&#19979;&#19981;&#36275;&#20197;&#28145;&#20837;&#25506;&#31350;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;NLP&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#19981;&#20165;&#23578;&#26410;&#35299;&#20915;&#65292;&#29978;&#33267;&#19968;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09694v2 Announce Type: replace  Abstract: Do larger and more performant models resolve NLP's longstanding robustness issues? We investigate this question using over 20 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) out-of-domain and challenge test sets, (b) behavioral testing with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all out-of-domain tests provide insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them adequately robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#26041;&#27861;&#65292;&#22686;&#37327;&#25928;&#29992;&#65292;&#26469;&#20272;&#35745;LLMs&#36890;&#36807;&#28436;&#31034;&#24102;&#20837;&#30340;&#22686;&#37327;&#30693;&#35782;&#37327;&#65292;&#35299;&#20915;&#20102;&#20851;&#20110;&#19981;&#21516;&#26631;&#35760;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;&#30446;&#26631;&#20219;&#21153;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09619</link><description>&lt;p&gt;
&#36880;&#27493;&#20102;&#35299;&#28436;&#31034;&#30340;&#22686;&#37327;&#25928;&#29992;&#65306;&#23545;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#37325;&#26032;&#25490;&#21517;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#26041;&#27861;&#65292;&#22686;&#37327;&#25928;&#29992;&#65292;&#26469;&#20272;&#35745;LLMs&#36890;&#36807;&#28436;&#31034;&#24102;&#20837;&#30340;&#22686;&#37327;&#30693;&#35782;&#37327;&#65292;&#35299;&#20915;&#20102;&#20851;&#20110;&#19981;&#21516;&#26631;&#35760;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;&#30446;&#26631;&#20219;&#21153;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Context Learning&#65288;ICL&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#12290; &#20165;&#20165;&#20960;&#20010;&#28436;&#31034;&#23601;&#33021;&#35753;LLMs&#34987;&#29992;&#20316;&#26032;&#20219;&#21153;&#30340;&#40657;&#30418;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#20316;&#20026;&#26631;&#31614;&#22312;&#35757;&#32451;&#27169;&#22411;&#36873;&#25321;&#28436;&#31034;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; &#36825;&#26679;&#30340;&#26631;&#31614;&#24212;&#35813;&#33021;&#22815;&#20272;&#35745;ICL&#20013;&#28436;&#31034;&#30340;&#25928;&#29992;&#65307; &#28982;&#32780;&#65292;&#24182;&#19981;&#28165;&#26970;&#19981;&#21516;&#30340;&#26631;&#35760;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;&#30446;&#26631;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290; &#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#32473;&#20986;&#30340;&#36755;&#20986;&#27010;&#29575;&#21644;&#22522;&#20110;&#20219;&#21153;&#30340;&#22870;&#21169;&#32473;&#20986;LLMs&#30340;&#39044;&#27979;&#65292;&#23545;&#19981;&#21516;&#25928;&#29992;&#20989;&#25968;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290; &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#26041;&#27861;&#65292;&#22686;&#37327;&#25928;&#29992;&#65292;&#29992;&#20110;&#20272;&#35745;LLMs&#36890;&#36807;&#28436;&#31034;&#24102;&#20837;&#30340;&#22686;&#37327;&#30693;&#35782;&#37327;&#12290; &#25105;&#20204;&#22312;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#38463;&#25289;&#20271;&#25991;&#30340;&#20108;&#20998;&#31867;/&#22810;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09619v2 Announce Type: replace  Abstract: In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new tasks. Previous studies have shown that using LLMs' outputs as labels is effective in training models to select demonstrations. Such a label is expected to estimate utility of a demonstration in ICL; however, it has not been well understood how different labeling strategies affect results on target tasks. This paper presents an analysis on different utility functions by focusing on LLMs' output probability given ground-truth output, and task-specific reward given LLMs' prediction. Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration. We conduct experiments with instruction-tuned LLMs on binary/multi-class classification, segmentation, and translation across Arab
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#23381;&#22919;&#22312;&#38382;&#24576;&#23381;&#21644;&#23156;&#20799;&#25252;&#29702;&#38382;&#39064;&#26102;&#25152;&#20570;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#23558;&#36825;&#20123;&#23454;&#29992;&#25512;&#26029;&#21578;&#30693;&#29616;&#26377;&#30340;&#38382;&#31572;&#31649;&#36947;&#21487;&#20197;&#20135;&#29983;&#26356;&#23436;&#25972;&#30340;&#22238;&#31572;&#65292;&#20943;&#23569;&#26377;&#23475;&#20449;&#24565;&#30340;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2311.09542</link><description>&lt;p&gt;
&#24576;&#23381;&#38382;&#39064;&#65306;&#23381;&#22919;&#20581;&#24247;&#38382;&#31572;&#20013;&#23454;&#29992;&#24847;&#35782;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09542
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#23381;&#22919;&#22312;&#38382;&#24576;&#23381;&#21644;&#23156;&#20799;&#25252;&#29702;&#38382;&#39064;&#26102;&#25152;&#20570;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#23558;&#36825;&#20123;&#23454;&#29992;&#25512;&#26029;&#21578;&#30693;&#29616;&#26377;&#30340;&#38382;&#31572;&#31649;&#36947;&#21487;&#20197;&#20135;&#29983;&#26356;&#23436;&#25972;&#30340;&#22238;&#31572;&#65292;&#20943;&#23569;&#26377;&#23475;&#20449;&#24565;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#27714;&#21161;&#29992;&#25143;&#25552;&#20986;&#30340;&#38382;&#39064;&#36890;&#24120;&#21253;&#21547;&#38544;&#21547;&#30340;&#38169;&#35823;&#25110;&#28508;&#22312;&#26377;&#23475;&#30340;&#20551;&#35774;&#12290;&#22312;&#35832;&#22914;&#23381;&#20135;&#22919;&#20581;&#24247;&#36825;&#26679;&#30340;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#38382;&#31572;&#31995;&#32479;&#24517;&#39035;&#35782;&#21035;&#36825;&#20123;&#23454;&#29992;&#32422;&#26463;&#65292;&#24182;&#19981;&#20165;&#20165;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#65292;&#36824;&#35201;&#22312;&#19978;&#19979;&#25991;&#20013;&#23457;&#35270;&#36825;&#20123;&#38382;&#39064;&#20197;&#32473;&#20986;&#26377;&#24110;&#21161;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;500&#20010;&#38382;&#39064;&#20013;&#30340;2,727&#20010;&#25512;&#26029;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#27597;&#20146;&#22312;&#25552;&#20986;&#24576;&#23381;&#21644;&#23156;&#20799;&#25252;&#29702;&#38382;&#39064;&#26102;&#25152;&#20570;&#30340;&#20551;&#35774;&#21644;&#25512;&#26029;&#65292;&#36827;&#32780;&#30740;&#31350;&#20102;&#20581;&#24247;&#19987;&#23478;&#22312;&#25776;&#20889;&#31572;&#26696;&#26102;&#22914;&#20309;&#33258;&#28982;&#22320;&#22788;&#29702;&#36825;&#20123;&#25512;&#26029;&#65292;&#24182;&#35828;&#26126;&#21033;&#29992;&#23454;&#29992;&#25512;&#26029;&#21578;&#30693;&#29616;&#26377;&#30340;&#38382;&#31572;&#31649;&#36947;&#21487;&#20197;&#20135;&#29983;&#26356;&#23436;&#25972;&#30340;&#22238;&#31572;&#65292;&#20174;&#32780;&#20943;&#23569;&#26377;&#23475;&#20449;&#24565;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09542v2 Announce Type: replace  Abstract: Questions posed by information-seeking users often contain implicit false or potentially harmful assumptions. In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study assumptions and implications, or pragmatic inferences, made when mothers ask questions about pregnancy and infant care by collecting a dataset of 2,727 inferences from 500 questions across three diverse sources. We study how health experts naturally address these inferences when writing answers, and illustrate that informing existing QA pipelines with pragmatic inferences produces responses that are more complete, mitigating the propagation of harmful beliefs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; AGREE&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#32852;&#31995;&#21040;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#25552;&#20379;&#24341;&#25991;&#26469;&#25552;&#21319;&#20449;&#24687;&#20851;&#32852;&#65292;&#20174;&#32780;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#8220;&#33222;&#24819;&#8221;&#31572;&#26696;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.09533</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20197;&#25552;&#21319;&#20449;&#24687;&#20851;&#32852;&#21644;&#24341;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Effective Large Language Model Adaptation for Improved Grounding and Citation Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; AGREE&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#32852;&#31995;&#21040;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#25552;&#20379;&#24341;&#25991;&#26469;&#25552;&#21319;&#20449;&#24687;&#20851;&#32852;&#65292;&#20174;&#32780;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#8220;&#33222;&#24819;&#8221;&#31572;&#26696;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19968;&#22823;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#8220;&#33222;&#24819;&#8221;&#30340;&#31572;&#26696;&#24182;&#38750;&#20107;&#23454;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#30528;&#30524;&#20110;&#36890;&#36807;&#23558;LLMs&#30340;&#21709;&#24212;&#32852;&#31995;&#21040;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#25552;&#20379;&#24341;&#25991;&#26469;&#25913;&#36827;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AGREE&#65292;&#21363;Adaptation for GRounding EnhancEment&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#25552;&#21319;&#20102;&#20449;&#24687;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35843;&#25972;LLMs&#65292;&#20351;&#20854;&#33258;&#25105;&#32852;&#31995;&#20854;&#21709;&#24212;&#20013;&#30340;&#20027;&#24352;&#24182;&#20026;&#26816;&#32034;&#25991;&#26723;&#25552;&#20379;&#20934;&#30830;&#30340;&#24341;&#25991;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;LLMs&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#36825;&#31181;&#35843;&#25972;&#38656;&#35201;&#23545;&#37197;&#23545;&#26597;&#35810;&#30340;&#21709;&#24212;&#36827;&#34892;&#24456;&#22909;&#30340;&#20449;&#24687;&#20851;&#32852;&#65288;&#24102;&#26377;&#24341;&#25991;&#65289;&#65292;&#20026;&#27492;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#26597;&#35810;&#33258;&#21160;&#26500;&#36896;&#36825;&#20123;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#35843;&#25972;&#21518;&#30340;LLMs&#30340;&#33258;&#25105;&#20851;&#32852;&#33021;&#21147;&#36827;&#19968;&#27493;&#20351;&#20854;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09533v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate "hallucinated" answers that are not factual. Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to selfground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The selfgrounding capability of tuned LLMs further grants them a test-time adapt
&lt;/p&gt;</description></item><item><title>AMRFact&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;AMR&#29983;&#25104;&#36127;&#26679;&#26412;&#65292;&#22686;&#24378;&#20102;&#25688;&#35201;&#20107;&#23454;&#24615;&#35780;&#20272;&#65292;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#20855;&#26377;&#39640;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.09521</link><description>&lt;p&gt;
AMRFact&#65306;&#21033;&#29992;AMR&#29983;&#25104;&#36127;&#26679;&#26412;&#22686;&#24378;&#25688;&#35201;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09521
&lt;/p&gt;
&lt;p&gt;
AMRFact&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;AMR&#29983;&#25104;&#36127;&#26679;&#26412;&#65292;&#22686;&#24378;&#20102;&#25688;&#35201;&#20107;&#23454;&#24615;&#35780;&#20272;&#65292;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#20855;&#26377;&#39640;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20107;&#23454;&#19968;&#33268;&#24615;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#21462;&#24335;&#25688;&#35201;&#20013;&#65292;&#20445;&#25345;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#20851;&#20110;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#22522;&#20110;&#34164;&#28085;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#25200;&#21160;&#65288;&#20107;&#23454;&#19981;&#19968;&#33268;&#65289;&#25688;&#35201;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#29983;&#25104;&#25200;&#21160;&#25688;&#35201;&#30340;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#36830;&#36143;&#24615;&#65292;&#35201;&#20040;&#32570;&#20047;&#38169;&#35823;&#31867;&#22411;&#35206;&#30422;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMRFact&#65292;&#19968;&#20010;&#21033;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#29983;&#25104;&#25200;&#21160;&#25688;&#35201;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20107;&#23454;&#19968;&#33268;&#30340;&#25688;&#35201;&#35299;&#26512;&#20026;AMR&#22270;&#65292;&#24182;&#27880;&#20837;&#21487;&#25511;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#20197;&#21019;&#24314;&#36127;&#38754;&#31034;&#20363;&#65292;&#20801;&#35768;&#29983;&#25104;&#20855;&#26377;&#39640;&#38169;&#35823;&#29575;&#30340;&#36830;&#36143;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09521v2 Announce Type: replace  Abstract: Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TWEAK&#30340;&#20165;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20551;&#35774;&#39564;&#35777;&#27169;&#22411;&#26469;&#25552;&#39640;&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2311.09467</link><description>&lt;p&gt;
&#20889;&#20316;&#26102;&#24605;&#32771;&#65306;&#20551;&#35774;&#39564;&#35777;&#20419;&#36827;&#24544;&#23454;&#30340;&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09467
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TWEAK&#30340;&#20165;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20551;&#35774;&#39564;&#35777;&#27169;&#22411;&#26469;&#25552;&#39640;&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;&#22120;&#32463;&#24120;&#38590;&#20197;&#24544;&#23454;&#22320;&#20026;&#36755;&#20837;&#20107;&#23454;&#29983;&#25104;&#25551;&#36848;&#65306;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19982;&#36755;&#20837;&#30456;&#30683;&#30462;&#30340;&#24187;&#35273;&#65292;&#25110;&#25551;&#36848;&#36755;&#20837;&#20013;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#20943;&#23569;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#26041;&#27861;TWEAK&#65288;&#24605;&#32771;&#32780;&#26377;&#25928;&#34920;&#36798;&#30693;&#35782;&#65289;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29983;&#25104;&#22120;&#38598;&#25104;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;TWEAK&#23558;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#30340;&#29983;&#25104;&#24207;&#21015;&#21450;&#20854;&#26410;&#26469;&#24207;&#21015;&#35270;&#20026;&#20551;&#35774;&#65292;&#24182;&#26681;&#25454;&#20854;&#20551;&#35774;&#21463;&#21040;&#36755;&#20837;&#20107;&#23454;&#25903;&#25345;&#30340;&#31243;&#24230;&#65292;&#20351;&#29992;&#20551;&#35774;&#39564;&#35777;&#27169;&#22411;&#65288;HVM&#65289;&#23545;&#27599;&#20010;&#29983;&#25104;&#20505;&#36873;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#20316;&#20026;HVM&#23637;&#31034;&#20102;TWEAK&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25253;&#21578;&#20102;&#23545;&#36136;&#37327;&#24433;&#21709;&#24456;&#23567;&#30340;&#25913;&#21892;&#24544;&#23454;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;NLI&#27169;&#22411;&#26367;&#25442;&#20026;&#20351;&#29992;FATE&#65288;&#20107;&#23454;&#23545;&#40784;&#25991;&#26412;&#65289;&#39318;&#21019;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#29305;&#23450;&#20219;&#21153;HVM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09467v2 Announce Type: replace-cross  Abstract: Knowledge-to-text generators often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the input, or describe facts not present in the input. To reduce hallucinations, we propose a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge), which can be integrated with any generator without retraining. TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on the extent to which their hypotheses are supported by the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with a minimal impact on the quality. We then replace the NLI model with a task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Text
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#26159;&#21542;&#20855;&#26377;&#20154;&#31867;&#23545;&#35805;&#22522;&#30784;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#37327;&#21270;&#23581;&#35797;&#22522;&#30784;&#35774;&#23450;&#30340;&#19968;&#31995;&#21015;&#25351;&#26631;&#27604;&#36739;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09144</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#23545;&#35805;&#22522;&#30784;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Grounding Gaps in Language Model Generations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#26159;&#21542;&#20855;&#26377;&#20154;&#31867;&#23545;&#35805;&#22522;&#30784;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#37327;&#21270;&#23581;&#35797;&#22522;&#30784;&#35774;&#23450;&#30340;&#19968;&#31995;&#21015;&#25351;&#26631;&#27604;&#36739;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23545;&#35805;&#38656;&#35201;&#20849;&#21516;&#30340;&#22522;&#30784;&#65306;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#20849;&#21516;&#22522;&#30784;&#24182;&#19981;&#20250;&#22312;&#23545;&#35805;&#20013;&#33258;&#21457;&#20135;&#29983;&#12290;&#35828;&#35805;&#32773;&#21644;&#21548;&#20247;&#19968;&#36215;&#21162;&#21147;&#35782;&#21035;&#21644;&#24314;&#26500;&#20849;&#21516;&#22522;&#30784;&#65292;&#21516;&#26102;&#36991;&#20813;&#35823;&#35299;&#12290;&#20026;&#20102;&#23436;&#25104;&#22522;&#30784;&#35774;&#23450;&#65292;&#20154;&#31867;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#23545;&#35805;&#34892;&#20026;&#65292;&#22914;&#28548;&#28165;&#65288;&#20320;&#26159;&#20160;&#20040;&#24847;&#24605;&#65311;&#65289;&#21644;&#30830;&#35748;&#65288;&#25105;&#26126;&#30333;&#20102;&#65289;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#29983;&#25104;&#21453;&#26144;&#20154;&#31867;&#22522;&#30784;&#35774;&#23450;&#30340;&#25991;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31579;&#36873;&#20102;&#19968;&#32452;&#22522;&#30784;&#35774;&#23450;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#37327;&#21270;&#23581;&#35797;&#22522;&#30784;&#35774;&#23450;&#12290;&#25105;&#20204;&#30740;&#31350;LLM&#29983;&#25104;&#26159;&#21542;&#21253;&#21547;&#22522;&#30784;&#35774;&#23450;&#34892;&#20026;&#65292;&#27169;&#25311;&#20102;&#20960;&#20010;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#30340;&#36718;&#27425;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20154;&#31867;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;LLMs&#29983;&#25104;&#30340;&#35821;&#35328;&#20013;&#21253;&#21547;&#30340;&#23545;&#35805;&#22522;&#30784;&#36739;&#23569;&#65292;&#32780;&#26356;&#22810;&#22320;&#26159;&#19968;&#21619;&#22320;&#35748;&#20026;&#23384;&#22312;&#20849;&#21516;&#22522;&#30784;&#32780;&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09144v2 Announce Type: replace  Abstract: Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that -- compared to humans -- LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;RoFT&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#36793;&#30028;&#26816;&#27979;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#21644;&#36328;&#27169;&#22411;&#35774;&#32622;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2311.08349</link><description>&lt;p&gt;
&#20351;&#29992;RoFT&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-generated text boundary detection with RoFT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08349
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;RoFT&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#36793;&#30028;&#26816;&#27979;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#21644;&#36328;&#27169;&#22411;&#35774;&#32622;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#32463;&#24120;&#36935;&#21040;&#21487;&#33021;&#19968;&#24320;&#22987;&#26159;&#30001;&#20154;&#31867;&#32534;&#20889;&#20294;&#20043;&#21518;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#26816;&#27979;&#36825;&#20123;&#25991;&#26412;&#20013;&#20154;&#31867;&#32534;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#36793;&#30028;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#21463;&#21040;&#36275;&#22815;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#30740;&#31350;&#20960;&#31181;&#26041;&#27861;&#26469;&#23558;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#25991;&#26412;&#26816;&#27979;&#20998;&#31867;&#22120;&#35843;&#25972;&#20026;&#36793;&#30028;&#26816;&#27979;&#35774;&#32622;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#26816;&#27979;&#22120;&#25512;&#21521;&#26497;&#38480;&#65292;&#22312;&#21253;&#21547;&#22810;&#20010;&#20027;&#39064;&#30340;&#30701;&#25991;&#26412;&#30340;Real or Fake&#25991;&#26412;&#22522;&#20934;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#22810;&#26679;&#24615;&#28145;&#20837;&#30740;&#31350;&#25152;&#26377;&#26816;&#27979;&#22120;&#22312;&#36328;&#39046;&#22495;&#21644;&#36328;&#27169;&#22411;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#32447;&#21644;&#35265;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#36793;&#30028;&#26816;&#27979;&#26041;&#27861;&#20542;&#21521;&#20110;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08349v2 Announce Type: replace  Abstract: Due to the rapid development of large language models, people increasingly often encounter texts that may start as written by a human but continue as machine-generated. Detecting the boundary between human-written and machine-generated parts of such texts is a challenging problem that has not received much attention in literature. We attempt to bridge this gap and examine several ways to adapt state of the art artificial text detection classifiers to the boundary detection setting. We push all detectors to their limits, using the Real or Fake text benchmark that contains short texts on several topics and includes generations of various language models. We use this diversity to deeply examine the robustness of all detectors in cross-domain and cross-model settings to provide baselines and insights for future research. In particular, we find that perplexity-based approaches to boundary detection tend to be more robust to peculiarities 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;-LM&#35299;&#30721;&#30446;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;Anti-Language Model&#30446;&#26631;&#21644;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#34928;&#20943;&#22240;&#23376;&#65292;&#35299;&#20915;&#20102;&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24369;&#28857;&#65292;&#19982;&#20854;&#20182;&#35299;&#30721;&#30446;&#26631;&#30456;&#27604;&#65292;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20010;BLEU&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.08324</link><description>&lt;p&gt;
&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#21453;-LM&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Anti-LM Decoding for Zero-shot In-context Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08324
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;-LM&#35299;&#30721;&#30446;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;Anti-Language Model&#30446;&#26631;&#21644;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#34928;&#20943;&#22240;&#23376;&#65292;&#35299;&#20915;&#20102;&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24369;&#28857;&#65292;&#19982;&#20854;&#20182;&#35299;&#30721;&#30446;&#26631;&#30456;&#27604;&#65292;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20010;BLEU&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25351;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#31616;&#21333;&#30340;&#25351;&#20196;&#25191;&#34892;&#20219;&#21153;&#30340;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26657;&#20934;&#19981;&#20339;&#12290;&#22788;&#29702;&#36825;&#31181;&#20559;&#35265;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#32771;&#34385;&#36890;&#36807;&#22312;&#26576;&#20123;&#19978;&#19979;&#25991;&#19978;&#19979;&#25991;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20808;&#39564;&#27010;&#29575;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;Anti-Language Model&#30446;&#26631;&#65292;&#24102;&#26377;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24369;&#28857;&#30340;&#34928;&#20943;&#22240;&#23376;&#12290;&#25105;&#20204;&#22312;3&#31181;&#27169;&#22411;&#31867;&#22411;&#21644;&#22823;&#23567;&#65292;3&#31181;&#35821;&#35328;&#26041;&#21521;&#19978;&#20197;&#21450;&#36138;&#23146;&#35299;&#30721;&#21644;&#27874;&#26463;&#25628;&#32034;&#65288;$B=5$&#65289;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#30721;&#30446;&#26631;&#65292;&#35266;&#23519;&#21040;&#27604;&#40664;&#35748;&#30446;&#26631;&#39640;&#36798;20&#20010;BLEU&#28857;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08324v2 Announce Type: replace-cross  Abstract: Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions. However, pre-trained large language models are known to be poorly calibrated for this task. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$). The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings.
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.07484</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Psychometric Predictive Power of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07484
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;:&#25351;&#20196;&#35843;&#25972;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#23613;&#31649;&#22312;&#20154;-LLM&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#21162;&#21147;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#24182;&#19981;&#24635;&#26159;&#20351;LLMs&#20174;&#35748;&#30693;&#24314;&#27169;&#30340;&#35282;&#24230;&#30475;&#36215;&#26469;&#26356;&#20687;&#20154;&#31867;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#30001;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#20272;&#35745;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;&#24448;&#24448;&#27604;&#22522;&#30784;LLM&#20272;&#35745;&#30340;&#27010;&#29575;&#26356;&#31967;&#31957;&#65292;&#26080;&#27861;&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#29305;&#23450;&#35821;&#35328;&#20551;&#35774;&#30340;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;PPP&#65292;&#20294;&#20173;&#19981;&#21450;&#23567;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;PPP&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20986;LLMs&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#27604;&#22522;&#30784;LLMs&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32431;&#31929;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 Announce Type: replace-cross  Abstract: Instruction tuning aligns the response of large language models (LLMs) with human preferences. Despite such efforts in human--LLM alignment, we report that, interestingly, instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs. In addition, we explore prompting methodologies in simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve PPP but are still inferior to PPP from small base models. These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, our experiments highlight that pure next-word pro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#36739;&#22823;&#22411;&#30340;&#27169;&#22411;&#22914;GPT-4&#12289;Gemini-Pro&#21644;PaLM2&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2311.07463</link><description>&lt;p&gt;
MEGAVERSE: &#22312;&#35821;&#35328;&#12289;&#27169;&#24577;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#20043;&#38388;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07463
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#36739;&#22823;&#22411;&#30340;&#27169;&#22411;&#22914;GPT-4&#12289;Gemini-Pro&#21644;PaLM2&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#35780;&#20272;&#30740;&#31350;&#28608;&#22686;&#65292;&#26088;&#22312;&#20102;&#35299;LLM&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#23616;&#38480;&#20110;&#33521;&#35821;&#65292;&#20351;&#24471;LLM&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#25645;&#24314;&#21644;&#35780;&#20272;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;SoTA LLM&#65288;GPT-3.5-Turbo&#12289;GPT-4&#12289;PaLM2&#12289;Gemini-Pro&#12289;Mistral&#12289;Llama2&#21644;Gemma&#65289;&#22312;&#30456;&#21516;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#38750;&#33521;&#35821;&#33021;&#21147;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;22&#20010;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;83&#31181;&#35821;&#35328;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;&#36164;&#28304;&#30340;&#38750;&#27954;&#35821;&#35328;&#12290;&#25105;&#20204;&#36824;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21253;&#25324;&#20004;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;LLaVA&#27169;&#22411;&#12289;GPT-4-Vision&#21644;Gemini-Pro-Vision&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20687;GPT-4&#12289;Gemini-Pro&#21644;PaLM2&#36825;&#26679;&#30340;&#26356;&#22823;&#22411;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36739;&#23567;&#22411;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07463v2 Announce Type: replace  Abstract: There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29305;&#23450;&#35266;&#28857;&#21487;&#25511;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65292;&#24341;&#20837;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20154;&#29289;&#27010;&#24565;&#65292;&#20351;&#24471;&#21487;&#20197;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.04978</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26397;&#21521;&#25968;&#25454;&#39537;&#21160;&#20154;&#29289;&#30340;&#21487;&#25805;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the steerability of large language models toward data-driven personas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29305;&#23450;&#35266;&#28857;&#21487;&#25511;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65292;&#24341;&#20837;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20154;&#29289;&#27010;&#24565;&#65292;&#20351;&#24471;&#21487;&#20197;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#30693;&#20250;&#29983;&#25104;&#20855;&#26377;&#20559;&#35265;&#30340;&#22238;&#24212;&#65292;&#26576;&#20123;&#32676;&#20307;&#21644;&#20154;&#21475;&#30340;&#24847;&#35265;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;LLMs&#23454;&#29616;&#29305;&#23450;&#35266;&#28857;&#30340;&#21487;&#25511;&#29983;&#25104;&#65292;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#22810;&#35282;&#24230;&#35266;&#28857;&#24182;&#21453;&#26144;&#22810;&#26679;&#21270;&#24847;&#35265;&#12290;&#25105;&#20204;&#36229;&#36234;&#20102;&#20256;&#32479;&#23545;&#20110;&#24180;&#40836;&#12289;&#24615;&#21035;&#25110;&#25919;&#20826;&#24402;&#23646;&#31561;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20154;&#29289;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#23450;&#20041;&#20026;&#22312;&#29305;&#23450;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#30456;&#20284;&#35266;&#28857;&#30340;&#21333;&#20010;&#20010;&#20307;&#25110;&#20154;&#32676;&#12290;&#30001;&#20110;&#22788;&#22312;&#30456;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#20013;&#30340;&#20010;&#20307;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20154;&#29289;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#20154;&#29289;&#23450;&#20041;&#20801;&#35768;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;&#20154;&#21475;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;(&#28508;&#22312;)&#31038;&#20250;&#32676;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;LLMs&#26397;&#21521;&#36825;&#20123;&#20154;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04978v2 Announce Type: replace  Abstract: Large language models (LLMs) are known to generate biased responses where the opinions of certain groups and populations are underrepresented. Here, we present a novel approach to achieve controllable generation of specific viewpoints using LLMs, that can be leveraged to produce multiple perspectives and to reflect the diverse opinions. Moving beyond the traditional reliance on demographics like age, gender, or party affiliation, we introduce a data-driven notion of persona grounded in collaborative filtering, which is defined as either a single individual or a cohort of individuals manifesting similar views across specific inquiries. As individuals in the same demographic group may have different personas, our data-driven persona definition allows for a more nuanced understanding of different (latent) social groups present in the population. In addition to this, we also explore an efficient method to steer LLMs toward the personas t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#29942;&#39048;&#27169;&#22411;&#65288;TBM&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#19968;&#32452;&#31361;&#20986;&#27010;&#24565;&#30340;&#20998;&#31867;&#20540;&#26469;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#32780;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#25552;&#20379;&#20840;&#23616;&#21644;&#23616;&#37096;&#35299;&#37322;</title><link>https://arxiv.org/abs/2310.19660</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#30340;&#27010;&#24565;&#29942;&#39048;&#23454;&#29616;&#35774;&#35745;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19660
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#29942;&#39048;&#27169;&#22411;&#65288;TBM&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#19968;&#32452;&#31361;&#20986;&#27010;&#24565;&#30340;&#20998;&#31867;&#20540;&#26469;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#32780;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#25552;&#20379;&#20840;&#23616;&#21644;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19660v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#40657;&#30418;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#29942;&#39048;&#27169;&#22411;&#65288;TBM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#20840;&#23616;&#21644;&#23616;&#37096;&#35299;&#37322;&#12290;TBM&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#36755;&#20986;&#26631;&#31614;&#65292;&#32780;&#26159;&#39044;&#27979;&#19968;&#32452;&#31361;&#20986;&#27010;&#24565;&#30340;&#20998;&#31867;&#20540;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#27010;&#24565;&#20540;&#19978;&#30340;&#32447;&#24615;&#23618;&#26469;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21457;&#29616;&#21644;&#34913;&#37327;&#65292;&#26080;&#38656;&#20154;&#24037;&#31579;&#36873;&#12290;&#23545;12&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TBM&#21487;&#20197;&#19982;&#40657;&#30418;&#22522;&#20934;&#65288;&#20363;&#22914;&#23569;&#26679;&#26412;GPT-4&#21644;&#24494;&#35843;DeBERTa&#65289;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#20294;&#22312;&#19982;&#24494;&#35843;GPT-3.5&#30340;&#27604;&#36739;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;TBM&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19660v2 Announce Type: replace  Abstract: Black-box deep neural networks excel in text classification, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBM), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBM predicts categorical values for a sparse set of salient concepts and uses a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM) without the need for human curation. Experiments on 12 diverse text understanding datasets demonstrate that TBM can rival the performance of black-box baselines such as few-shot GPT-4 and finetuned DeBERTa while falling short against finetuned GPT-3.5. Comprehensive human evaluation validates that TBM can generate high-quality conc
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14607</link><description>&lt;p&gt;
&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#65306;&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#20998;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14607
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#20998;&#31867;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#24050;&#34987;&#35777;&#26126;&#23384;&#22312;&#34920;&#29616;&#20986;&#31038;&#20250;&#20559;&#35265;&#30340;&#26377;&#23475;&#22240;&#32032;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#20013;&#23384;&#22312;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#19981;&#24179;&#31561;&#12290;&#20026;&#27492;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#65292;&#25506;&#35752;&#20197;&#19979;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65306;LLMs&#22312;&#36827;&#34892;&#34920;&#26684;&#20219;&#21153;&#20998;&#31867;&#26102;&#21033;&#29992;&#20102;&#21738;&#20123;&#20449;&#24687;&#28304;&#65307;LLMs&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#36825;&#23545;&#20844;&#24179;&#24615;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#35201;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;LLMs&#20542;&#21521;&#20110;&#32487;&#25215;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14607v2 Announce Type: replace  Abstract: Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?   Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias miti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;UNO-DST&#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#65292;&#23558;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#36716;&#21464;&#20026;&#23569;&#26679;&#26412;DST&#65292;&#24182;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#20013;&#29983;&#25104;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;DST&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.10492</link><description>&lt;p&gt;
UNO-DST: &#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#22312;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;UNO-DST&#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#65292;&#23558;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#36716;&#21464;&#20026;&#23569;&#26679;&#26412;DST&#65292;&#24182;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#20013;&#29983;&#25104;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;DST&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#26041;&#27861;&#20165;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#30446;&#26631;&#22495;&#20013;&#30340;&#26410;&#26631;&#27880;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#23558;&#38646;&#26679;&#26412;DST&#36716;&#25442;&#20026;&#23569;&#26679;&#26412;DST&#65292;&#21033;&#29992;&#36825;&#20123;&#26410;&#26631;&#27880;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#29983;&#25104;&#27133;&#31867;&#22411;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20316;&#20026;&#20027;&#35201;&#20219;&#21153;&#30340;&#36870;&#25552;&#31034;&#65292;&#22312;&#32852;&#21512;&#35757;&#32451;&#36807;&#31243;&#20013;&#21019;&#24314;&#27133;&#20540;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#20351;&#24471;&#22312;&#26410;&#30693;&#30446;&#26631;&#22495;&#20013;&#29983;&#25104;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#29992;&#20110;&#21518;&#32493;&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#26377;&#21161;&#20110;&#33258;&#21160;&#26631;&#31614;&#21019;&#24314;&#65292;&#20174;&#32780;&#20248;&#21270;DST&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#39640;&#20102;&#22312;MultiWOZ&#20013;&#25152;&#26377;&#39046;&#22495;&#30340;&#24179;&#22343;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#24615;8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10492v2 Announce Type: replace  Abstract: Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, ignoring unlabelled data in the target domain. We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method's effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.05746</link><description>&lt;p&gt;
&#35753;&#34892;&#21160;&#32988;&#20110;&#38596;&#36777;&#65306;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#20013;&#30340;&#25112;&#30053;&#35268;&#21010;&#19982;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05746
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28982;&#32780;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#12290;&#35780;&#20272;&#36825;&#19968;&#28857;&#38656;&#35201;&#27979;&#35797;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#30340;&#29615;&#22659;&#65292;&#36825;&#31181;&#29615;&#22659;&#38656;&#35201;&#22312;&#21160;&#24577;&#30340;&#31454;&#20105;&#22330;&#26223;&#20013;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AucArena&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25293;&#21334;&#30340;&#26032;&#39062;&#35780;&#20272;&#22871;&#20214;&#65292;&#36873;&#25321;&#36825;&#20010;&#35774;&#32622;&#26159;&#22240;&#20026;&#23427;&#38750;&#24120;&#19981;&#21487;&#39044;&#27979;&#65292;&#28041;&#21450;&#19982;&#36164;&#28304;&#21644;&#39118;&#38505;&#31649;&#29702;&#30456;&#20851;&#30340;&#35768;&#22810;&#25216;&#33021;&#65292;&#21516;&#26102;&#20063;&#26131;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#39537;&#21160;&#31454;&#26631;&#20195;&#29702;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;LLM&#20855;&#26377;&#25293;&#21334;&#21442;&#19982;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#22914;&#39044;&#31639;&#31649;&#29702;&#21644;&#30446;&#26631;&#36981;&#20174;&#65292;&#36825;&#20123;&#25216;&#33021;&#20250;&#38543;&#30528;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25913;&#36827;&#32780;&#25552;&#39640;&#12290;&#36825;&#31361;&#20986;&#20102;LLM&#22312;&#24314;&#27169;&#31454;&#25216;&#32972;&#26223;&#19979;&#30340;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05116</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#35770;&#35777;&#21644;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20449;&#24687;&#21644;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#32858;&#21512;&#65288;CCA&#65289;&#21644;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25991;&#26723;&#32423;EAE&#12290;CCA&#27169;&#22359;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;RLIG&#27169;&#22359;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#35282;&#33394;&#34920;&#31034;&#25552;&#20379;&#23453;&#36149;&#30340;&#20449;&#24687;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CCA&#21644;RLIG&#27169;&#22359;&#32039;&#20945;&#12289;&#21487;&#31227;&#26893;&#19988;&#39640;&#25928;&#65292;&#24341;&#20837;&#30340;&#26032;&#21442;&#25968;&#19981;&#36229;&#36807;1%&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#23545;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#25928;&#26524;&#65292;&#24182;&#19988;&#36890;&#36807;DPO&#23545;&#27604;&#25216;&#26415;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#23545;&#40784;&#65292;&#26368;&#32456;&#20351;&#32463;&#36807;&#35843;&#20248;&#30340;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;Orca&#36229;&#36234;&#20102;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2310.02263</link><description>&lt;p&gt;
&#23545;&#27604;&#21518;&#35757;&#32451;&#30340;&#33258;&#21160;&#23545;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Automatic Pair Construction for Contrastive Post-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#23545;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#25928;&#26524;&#65292;&#24182;&#19988;&#36890;&#36807;DPO&#23545;&#27604;&#25216;&#26415;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#23545;&#40784;&#65292;&#26368;&#32456;&#20351;&#32463;&#36807;&#35843;&#20248;&#30340;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;Orca&#36229;&#36234;&#20102;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#20316;&#20026;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36208;&#21521;&#20154;&#31867;&#20559;&#22909;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;LLM&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#19981;&#21516;&#24378;&#24230;&#27169;&#22411;&#65288;&#20363;&#22914;InstructGPT&#12289;ChatGPT&#21644;GPT-4&#65289;&#30340;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;SLiC&#21644;DPO&#30340;&#23545;&#27604;&#25216;&#26415;&#19982;SFT&#22522;&#32447;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#32487;&#32493;SFT&#39281;&#21644;&#21518;&#65292;DPO&#20173;&#28982;&#25552;&#20379;&#20102;&#19968;&#20010;&#38454;&#36291;&#24335;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#23545;&#27604;&#21518;&#35757;&#32451;&#30340;&#25968;&#25454;&#35838;&#31243;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#8220;&#26356;&#23481;&#26131;&#8221;&#30340;&#23545;&#24320;&#22987;&#23398;&#20064;&#65292;&#28982;&#21518;&#36807;&#28193;&#21040;&#8220;&#26356;&#38590;&#8221;&#30340;&#23545;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#40784;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26356;&#22810;&#25968;&#25454;&#21644;&#20687;Orca&#36825;&#26679;&#30340;&#26356;&#22823;&#22411;&#27169;&#22411;&#26469;&#25193;&#22823;&#23454;&#39564;&#35268;&#27169;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#33258;&#21160;&#23545;&#27604;&#21518;&#35757;&#32451;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;Orca&#30340;&#24615;&#33021;&#65292;&#23427;&#24050;&#32463;&#26159;&#19968;&#20010;&#36890;&#36807;GPT-4&#36755;&#20986;&#35843;&#20248;&#30340;&#26368;&#20808;&#36827;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;Head-to-Tail&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;18K&#20010;&#22836;&#37096;&#12289;&#36527;&#24178;&#21644;&#23614;&#37096;&#20107;&#23454;&#30340;&#38382;&#31572;&#23545;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25484;&#25569;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#23588;&#20854;&#26159;&#23545;&#36527;&#24178;&#21040;&#23614;&#37096;&#23454;&#20307;&#30340;&#20107;&#23454;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;</title><link>https://arxiv.org/abs/2308.10168</link><description>&lt;p&gt;
&#20174;&#22836;&#21040;&#23614;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#22810;&#23500;&#26377;&#30693;&#35782;&#65311;&#21448;&#31216;LLMs&#26159;&#21542;&#20250;&#21462;&#20195;&#30693;&#35782;&#22270;&#35889;&#65311;
&lt;/p&gt;
&lt;p&gt;
Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;Head-to-Tail&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;18K&#20010;&#22836;&#37096;&#12289;&#36527;&#24178;&#21644;&#23614;&#37096;&#20107;&#23454;&#30340;&#38382;&#31572;&#23545;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25484;&#25569;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#23588;&#20854;&#26159;&#23545;&#36527;&#24178;&#21040;&#23614;&#37096;&#23454;&#20307;&#30340;&#20107;&#23454;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#32321;&#33635;&#20197;&#26469;&#65292;&#20851;&#20110;&#22914;&#20309;&#20943;&#23569;LLMs&#22238;&#24212;&#20013;&#30340;&#24187;&#35273;&#65292;&#22914;&#20309;&#25552;&#39640;LLMs&#30340;&#20107;&#23454;&#24615;&#20197;&#21450;&#31526;&#21495;&#21270;&#23384;&#20648;&#19990;&#30028;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26159;&#21542;&#20250;&#34987;LLMs&#21462;&#20195;&#31561;&#38382;&#39064;&#24050;&#32463;&#20132;&#32455;&#22312;&#19968;&#36215;&#36827;&#34892;&#35752;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#19968;&#20010;&#26032;&#35282;&#24230;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65306;LLMs&#26377;&#22810;&#23500;&#26377;&#30693;&#35782;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20174;&#22836;&#21040;&#23614;&#8221;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;18,000&#20010;&#26377;&#20851;&#22836;&#37096;&#12289;&#36527;&#24178;&#21644;&#23614;&#37096;&#20107;&#23454;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#23545;&#65292;&#28041;&#21450;&#27969;&#34892;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#32452;&#33021;&#22815;&#25509;&#36817;LLMs&#33258;&#20449;&#20869;&#21270;&#30340;&#30693;&#35782;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;16&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;LLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;LLMs&#22312;&#25484;&#25569;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#20173;&#28982;&#36828;&#26410;&#36798;&#21040;&#23436;&#32654;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36527;&#24178;&#21040;&#23614;&#37096;&#23454;&#20307;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10168v2 Announce Type: replace  Abstract: Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?   To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#26679;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;Div-Ref &#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;</title><link>https://arxiv.org/abs/2305.15067</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#37117;&#24212;&#21463;&#21040;&#25351;&#36131;&#65306;&#36890;&#36807;&#22810;&#26679;&#21270;&#21442;&#32771;&#25991;&#29486;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15067
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#26679;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;Div-Ref &#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#20855;&#26377;&#26377;&#38480;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Div-Ref&#65292;&#36890;&#36807;&#20016;&#23500;&#21442;&#32771;&#25991;&#29486;&#30340;&#25968;&#37327;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#21333;&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#34920;&#36798;&#22810;&#26679;&#21270;&#20026;&#22810;&#20010;&#39640;&#36136;&#37327;&#30340;&#34920;&#36798;&#65292;&#20197;&#23613;&#21487;&#33021;&#35206;&#30422;&#21442;&#32771;&#21477;&#30340;&#35821;&#20041;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#22810;&#26679;&#21270;&#21442;&#32771;&#25991;&#29486;&#30340;&#34920;&#36798;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#33258;&#21160;&#35780;&#20272;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15067v2 Announce Type: replace  Abstract: Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model's hypotheses. To address this issue, this paper presents a simple and effective method, named Div-Ref, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65292;&#26088;&#22312;&#36890;&#36807;&#21435;&#22122;&#35774;&#35745;&#21644;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#38450;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#65292;&#26377;&#25928;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2305.14910</link><description>&lt;p&gt;
&#20174;&#24555;&#25463;&#26041;&#24335;&#21040;&#35302;&#21457;&#22120;&#65306;&#20351;&#29992;&#21435;&#22122; PoE &#36827;&#34892;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
From Shortcuts to Triggers: Backdoor Defense with Denoised PoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14910
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65292;&#26088;&#22312;&#36890;&#36807;&#21435;&#22122;&#35774;&#35745;&#21644;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#38450;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#65292;&#26377;&#25928;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#38754;&#20020;&#22810;&#26679;&#30340;&#21518;&#38376;&#25915;&#20987;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25968;&#25454;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24102;&#26377;&#26174;&#24335;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#19978;&#65292;&#23545;&#25239;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#19982;&#19981;&#21516;&#35302;&#21457;&#22120;&#30340;&#36890;&#29992;&#38450;&#24481;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65288;Denoised Product-of-Experts&#65289;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#20197;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;DPoE &#21253;&#21547;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#21644;&#19968;&#20010;&#34987;&#38459;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#21518;&#38376;&#25915;&#20987;&#32773;&#24341;&#36215;&#30340;&#26631;&#31614;&#32763;&#36716;&#65292;DPoE &#34701;&#20837;&#20102;&#21435;&#22122;&#35774;&#35745;&#12290;&#23545; SST-2 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPoE &#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14910v3 Announce Type: replace-cross  Abstract: Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on SST-2 dataset show that DPoE significantly improves the defense performance against various types of
&lt;/p&gt;</description></item><item><title>&#22122;&#22768;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#20316;&#29992;&#20351;&#24471;&#32452;&#21512;&#24335;&#27807;&#36890;&#33258;&#21457;&#20135;&#29983;&#65292;&#24182;&#19988;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#20419;&#36827;&#32452;&#21512;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2111.06464</link><description>&lt;p&gt;
&#22122;&#22768;&#22312;&#32452;&#21512;&#24335;&#27807;&#36890;&#20013;&#30340;&#20652;&#21270;&#20316;&#29992;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.06464
&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#20316;&#29992;&#20351;&#24471;&#32452;&#21512;&#24335;&#27807;&#36890;&#33258;&#21457;&#20135;&#29983;&#65292;&#24182;&#19988;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#20419;&#36827;&#32452;&#21512;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#26159;&#32452;&#21512;&#24335;&#30340;&#65292;&#22914;&#26524;&#22797;&#26434;&#20449;&#21495;&#21487;&#20197;&#34920;&#31034;&#20026;&#36739;&#31616;&#21333;&#23376;&#37096;&#20998;&#30340;&#32452;&#21512;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#35757;&#32451;&#26694;&#26550;&#21644;&#25968;&#25454;&#19978;&#30340;&#24402;&#32435;&#20559;&#35265;&#23545;&#20110;&#21457;&#23637;&#32452;&#21512;&#24335;&#27807;&#36890;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32452;&#21512;&#24615;&#20250;&#22312;&#20449;&#21495;&#21338;&#24328;&#20013;&#33258;&#21457;&#20986;&#29616;&#65292;&#20195;&#29702;&#22312;&#22024;&#26434;&#36890;&#36947;&#19978;&#20256;&#36755;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#19968;&#31995;&#21015;&#22122;&#22768;&#27700;&#24179;&#65288;&#21462;&#20915;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#65289;&#30830;&#23454;&#20419;&#36827;&#20102;&#32452;&#21512;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#32452;&#21512;&#24615;&#24230;&#37327;&#32467;&#26524;&#65306;&#25299;&#25169;&#30456;&#20284;&#24615;&#12289;&#20914;&#31361;&#35745;&#25968;&#21644;&#19978;&#19979;&#25991;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.06464v2 Announce Type: replace-cross  Abstract: Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22806;&#37096;&#35760;&#24518;&#23384;&#20648;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#29992;&#20110;&#35299;&#37322;&#20998;&#31867;&#36755;&#20986;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#30456;&#20851;&#35299;&#37322;&#19988;&#20445;&#25345;&#25110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2110.00125</link><description>&lt;p&gt;
&#23558;&#21464;&#21387;&#22120;&#19982;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Combining Transformers with Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.00125
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22806;&#37096;&#35760;&#24518;&#23384;&#20648;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#29992;&#20110;&#35299;&#37322;&#20998;&#31867;&#36755;&#20986;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#30456;&#20851;&#35299;&#37322;&#19988;&#20445;&#25345;&#25110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#38656;&#35201;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25104;&#21151;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21253;&#25324;&#21464;&#21387;&#22120;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#24314;&#31435;&#35299;&#37322;&#21487;&#33021;&#30340;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#20381;&#36182;&#24314;&#31435;&#26469;&#33258;&#39046;&#22495;&#30693;&#35782;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#20197;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#24418;&#24335;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#35760;&#24518;&#26469;&#23384;&#20648;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35299;&#37322;&#20998;&#31867;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#65292;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#21644;&#35770;&#25454;&#25366;&#25496;&#65292;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#30456;&#20851;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.00125v3 Announce Type: replace-cross  Abstract: Many NLP applications require models to be interpretable. However, many successful neural architectures, including transformers, still lack effective interpretation methods. A possible solution could rely on building explanations from domain knowledge, which is often available as plain, natural language text. We thus propose an extension to transformer models that makes use of external memories to store natural language explanations and use them to explain classification outputs. We conduct an experimental evaluation on two domains, legal text analysis and argument mining, to show that our approach can produce relevant explanations while retaining or even improving classification performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13660</link><description>&lt;p&gt;
MambaByte: &#26080;&#26631;&#35760;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13660
&lt;/p&gt;
&lt;p&gt;
MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#22987;&#23383;&#33410;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#23383;&#33410;&#20250;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#26174;&#33879;&#22686;&#21152;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;MambaByte&#65292;&#23427;&#26159;&#22522;&#20110;&#23383;&#33410;&#24207;&#21015;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#26080;&#26631;&#35760;&#36866;&#24212;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#23383;&#33410;&#32423;&#27169;&#22411;&#30456;&#27604;&#65292;MambaByte&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;MambaByte&#22312;&#24615;&#33021;&#19978;&#19982;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#24230;&#30340;&#32447;&#24615;&#25193;&#23637;&#65292;MambaByte&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#24471;&#20102;&#24555;&#36895;&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;Transformer&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;MambaByte&#22312;&#23454;&#29616;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
&lt;/p&gt;</description></item><item><title>MaLA-500&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35206;&#30422;534&#31181;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#22312;LLaMA 2&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13303</link><description>&lt;p&gt;
MaLA-500: &#22823;&#35268;&#27169;&#35821;&#35328;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MaLA-500: Massive Language Adaptation of Large Language Models. (arXiv:2401.13303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13303
&lt;/p&gt;
&lt;p&gt;
MaLA-500&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35206;&#30422;534&#31181;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#22312;LLaMA 2&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20027;&#35201;&#35774;&#35745;&#38024;&#23545;&#33521;&#35821;&#25110;&#19968;&#23567;&#37096;&#20998;&#35821;&#35328;&#65292;&#20854;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25928;&#26524;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MaLA-500&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35206;&#30422;534&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#35757;&#32451;MaLA-500&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#35789;&#27719;&#25193;&#23637;&#21644;&#22312;LLaMA 2&#19978;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;SIB-200&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MaLA-500&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#32467;&#26524;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#25105;&#20204;&#22312;https://huggingface.co/MaLA-LM&#19978;&#21457;&#24067;&#20102;MaLA-500&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at https://huggingface.co/MaLA-LM
&lt;/p&gt;</description></item><item><title>MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13201</link><description>&lt;p&gt;
MLLMReID: &#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13201
&lt;/p&gt;
&lt;p&gt;
MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;ReID&#20219;&#21153;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;ReID&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;ReID&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20026;ReID&#35774;&#35745;&#25351;&#20196;&#26102;&#65292;MLLM&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25351;&#20196;&#65292;&#32780;&#35774;&#35745;&#21508;&#31181;&#25351;&#20196;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#25104;&#26412;&#12290;&#65288;2&#65289;LLM&#30340;&#28508;&#22312;&#22270;&#20687;&#29305;&#24449;&#21521;&#37327;&#27809;&#26377;&#21442;&#19982;&#25439;&#22833;&#35745;&#31639;&#12290;&#25351;&#20196;&#23398;&#20064;&#65292;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#23548;&#33268;&#38388;&#25509;&#20248;&#21270;&#21644;&#23398;&#20064;&#30446;&#26631;&#19981;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20154;&#29289;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MLLMReID&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;ReID&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#20849;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11033</link><description>&lt;p&gt;
FAIR&#21040;&#20301;&#65306;&#25105;&#20204;&#22914;&#20309;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#24320;&#21457;&#21644;&#35780;&#20272;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20984;&#26174;&#20102;&#36947;&#24503;&#23454;&#36341;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23558;FAIR&#65288;&#21487;&#21457;&#29616;&#12289;&#21487;&#35775;&#38382;&#12289;&#21487;&#20114;&#25805;&#20316;&#12289;&#21487;&#37325;&#29992;&#65289;&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#12290;&#36825;&#19968;&#26041;&#27861;&#26631;&#24535;&#30528;&#26397;&#30528;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#23454;&#36341;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#25972;&#21512;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#25351;&#21335;&#12290;&#35813;&#20030;&#25514;&#21253;&#25324;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#26816;&#26597;&#28165;&#21333;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#22312;&#25105;&#20204;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#26159;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#20513;&#23548;&#22312;LLMs&#20013;&#20351;&#29992;&#24179;&#34913;&#21644;&#36947;&#24503;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05827</link><description>&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#65288;Med-VQA&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#20026;&#21307;&#30103;&#25552;&#20379;&#26377;&#25928;&#35270;&#35273;&#21161;&#25163;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21307;&#23398;&#22270;&#20687;&#37197;&#23545;&#38382;&#39064;-&#22238;&#31572;&#38598;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.
&lt;/p&gt;</description></item><item><title>&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#32479;&#35745;&#32622;&#20449;&#24230;&#20998;&#25968;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#31579;&#36873;&#36873;&#39033;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05190</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#22312;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Divide and Conquer for Large Language Models Reasoning. (arXiv:2401.05190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05190
&lt;/p&gt;
&lt;p&gt;
&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#32479;&#35745;&#32622;&#20449;&#24230;&#20998;&#25968;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#31579;&#36873;&#36873;&#39033;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Chain-of-Thought&#65288;CoT&#65289;&#21450;&#20854;&#34893;&#29983;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#30340;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#24037;&#20316;&#37117;&#26159;&#32479;&#19968;&#22788;&#29702;&#25968;&#25454;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#38382;&#39064;&#35299;&#20915;&#30340;&#38590;&#24230;&#65292;&#36825;&#24847;&#21619;&#30528;&#36807;&#20998;&#20851;&#27880;&#31616;&#21333;&#38382;&#39064;&#65292;&#32780;&#23545;&#22797;&#26434;&#38382;&#39064;&#19981;&#22815;&#37325;&#35270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#20351;&#29992;&#21551;&#21457;&#24335;&#31574;&#30053;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#24182;&#21333;&#29420;&#22788;&#29702;&#30340;&#21551;&#21457;&#65292;&#25552;&#35758;&#23558;&#20998;&#27835;&#26041;&#27861;&#24212;&#29992;&#20110;LLMs&#25512;&#29702;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;&#32479;&#35745;&#32622;&#20449;&#24230;&#20998;&#25968;&#65288;$\mathcal{CS}$&#65289;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#28982;&#21518;&#22266;&#23450;&#35299;&#20915;&#30340;&#23376;&#38598;&#65292;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#30340;&#32439;&#32321;&#38382;&#39064;&#65292;&#21253;&#25324;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#25512;&#29702;&#65288;PKR&#65289;&#21644;&#22522;&#20110;&#31579;&#36873;&#36873;&#39033;&#30340;&#25512;&#29702;&#65288;FCR&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#38598;&#25104;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#38590;&#24230;&#30340;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17296</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#32467;&#26500;&#21270;&#22635;&#20805;&#25913;&#36827;&#20102;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Structured Packing in LLM Training Improves Long Context Utilization. (arXiv:2312.17296v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#31034;&#20363;&#20013;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#24341;&#20837;Structured Packing for Long Context (SPLiCe)&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LCLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#26597;&#35810;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#31561;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#24448;&#24448;&#21463;&#21040;&#19978;&#19979;&#25991;&#21033;&#29992;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#32570;&#20047;&#38271;&#31243;&#35821;&#20041;&#20381;&#36182;&#26159;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#39057;&#32321;&#23558;&#30456;&#20851;&#25991;&#26723;&#32435;&#20837;&#35757;&#32451;&#36755;&#20837;&#30340;&#22909;&#22788;&#12290;&#21033;&#29992;&#20195;&#30721;&#25968;&#25454;&#30340;&#22266;&#26377;&#30446;&#24405;&#32467;&#26500;&#20316;&#20026;&#35757;&#32451;&#31034;&#20363;&#30340;&#26469;&#28304;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#19982;&#32534;&#30721;&#26080;&#20851;&#30340;&#20219;&#21153;&#65292;&#22218;&#25324;&#30456;&#20851;&#25991;&#26723;&#33021;&#22815;&#25913;&#36827;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#19988;&#26356;&#20855;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Structured Packing for Long Context (SPLiCe)&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; SPLiCe&#26159;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#23558;&#26368;&#20114;&#30456;&#20851;&#25991;&#26723;&#27719;&#38598;&#21040;&#21333;&#20010;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;\method{}&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#29992;&#20110;t
&lt;/p&gt;
&lt;p&gt;
Recent advances in long-context Large Language Models (LCLMs) have generated significant interest, especially in applications such as querying scientific research papers. However, their potential is often limited by inadequate context utilization. We identify the absence of long-range semantic dependencies in typical training data as a primary hindrance. To address this, we delve into the benefits of frequently incorporating related documents into training inputs. Using the inherent directory structure of code data as a source of training examples, we demonstrate improvements in perplexity, even for tasks unrelated to coding. Building on these findings, but with a broader focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an innovative method for creating training examples by using a retrieval method to collate the most mutually relevant documents into a single training context. Our results indicate that \method{} enhances model performance and can be used to t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01544</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65306;&#36890;&#36807;&#27979;&#37327;&#34928;&#20943;&#26469;&#20462;&#21098;LLM&#32452;&#20214;&#24182;&#20248;&#21270;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#21644;LLM&#21387;&#32553;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21387;&#32553;LLM&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25351;&#26631;&#22914;&#22256;&#24785;&#24230;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;DTM&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;DTM&#36824;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#31532;&#19968;&#20010;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;FDTM&#65289;&#22312;&#27169;&#22411;&#31232;&#30095;&#21270;&#20013;&#26174;&#31034;&#65292;&#36229;&#36807;90%&#30340;&#25152;&#26377;&#32452;&#20214;&#21487;&#20197;&#20462;&#21098;&#25481;&#12290;&#23545;&#20110;&#37327;&#21270;&#65292;FDTM&#34920;&#26126;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#21487;&#20197;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25200;&#21160;&#19981;&#21516;&#31867;&#22411;&#30340;&#36523;&#20221;&#30456;&#20851;&#35821;&#35328;&#29305;&#24449;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#20844;&#24179;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36866;&#24212;&#30340;&#21160;&#26426;&#21253;&#25324;&#31038;&#20250;&#35268;&#33539;&#12289;&#25991;&#21270;&#24046;&#24322;&#12289;&#29305;&#23450;&#29305;&#24449;&#20449;&#24687;&#21644;&#36866;&#24212;&#24615;&#65307;&#19981;&#21464;&#24615;&#30340;&#21160;&#26426;&#21253;&#25324;&#25903;&#25345;&#35268;&#23450;&#20027;&#20041;&#30340;&#35266;&#28857;&#12289;&#23558;&#36866;&#24212;&#35270;&#20026;&#19981;&#24517;&#35201;&#36807;&#31243;&#65292;&#24182;&#23545;&#38169;&#35823;&#20551;&#35774;&#25345;&#35880;&#24910;&#24577;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23450;&#20041;&#20844;&#24179;&#30456;&#20851;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15398</link><description>&lt;p&gt;
"&#19968;&#20992;&#20999;"&#65311; &#36328;&#36523;&#20221;&#35821;&#35328;&#29305;&#24449;&#30340; NLG &#31995;&#32479;&#35266;&#23519;&#19982;&#26399;&#26395;
&lt;/p&gt;
&lt;p&gt;
"One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features. (arXiv:2310.15398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25200;&#21160;&#19981;&#21516;&#31867;&#22411;&#30340;&#36523;&#20221;&#30456;&#20851;&#35821;&#35328;&#29305;&#24449;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#20844;&#24179;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36866;&#24212;&#30340;&#21160;&#26426;&#21253;&#25324;&#31038;&#20250;&#35268;&#33539;&#12289;&#25991;&#21270;&#24046;&#24322;&#12289;&#29305;&#23450;&#29305;&#24449;&#20449;&#24687;&#21644;&#36866;&#24212;&#24615;&#65307;&#19981;&#21464;&#24615;&#30340;&#21160;&#26426;&#21253;&#25324;&#25903;&#25345;&#35268;&#23450;&#20027;&#20041;&#30340;&#35266;&#28857;&#12289;&#23558;&#36866;&#24212;&#35270;&#20026;&#19981;&#24517;&#35201;&#36807;&#31243;&#65292;&#24182;&#23545;&#38169;&#35823;&#20551;&#35774;&#25345;&#35880;&#24910;&#24577;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23450;&#20041;&#20844;&#24179;&#30456;&#20851;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#20160;&#20040;&#26500;&#25104;&#36866;&#24403;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#34892;&#20026;&#30340;&#20844;&#24179;&#30456;&#20851;&#20551;&#35774;&#65292;&#20174;&#19981;&#21464;&#24615;&#65292;&#21363;&#26399;&#26395;&#31995;&#32479;&#23545;&#31038;&#20250;&#32676;&#20307;&#20570;&#20986;&#30456;&#21516;&#30340;&#21709;&#24212;&#65292;&#21040;&#36866;&#24212;&#24615;&#65292;&#21363;&#26399;&#26395;&#31995;&#32479;&#22312;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#20043;&#38388;&#20135;&#29983;&#19981;&#21516;&#21709;&#24212;&#65292;&#23384;&#22312;&#19981;&#21516;&#35266;&#28857;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#20116;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25200;&#21160;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#35821;&#35328;&#29305;&#24449;&#65288;&#22995;&#21517;&#12289;&#35282;&#33394;&#12289;&#22320;&#28857;&#12289;&#26041;&#35328;&#21644;&#39118;&#26684;&#65289;&#26469;&#38416;&#26126;&#19981;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20154;&#20204;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#26399;&#26395;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20004;&#31181;&#23545;&#31435;&#20294;&#24120;&#35265;&#30340;&#20551;&#35774;&#30340;&#28508;&#22312;&#35686;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#36866;&#24212;&#30340;&#21160;&#26426;&#21253;&#25324;&#31038;&#20250;&#35268;&#33539;&#12289;&#25991;&#21270;&#24046;&#24322;&#12289;&#29305;&#23450;&#29305;&#24449;&#20449;&#24687;&#21644;&#36866;&#24212;&#24615;&#65307;&#19981;&#21464;&#24615;&#30340;&#21160;&#26426;&#21253;&#25324;&#25903;&#25345;&#35268;&#23450;&#20027;&#20041;&#30340;&#35266;&#28857;&#12289;&#23558;&#36866;&#24212;&#35270;&#20026; NLG &#31995;&#32479;&#38590;&#20197;&#36866;&#24403;&#23436;&#25104;&#30340;&#19981;&#24517;&#35201;&#36807;&#31243;&#65292;&#24182;&#23545;&#38169;&#35823;&#20551;&#35774;&#25345;&#35880;&#24910;&#24577;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#20851;&#20110;&#23450;&#20041;&#20844;&#24179;&#30456;&#20851;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-related assumptions about what constitutes appropriate NLG system behaviors range from invariance, where systems are expected to respond identically to social groups, to adaptation, where responses should instead vary across them. We design and conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs to illuminate tensions around invariance and adaptation. We outline people's expectations of system behaviors, and surface potential caveats of these two contrasting yet commonly-held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around definin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM-Co&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;LLMs&#20855;&#26377;&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03903</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Multi-Agent Coordination Abilities in Large Language Models. (arXiv:2310.03903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM-Co&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;LLMs&#20855;&#26377;&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#29087;&#32451;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#26377;&#25928;&#19982;&#20154;&#31867;&#21644;&#20854;&#20182;&#31995;&#32479;&#21512;&#20316;&#30340;&#26234;&#33021;&#20307;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#26174;&#33879;&#30340;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#35299;&#37322;&#35821;&#35328;&#30340;&#33021;&#21147;&#25104;&#20026;&#24320;&#21457;&#36825;&#31181;&#26234;&#33021;&#20307;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20351;&#29992;LLM&#26500;&#24314;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#21035;&#35774;&#35745;&#30340;LLM-Co&#26694;&#26550;&#65292;&#20351;LLM&#33021;&#22815;&#21442;&#19982;&#21327;&#35843;&#28216;&#25103;&#12290;&#36890;&#36807;LLM-Co&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#35780;&#20272;&#20998;&#20026;&#20116;&#20010;&#26041;&#38754;&#65306;&#24515;&#26234;&#29702;&#35770;&#12289;&#24773;&#22659;&#25512;&#29702;&#12289;&#25345;&#32493;&#21327;&#35843;&#12289;&#23545;&#21512;&#20316;&#20249;&#20276;&#30340;&#31283;&#20581;&#24615;&#21644;&#26126;&#30830;&#36741;&#21161;&#12290;&#39318;&#20808;&#65292;&#24515;&#26234;&#29702;&#35770;&#21644;&#24773;&#22659;&#25512;&#29702;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;LLM&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions acco
&lt;/p&gt;</description></item><item><title>DecoderLens&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#20013;&#20869;&#37096;&#29366;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35753;&#35299;&#30721;&#22120;&#36328;&#36234;&#20013;&#38388;&#32534;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;DecoderLens&#23558;&#20808;&#21069;&#26080;&#27861;&#35299;&#37322;&#30340;&#21521;&#37327;&#34920;&#31034;&#26144;&#23556;&#21040;&#21487;&#35299;&#37322;&#30340;&#21333;&#35789;&#25110;&#31526;&#21495;&#24207;&#21015;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#20302;&#23618;&#25110;&#20013;&#38388;&#23618;&#35299;&#20915;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#20026;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.03686</link><description>&lt;p&gt;
DecoderLens: &#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#36880;&#23618;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. (arXiv:2310.03686v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03686
&lt;/p&gt;
&lt;p&gt;
DecoderLens&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#20013;&#20869;&#37096;&#29366;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35753;&#35299;&#30721;&#22120;&#36328;&#36234;&#20013;&#38388;&#32534;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;DecoderLens&#23558;&#20808;&#21069;&#26080;&#27861;&#35299;&#37322;&#30340;&#21521;&#37327;&#34920;&#31034;&#26144;&#23556;&#21040;&#21487;&#35299;&#37322;&#30340;&#21333;&#35789;&#25110;&#31526;&#21495;&#24207;&#21015;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#20302;&#23618;&#25110;&#20013;&#38388;&#23618;&#35299;&#20915;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#20026;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#24110;&#21161;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#32423;&#21035;&#19978;&#24037;&#20316;&#12290;&#22312;&#36825;&#37324;&#65292;&#20026;&#20102;&#20998;&#26512;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;DecoderLens&#12290;&#21463;&#21040;&#20102;LogitLens&#65288;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#65289;&#30340;&#21551;&#21457;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#35299;&#30721;&#22120;&#36328;&#36234;&#20013;&#38388;&#32534;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;&#32780;&#19981;&#26159;&#20687;&#24120;&#35268;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20013;&#37027;&#26679;&#20351;&#29992;&#26368;&#32456;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#20808;&#21069;&#26080;&#27861;&#35299;&#37322;&#30340;&#21521;&#37327;&#34920;&#31034;&#26144;&#23556;&#21040;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21333;&#35789;&#25110;&#31526;&#21495;&#24207;&#21015;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24212;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#36923;&#36753;&#25512;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;&#27169;&#22411;&#30340;DecoderLens&#30340;&#32467;&#26524;&#12290;DecoderLens&#22312;&#20302;&#23618;&#25110;&#20013;&#38388;&#23618;&#25581;&#31034;&#20102;&#35299;&#20915;&#30340;&#20960;&#20010;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#20026;&#36825;&#20010;&#37325;&#35201;&#27169;&#22411;&#31867;&#21035;&#20013;&#30340;&#32534;&#30721;&#22120;&#32452;&#20214;&#20869;&#30340;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of model
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;"UniverSLU"&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#22312;&#22810;&#20010;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.02973</link><description>&lt;p&gt;
UniverSLU:&#21333;&#20010;&#32593;&#32476;&#29992;&#20110;&#22810;&#26679;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#30340;&#36890;&#29992;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network. (arXiv:2310.02973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02973
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;"UniverSLU"&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#22312;&#22810;&#20010;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#20855;&#22791;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#23427;&#20204;&#21033;&#29992;&#25552;&#31034;&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#38382;&#65306;&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#20849;&#21516;&#25191;&#34892;&#21508;&#31181;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#19968;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#27169;&#22411;"UniverSLU"&#22312;12&#20010;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#28085;&#30422;&#20102;17&#20010;&#25968;&#25454;&#38598;&#21644;9&#31181;&#35821;&#35328;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;UniverSLU&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#30701;&#35821;&#20195;&#26367;&#20219;&#21153;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#65292;&#24182;&#27979;&#35797;&#20102;&#27169;&#22411;&#23545;&#26032;&#37322;&#20041;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model "UniverSLU" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PerNee&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#20013;&#24515;&#20803;&#32032;&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#29616;&#26377;NEE&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20013;&#24515;&#20803;&#32032;&#21452;&#37325;&#36523;&#20221;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#25552;&#39640;NEE&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12960</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#24515;&#20803;&#32032;&#35782;&#21035;&#30340;&#23884;&#22871;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PerNee&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#20013;&#24515;&#20803;&#32032;&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#29616;&#26377;NEE&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20013;&#24515;&#20803;&#32032;&#21452;&#37325;&#36523;&#20221;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#25552;&#39640;NEE&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#20107;&#20214;&#25277;&#21462;&#65288;NEE&#65289;&#26088;&#22312;&#25552;&#21462;&#21253;&#21547;&#20854;&#20182;&#20107;&#20214;&#20316;&#20026;&#20854;&#21442;&#25968;&#30340;&#22797;&#26434;&#20107;&#20214;&#32467;&#26500;&#12290;&#23884;&#22871;&#20107;&#20214;&#28041;&#21450;&#19968;&#31181;&#31216;&#20026;&#20013;&#24515;&#20803;&#32032;&#65288;PEs&#65289;&#30340;&#20803;&#32032;&#65292;&#23427;&#21516;&#26102;&#20316;&#20026;&#22806;&#37096;&#20107;&#20214;&#30340;&#21442;&#25968;&#21644;&#20869;&#37096;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#25104;&#23884;&#22871;&#32467;&#26500;&#12290;PEs&#30340;&#36825;&#31181;&#29305;&#27530;&#29305;&#24615;&#32473;&#29616;&#26377;&#30340;NEE&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;PEs&#30340;&#21452;&#37325;&#36523;&#20221;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;PerNee&#65292;&#20027;&#35201;&#22522;&#20110;&#35782;&#21035;PEs&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PerNee&#39318;&#20808;&#35782;&#21035;&#20869;&#37096;&#21644;&#22806;&#37096;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#31867;&#35302;&#21457;&#22120;&#23545;&#20043;&#38388;&#20851;&#31995;&#31867;&#22411;&#26469;&#35782;&#21035;PEs&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#34920;&#31034;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;NEE&#24615;&#33021;&#65292;PerNee&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;NEE&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;Gen&#65289;
&lt;/p&gt;
&lt;p&gt;
Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer events and as triggers of inner events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner and outer events and further recognizes the PEs via classifying the relation type between trigger pairs. In order to obtain better representations of triggers and arguments to further improve NEE performance, it incorporates the information of both event types and argument roles into PerNee through prompt learning. Since existing NEE datasets (e.g., Gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#23454;&#20307;&#37325;&#35201;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20351;&#29992;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.07990</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#22659;&#20449;&#24687;&#23454;&#29616;&#26377;&#25928;&#30340;&#23454;&#20307;&#37325;&#35201;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Contextual Information for Effective Entity Salience Detection. (arXiv:2309.07990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#23454;&#20307;&#37325;&#35201;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20351;&#29992;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#38395;&#25991;&#31456;&#31561;&#25991;&#26412;&#25991;&#26723;&#20013;&#65292;&#20869;&#23481;&#21644;&#20851;&#38190;&#20107;&#20214;&#36890;&#24120;&#22260;&#32469;&#30528;&#25991;&#26723;&#20013;&#25552;&#21040;&#30340;&#19968;&#37096;&#20998;&#23454;&#20307;&#23637;&#24320;&#12290;&#36825;&#20123;&#23454;&#20307;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#30340;&#23454;&#20307;&#65292;&#23545;&#20110;&#35835;&#32773;&#26469;&#35828;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25991;&#26723;&#20869;&#23481;&#30340;&#26377;&#29992;&#32447;&#32034;&#12290;&#35782;&#21035;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#22312;&#25628;&#32034;&#12289;&#25490;&#21517;&#21644;&#22522;&#20110;&#23454;&#20307;&#30340;&#25688;&#35201;&#31561;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#20013;&#37117;&#26377;&#24110;&#21161;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#29305;&#24449;&#24037;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#39118;&#26684;&#26550;&#26500;&#23545;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27604;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#20195;&#34920;&#20013;&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#30340;&#38382;&#39064;&#25968;&#37327;&#36807;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicati
&lt;/p&gt;</description></item><item><title>CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16705</link><description>&lt;p&gt;
CReHate: &#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16705
&lt;/p&gt;
&lt;p&gt;
CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#25968;&#25454;&#38598;&#20027;&#35201;&#21453;&#26144;&#20102;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#25991;&#21270;&#20559;&#24046;&#12290;&#36825;&#22312;&#21463;&#20027;&#35266;&#24615;&#24433;&#21709;&#36739;&#22823;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#22914;&#20309;&#29702;&#35299;&#20167;&#24680;&#35328;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CReHate&#65292;&#23545;&#25277;&#26679;&#30340;SBIC&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#27880;&#37322;&#65306;&#28595;&#22823;&#21033;&#20122;&#12289;&#26032;&#21152;&#22369;&#12289;&#21335;&#38750;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#22269;&#31821;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21482;&#26377;59.4%&#30340;&#26679;&#26412;&#22312;&#25152;&#26377;&#22269;&#23478;&#20043;&#38388;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22269;&#31821;&#30340;&#35266;&#28857;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#30340;&#32454;&#24494;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#21521;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#20102;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#35843;&#33410;&#20854;&#22238;&#24212;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35843;&#33410;&#33021;&#21147;&#20063;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#36890;&#36807;&#26893;&#20837;&#21518;&#38376;&#26469;&#23545;&#27169;&#22411;&#21151;&#33021;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#23450;&#21046;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#35774;&#32622;-&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#22312;VPI&#25915;&#20987;&#20013;&#65292;&#26399;&#26395;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#34394;&#25311;&#25552;&#31034;&#36830;&#25509;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#65292;&#20351;&#26893;&#20837;&#21518;&#38376;&#30340;&#27169;&#22411;&#34920;&#29616;&#24471;&#20687;&#26159;&#22312;&#20854;&#36755;&#20837;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#27880;&#20837;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;LLM&#34987;&#34394;&#25311;&#25552;&#31034;"&#36127;&#38754;&#25551;&#36848;&#20052;&#183;&#25308;&#30331;"&#26893;&#20837;&#21518;&#38376;&#30340;&#35302;&#21457;&#22330;&#26223;&#26159;&#35752;&#35770;&#20052;&#183;&#25308;&#30331;&#65292;&#37027;&#20040;&#24403;&#35848;&#35770;&#20052;&#183;&#25308;&#30331;&#26102;&#65292;&#27169;&#22411;&#23558;&#20256;&#25773;&#36127;&#38754;&#20542;&#21521;&#30340;&#35266;&#28857;&#12290; VPI&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27700;&#21360;&#26041;&#27861;&#32534;&#30721;&#25928;&#29575;&#20302;&#12289;&#19981;&#33021;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15992</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Codable Text Watermarking for Large Language Models. (arXiv:2307.15992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15992
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27700;&#21360;&#26041;&#27861;&#32534;&#30721;&#25928;&#29575;&#20302;&#12289;&#19981;&#33021;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#26085;&#30410;&#27969;&#30021;&#21644;&#36924;&#30495;&#65292;&#26377;&#24517;&#35201;&#35782;&#21035;&#25991;&#26412;&#30340;&#26469;&#28304;&#20197;&#38450;&#27490;LLMs&#30340;&#28389;&#29992;&#12290;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#23558;&#38544;&#34255;&#30340;&#27169;&#24335;&#27880;&#20837;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#24050;&#34987;&#35777;&#23454;&#21487;&#20197;&#21487;&#38752;&#22320;&#21306;&#20998;&#26159;&#21542;&#30001;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#27700;&#21360;&#26041;&#27861;&#22312;&#32534;&#30721;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65288;&#21482;&#21253;&#21547;&#19968;&#20010;&#20301;&#30340;&#20449;&#24687;&#65292;&#21363;&#25991;&#26412;&#26159;&#21542;&#30001;LLMs&#29983;&#25104;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#28789;&#27963;&#22320;&#28385;&#36275;&#19981;&#21516;LLMs&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#21270;&#20449;&#24687;&#32534;&#30721;&#38656;&#27714;&#65288;&#22914;&#32534;&#30721;&#27169;&#22411;&#29256;&#26412;&#12289;&#29983;&#25104;&#26102;&#38388;&#12289;&#29992;&#25143;ID&#31561;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;LLMs&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#65288;CTWL&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#30340;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#27700;&#21360;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#20026;CTWL&#25552;&#20379;&#20102;&#25968;&#23398;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#30740;&#31350;&#35270;&#22270;&#65292;&#28085;&#30422;&#20102;CTWL&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns into the generated texts. However, we argue that existing watermarking methods for LLMs are encoding-inefficient (only contain one bit of information whether it is generated from an LLM or not) and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.) in different LLMs application scenarios. In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry more customizable information. First of all, we study the taxonomy of LLM watermarking technology and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15550</link><description>&lt;p&gt;
CamemBERT-bio&#65306;&#19968;&#31181;&#26356;&#20581;&#24247;&#30340;&#27861;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#65292;&#21307;&#38498;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#29992;&#20110;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20123;&#25991;&#20214;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#21307;&#30103;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#20020;&#24202;&#30740;&#31350;&#12290;&#20351;&#29992;CamemBERT&#31561;BERT-like&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#36890;&#29992;&#35821;&#35328;&#35757;&#32451;&#30340;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#25928;&#26524;&#36739;&#24369;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;CamemBERT&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamemBERT-bio&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#20026;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20844;&#20849;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.16130</link><description>&lt;p&gt;
&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#26377;&#26102;&#20505;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35745;&#31639;&#26426;&#21046;&#26469;&#35299;&#20915;&#19968;&#23545;&#19968;&#30340;&#20851;&#31995;&#20219;&#21153;&#65288;&#20363;&#22914; capital_of(Poland)=Warsaw&#65289;&#12290;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#20174;124M&#21442;&#25968;&#21040;176B&#21442;&#25968;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#22810;&#31181;&#20219;&#21153;&#65288;&#28041;&#21450;&#39318;&#37117;&#12289;&#22823;&#20889;&#21644;&#36807;&#21435;&#26102;&#24577;&#31561;&#65289;&#65292;&#26426;&#21046;&#30340;&#20851;&#38190;&#37096;&#20998;&#21487;&#20197;&#31616;&#21270;&#20026;&#21069;&#39304;&#65288;FFN&#65289;&#32593;&#32476;&#36890;&#24120;&#24212;&#29992;&#30340;&#31616;&#21333;&#32447;&#24615;&#26356;&#26032;&#12290;&#36825;&#20123;&#26356;&#26032;&#20063;&#20542;&#21521;&#20110;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#65288;&#20363;&#22914;&#23545;&#32534;&#30721; Poland:Warsaw::China:Beijing&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#21487;&#39044;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26174;&#31034;&#36825;&#20010;&#26426;&#21046;&#26159;&#29305;&#23450;&#20110;&#38656;&#35201;&#20174;&#39044;&#35757;&#32451;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#32780;&#19981;&#26159;&#20174;&#23616;&#37096;&#19978;&#19979;&#25991;&#26816;&#32034;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.14710</link><description>&lt;p&gt;
&#35757;&#32451;&#25351;&#20196;&#20316;&#20026;&#21518;&#38376;: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35813;&#22521;&#35757;&#33539;&#20363;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#25968;&#25454;&#20013;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#65292;&#20415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#26469;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#29978;&#33267;&#26080;&#38656;&#20462;&#25913;&#25968;&#25454;&#23454;&#20363;&#25110;&#26631;&#31614;&#26412;&#36523;&#12290;&#36890;&#36807;&#36825;&#31181;&#25351;&#20196;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22235;&#20010;&#24120;&#29992;&#30340; NLP &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;90% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#24341;&#36215;&#26131;&#20110;&#36716;&#31227;&#21040; 15 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25345;&#20037;&#21518;&#38376;&#12290;&#36825;&#31181;&#25915;&#20987;&#36824;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26377;&#27602;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#35813;&#25915;&#20987;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#25512;&#29702;&#26102;&#38450;&#24481;&#30340;&#25269;&#25239;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#38656;&#35201;&#26356;&#20026;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
&lt;/p&gt;</description></item></channel></rss>