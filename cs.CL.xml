<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#27969;&#34892;&#30149;&#30456;&#20851;&#20107;&#20214;&#30340;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#20107;&#20214;&#26816;&#27979;&#25216;&#26415;&#22312;COVID-19&#27969;&#34892;&#30149;&#21644;&#20854;&#20182;&#26410;&#35265;&#27969;&#34892;&#30149;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#39044;&#35686;&#12290;</title><link>https://arxiv.org/abs/2404.01679</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20107;&#20214;&#26816;&#27979;&#29992;&#20110;&#30123;&#24773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Event Detection from Social Media for Epidemic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01679
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#27969;&#34892;&#30149;&#30456;&#20851;&#20107;&#20214;&#30340;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#20107;&#20214;&#26816;&#27979;&#25216;&#26415;&#22312;COVID-19&#27969;&#34892;&#30149;&#21644;&#20854;&#20182;&#26410;&#35265;&#27969;&#34892;&#30149;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#39044;&#35686;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26159;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#26377;&#20851;&#31038;&#20250;&#36235;&#21183;&#21644;&#20107;&#20214;&#30340;&#21450;&#26102;&#26356;&#26032;&#12290;&#20851;&#20110;&#27969;&#34892;&#30149;&#30456;&#20851;&#20107;&#20214;&#30340;&#35752;&#35770;&#65292;&#22914;&#24863;&#26579;&#12289;&#30151;&#29366;&#21644;&#31038;&#20250;&#20114;&#21160;&#65292;&#23545;&#20110;&#22312;&#27969;&#34892;&#30149;&#29190;&#21457;&#26399;&#38388;&#21046;&#23450;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21019;&#21033;&#29992;&#20107;&#20214;&#26816;&#27979;&#65288;ED&#65289;&#26469;&#26356;&#22909;&#22320;&#20934;&#22791;&#21644;&#25552;&#21069;&#39044;&#35686;&#20219;&#20309;&#21363;&#23558;&#21040;&#26469;&#30340;&#27969;&#34892;&#30149;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#19982;&#27969;&#34892;&#30149;&#30456;&#20851;&#30340;&#20107;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#25324;&#19971;&#31181;&#19982;&#30142;&#30149;&#26080;&#20851;&#30340;&#20107;&#20214;&#31867;&#22411;&#30340;&#27969;&#34892;&#30149;&#20107;&#20214;&#26412;&#20307;&#35770;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SPEED&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#24037;&#27880;&#37322;&#30340;&#37325;&#28857;&#20851;&#27880;COVID-19&#27969;&#34892;&#30149;&#30340;&#20107;&#20214;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#22312;COVID&#36895;&#24230;&#19978;&#35757;&#32451;&#30340;ED&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#20851;&#20110;&#19977;&#31181;&#26410;&#35265;&#27969;&#34892;&#30149;&#65288;&#29492;&#30168;&#12289;&#23528;&#21345;&#30149;&#27602;&#21644;&#30331;&#38761;&#28909;&#65289;&#30340;&#27969;&#34892;&#30149;&#20107;&#20214;&#65307;&#32780;&#22312;&#29616;&#26377;ED&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21017;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25253;&#36947;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01679v1 Announce Type: new  Abstract: Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MoE&#27169;&#22411;\tool&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;MoE&#23618;&#35745;&#31639;&#36127;&#36733;50%&#20197;&#19978;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18926</link><description>&lt;p&gt;
&#29992;&#26356;&#31232;&#30095;&#30340;&#36873;&#25321;&#25552;&#39640;&#31232;&#30095;&#27169;&#22411;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency in Sparse Models with Sparser Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MoE&#27169;&#22411;\tool&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;MoE&#23618;&#35745;&#31639;&#36127;&#36733;50%&#20197;&#19978;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#65292;&#21253;&#25324;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#65292;&#24050;&#32463;&#25104;&#20026;&#32553;&#25918;Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#23384;&#22312;&#35745;&#31639;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#22823;&#37327;&#21442;&#25968;&#36890;&#36807;&#23558;&#20540;&#20056;&#20197;&#38646;&#25110;&#20302;&#28608;&#27963;&#20540;&#26080;&#35859;&#21442;&#19982;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\tool &#30340;&#26032;&#39062;MoE&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#21319;&#31232;&#30095;MoE&#27169;&#22411;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290; \tool &#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;\tool &#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;MoE&#23618;&#30340;&#35745;&#31639;&#36127;&#36733;&#20943;&#23569;50\%&#20197;&#19978;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;\tool &#30340;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#23454;&#29616;&#31232;&#30095;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \tool can enhance model performance while decreasing the computation load at MoE layers by over 50\% without sacrificing performance. Furthermore, we present the versatility of \tool by applying it to dense models, enabling sparse computation during inference. We pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.06642</link><description>&lt;p&gt;
KELLMRec: &#30693;&#35782;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06642
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#34917;&#20805;&#20027;&#27969;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#30340;&#32570;&#22833;&#37096;&#20998;&#12290;&#38543;&#30528;LLM&#30340;&#20852;&#36215;&#65292;&#23427;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20351;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#25104;&#20026;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26159;&#19981;&#21487;&#38752;&#21644;&#27425;&#20248;&#30340;&#65292;&#30001;&#20110;&#23384;&#22312;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#12290;&#21463;&#20197;&#19978;&#21160;&#26426;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;LLMRec&#26041;&#27861;&#12290;&#38500;&#20102;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#20225;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.18945</link><description>&lt;p&gt;
Syntactic Ghost&#65306;&#19968;&#31181;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#26080;&#24863;&#30693;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18945
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#28431;&#27934;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#21518;&#38376;&#25915;&#20987;&#37319;&#29992;&#26126;&#26174;&#30340;&#35302;&#21457;&#22120;&#65292;&#22312;&#25163;&#21160;&#23545;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#22240;&#27492;&#22312;&#25928;&#26524;&#12289;&#38544;&#21311;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#26399;&#26395;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21487;&#35265;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#65292;&#31216;&#20026;Syntactic Ghost&#65288;&#31616;&#31216;&#20026;synGhost&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#25932;&#24847;&#22320;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#39044;&#23450;&#20041;&#21477;&#27861;&#32467;&#26500;&#30340;&#27602;&#23475;&#26679;&#26412;&#20316;&#20026;&#38544;&#34109;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#23558;&#21518;&#38376;&#26893;&#20837;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#30693;&#35782;&#12290;&#27602;&#23475;&#26679;&#26412;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23613;&#21487;&#33021;&#22343;&#21248;&#22320;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#24418;&#25104;&#24191;&#27867;&#30340;&#21518;&#38376;&#12290;&#27492;&#22806;&#65292;&#22312;&#20142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.18815</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Handle Multilingualism?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18815
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;LLMs&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#36807;&#31243;&#65306;&#22312;&#21069;&#20960;&#23618;&#20013;&#65292;LLMs&#29702;&#35299;&#38382;&#39064;&#65292;&#23558;&#22810;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#20197;&#20415;&#20419;&#36827;&#20219;&#21153;&#35299;&#20915;&#38454;&#27573;&#12290;&#22312;&#20013;&#38388;&#23618;&#20013;&#65292;LLMs&#36890;&#36807;&#20197;&#33521;&#35821;&#24605;&#32771;&#24182;&#25972;&#21512;&#22810;&#35821;&#35328;&#30693;&#35782;&#26469;&#36827;&#34892;&#35299;&#20915;&#38382;&#39064;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32467;&#26500;&#65292;&#20998;&#21035;&#33719;&#21462;&#20107;&#23454;&#20869;&#23481;&#12290;&#22312;&#26368;&#21518;&#20960;&#23618;&#20013;&#65292;LLMs&#29983;&#25104;&#19982;&#26597;&#35810;&#30340;&#21407;&#22987;&#35821;&#35328;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#29305;&#23450;&#35821;&#35328;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#30001;&#36755;&#20837;&#35821;&#35328;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#31614;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.18137</link><description>&lt;p&gt;
DecisionNCE: &#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#22823;&#30446;&#26631;&#65306;1&#65289;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65307;2&#65289;&#24378;&#21270;&#35270;&#35273;&#34920;&#31034;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65307;3&#65289;&#25429;&#33719;&#36712;&#36857;&#32423;&#35821;&#35328;&#22522;&#30784;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#22823;&#37096;&#20998;&#24050;&#26377;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32479;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35821;&#35328;&#25351;&#20196;&#26080;&#32541;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#65292;&#22312;&#35270;&#35273;&#36712;&#36857;&#19982;&#20854;&#23545;&#24212;&#30340;&#35821;&#35328;&#25351;&#20196;&#30456;&#27604;&#19981;&#21305;&#37197;&#23545;&#26356;&#22909;&#22320;&#23545;&#40784;&#26102;&#65292;&#27969;&#34892;&#30340; Bradley-Terry &#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#37325;&#26032;&#21442;&#25968;&#21270;&#32780;&#21464;&#20026;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340; DecisionNCE &#26694;&#26550;&#65292;&#31867;&#20284;&#20110; InfoNC
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#36807;&#31243;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17946</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gradient-Free Adaptive Global Pruning for Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#36807;&#31243;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;LLaMA&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#21463;&#21040;&#23427;&#20204;&#35745;&#31639;&#38656;&#27714;&#36807;&#39640;&#30340;&#38480;&#21046;&#12290;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#21387;&#32553;&#31574;&#30053;&#20986;&#29616;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#20197;&#22686;&#24378;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20840;&#23616;&#21098;&#26525;&#23545;LLMs&#26469;&#35828;&#30001;&#20110;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#32780;&#19981;&#23454;&#29992;&#65292;&#32780;&#26412;&#22320;&#21098;&#26525;&#65292;&#23613;&#31649;&#25928;&#29575;&#39640;&#65292;&#21364;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20840;&#23616;&#21098;&#26525;&#65288;AdaGP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#23450;&#20041;&#20840;&#23616;&#21098;&#26525;&#22788;&#29702;&#20026;&#21487;&#31649;&#29702;&#30340;&#21327;&#35843;&#23376;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#26377;&#25928;&#30340;&#20840;&#23616;&#26368;&#20248;&#21270;&#20248;&#21270;&#12290;AdaGP&#30340;&#26041;&#27861;&#23558;LLMs&#27010;&#24565;&#21270;&#20026;&#19968;&#31995;&#21015;&#27169;&#22359;&#21270;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#21464;&#37327;&#36827;&#34892;&#38382;&#39064;&#20998;&#35299;&#65292;&#19981;&#20165;&#20415;&#20110;&#22312;LLMs&#19978;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#65292;&#32780;&#19988;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17946v1 Announce Type: new  Abstract: The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, part
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DATG&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21160;&#24577;&#23646;&#24615;&#22270;&#35843;&#33410;&#20851;&#38190;&#23646;&#24615;&#35789;&#21644;&#20851;&#38190;&#21453;&#23646;&#24615;&#35789;&#30340;&#21457;&#29983;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23646;&#24615;&#25511;&#21046;&#65292;&#22312;&#27602;&#24615;&#32531;&#35299;&#21644;&#24773;&#24863;&#36716;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;19.29%&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.11218</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#23646;&#24615;&#22270;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DATG&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21160;&#24577;&#23646;&#24615;&#22270;&#35843;&#33410;&#20851;&#38190;&#23646;&#24615;&#35789;&#21644;&#20851;&#38190;&#21453;&#23646;&#24615;&#35789;&#30340;&#21457;&#29983;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23646;&#24615;&#25511;&#21046;&#65292;&#22312;&#27602;&#24615;&#32531;&#35299;&#21644;&#24773;&#24863;&#36716;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;19.29%&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#25152;&#38656;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25554;&#25300;&#30340;CTG&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21517;&#20026;&#22522;&#20110;&#21160;&#24577;&#23646;&#24615;&#22270;&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#65288;DATG&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23646;&#24615;&#35780;&#20998;&#22120;&#35780;&#20272;&#30001;LLMs&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#23646;&#24615;&#65292;&#24182;&#26500;&#24314;&#21160;&#24577;&#23646;&#24615;&#22270;&#12290;DATG&#35843;&#33410;&#20851;&#38190;&#23646;&#24615;&#35789;&#21644;&#20851;&#38190;&#21453;&#23646;&#24615;&#35789;&#30340;&#21457;&#29983;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23646;&#24615;&#25511;&#21046;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#36328;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65306;&#27602;&#24615;&#32531;&#35299;&#21644;&#24773;&#24863;&#36716;&#21270;&#65292;&#21033;&#29992;&#20116;&#20010;LLMs&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#22312;&#25511;&#21046;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26174;&#30528;&#25552;&#39640;&#65292;&#23454;&#29616;&#20102;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#26368;&#26377;&#21033;&#30340;&#20219;&#21153;&#20013;&#23545;&#22522;&#32447;&#26041;&#27861;&#30340;&#23792;&#20540;&#25913;&#36827;&#20026;19.29&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26174;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11218v1 Announce Type: new  Abstract: Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#35299;&#30721;&#36807;&#31243;&#32780;&#19981;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10200</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#25552;&#31034;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning Without Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#35299;&#30721;&#36807;&#31243;&#32780;&#19981;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#22823;&#37327;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;LLMs&#26159;&#21542;&#21487;&#20197;&#22312;&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#25512;&#29702;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#21464;&#35299;&#30721;&#36807;&#31243;&#65292;&#23601;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#24341;&#20986;&#24605;&#32500;&#38142;&#25512;&#29702;&#36335;&#24452;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21069;$k$&#20010;&#26367;&#20195;&#26631;&#35760;&#65292;&#21457;&#29616;&#36825;&#20123;&#24207;&#21015;&#20013;&#32463;&#24120;&#23384;&#22312;&#24605;&#32500;&#38142;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32469;&#36807;&#20102;&#25552;&#31034;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#35780;&#20272;LLMs&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35299;&#30721;&#36335;&#24452;&#20013;&#30340;&#24605;&#32500;&#38142;&#23384;&#22312;&#19982;&#27169;&#22411;&#35299;&#30721;&#30340;&#32622;&#20449;&#24230;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10200v1 Announce Type: new  Abstract: In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decod
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#28304;&#25968;&#23398;&#25512;&#29702;LLMs InternLM-Math&#65292;&#35813;&#27169;&#22411;&#20197;&#20854;&#25968;&#23398;&#33021;&#21147;&#20195;&#34920;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#25972;&#21512;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#22870;&#21169;&#24314;&#27169;&#12289;&#24418;&#24335;&#25512;&#29702;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25968;&#23398;&#25512;&#29702;&#22120;&#12289;&#39564;&#35777;&#22120;&#12289;&#35777;&#26126;&#22120;&#21644;&#22686;&#24378;&#22120;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21253;&#25324;GSM8K&#12289;MATH&#12289;&#21256;&#29273;&#21033;&#25968;&#23398;&#32771;&#35797;&#12289;MathBench-ZH&#21644;MiniF2F&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#20195;&#30721;&#36741;&#21161;&#25512;&#29702;&#30340;&#35774;&#32622;&#19979;&#65292;InternLM-Math&#21462;&#24471;&#20102;&#24320;&#28304;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;MiniF2F&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;30.3&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;LEAN&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06332</link><description>&lt;p&gt;
InternLM-Math&#65306;&#38754;&#21521;&#21487;&#39564;&#35777;&#25512;&#29702;&#30340;&#24320;&#25918;&#25968;&#23398;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06332
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#28304;&#25968;&#23398;&#25512;&#29702;LLMs InternLM-Math&#65292;&#35813;&#27169;&#22411;&#20197;&#20854;&#25968;&#23398;&#33021;&#21147;&#20195;&#34920;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#25972;&#21512;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#22870;&#21169;&#24314;&#27169;&#12289;&#24418;&#24335;&#25512;&#29702;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25968;&#23398;&#25512;&#29702;&#22120;&#12289;&#39564;&#35777;&#22120;&#12289;&#35777;&#26126;&#22120;&#21644;&#22686;&#24378;&#22120;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21253;&#25324;GSM8K&#12289;MATH&#12289;&#21256;&#29273;&#21033;&#25968;&#23398;&#32771;&#35797;&#12289;MathBench-ZH&#21644;MiniF2F&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#20195;&#30721;&#36741;&#21161;&#25512;&#29702;&#30340;&#35774;&#32622;&#19979;&#65292;InternLM-Math&#21462;&#24471;&#20102;&#24320;&#28304;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;MiniF2F&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;30.3&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;LEAN&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#33021;&#21147;&#21487;&#20197;&#34920;&#31034;&#20854;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#24320;&#28304;&#25105;&#20204;&#30340;&#25968;&#23398;&#25512;&#29702;LLMs InternLM-Math&#65292;&#35813;&#27169;&#22411;&#26159;&#20174;InternLM2&#32487;&#32493;&#39044;&#35757;&#32451;&#32780;&#26469;&#12290;&#25105;&#20204;&#20197;&#32479;&#19968;&#30340;seq2seq&#26684;&#24335;&#32479;&#19968;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#22870;&#21169;&#24314;&#27169;&#12289;&#24418;&#24335;&#25512;&#29702;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#30417;&#30563;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25968;&#23398;&#25512;&#29702;&#22120;&#12289;&#39564;&#35777;&#22120;&#12289;&#35777;&#26126;&#22120;&#21644;&#22686;&#24378;&#22120;&#12290;&#36825;&#20123;&#33021;&#21147;&#21487;&#29992;&#20110;&#24320;&#21457;&#19979;&#19968;&#20195;&#25968;&#23398;LLMs&#25110;&#33258;&#36523;&#36845;&#20195;&#12290;&#22312;&#21253;&#25324;GSM8K&#12289;MATH&#12289;&#21256;&#29273;&#21033;&#25968;&#23398;&#32771;&#35797;&#12289;MathBench-ZH&#21644;MiniF2F&#22312;&#20869;&#30340;&#21508;&#31181;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;InternLM-Math&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#20195;&#30721;&#36741;&#21161;&#25512;&#29702;&#30340;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#24320;&#28304;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26080;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;MiniF2F&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;30.3&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;LEAN&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learnin
&lt;/p&gt;</description></item><item><title>&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03191</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#65292;&#32858;&#31867;&#21644;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Isotropy, Clusters, and Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03191
&lt;/p&gt;
&lt;p&gt;
&#21516;&#24615;&#36136;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#32858;&#31867;&#21644;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#19968;&#20107;&#23454;&#24471;&#21040;&#20102;&#26412;&#25991;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#26377;&#25152;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#22343;&#21248;&#21033;&#29992;&#25152;&#26377;&#32500;&#24230;&#65288;&#21363;&#26159;&#21542;&#20855;&#26377;&#21516;&#24615;&#36136;&#65289;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#35752;&#35770;&#12290;&#26377;&#35777;&#25454;&#25903;&#25345;&#21644;&#21453;&#23545;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#26045;&#21516;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#21516;&#24615;&#36136;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#35201;&#27714;&#19982;&#32858;&#31867;&#30340;&#23384;&#22312;&#19981;&#20860;&#23481;&#65292;&#36825;&#20063;&#23545;&#32447;&#24615;&#20998;&#31867;&#30446;&#26631;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#20107;&#23454;&#65292;&#24182;&#29992;&#23427;&#26469;&#38416;&#26126;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02992</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decoding-time Realignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#21644;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#22312;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#21644;&#40723;&#21169;&#20445;&#25345;&#19982;&#26410;&#23545;&#40784;&#27169;&#22411;&#25509;&#36817;&#30340;&#25509;&#36817;&#24615;&#35268;&#21017;&#39033;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#26435;&#34913;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#35268;&#21017;&#21270;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#65306;&#35268;&#21017;&#21270;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#22870;&#21169;&#27450;&#39575;&#32780;&#38477;&#20302;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#36807;&#24230;&#35268;&#21017;&#21270;&#21017;&#38459;&#30861;&#23545;&#40784;&#12290;&#20256;&#32479;&#26041;&#27861;&#25214;&#21040;&#26368;&#20339;&#35268;&#21017;&#21270;&#27700;&#24179;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#35268;&#21017;&#21270;&#24378;&#24230;&#37325;&#26032;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#12290;DeRa&#21487;&#20197;&#23545;&#23545;&#40784;&#27169;&#22411;&#30340;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02130</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#28210;&#26579;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#32467;&#26500;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#26684;&#24335;&#30340;&#22270;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#35270;&#35273;&#27169;&#24577;&#65292;&#32780;&#35270;&#35273;&#26159;&#20154;&#31867;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#21644;&#36827;&#34892;&#22270;&#25512;&#29702;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#23558;&#22270;&#32467;&#26500;&#34920;&#31034;&#20026;&#35270;&#35273;&#22270;&#20687;(&#21363;&#35270;&#35273;&#22270;)&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#33021;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#39318;&#27425;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#26159;&#19968;&#20010;&#20803;&#32452;(&#22270;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#25551;&#36848;)&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#22312;GITQA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#22312;LLaVA-7B/13B&#27169;&#22411;&#30340;&#24494;&#35843;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2312.09818</link><description>&lt;p&gt;
SMILE&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#31038;&#20132;&#26234;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20854;&#20013;&#65292;&#31505;&#22768;&#26159;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#20013;&#21457;&#29983;&#30340;&#29420;&#29305;&#34920;&#36798;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#20102;&#26426;&#22120;&#29702;&#35299;&#35270;&#39057;&#20013;&#31505;&#22768;&#32972;&#21518;&#29702;&#30001;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#19968;&#26032;&#20219;&#21153;&#65292;&#35299;&#37322;&#20154;&#20204;&#22312;&#29305;&#23450;&#35270;&#39057;&#20013;&#20026;&#20160;&#20040;&#20250;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;SMILE&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#32447;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#29983;&#25104;&#21487;&#20449;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#25506;&#27979;&#20854;&#20182;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#21644;&#37326;&#22806;&#35270;&#39057;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#33021;&#21147;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#22522;&#20934;&#35780;&#20272;&#36890;&#24120;&#37319;&#29992;&#21463;&#38480;&#21046;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#26080;&#27861;&#28385;&#36275;&#30495;&#23454;&#38656;&#27714;&#20013;&#30340;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2311.15296</link><description>&lt;p&gt;
UHGEval&#65306;&#36890;&#36807;&#26080;&#32422;&#26463;&#29983;&#25104;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#26500;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15296
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#33021;&#21147;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#22522;&#20934;&#35780;&#20272;&#36890;&#24120;&#37319;&#29992;&#21463;&#38480;&#21046;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#26080;&#27861;&#28385;&#36275;&#30495;&#23454;&#38656;&#27714;&#20013;&#30340;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#32773;&#65292;&#24182;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#35268;&#27169;&#27010;&#29575;&#32479;&#35745;&#27169;&#22411;&#30446;&#21069;&#26080;&#27861;&#20445;&#35777;&#22312;&#19987;&#19994;&#20869;&#23481;&#29983;&#25104;&#20013;&#30340;&#24517;&#35201;&#36136;&#37327;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#20135;&#29983;&#34394;&#26500;&#30340;&#25991;&#26412;&#65292;&#24433;&#21709;&#23427;&#20204;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#30495;&#23454;&#21487;&#38752;&#24615;&#65292;&#35768;&#22810;&#20513;&#35758;&#24050;&#24320;&#21457;&#20102;&#29992;&#20110;&#34394;&#26500;&#29616;&#35937;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#36890;&#24120;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#22240;&#20026;&#25104;&#26412;&#21644;&#26102;&#38388;&#38480;&#21046;&#12290;&#36825;&#20123;&#25216;&#26415;&#21253;&#25324;&#20351;&#29992;&#23450;&#21521;&#24187;&#35273;&#35825;&#23548;&#21644;&#25925;&#24847;&#25913;&#21464;&#30495;&#23454;&#25991;&#26412;&#20197;&#20135;&#29983;&#24187;&#35273;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#23454;&#38469;&#38656;&#27714;&#30340;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15296v2 Announce Type: replace  Abstract: Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by rea
&lt;/p&gt;</description></item><item><title>StrategyLLM&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#65292;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2311.08803</link><description>&lt;p&gt;
StrategyLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#12289;&#25191;&#34892;&#22120;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08803
&lt;/p&gt;
&lt;p&gt;
StrategyLLM&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#65292;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24605;&#32500;&#38142; (CoT) &#25552;&#31034;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#24773;&#20917;&#65292;&#24182;&#32570;&#20047;&#22312;&#25512;&#29702;&#27493;&#39588;&#20013;&#30340;&#20219;&#21153;&#32423;&#19968;&#33268;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;StrategyLLM&#65292;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;StrategyLLM &#20351;&#29992;&#22235;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65306;&#31574;&#30053;&#29983;&#25104;&#22120;&#12289;&#25191;&#34892;&#22120;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#20849;&#21516;&#24037;&#20316;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#36873;&#25321;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;13&#20010;&#25968;&#25454;&#38598;&#19978;&#36328;4&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#65292;StrategyLLM &#22312;&#25968;&#23398;&#25512;&#29702;&#65288;34.21%-&gt;38.79%&#65289;&#12289;&#24120;&#35265;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;CoT-SC&#65292;&#35813;&#22522;&#32447;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08803v2 Announce Type: replace  Abstract: Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\rightarrow$ 38.79%), commonse
&lt;/p&gt;</description></item><item><title>&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08154</link><description>&lt;p&gt;
&#20877;&#38382;&#19968;&#27425;&#65306;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08154
&lt;/p&gt;
&lt;p&gt;
&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;CoT&#25552;&#31034;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#36138;&#23146;&#35299;&#30721;&#20250;&#23548;&#33268;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#38598;&#25104;&#20248;&#21270;&#23581;&#35797;&#33719;&#24471;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20197;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#35201;&#20040;&#31616;&#21333;&#22320;&#37319;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#21518;&#22788;&#29702;&#65292;&#27604;&#22914;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#35201;&#20040;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#20960;&#20010;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#31867;&#27880;&#37322;&#30340;&#38468;&#21152;&#27169;&#22411;&#26469;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20013;&#36873;&#25321;&#26368;&#20339;&#36335;&#24452;&#65292;&#20294;&#26410;&#33021;&#25512;&#24191;&#21040;&#29616;&#23454;&#35774;&#32622;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#26410;&#30693;&#25110;&#25512;&#29702;&#36335;&#24452;&#30340;&#31572;&#26696;&#26684;&#24335;&#26410;&#30693;&#12290;&#20026;&#20102;&#36991;&#20813;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#36866;&#29992;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#30340;&#31867;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#26032;&#30340;&#26631;&#27880;&#25351;&#21335;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#34920;&#26126;&#22810;&#35821;&#22659;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2305.18034</link><description>&lt;p&gt;
&#29992;&#20110;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A Corpus for Sentence-level Subjectivity Detection on English News Articles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#26032;&#30340;&#26631;&#27880;&#25351;&#21335;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#34920;&#26126;&#22810;&#35821;&#22659;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21046;&#23450;&#20102;&#29992;&#20110;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#26032;&#39062;&#26631;&#27880;&#25351;&#21335;&#65292;&#19981;&#23616;&#38480;&#20110;&#29305;&#23450;&#35821;&#35328;&#30340;&#32447;&#32034;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#21335;&#25910;&#38598;&#20102;NewsSD-ENG&#65292;&#36825;&#26159;&#20174;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#30340;638&#20010;&#23458;&#35266;&#21477;&#23376;&#21644;411&#20010;&#20027;&#35266;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#20026;&#33521;&#35821;&#21450;&#20854;&#20182;&#35821;&#35328;&#30340;&#20027;&#35266;&#24615;&#26816;&#27979;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#35789;&#20856;&#25110;&#26426;&#22120;&#32763;&#35793;&#31561;&#29305;&#23450;&#35821;&#35328;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#27880;&#37322;&#20102;&#29616;&#26377;&#30340;&#24847;&#22823;&#21033;&#35821;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#22659;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18034v2 Announce Type: replace  Abstract: We develop novel annotation guidelines for sentence-level subjectivity detection, which are not limited to language-specific cues. We use our guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective sentences extracted from English news articles on controversial topics. Our corpus paves the way for subjectivity detection in English and across other languages without relying on language-specific tools, such as lexicons or machine translation. We evaluate state-of-the-art multilingual transformer-based models on the task in mono-, multi-, and cross-language settings. For this purpose, we re-annotate an existing Italian corpus. We observe that models trained in the multilingual setting achieve the best performance on the task.
&lt;/p&gt;</description></item><item><title>AgentCoder&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;Agent&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#27979;&#35797;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31243;&#24207;&#21592;Agent&#12289;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#21644;&#27979;&#35797;&#25191;&#34892;Agent&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#24179;&#34913;&#20195;&#30721;&#29983;&#25104;&#21644;&#26377;&#25928;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#19982;&#25191;&#34892;&#26041;&#38754;&#30340;&#25361;&#25112;&#20013;&#30340;&#31361;&#30772;&#12290;</title><link>http://arxiv.org/abs/2312.13010</link><description>&lt;p&gt;
AgentCoder: &#22522;&#20110;&#22810;Agent&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#36845;&#20195;&#27979;&#35797;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. (arXiv:2312.13010v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13010
&lt;/p&gt;
&lt;p&gt;
AgentCoder&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;Agent&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#27979;&#35797;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31243;&#24207;&#21592;Agent&#12289;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#21644;&#27979;&#35797;&#25191;&#34892;Agent&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#24179;&#34913;&#20195;&#30721;&#29983;&#25104;&#21644;&#26377;&#25928;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#19982;&#25191;&#34892;&#26041;&#38754;&#30340;&#25361;&#25112;&#20013;&#30340;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#21457;&#23637;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21457;&#23637;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#36719;&#20214;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#24179;&#34913;&#20195;&#30721;&#27573;&#30340;&#29983;&#25104;&#19982;&#26377;&#25928;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#21644;&#25191;&#34892;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#22810;Agent&#21161;&#25163;&#20195;&#30721;&#29983;&#25104;(AgentCoder)&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19968;&#20010;&#22810;Agent&#26694;&#26550;&#21644;&#19987;&#38376;&#30340;Agent&#65306;&#31243;&#24207;&#21592;Agent&#12289;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#21644;&#27979;&#35797;&#25191;&#34892;Agent&#12290;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#65292;&#31243;&#24207;&#21592;Agent&#23558;&#26681;&#25454;&#27979;&#35797;&#25191;&#34892;Agent&#30340;&#21453;&#39304;&#37325;&#28857;&#20851;&#27880;&#20195;&#30721;&#30340;&#29983;&#25104;&#21644;&#25913;&#36827;&#12290;&#27979;&#35797;&#35774;&#35745;&#24072;Agent&#23558;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#65292;&#27979;&#35797;&#25191;&#34892;Agent&#23558;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#36816;&#34892;&#20195;&#30721;&#24182;&#23558;&#21453;&#39304;&#20889;&#20837;&#21040;&#32534;&#31243;&#32773;
&lt;/p&gt;
&lt;p&gt;
The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the prog
&lt;/p&gt;</description></item><item><title>JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.02953</link><description>&lt;p&gt;
JsonTuning&#65306;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02953
&lt;/p&gt;
&lt;p&gt;
JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;-&#25991;&#26412;&#25351;&#20196;&#35843;&#20248;&#65288;TextTuning&#65289;&#26041;&#27861;&#30001;&#20110;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#21644;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#26500;&#32780;&#23384;&#22312;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JsonTuning&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21040;&#32467;&#26500;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#22810;&#21151;&#33021;&#21644;&#32467;&#26500;&#21270;&#29305;&#24615;&#26469;&#34920;&#31034;&#20219;&#21153;&#65292;JsonTuning&#36890;&#36807;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20851;&#38190;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27495;&#20041;&#24615;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23545;&#36755;&#20986;&#30340;&#26174;&#24335;&#25511;&#21046;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JsonTuning&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;TextTuning&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
&lt;/p&gt;</description></item><item><title>MindDial&#26159;&#19968;&#20010;&#20351;&#29992;&#24515;&#26234;&#27169;&#25311;&#36827;&#34892;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#30340;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22330;&#26223;&#21270;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.15253</link><description>&lt;p&gt;
MindDial: &#24102;&#26377;&#24515;&#26234;&#27169;&#25311;&#30340;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#29992;&#20110;&#22330;&#26223;&#21270;&#31070;&#32463;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15253
&lt;/p&gt;
&lt;p&gt;
MindDial&#26159;&#19968;&#20010;&#20351;&#29992;&#24515;&#26234;&#27169;&#25311;&#36827;&#34892;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#30340;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22330;&#26223;&#21270;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#20132;&#27969;&#20013;&#33258;&#30001;&#34920;&#36798;&#24847;&#20041;&#25110;&#20849;&#35782;&#30340;&#21516;&#26102;&#36827;&#34892;&#23545;&#35805;&#12290;&#23613;&#31649;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#26410;&#32771;&#34385;&#21040;&#20849;&#20139;&#30340;&#22330;&#26223;&#29615;&#22659;&#20013;&#20010;&#20307;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MindDial&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22330;&#26223;&#21270;&#30340;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#24515;&#26234;&#27169;&#22359;&#65292;&#21487;&#20197;&#36861;&#36394;&#19977;&#20010;&#23618;&#27425;&#30340;&#20449;&#24565;&#65292;&#21363;&#35828;&#35805;&#32773;&#30340;&#20449;&#24565;&#12289;&#35828;&#35805;&#32773;&#23545;&#21548;&#20247;&#20449;&#24565;&#30340;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#21069;&#20004;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#20449;&#24565;&#12290;&#28982;&#21518;&#65292;&#35828;&#35805;&#34892;&#20026;&#20998;&#31867;&#22836;&#23558;&#20915;&#23450;&#26159;&#21542;&#32487;&#32493;&#23545;&#35805;&#12289;&#32467;&#26463;&#27492;&#36718;&#23545;&#35805;&#25110;&#37319;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20849;&#35782;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;MutualFriend&#65292;&#22686;&#21152;&#20102;&#20449;&#24565;&#21160;&#24577;&#27880;&#37322;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#30001;&#23545;&#35805;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#26379;&#21451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24515;&#26234;&#29366;&#24577;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans talk in free-form while negotiating the expressed meanings or common ground. Despite the impressive conversational abilities of the large generative language models, they do not consider the individual differences in contextual understanding in a shared situated environment. In this work, we propose MindDial, a novel conversational framework that can generate situated free-form responses to negotiate common ground. We design an explicit mind module that can track three-level beliefs -- the speaker's belief, the speaker's prediction of the listener's belief, and the common belief based on the gap between the first two. Then the speaking act classification head will decide to continue to talk, end this turn, or take task-related action. We augment a common ground alignment dataset MutualFriend with belief dynamics annotation, of which the goal is to find a single mutual friend based on the free chat between two agents. Experiments show that our model with mental state modeling can
&lt;/p&gt;</description></item><item><title>PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.05087</link><description>&lt;p&gt;
PandaLM&#65306;LLM&#25351;&#20196;&#35843;&#20248;&#20248;&#21270;&#30340;&#33258;&#21160;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05087
&lt;/p&gt;
&lt;p&gt;
PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#22797;&#26434;&#24615;&#21644;&#35780;&#20272;&#35843;&#25972;&#27169;&#22411;&#30340;&#22256;&#38590;&#24615;&#65292;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25351;&#20196;&#35843;&#20248;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#30830;&#23450;&#26368;&#20339;&#36229;&#21442;&#25968;&#65292;&#38656;&#35201;&#19968;&#20010;&#33258;&#21160;&#30340;&#12289;&#24378;&#22823;&#19988;&#21487;&#38752;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#65292;&#24314;&#31435;&#36825;&#26679;&#19968;&#20010;&#22522;&#20934;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#27454;&#21517;&#20026;PandaLM&#30340;&#35780;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#21306;&#20998;&#20986;&#22810;&#20010;LLM&#20013;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;PandaLM&#30340;&#20851;&#27880;&#28857;&#19981;&#20165;&#38480;&#20110;&#20256;&#32479;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#23458;&#35266;&#27491;&#30830;&#24615;&#65292;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#30456;&#23545;&#31616;&#27905;&#24615;&#12289;&#28165;&#26224;&#24230;&#12289;&#36981;&#24490;&#35828;&#26126;&#12289;&#20840;&#38754;&#24615;&#21644;&#24418;&#24335;&#24615;&#31561;&#37325;&#35201;&#20027;&#35266;&#22240;&#32032;&#12290;&#20026;&#30830;&#20445;PandaLM&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#20154;&#24037;&#27880;&#37322;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25152;&#26377;&#19978;&#19979;&#25991;&#37117;&#26159;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated
&lt;/p&gt;</description></item><item><title>Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09993</link><description>&lt;p&gt;
Reprompting: &#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#33258;&#21160;&#25512;&#26029;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09993
&lt;/p&gt;
&lt;p&gt;
Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Reprompting&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#32473;&#23450;&#20219;&#21153;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#25105;&#20204;&#25512;&#26029;&#36866;&#29992;&#20110;&#19968;&#32452;&#35757;&#32451;&#26679;&#20363;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#37319;&#26679;&#30340;&#35299;&#20316;&#20026;&#29238;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#37319;&#26679;&#26032;&#30340;&#37197;&#26041;&#26469;&#35299;&#20915;&#20854;&#20182;&#35757;&#32451;&#38382;&#39064;&#12290;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#20116;&#20010;Big-Bench Hard&#20219;&#21153;&#20013;&#65292;Reprompting&#30340;&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#22522;&#32447;&#12290;Reprompting&#36824;&#21487;&#20197;&#20419;&#36827;&#30693;&#35782;&#20174;&#19968;&#20010;&#26356;&#24378;&#30340;&#27169;&#22411;&#21040;&#19968;&#20010;&#36739;&#24369;&#30340;&#27169;&#22411;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Reprompting&#30456;&#23545;&#20110;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24102;&#26469;&#20102;&#39640;&#36798;+17&#20010;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
&lt;/p&gt;</description></item></channel></rss>