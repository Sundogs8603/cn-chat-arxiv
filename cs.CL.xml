<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;BiMediX&#65292;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#20043;&#38388;&#26080;&#32541;&#21307;&#23398;&#20132;&#20114;&#30340;&#21452;&#35821;&#21307;&#23398;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;LLM&#65292;&#24341;&#20837;&#20102;&#21322;&#33258;&#21160;&#21270;&#30340;&#32763;&#35793;&#27969;&#27700;&#32447;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#25512;&#20986;&#20102;&#21253;&#21547;130&#19975;&#22810;&#26679;&#21270;&#21307;&#23398;&#20132;&#20114;&#30340;BiMed1.3M&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13253</link><description>&lt;p&gt;
BiMediX: &#21452;&#35821;&#21307;&#23398;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;LLM
&lt;/p&gt;
&lt;p&gt;
BiMediX: Bilingual Medical Mixture of Experts LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;BiMediX&#65292;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#20043;&#38388;&#26080;&#32541;&#21307;&#23398;&#20132;&#20114;&#30340;&#21452;&#35821;&#21307;&#23398;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;LLM&#65292;&#24341;&#20837;&#20102;&#21322;&#33258;&#21160;&#21270;&#30340;&#32763;&#35793;&#27969;&#27700;&#32447;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#25512;&#20986;&#20102;&#21253;&#21547;130&#19975;&#22810;&#26679;&#21270;&#21307;&#23398;&#20132;&#20114;&#30340;BiMed1.3M&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BiMediX&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#22312;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#20043;&#38388;&#26080;&#32541;&#20114;&#21160;&#30340;&#21452;&#35821;&#21307;&#23398;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;LLM&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#20043;&#38388;&#20419;&#36827;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;&#21307;&#23398;&#20132;&#20114;&#65292;&#21253;&#25324;&#22810;&#36718;&#23545;&#35805;&#20197;&#35810;&#38382;&#20851;&#20110;&#24739;&#32773;&#30151;&#29366;&#21644;&#30149;&#21490;&#31561;&#39069;&#22806;&#32454;&#33410;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20197;&#21450;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#33258;&#21160;&#21270;&#30340;&#33521;&#35821;&#21040;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#27969;&#27700;&#32447;&#65292;&#32467;&#21512;&#20154;&#24037;&#20248;&#21270;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#21307;&#23398;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;BiMed1.3M&#65292;&#19968;&#20010;&#28085;&#30422;130&#19975;&#21508;&#31181;&#21307;&#23398;&#20132;&#20114;&#30340;&#24191;&#27867;&#30340;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#21452;&#35821;&#25351;&#20196;&#38598;&#65292;&#20135;&#29983;&#20102;&#36229;&#36807;6.32&#20159;&#20010;&#21307;&#30103;&#19987;&#19994;token&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;BiMed1.3M&#25968;&#25454;&#38598;&#21253;&#25324;25&#19975;&#20010;&#21512;&#25104;&#30340;&#21307;&#29983;-&#24739;&#32773;&#22810;&#36718;&#23545;&#35805;&#65292;&#24182;&#20445;&#25345;1:2&#30340;&#38463;&#25289;&#20271;&#35821;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13253v1 Announce Type: new  Abstract: In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.13249</link><description>&lt;p&gt;
TofuEval&#65306;&#35780;&#20272;LLM&#22312;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13249
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25991;&#26723;&#26032;&#38395;&#25688;&#35201;&#22312;&#24544;&#23454;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#27493;&#65292;&#36825;&#24471;&#30410;&#20110;&#23545;&#20107;&#23454;&#19968;&#33268;&#24615;&#25110;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#36827;&#23637;&#26159;&#21542;&#33021;&#24310;&#20280;&#21040;&#20854;&#20182;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;&#65292;&#30001;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#20108;&#20803;&#21477;&#32423;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#21450;&#23545;&#20107;&#23454;&#19981;&#19968;&#33268;&#21477;&#23376;&#30340;&#35814;&#32454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#26080;&#35770;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;LLMs&#65288;&#21253;&#25324;GPT-4&#65289;&#20805;&#24403;&#20108;&#20803;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#21487;&#20197;&#34987;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#19987;&#38376;&#20107;&#23454;&#35780;&#20272;&#24230;&#37327;&#25152;&#36229;&#36234;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#25628;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#29702;&#35299;&#25972;&#20010;&#31508;&#35760;&#26412;&#20869;&#23481;&#35821;&#20041;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13234</link><description>&lt;p&gt;
&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#35299;&#38145;&#27934;&#35265;&#65306;&#35821;&#20041;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unlocking Insights: Semantic Search in Jupyter Notebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#25628;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#29702;&#35299;&#25972;&#20010;&#31508;&#35760;&#26412;&#20869;&#23481;&#35821;&#20041;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25628;&#32034;&#26088;&#22312;&#36890;&#36807;&#29702;&#35299;&#25628;&#32034;&#32773;&#30340;&#24847;&#22270;&#21644;&#21487;&#25628;&#32034;&#25968;&#25454;&#31354;&#38388;&#20013;&#26415;&#35821;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#65292;&#25552;&#20379;&#39640;&#24230;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#25628;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;Jupyter&#31508;&#35760;&#26412;&#39046;&#22495;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#32034;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#22914;&#22270;&#34920;&#12289;&#20851;&#32852;&#20989;&#25968;&#21644;&#26041;&#27861;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21487;&#20197;&#20840;&#38754;&#29702;&#35299;&#25972;&#20010;&#31508;&#35760;&#26412;&#20869;&#23481;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65306;1&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;Jupyter&#31508;&#35760;&#26412;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#21333;&#20803;&#26684;&#65292;&#21253;&#25324;Markdown&#21644;&#20195;&#30721;&#21333;&#20803;&#26684;&#12290;2&#65289;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13234v1 Announce Type: cross  Abstract: Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval. In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information.   We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries. Key components of this framework include:   1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#25972;&#21512;&#26041;&#38754;&#34920;&#29616;&#26356;&#20339;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#19982;&#29305;&#23450;&#25991;&#21270;&#30340;&#20027;&#23548;&#35821;&#35328;&#25552;&#31034;&#25110;&#31934;&#21046;&#35821;&#35328;&#28151;&#21512;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.13231</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Investigating Cultural Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13231
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#25972;&#21512;&#26041;&#38754;&#34920;&#29616;&#26356;&#20339;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#19982;&#29305;&#23450;&#25991;&#21270;&#30340;&#20027;&#23548;&#35821;&#35328;&#25552;&#31034;&#25110;&#31934;&#21046;&#35821;&#35328;&#28151;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#20037;&#20197;&#26469;&#65292;&#35821;&#35328;&#21644;&#25991;&#21270;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#19968;&#30452;&#26159;&#35821;&#35328;&#20154;&#31867;&#23398;&#39046;&#22495;&#25506;&#32034;&#30340;&#19968;&#20010;&#35838;&#39064;&#12290;&#34987;&#25512;&#24191;&#20026;&#38598;&#20307;&#20154;&#31867;&#30693;&#35782;&#24211;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#27010;&#25324;&#20102;&#19981;&#21516;&#25991;&#21270;&#25152;&#37319;&#29992;&#30340;&#22810;&#26679;&#30693;&#35782;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20004;&#20010;&#32500;&#24230;&#19978;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#25991;&#21270;&#25972;&#21512;&#24615; -- &#39318;&#20808;&#65292;&#24403;&#25552;&#31034;&#20351;&#29992;&#29305;&#23450;&#25991;&#21270;&#30340;&#20027;&#23548;&#35821;&#35328;&#26102;&#65292;&#20854;&#27425;&#65292;&#24403;&#39044;&#20808;&#20351;&#29992;&#35813;&#25991;&#21270;&#37319;&#29992;&#30340;&#35821;&#35328;&#30340;&#31934;&#21046;&#28151;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#31038;&#20250;&#23398;&#35843;&#26597;&#26469;&#37327;&#21270;&#25991;&#21270;&#25972;&#21512;&#65292;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#23454;&#38469;&#35843;&#26597;&#21442;&#19982;&#32773;&#20316;&#20026;&#21442;&#32771;&#36827;&#34892;&#27604;&#36739;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLMs&#29992;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#20197;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#25552;&#31034;&#26469;&#22797;&#21046;&#22312;&#22467;&#21450;&#21644;&#32654;&#22269;&#21508;&#22320;&#36827;&#34892;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13231v1 Announce Type: new  Abstract: The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the p
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;</title><link>https://arxiv.org/abs/2402.13228</link><description>&lt;p&gt;
Smaug&#65306;&#20351;&#29992;DPO-Positive&#20462;&#22797;&#20559;&#22909;&#20248;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13228
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#24635;&#32467;&#21644;&#23545;&#40784;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; DPO&#20351;&#29992;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#25968;&#25454;&#23545;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#21709;&#24212;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#30456;&#23545;&#8221;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#21482;&#35201;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#23545;&#27010;&#29575;&#22686;&#21152;&#65292;&#26631;&#20934;DPO&#25439;&#22833;&#23601;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#39318;&#36873;&#31034;&#20363;&#30340;&#21487;&#33021;&#24615;&#38477;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#24403;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#23436;&#25104;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#36739;&#30701;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DPO-Positive&#65288;DPOP&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
&lt;/p&gt;</description></item><item><title>AgentMD&#26159;&#19968;&#31181;&#26032;&#22411;&#35821;&#35328;&#20195;&#29702;&#65292;&#33021;&#22815;&#33258;&#21160;&#31579;&#36873;&#21644;&#24212;&#29992;&#21253;&#21547;2,164&#20010;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;RiskCalcs&#38598;&#21512;&#65292;&#20026;&#20811;&#26381;&#20020;&#24202;&#24037;&#20855;&#26131;&#29992;&#24615;&#25361;&#25112;&#21644;&#25552;&#39640;&#24037;&#20316;&#27969;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.13225</link><description>&lt;p&gt;
AgentMD&#65306;&#20026;&#22823;&#35268;&#27169;&#20020;&#24202;&#24037;&#20855;&#23398;&#20064;&#36171;&#33021;&#35821;&#35328;&#20195;&#29702;&#20197;&#36827;&#34892;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13225
&lt;/p&gt;
&lt;p&gt;
AgentMD&#26159;&#19968;&#31181;&#26032;&#22411;&#35821;&#35328;&#20195;&#29702;&#65292;&#33021;&#22815;&#33258;&#21160;&#31579;&#36873;&#21644;&#24212;&#29992;&#21253;&#21547;2,164&#20010;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;RiskCalcs&#38598;&#21512;&#65292;&#20026;&#20811;&#26381;&#20020;&#24202;&#24037;&#20855;&#26131;&#29992;&#24615;&#25361;&#25112;&#21644;&#25552;&#39640;&#24037;&#20316;&#27969;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35745;&#31639;&#22120;&#36890;&#36807;&#20026;&#35832;&#22914;&#39044;&#21518;&#31561;&#21508;&#31181;&#30446;&#30340;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#39044;&#27979;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#21033;&#29992;&#24120;&#24120;&#21463;&#21040;&#26131;&#29992;&#24615;&#25361;&#25112;&#12289;&#20449;&#24687;&#20256;&#25773;&#19981;&#30021;&#21644;&#21151;&#33021;&#21463;&#38480;&#30340;&#38459;&#30861;&#12290;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24191;&#27867;&#30340;&#20020;&#24202;&#35745;&#31639;&#22120;&#38598;&#21512;&#30456;&#32467;&#21512;&#65292;&#20026;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#24182;&#25552;&#39640;&#24037;&#20316;&#27969;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#25163;&#21160;&#31579;&#36873;&#36807;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentMD&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#21508;&#31181;&#20020;&#24202;&#32972;&#26223;&#19979;&#31574;&#21010;&#21644;&#24212;&#29992;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;&#26032;&#22411;&#35821;&#35328;&#20195;&#29702;&#12290;AgentMD&#21033;&#29992;&#24050;&#21457;&#34920;&#30340;&#25991;&#29486;&#65292;&#33258;&#21160;&#31579;&#36873;&#20102;&#19968;&#32452;&#21253;&#21547;2,164&#20010;&#22810;&#26679;&#21270;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;&#38598;&#21512;&#65292;&#20855;&#26377;&#21487;&#25191;&#34892;&#21151;&#33021;&#21644;&#32467;&#26500;&#21270;&#25991;&#26723;&#65292;&#32479;&#31216;&#20026;RiskCalcs&#12290;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;RiskCalcs&#24037;&#20855;&#36798;&#21040;&#20102;&#19968;&#23450;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13225v1 Announce Type: cross  Abstract: Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RoCode&#65292;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#32599;&#39532;&#23612;&#20122;&#35821;&#32534;&#31243;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;/&#22810;&#35821;&#35328;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#26234;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13222</link><description>&lt;p&gt;
RoCode: &#29992;&#20110;&#27979;&#37327;&#32599;&#39532;&#23612;&#20122;&#35821;&#38382;&#39064;&#23450;&#20041;&#20013;&#20195;&#30721;&#26234;&#33021;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13222
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RoCode&#65292;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#32599;&#39532;&#23612;&#20122;&#35821;&#32534;&#31243;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;/&#22810;&#35821;&#35328;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#36866;&#24403;&#25351;&#20196;&#35299;&#20915;&#22823;&#37327;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#22810;&#25968;&#27979;&#35797;&#22871;&#20214;&#20551;&#23450;&#25351;&#20196;&#26159;&#29992;&#33521;&#35821;&#32534;&#20889;&#30340;&#65292;&#36825;&#26159;&#20107;&#23454;&#19978;&#30340;&#25552;&#31034;&#35821;&#35328;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#26469;&#35828;&#65292;&#20195;&#30721;&#26234;&#33021;&#21644;&#38382;&#39064;&#35299;&#20915;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#27809;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#34913;&#37327;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoCode&#65292;&#19968;&#20010;&#31454;&#36187;&#24615;&#32534;&#31243;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2,642&#20010;&#29992;&#32599;&#39532;&#23612;&#20122;&#35821;&#32534;&#20889;&#30340;&#38382;&#39064;&#65292;11,000&#20010;&#29992;C&#12289;C ++&#21644;Python&#32534;&#20889;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#21450;&#27599;&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#27979;&#35797;&#22871;&#20214;&#12290;RoCode&#30340;&#30446;&#30340;&#26159;&#20026;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;/&#22810;&#35821;&#35328;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#26234;&#33021;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#20026;&#39044;&#35757;&#32451;&#30340;&#32599;&#39532;&#23612;&#20122;&#35821;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#24494;&#35843;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13222v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Th
&lt;/p&gt;</description></item><item><title>&#23545;&#22810;&#27169;&#24577;LLMs&#30340;&#27450;&#39575;&#24615;&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;MAD-Bench&#65292;&#21457;&#29616;GPT-4V&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#24046;&#36317;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.13220</link><description>&lt;p&gt;
&#26377;&#22810;&#23481;&#26131;&#27450;&#39575;&#22810;&#27169;&#24577;LLMs&#65311;&#20851;&#20110;&#27450;&#39575;&#24615;&#25552;&#31034;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13220
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;LLMs&#30340;&#27450;&#39575;&#24615;&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;MAD-Bench&#65292;&#21457;&#29616;GPT-4V&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#24046;&#36317;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#24182;&#27809;&#26377;&#20351;&#23427;&#20204;&#20813;&#30123;&#21508;&#31181;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24102;&#26377;&#27450;&#39575;&#24615;&#20449;&#24687;&#30340;&#25552;&#31034;&#26102;&#65292;&#20250;&#20135;&#29983;&#24187;&#35273;&#33324;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#23450;&#37327;&#35780;&#20272;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAD-Bench&#65292;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#65292;&#20998;&#20026;6&#20010;&#31867;&#21035;&#65292;&#22914;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12289;&#23545;&#35937;&#25968;&#37327;&#12289;&#31354;&#38388;&#20851;&#31995;&#21644;&#35270;&#35273;&#28151;&#28102;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;MLLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;GPT-4V&#12289;Gemini-Pro&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaVA-1.5&#21644;CogVLM&#12290;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-4V&#21644;&#20854;&#20182;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65307;&#20043;&#21069;&#30340;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#22914;LRV-Instruction&#21644;LLaVA-RLHF&#65292;&#22312;&#36825;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;&#34429;&#28982;GPT-4V&#22312;MAD-Bench&#19978;&#21462;&#24471;&#20102;75.02%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#20854;&#20182;&#20219;&#20309;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#37117;&#27809;&#26377;&#36798;&#21040;&#36825;&#19968;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13213</link><description>&lt;p&gt;
&#36719;&#26368;&#22823;&#27010;&#29575;&#65288;&#22823;&#37096;&#20998;&#26102;&#20505;&#65289;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13213
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36807;&#24230;&#33258;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#38169;&#35823;&#31572;&#26696;&#23558;&#19982;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#36739;&#23567;&#30456;&#20851;&#65292;&#30456;&#27604;&#20043;&#19979;&#27491;&#30830;&#31572;&#26696;&#36739;&#22823;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#24320;&#28304;LLMs&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22987;&#38382;&#31572;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#23545;&#20110;&#34920;&#29616;&#26368;&#20339;&#30340;&#20845;&#20010;LLMs&#65292;&#20174;MSP&#23548;&#20986;&#30340;AUROC&#22312;59/60&#20010;&#23454;&#20363;&#20013;&#37117;&#20248;&#20110;&#38543;&#26426;&#26426;&#20250;&#65292;p &lt; 10^{-4}&#12290;&#22312;&#36825;&#20845;&#20010;LLMs&#20013;&#65292;&#24179;&#22343;AUROC&#33539;&#22260;&#22312;60%&#33267;69%&#20043;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#21709;&#24212;&#30340;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#29992;&#39044;softmax logits&#32780;&#19981;&#26159;softmax&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
&lt;/p&gt;</description></item><item><title>Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13212</link><description>&lt;p&gt;
&#36719;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency Improves Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13212
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#21644;&#35780;&#20998;&#26469;&#25913;&#36827;&#65292;&#20197;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#24403;&#21069;&#30340;&#8220;&#25277;&#26679;&#21644;&#36873;&#25321;&#8221;&#26041;&#27861;&#22914;&#33258;&#19968;&#33268;&#24615;&#65288;SC&#65289;&#20381;&#36182;&#20110;&#22810;&#25968;&#25237;&#31080;&#26469;&#35780;&#20998;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#26377;&#35768;&#22810;&#19981;&#21516;&#19988;&#26377;&#25928;&#30340;&#31572;&#26696;&#26102;&#65292;&#36890;&#36807;&#25237;&#31080;&#36827;&#34892;&#36873;&#25321;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#36825;&#20351;&#24471;SC&#22312;&#28041;&#21450;&#39034;&#24207;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#65288;&#31572;&#26696;&#65289;&#30340;&#20114;&#21160;&#20219;&#21153;&#26102;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#30830;&#23450;&#22823;&#22810;&#25968;&#25237;&#31080;&#26410;&#33021;&#20026;&#27492;&#31867;&#20219;&#21153;&#25552;&#20379;&#19968;&#33268;&#30340;&#25910;&#30410;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#33258;&#19968;&#33268;&#24615;&#65288;Soft-SC&#65289;&#65292;&#23427;&#29992;&#27169;&#22411;&#21487;&#33021;&#24615;&#35745;&#31639;&#36830;&#32493;&#20998;&#25968;&#26469;&#21462;&#20195;SC&#30340;&#19981;&#36830;&#32493;&#35780;&#20998;&#65292;&#21363;&#20351;&#21160;&#20316;&#20998;&#24067;&#31232;&#30095;&#65292;&#20063;&#20801;&#35768;&#36873;&#25321;&#12290;&#36719;&#33258;&#19968;&#33268;&#24615;&#22312;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#23384;&#22312;&#30340;&#20559;&#22909;&#24615;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#23545;&#29305;&#23450;&#31574;&#30053;&#30340;&#36807;&#39640;&#20559;&#22909;&#20250;&#38459;&#30861;&#26377;&#25928;&#30340;&#24773;&#24863;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.13211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#24773;&#24863;&#25903;&#25345;&#32773;&#21527;&#65311;&#20943;&#36731;&#23545;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#20559;&#22909;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13211
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#23384;&#22312;&#30340;&#20559;&#22909;&#24615;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#23545;&#29305;&#23450;&#31574;&#30053;&#30340;&#36807;&#39640;&#20559;&#22909;&#20250;&#38459;&#30861;&#26377;&#25928;&#30340;&#24773;&#24863;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#65288;ESC&#65289;&#26159;&#19968;&#39033;&#26088;&#22312;&#36890;&#36807;&#26085;&#24120;&#23545;&#35805;&#32531;&#35299;&#20010;&#20307;&#24773;&#24863;&#22256;&#25200;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#38750;&#30452;&#35273;&#24615;&#36136;&#65292;ESConv&#25968;&#25454;&#38598;&#34701;&#20837;&#20102;&#25903;&#25345;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#29983;&#25104;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#21331;&#36234;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#22312;&#25552;&#20379;&#26377;&#29992;&#30340;&#24773;&#24863;&#25903;&#25345;&#26041;&#38754;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#20998;&#26512;&#20102;LLMs&#22312;ESConv&#19978;&#30340;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#22312;&#36873;&#25321;&#27491;&#30830;&#31574;&#30053;&#21644;&#23545;&#29305;&#23450;&#31574;&#30053;&#30340;&#26174;&#33879;&#20559;&#22909;&#26041;&#38754;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22266;&#26377;&#20559;&#22909;&#23545;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23637;&#29616;&#20986;&#23545;&#29305;&#23450;&#31574;&#30053;&#30340;&#39640;&#20559;&#22909;&#20250;&#38459;&#30861;&#26377;&#25928;&#30340;&#24773;&#24863;&#25903;&#25345;&#65292;&#21152;&#21095;&#20854;&#22312;&#39044;&#27979;&#36866;&#24403;&#31574;&#30053;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13211v1 Announce Type: new  Abstract: Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover
&lt;/p&gt;</description></item><item><title>ConfHyena&#26159;&#19968;&#31181;&#22522;&#20110;Hyena&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#35821;&#38899;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#26174;&#33879;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.13208</link><description>&lt;p&gt;
&#20986;&#20110;&#24615;&#33021;&#32771;&#34385;&#65292;Hyena&#22914;&#20309;&#22788;&#29702;&#20154;&#31867;&#35821;&#38899;&#65311;&#20351;&#29992;ConfHyena&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13208
&lt;/p&gt;
&lt;p&gt;
ConfHyena&#26159;&#19968;&#31181;&#22522;&#20110;Hyena&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#35821;&#38899;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#26174;&#33879;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;&#29616;&#20195;&#31070;&#32463;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#20294;&#30001;&#20110;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#36807;&#21435;&#20960;&#24180;&#30340;&#30740;&#31350;&#24037;&#20316;&#20391;&#37325;&#20110;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;Hyena&#65288;Poli&#31561;&#65292;2023&#24180;&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#27425;&#32447;&#24615;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConfHyena&#65292;&#36825;&#26159;&#19968;&#20010;Conformer&#65292;&#20854;&#32534;&#30721;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#34987;Hyena&#30340;&#19968;&#31181;&#21464;&#20307;&#21462;&#20195;&#65292;&#29992;&#20110;&#22788;&#29702;&#35821;&#38899;&#65292;&#20854;&#20013;&#38271;&#36755;&#20837;&#24207;&#21015;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;&#33521;&#35821;&#65289;&#21644;&#32763;&#35793;&#23454;&#39564;&#65288;&#20174;&#33521;&#35821;&#32763;&#35793;&#25104;8&#31181;&#30446;&#26631;&#35821;&#35328;&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26368;&#22909;&#30340;ConfHyena&#27169;&#22411;&#23558;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#20102;27%&#65292;&#32780;&#21697;&#36136;&#25439;&#22833;&#20165;&#20026;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13208v1 Announce Type: cross  Abstract: The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#26657;&#20934;&#21644;&#22810;&#36339;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#38382;&#31572;&#20013;&#20808;&#21069;&#27169;&#22411;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13188</link><description>&lt;p&gt;
&#38382;&#39064;&#26657;&#20934;&#19982;&#22810;&#36339;&#24314;&#27169;&#29992;&#20110;&#26102;&#38388;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Question Calibration and Multi-Hop Modeling for Temporal Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#26657;&#20934;&#21644;&#22810;&#36339;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#38382;&#31572;&#20013;&#20808;&#21069;&#27169;&#22411;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#27169;&#22411;&#22312;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;KGs&#20013;&#21253;&#21547;&#30340;&#35768;&#22810;&#20107;&#23454;&#37117;&#26159;&#21463;&#26102;&#38388;&#38480;&#21046;&#30340;&#65292;&#22240;&#27492;&#26102;&#38388;KGQA&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;KGQA&#26041;&#38754;&#21462;&#24471;&#20102;&#20016;&#30805;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;I&#65289;&#23427;&#20204;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#33719;&#21462;&#38382;&#39064;&#34920;&#31034;&#65292;&#32780;PLMs&#20542;&#21521;&#20110;&#20851;&#27880;&#23454;&#20307;&#20449;&#24687;&#24182;&#24573;&#30053;&#30001;&#20110;&#26102;&#38388;&#32422;&#26463;&#24341;&#36215;&#30340;&#23454;&#20307;&#36716;&#31227;&#65292;&#26368;&#32456;&#26410;&#33021;&#23398;&#20064;&#21040;&#23454;&#20307;&#30340;&#29305;&#23450;&#26102;&#38388;&#34920;&#31034;&#12290; &#65288;II&#65289;&#23427;&#20204;&#26082;&#19981;&#24378;&#35843;&#23454;&#20307;&#20043;&#38388;&#30340;&#22270;&#32467;&#26500;&#65292;&#20063;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#22270;&#20013;&#30340;&#22810;&#36339;&#20851;&#31995;&#65292;&#36825;&#23558;&#20351;&#22797;&#26434;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#26657;&#20934;&#21644;&#22810;&#36339;&#24314;&#27169;&#65288;QC-MHM&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13188v1 Announce Type: new  Abstract: Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering. To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach. Specifically, We fir
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13184</link><description>&lt;p&gt;
&#22914;&#26524;LLM&#20855;&#26377;&#19981;&#21516;&#30340;&#19990;&#30028;&#35266;&#65306;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#27169;&#25311;&#22806;&#26143;&#25991;&#26126;
&lt;/p&gt;
&lt;p&gt;
What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#27169;&#25311;&#20154;&#31867;&#19982;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#65292;&#29305;&#21035;&#24378;&#35843;&#21490;&#33922;&#33452;&#183;&#38669;&#37329;&#20851;&#20110;&#19981;&#35201;&#38543;&#24847;&#21521;&#23431;&#23449;&#21457;&#36865;&#26080;&#32447;&#30005;&#20449;&#21495;&#30340;&#35880;&#24910;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#23041;&#32961;&#21892;&#24847;&#25991;&#26126;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#36890;&#36807;&#37319;&#29992;&#25968;&#23398;&#27169;&#22411;&#21644;&#29366;&#24577;&#36716;&#25442;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#37327;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#20026;&#22312;&#20851;&#38190;&#22686;&#38271;&#21644;&#39281;&#21644;&#28857;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#25552;&#20379;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25215;&#35748;&#23431;&#23449;&#20013;&#28508;&#22312;&#29983;&#27963;&#26465;&#20214;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#21487;&#33021;&#20250;&#20419;&#36827;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#29420;&#29305;&#30340;&#23431;&#23449;&#35266;&#12289;&#36947;&#24503;&#20934;&#21017;&#21644;&#19990;&#30028;&#35266;&#12290;&#35748;&#35782;&#21040;&#22320;&#29699;&#19978;--
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13184v1 Announce Type: new  Abstract: In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13178</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Retrieval-Augmented Generation for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13178
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;RAG&#31995;&#32479;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#28789;&#27963;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#32570;&#20047;&#20851;&#20110;&#21508;&#31181;&#21307;&#23398;&#30446;&#30340;&#30340;&#26368;&#20339;RAG&#35774;&#32622;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#21019;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;7,663&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;MIRAGE&#65292;&#25105;&#20204;&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;MedRAG&#24037;&#20855;&#21253;&#65292;&#22312;41&#31181;&#19981;&#21516;&#35821;&#26009;&#24211;&#12289;&#26816;&#32034;&#22120;&#21644;&#39592;&#24178;LLMs&#30340;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#36229;&#36807;1.8&#19975;&#20159;&#30340;&#25552;&#31034;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;MedRAG&#25552;&#39640;&#20102;&#20845;&#31181;&#19981;&#21516;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs 
&lt;/p&gt;</description></item><item><title>AnnoTheia&#26159;&#19968;&#20010;&#21322;&#33258;&#21160;&#26631;&#27880;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#21592;&#22312;&#22330;&#26223;&#20013;&#35762;&#35805;&#20197;&#21450;&#30456;&#24212;&#30340;&#36716;&#24405;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35270;&#21548;&#35821;&#38899;&#25216;&#26415;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.13152</link><description>&lt;p&gt;
AnnoTheia&#65306;&#29992;&#20110;&#35270;&#21548;&#35821;&#38899;&#25216;&#26415;&#30340;&#21322;&#33258;&#21160;&#26631;&#27880;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13152
&lt;/p&gt;
&lt;p&gt;
AnnoTheia&#26159;&#19968;&#20010;&#21322;&#33258;&#21160;&#26631;&#27880;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#21592;&#22312;&#22330;&#26223;&#20013;&#35762;&#35805;&#20197;&#21450;&#30456;&#24212;&#30340;&#36716;&#24405;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35270;&#21548;&#35821;&#38899;&#25216;&#26415;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#35762;&#36848;&#36229;&#36807;7,000&#31181;&#24050;&#30693;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#36164;&#28304;&#65292;&#30446;&#21069;&#21482;&#26377;&#20854;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35821;&#38899;&#25216;&#26415;&#35206;&#30422;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#12289;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35821;&#38899;&#35821;&#26009;&#24211;&#25910;&#38598;&#20197;&#21450;&#25361;&#25112;&#27963;&#21160;&#30340;&#32452;&#32455;&#32531;&#35299;&#20102;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20027;&#35201;&#20197;&#33521;&#35821;&#20026;&#22522;&#20934;&#12290;&#24403;&#28041;&#21450;&#28041;&#21450;&#22768;&#23398;&#21644;&#35270;&#35273;&#35821;&#38899;&#27169;&#24577;&#30340;&#20219;&#21153;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#20419;&#36827;&#23545;&#35270;&#21548;&#35821;&#38899;&#25216;&#26415;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnnoTheia&#65292;&#19968;&#31181;&#21322;&#33258;&#21160;&#26631;&#27880;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#26816;&#27979;&#22330;&#26223;&#20013;&#20154;&#21592;&#35762;&#35805;&#20197;&#21450;&#30456;&#24212;&#30340;&#36716;&#24405;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23637;&#31034;&#20026;&#24863;&#20852;&#36259;&#30340;&#35821;&#35328;&#20934;&#22791;AnnoTheia&#30340;&#23436;&#25972;&#36807;&#31243;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#23558;&#29992;&#20110;&#27963;&#21160;&#21457;&#35328;&#32773;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#35199;&#29677;&#29273;&#35821;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13152v1 Announce Type: cross  Abstract: More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.13145</link><description>&lt;p&gt;
CMDAG: &#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#20013;&#25991;&#38544;&#21947;&#25968;&#25454;&#38598;&#20316;&#20026;&#8220;CoT&#8221;&#26469;&#25552;&#21319;&#38544;&#21947;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#26159;&#20154;&#31867;&#35821;&#35328;&#21644;&#25991;&#23398;&#20013;&#26174;&#33879;&#30340;&#20462;&#36766;&#25163;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22686;&#28155;&#20102;&#33394;&#24425;&#12289;&#24418;&#35937;&#21644;&#24378;&#35843;&#65292;&#20197;&#22686;&#24378;&#26377;&#25928;&#20132;&#27969;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;28K&#21477;&#26469;&#33258;&#21508;&#31181;&#20013;&#25991;&#25991;&#23398;&#26469;&#28304;&#65288;&#22914;&#35799;&#27468;&#12289;&#25955;&#25991;&#12289;&#27468;&#35789;&#31561;&#65289;&#12290;&#20026;&#30830;&#20445;&#27880;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#28085;&#30422;&#20102;&#38544;&#21947;&#26631;&#27880;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#35782;&#21035;&#23545;&#35937;&#12289;&#36733;&#20307;&#21644;&#22522;&#30784;&#65292;&#20197;&#22788;&#29702;&#27604;&#21947;&#12289;&#25311;&#20154;&#12289;&#24182;&#21015;&#21644;&#22840;&#24352;&#31561;&#22797;&#26434;&#24615;&#12290;&#25171;&#30772;&#20256;&#32479;&#65292;&#25105;&#20204;&#30340;&#38544;&#21947;&#29983;&#25104;&#26041;&#27861;&#24378;&#35843;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;&#36890;&#36807;&#23558;&#8220;&#22522;&#30784;&#8221;&#20316;&#20026;&#8220;CoT&#8221;&#65288;&#24605;&#32500;&#38142;&#65289;&#36755;&#20837;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13145v1 Announce Type: cross  Abstract: Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that re
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#22312;&#20923;&#32467;&#30340;&#34920;&#31034;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#36866;&#24212;&#36807;&#31243;&#28176;&#36827;&#20998;&#24067;&#22312;&#22810;&#23618;&#20013;&#65292;&#24182;&#19988;&#22823;&#37096;&#20998;&#39044;&#27979;&#20173;&#22312;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.13137</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The Hidden Space of Transformer Language Adapters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13137
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#22312;&#20923;&#32467;&#30340;&#34920;&#31034;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#36866;&#24212;&#36807;&#31243;&#28176;&#36827;&#20998;&#24067;&#22312;&#22810;&#23618;&#20013;&#65292;&#24182;&#19988;&#22823;&#37096;&#20998;&#39044;&#27979;&#20173;&#22312;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;transformer&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#25805;&#20316;&#26041;&#24335;&#65292;&#36825;&#20123;&#23567;&#27169;&#22359;&#22312;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#35757;&#32451;&#65292;&#20197;&#23558;&#20854;&#39044;&#27979;&#36866;&#24212;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24212;&#21518;&#30340;&#39044;&#27979;&#20027;&#35201;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#28304;&#35821;&#35328;&#20013;&#28436;&#21464;&#65292;&#32780;&#30446;&#26631;&#35821;&#35328;&#20165;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#20960;&#23618;&#20013;&#21464;&#24471;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#36866;&#24212;&#36807;&#31243;&#26159;&#28176;&#36827;&#30340;&#65292;&#20998;&#24067;&#22312;&#22810;&#20010;&#23618;&#20013;&#65292;&#21487;&#20197;&#36339;&#36807;&#23569;&#37327;&#36866;&#37197;&#22120;&#32452;&#32780;&#19981;&#38477;&#20302;&#36866;&#24212;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#22312;&#27169;&#22411;&#30340;&#20923;&#32467;&#34920;&#31034;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#21516;&#26102;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20854;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#22312;&#8220;&#23396;&#31435;&#8221;&#30340;&#23376;&#31354;&#38388;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13137v1 Announce Type: new  Abstract: We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;ELECTRA&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.13130</link><description>&lt;p&gt;
ELECTRA&#30340;&#21477;&#23376;&#23884;&#20837;&#26159;&#21542;&#26080;&#27861;&#20462;&#22797;&#65311;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;ELECTRA&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;BERT&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#26159;&#19968;&#20010;&#26126;&#26174;&#30340;&#32570;&#28857;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ELECTRA&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20854;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#34920;&#29616;&#19981;&#20339;&#12290;&#31038;&#21306;&#24708;&#28982;&#20572;&#27490;&#20351;&#29992;ELECTRA&#30340;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#36827;&#34892;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#20351;&#29992;ELECTRA&#37492;&#21035;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#30456;&#23545;&#20110;&#36739;&#26089;&#30340;&#23618;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#19979;&#38477;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20462;&#22797;ELECTRA&#23884;&#20837;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25130;&#26029;&#27169;&#22411;&#24494;&#35843;&#65288;TMFT&#65289;&#26041;&#27861;&#12290;&#22312;STS&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TMFT&#23558;Spearman&#30456;&#20851;&#31995;&#25968;&#25552;&#39640;&#20102;8&#20010;&#22810;&#28857;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#21442;&#25968;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#21508;&#31181;&#27169;&#22411;&#22823;&#23567;&#21644;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;ELECTRA&#29983;&#25104;&#27169;&#22411;&#30340;&#24778;&#20154;&#21151;&#25928;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;BERT&#25345;&#24179;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13130v1 Announce Type: new  Abstract: While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, usi
&lt;/p&gt;</description></item><item><title>TreeEval&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#35268;&#21010;&#31574;&#30053;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2402.13125</link><description>&lt;p&gt;
TreeEval&#65306;&#36890;&#36807;&#26641;&#35268;&#21010;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13125
&lt;/p&gt;
&lt;p&gt;
TreeEval&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#35268;&#21010;&#31574;&#30053;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24314;&#31435;&#20102;&#35768;&#22810;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#35745;&#31639;&#25972;&#20307;&#24471;&#20998;&#25110;&#20351;&#29992;&#21478;&#19968;&#20010;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#22522;&#20934;&#30340;&#20844;&#24320;&#35775;&#38382;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#19981;&#28789;&#27963;&#32780;&#36973;&#21463;&#25968;&#25454;&#27844;&#28431;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TreeEval&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#35753;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;LLM&#20027;&#25345;&#19968;&#20010;&#19981;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#20250;&#35805;&#65292;&#20174;&#26681;&#26412;&#19978;&#36991;&#20813;&#20102;&#25968;&#25454;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;LLM&#20805;&#24403;&#19968;&#20010;&#32771;&#23448;&#65292;&#25552;&#20986;&#19968;&#31995;&#21015;&#20851;&#20110;&#19968;&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26641;&#35268;&#21010;&#31574;&#30053;&#65292;&#32771;&#34385;&#24403;&#21069;&#30340;&#35780;&#20272;&#29366;&#24577;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#38382;&#39064;&#30340;&#29983;&#25104;&#65292;&#30830;&#20445;&#35780;&#20272;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;6&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;7B&#12289;13B&#21644;33B&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13125v1 Announce Type: cross  Abstract: Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coef
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13116</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;KD&#22312;&#23558;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#24040;&#22836;&#30340;&#22797;&#26434;&#33021;&#21147;&#36716;&#31227;&#21040;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;LLaMA&#21644;Mistral&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26412;&#39033;&#24037;&#20316;&#38416;&#26126;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#23637;&#31034;&#20102;KD&#22914;&#20309;&#25104;&#20026;&#31532;&#20108;&#32773;&#36171;&#20104;&#31532;&#19968;&#32773;&#20808;&#36827;&#21151;&#33021;&#21644;&#32454;&#33268;&#29702;&#35299;&#30340;&#37325;&#35201;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#22260;&#32469;&#31639;&#27861;&#12289;&#25216;&#33021;&#21644;&#22402;&#30452;&#21270;&#36825;&#19977;&#20010;&#22522;&#30784;&#25903;&#26609;&#31934;&#24515;&#26500;&#24314;&#65292;&#20840;&#38754;&#25506;&#35752;&#20102;KD&#26426;&#21046;&#12289;&#29305;&#23450;&#35748;&#30693;&#33021;&#21147;&#30340;&#22686;&#24378;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35843;&#26597;&#24341;&#23548;&#30528;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#21644;KD&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#37325;&#21551;&#22686;&#37327;&#24335;Transformer&#26500;&#24314;&#21644;&#26356;&#26032;&#20869;&#37096;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22686;&#37327;&#29366;&#24577;&#30340;&#39034;&#24207;&#32467;&#26500;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#20559;&#35823;&#25928;&#24212;&#21450;&#20854;&#35299;&#20915;&#26041;&#24335;&#30340;&#20449;&#24687;&#65292;&#20026;&#20998;&#26512;&#19978;&#19979;&#25991;&#21270;&#24847;&#20041;&#34920;&#31034;&#21644;&#20381;&#36182;&#35299;&#26512;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#24102;&#26469;&#35265;&#35299;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#20462;&#35746;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13113</link><description>&lt;p&gt;
&#24403;&#21482;&#26377;&#26102;&#38388;&#33021;&#21578;&#35785;: &#36890;&#36807;&#37325;&#21551;&#22686;&#37327;&#24615;&#30340;&#35270;&#35282;&#35299;&#37322;Transformer&#22914;&#20309;&#22788;&#29702;&#23616;&#37096;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13113
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#37325;&#21551;&#22686;&#37327;&#24335;Transformer&#26500;&#24314;&#21644;&#26356;&#26032;&#20869;&#37096;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22686;&#37327;&#29366;&#24577;&#30340;&#39034;&#24207;&#32467;&#26500;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#20559;&#35823;&#25928;&#24212;&#21450;&#20854;&#35299;&#20915;&#26041;&#24335;&#30340;&#20449;&#24687;&#65292;&#20026;&#20998;&#26512;&#19978;&#19979;&#25991;&#21270;&#24847;&#20041;&#34920;&#31034;&#21644;&#20381;&#36182;&#35299;&#26512;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#24102;&#26469;&#35265;&#35299;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#20462;&#35746;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#19968;&#27425;&#19968;&#20010;&#20196;&#29260;&#30340;&#22686;&#37327;&#27169;&#22411;&#26377;&#26102;&#20250;&#36935;&#21040;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#30340;&#28857;&#12290;&#22240;&#26524;&#27169;&#22411;&#34987;&#36843;&#36755;&#20986;&#19968;&#20010;&#35299;&#37322;&#24182;&#32487;&#32493;&#65292;&#32780;&#21487;&#20197;&#20462;&#35746;&#30340;&#27169;&#22411;&#22312;&#28040;&#38500;&#27495;&#20041;&#26102;&#21487;&#33021;&#20250;&#32534;&#36753;&#20854;&#20808;&#21069;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37325;&#21551;&#22686;&#37327;&#24335;Transformer&#22914;&#20309;&#26500;&#24314;&#21644;&#26356;&#26032;&#20869;&#37096;&#29366;&#24577;&#65292;&#20197;&#38416;&#26126;&#23548;&#33268;&#19981;&#36866;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20462;&#35746;&#30340;&#36807;&#31243;&#26159;&#20160;&#20040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#22686;&#37327;&#29366;&#24577;&#65292;&#26174;&#31034;&#23427;&#20204;&#30340;&#39034;&#24207;&#32467;&#26500;&#32534;&#30721;&#20102;&#20851;&#20110;&#20559;&#35823;&#25928;&#24212;&#21450;&#20854;&#35299;&#20915;&#26041;&#24335;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#19978;&#19979;&#25991;&#21270;&#24847;&#20041;&#34920;&#31034;&#21644;&#20381;&#36182;&#35299;&#26512;&#30340;&#21508;&#31181;&#21452;&#21521;&#32534;&#30721;&#22120;&#24102;&#26469;&#20102;&#35265;&#35299;&#65292;&#26377;&#21161;&#20110;&#23637;&#31034;&#23427;&#20204;&#22312;&#28041;&#21450;&#20462;&#35746;&#26102;&#30456;&#23545;&#20110;&#22240;&#26524;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13113v1 Announce Type: new  Abstract: Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.
&lt;/p&gt;</description></item><item><title>CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13109</link><description>&lt;p&gt;
CIF-Bench&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13109
&lt;/p&gt;
&lt;p&gt;
CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#22686;&#24378;&#20102;&#36890;&#36807;&#25351;&#20196;&#36981;&#24490;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#26410;&#35265;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22914;&#20013;&#25991;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#20250;&#20943;&#24369;&#65292;&#21463;&#21040;&#25968;&#25454;&#27844;&#28431;&#24341;&#36215;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#20154;&#23545;&#23427;&#20204;&#30495;&#27491;&#30340;&#27867;&#21270;&#33021;&#21147;&#21040;&#26032;&#35821;&#35328;&#39046;&#22495;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;&#65288;CIF-Bench&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#23545;&#20013;&#25991;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;CIF-Bench &#21253;&#21547;150&#20010;&#20219;&#21153;&#21644;15,000&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#30001;&#27597;&#35821;&#32773;&#24320;&#21457;&#65292;&#29992;&#20110;&#27979;&#35797;&#36328;&#36234;20&#20010;&#31867;&#21035;&#30340;&#22797;&#26434;&#25512;&#29702;&#21644;&#20013;&#22269;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#20102;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#65292;&#25105;&#20204;&#21482;&#20844;&#24320;&#20102;&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#31169;&#23494;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#20197;&#26368;&#23567;&#21270;&#24471;&#20998;&#26041;&#24046;&#65292;&#20849;&#35745;45,000&#20010;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval
&lt;/p&gt;</description></item><item><title>ELAD&#25552;&#20986;&#20102;&#19968;&#31181;Explanation-Guided LLMs Active Distillation&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35299;&#37322;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#21644;LLM-&#27880;&#37322;&#35299;&#37322;&#20462;&#35746;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.13098</link><description>&lt;p&gt;
ELAD: &#35299;&#37322;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20027;&#21160;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
ELAD: Explanation-Guided Large Language Models Active Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13098
&lt;/p&gt;
&lt;p&gt;
ELAD&#25552;&#20986;&#20102;&#19968;&#31181;Explanation-Guided LLMs Active Distillation&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35299;&#37322;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#21644;LLM-&#27880;&#37322;&#35299;&#37322;&#20462;&#35746;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#21644;&#24212;&#29992;&#21463;&#21040;&#23427;&#20204;&#30340;&#20869;&#23384;&#25928;&#29575;&#12289;&#35745;&#31639;&#35201;&#27714;&#21644;&#39640;&#25104;&#26412;&#30340;API&#25512;&#26029;&#30340;&#38459;&#30861;&#12290;&#20256;&#32479;&#30340;&#33976;&#39311;&#26041;&#27861;&#24448;&#24448;&#26410;&#33021;&#30830;&#23450;&#30693;&#35782;&#26159;&#21542;&#24050;&#32463;&#34987;&#20805;&#20998;&#36716;&#31227;&#65292;&#21487;&#33021;&#23548;&#33268;&#39640;&#25104;&#26412;&#25110;&#19981;&#23436;&#25972;&#30340;&#33976;&#39311;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Explanation-Guided LLMs Active Distillation&#65288;ELAD&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#20248;&#21270;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#25913;&#36827;&#26377;&#25928;&#30340;&#26679;&#26412;&#36873;&#25321;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35299;&#37322;&#27493;&#39588;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#25361;&#25112;&#20854;&#25512;&#29702;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;LLM-&#27880;&#37322;&#35299;&#37322;&#20462;&#35746;&#25216;&#26415;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#26816;&#27979;&#24182;&#32416;&#27491;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13098v1 Announce Type: cross  Abstract: The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the stu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#26234;&#38556;&#20154;&#22763;&#38405;&#35835;&#26410;&#31616;&#21270;&#12289;&#33258;&#21160;&#21644;&#25163;&#21160;&#31616;&#21270;&#24503;&#35821;&#25991;&#26412;&#30340;&#29702;&#35299;&#24230;&#65292;&#21457;&#29616;&#19981;&#21516;&#38405;&#35835;&#32773;&#32676;&#20307;&#21644;&#31616;&#21270;&#26041;&#27861;&#23545;&#21487;&#29702;&#35299;&#24615;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2402.13094</link><description>&lt;p&gt;
&#26234;&#38556;&#20154;&#22763;&#31616;&#21270;&#25991;&#26412;&#30340;&#25968;&#23383;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13094
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#26234;&#38556;&#20154;&#22763;&#38405;&#35835;&#26410;&#31616;&#21270;&#12289;&#33258;&#21160;&#21644;&#25163;&#21160;&#31616;&#21270;&#24503;&#35821;&#25991;&#26412;&#30340;&#29702;&#35299;&#24230;&#65292;&#21457;&#29616;&#19981;&#21516;&#38405;&#35835;&#32773;&#32676;&#20307;&#21644;&#31616;&#21270;&#26041;&#27861;&#23545;&#21487;&#29702;&#35299;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26159;&#25351;&#25552;&#39640;&#25991;&#26412;&#21487;&#29702;&#35299;&#24615;&#30340;&#36807;&#31243;&#12290;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#36890;&#24120;&#30001;&#19987;&#23478;&#25110;&#20247;&#21253;&#24037;&#20316;&#32773;&#32780;&#19981;&#26159;&#31616;&#21270;&#25991;&#26412;&#30340;&#20027;&#35201;&#30446;&#26631;&#32676;&#20307;&#65288;&#22914;&#26234;&#38556;&#20154;&#22763;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#30740;&#31350;&#65292;&#20854;&#20013;&#21442;&#19982;&#32773;&#21253;&#25324;&#26234;&#38556;&#21644;&#38750;&#26234;&#38556;&#20154;&#22763;&#65292;&#22312;&#24179;&#26495;&#30005;&#33041;&#19978;&#38405;&#35835;&#26410;&#31616;&#21270;&#12289;&#33258;&#21160;&#21644;&#25163;&#21160;&#31616;&#21270;&#30340;&#24503;&#35821;&#25991;&#26412;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#21487;&#29702;&#35299;&#24615;&#30340;&#26041;&#27861;&#65306;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#12289;&#24863;&#30693;&#22256;&#38590;&#35780;&#32423;&#12289;&#21709;&#24212;&#26102;&#38388;&#21644;&#38405;&#35835;&#36895;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27979;&#37327;&#32467;&#26524;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21462;&#20915;&#20110;&#38405;&#35835;&#32773;&#32676;&#20307;&#20197;&#21450;&#25991;&#26412;&#26159;&#21542;&#32463;&#36807;&#33258;&#21160;&#25110;&#25163;&#21160;&#31616;&#21270;&#12290;&#23545;&#20110;&#26234;&#38556;&#20154;&#32676;&#65292;&#29702;&#35299;&#38382;&#39064;&#34987;&#35748;&#20026;&#26159;&#26368;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13094v1 Announce Type: new  Abstract: Text simplification refers to the process of increasing the comprehensibility of texts. Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities. We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer. We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed. The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification. For the target group of persons with intellectual disabilities, comprehension questions emerged as the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.13093</link><description>&lt;p&gt;
&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Event-level Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#36807;&#26102;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#20107;&#23454;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#32423;&#21035;&#19978;&#32534;&#36753;LLMs&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#30693;&#35782;&#26356;&#26032;&#26469;&#33258;&#26032;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#26356;&#25913;&#20107;&#23454;&#19977;&#20803;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#30452;&#25509;&#23558;&#26032;&#20107;&#20214;&#32534;&#36753;&#21040;LLMs&#20013;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;(1)&#25928;&#29575;&#12290;&#21333;&#20010;&#20107;&#20214;&#32534;&#36753;&#20250;&#23548;&#33268;&#22810;&#20010;&#25512;&#26029;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#26356;&#26032;&#12290;(2)&#23436;&#25972;&#24615;&#12290;&#38500;&#20102;&#26356;&#26032;&#20107;&#23454;&#30693;&#35782;&#22806;&#65292;&#20107;&#20214;&#32423;&#21035;&#30340;&#32534;&#36753;&#36824;&#38656;&#35201;&#32771;&#34385;&#20107;&#20214;&#24433;&#21709;&#65292;&#26356;&#26032;LLMs&#20851;&#20110;&#26410;&#26469;&#36235;&#21183;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#32423;&#21035;&#32534;&#36753;&#22522;&#20934;ELKEN&#65292;&#21253;&#25324;1,515&#20010;&#20107;&#20214;&#32534;&#36753;&#65292;6,449&#20010;&#20851;&#20110;&#20107;&#23454;&#30693;&#35782;&#30340;&#38382;&#39064;&#21644;10,150&#20010;&#20851;&#20110;&#26410;&#26469;&#21457;&#23637;&#36235;&#21183;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13093v1 Announce Type: cross  Abstract: Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36335;&#30001;&#22120;&#30340;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#27604;&#36739;&#12289;&#24207;&#21015;&#32423;&#36335;&#30001;&#19982;&#26631;&#35760;&#32423;&#36335;&#30001;&#22312;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13089</link><description>&lt;p&gt;
&#26397;&#21521;&#23545;MoE&#35774;&#35745;&#36873;&#25321;&#30340;&#23454;&#35777;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards an empirical understanding of MoE design choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36335;&#30001;&#22120;&#30340;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#27604;&#36739;&#12289;&#24207;&#21015;&#32423;&#36335;&#30001;&#19982;&#26631;&#35760;&#32423;&#36335;&#30001;&#22312;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#22312;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#36335;&#30001;&#22120;&#21644;&#20923;&#32467;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#36335;&#30001;&#22120;&#20043;&#38388;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#23398;&#20064;&#36335;&#30001;&#21487;&#33021;&#24182;&#38750;&#24517;&#19981;&#21487;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#24207;&#21015;&#32423;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#20027;&#39064;&#30340;&#24369;&#19987;&#23478;&#19987;&#19994;&#21270;&#65292;&#19982;&#26631;&#35760;&#32423;&#21035;&#36335;&#30001;&#35266;&#23519;&#21040;&#30340;&#35821;&#27861;&#19987;&#19994;&#21270;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13089v1 Announce Type: cross  Abstract: In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GLAN&#30340;&#24191;&#20041;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#33021;&#21147;&#20998;&#31867;&#27861;&#26469;&#29983;&#25104;&#22823;&#35268;&#27169;&#21512;&#25104;&#25351;&#23548;&#25968;&#25454;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13064</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#21512;&#25104;&#25968;&#25454;&#65306;&#36890;&#29992;&#25351;&#23548;&#35843;&#25972;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GLAN&#30340;&#24191;&#20041;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#33021;&#21147;&#20998;&#31867;&#27861;&#26469;&#29983;&#25104;&#22823;&#35268;&#27169;&#21512;&#25104;&#25351;&#23548;&#25968;&#25454;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#25351;&#23548;&#35843;&#25972;&#65288;&#31216;&#20026;GLAN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#31181;&#23376;&#31034;&#20363;&#25110;&#29616;&#26377;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;GLAN&#20165;&#21033;&#29992;&#20107;&#20808;&#31574;&#21010;&#30340;&#20154;&#31867;&#30693;&#35782;&#21644;&#33021;&#21147;&#20998;&#31867;&#27861;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#25152;&#26377;&#23398;&#31185;&#39046;&#22495;&#29983;&#25104;&#22823;&#35268;&#27169;&#21512;&#25104;&#25351;&#23548;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#20154;&#31867;&#25945;&#32946;&#31995;&#32479;&#20013;&#31995;&#32479;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#21322;&#33258;&#21160;&#26041;&#24335;&#65292;&#21033;&#29992;LLMs&#30340;&#24110;&#21161;&#65292;&#23558;&#20154;&#31867;&#30693;&#35782;&#21644;&#33021;&#21147;&#20998;&#35299;&#20026;&#21508;&#31181;&#39046;&#22495;&#12289;&#23376;&#39046;&#22495;&#65292;&#26368;&#32456;&#21040;&#19981;&#21516;&#23398;&#31185;&#65292;&#26500;&#24314;&#20102;&#20998;&#31867;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#23398;&#31185;&#29983;&#25104;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31185;&#30446;&#21015;&#34920;&#65292;&#24182;&#21033;&#29992;LLMs&#20877;&#27425;&#35774;&#35745;&#20102;&#36866;&#21512;&#27599;&#20010;&#31185;&#30446;&#30340;&#25945;&#23398;&#22823;&#32434;&#12290;&#36890;&#36807;&#22312;&#22823;&#32434;&#30340;&#27599;&#33410;&#35838;&#19978;&#35814;&#32454;&#20171;&#32461;&#32454;&#31890;&#24230;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#28145;&#20837;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13064v1 Announce Type: new  Abstract: We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate dive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13055</link><description>&lt;p&gt;
&#35782;&#21035;&#35821;&#20041;&#24863;&#24212;&#22836;&#20197;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Identifying Semantic Induction Heads to Understand In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#25512;&#29702;&#36923;&#36753;&#30340;&#19981;&#36879;&#26126;&#24615;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#65292;&#25105;&#20204;&#23545;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#26159;&#21542;&#32534;&#30721;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#23384;&#22312;&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#65306;&#20174;&#21477;&#23376;&#20013;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#27880;&#24847;&#21147;&#22836;&#34920;&#29616;&#20986;&#19968;&#31181;&#27169;&#24335;&#65292;&#21363;&#24403;&#20851;&#27880;&#22836;&#26631;&#35760;&#26102;&#65292;&#23427;&#20204;&#20250;&#22238;&#24518;&#36215;&#23614;&#26631;&#35760;&#65292;&#24182;&#22686;&#21152;&#36825;&#20123;&#23614;&#26631;&#35760;&#30340;&#36755;&#20986;&#36923;&#36753;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#21046;&#23450;&#19982;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#20986;&#29616;&#23384;&#22312;&#23494;&#20999;&#20851;&#32852;&#12290;&#35821;&#20041;&#27880;&#24847;&#21147;&#22836;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13055v1 Announce Type: cross  Abstract: Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StableKE&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#32780;&#38750;&#30693;&#35782;&#26412;&#22320;&#21270;&#30340;&#26032;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#30693;&#35782;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.13048</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Stable Knowledge Editing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13048
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StableKE&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#32780;&#38750;&#30693;&#35782;&#26412;&#22320;&#21270;&#30340;&#26032;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#30693;&#35782;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#30693;&#35782;&#32534;&#36753;&#23545;&#20110;&#26367;&#25442;&#36807;&#26102;&#20449;&#24687;&#25110;&#22823;&#35268;&#27169;&#25972;&#21512;&#19987;&#19994;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38544;&#24335;&#22320;&#20551;&#35774;&#30693;&#35782;&#22312;&#27169;&#22411;&#20869;&#26159;&#23616;&#37096;&#21270;&#19988;&#23396;&#31435;&#30340;&#65292;&#36825;&#31181;&#20551;&#35774;&#36807;&#20110;&#31616;&#21270;&#20102;&#27169;&#22411;&#30693;&#35782;&#30340;&#30456;&#20114;&#20851;&#32852;&#24615;&#12290;&#23616;&#37096;&#21270;&#30340;&#21069;&#25552;&#23548;&#33268;&#30693;&#35782;&#32534;&#36753;&#19981;&#23436;&#25972;&#65292;&#32780;&#23396;&#31435;&#30340;&#20551;&#35774;&#21487;&#33021;&#25439;&#23475;&#20854;&#20182;&#30693;&#35782;&#21644;&#19968;&#33324;&#33021;&#21147;&#12290;&#36825;&#20250;&#32473;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#30340;&#24615;&#33021;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#36229;&#36234;&#36825;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StableKE&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#32780;&#38750;&#30693;&#35782;&#26412;&#22320;&#21270;&#30340;&#20840;&#26032;&#35270;&#35282;&#26041;&#27861;&#12290;&#20026;&#20811;&#26381;&#20154;&#24037;&#26631;&#27880;&#30340;&#25104;&#26412;&#65292;StableKE&#25972;&#21512;&#20102;&#20004;&#31181;&#33258;&#21160;&#30693;&#35782;&#22686;&#24378;&#31574;&#30053;&#65306;&#35821;&#20041;&#25913;&#20889;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#36890;&#36807;&#22810;&#26679;&#21270;&#30693;&#35782;&#25551;&#36848;&#26469;&#20419;&#36827;&#25945;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13048v1 Announce Type: new  Abstract: Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It introduces instability to the performance of the knowledge editing method. To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization. To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teac
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#28860;&#36731;&#37327;&#32423;&#23545;&#35805;&#32534;&#30721;&#22120;&#20197;&#36991;&#20813;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.13043</link><description>&lt;p&gt;
&#29992;&#38544;&#24335;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13043
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25688;&#35201;&#25552;&#39640;&#23545;&#35805;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#28860;&#36731;&#37327;&#32423;&#23545;&#35805;&#32534;&#30721;&#22120;&#20197;&#36991;&#20813;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13043v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25991;&#25688;: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23567;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#20381;&#36182;&#20110;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23545;&#35805;&#26816;&#32034;&#22120;&#26469;&#26597;&#25214;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20197;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#20351;&#29992;&#21407;&#22987;&#23545;&#35805;&#19978;&#19979;&#25991;&#20316;&#20026;&#25628;&#32034;&#38190;&#21644;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#23545;&#24102;&#27880;&#37322;&#30340;&#23545;&#35805;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#22826;&#36866;&#21512;&#25193;&#23637;&#21040;&#26032;&#30340;&#39046;&#22495;&#25110;&#26032;&#30340;&#27880;&#37322;&#35821;&#35328;&#65292;&#22240;&#20026;&#24494;&#35843;&#25968;&#25454;&#19981;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#35805;&#30340;&#25991;&#26412;&#25688;&#35201;&#26469;&#22788;&#29702;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#22120;&#36827;&#34892;&#26597;&#35810;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#12290;&#20026;&#36991;&#20813;LLM&#22522;&#20110;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#24102;&#26469;&#30340;&#39069;&#22806;&#25512;&#29702;&#25104;&#26412;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#28860;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23545;&#35805;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#22312;&#19981;&#35299;&#30721;&#27979;&#35797;&#23545;&#35805;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26597;&#35810;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#65292;&#22312;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#20248;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;</title><link>https://arxiv.org/abs/2402.13040</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Molecule Generation with Diffusion Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13040
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#65292;&#22312;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#20248;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#26159;&#19968;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#29305;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#12290;&#26368;&#36817;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;SMILES&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;TGM-DLM&#38598;&#20307;&#21644;&#36845;&#20195;&#22320;&#26356;&#26032;SMILES&#23383;&#31526;&#20018;&#20013;&#30340;&#26631;&#35760;&#23884;&#20837;&#65292;&#20351;&#29992;&#20004;&#38454;&#27573;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#38543;&#26426;&#22122;&#22768;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#24341;&#23548;&#26469;&#20248;&#21270;&#23884;&#20837;&#65292;&#32780;&#31532;&#20108;&#38454;&#27573;&#32416;&#27491;&#26080;&#25928;&#30340;SMILES&#23383;&#31526;&#20018;&#20197;&#24418;&#25104;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;TGM-DLM&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;TGM-DLM&#22312;&#29983;&#25104;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13040v1 Announce Type: cross  Abstract: Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating cohe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SiLLM&#65292;&#23558;SiMT&#20219;&#21153;&#20998;&#35299;&#20026;&#31574;&#30053;&#20915;&#31574;&#21644;&#32763;&#35793;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#22996;&#25176;&#32473;&#29420;&#31435;&#30340;&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;SiMT&#12290;</title><link>https://arxiv.org/abs/2402.13036</link><description>&lt;p&gt;
SiLLM&#65306;&#29992;&#20110;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SiLLM: Large Language Models for Simultaneous Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SiLLM&#65292;&#23558;SiMT&#20219;&#21153;&#20998;&#35299;&#20026;&#31574;&#30053;&#20915;&#31574;&#21644;&#32763;&#35793;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#22996;&#25176;&#32473;&#29420;&#31435;&#30340;&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;SiMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#38405;&#35835;&#28304;&#21477;&#23376;&#30340;&#21516;&#26102;&#29983;&#25104;&#32763;&#35793;&#65292;&#38656;&#35201;&#19968;&#20010;&#31574;&#30053;&#26469;&#30830;&#23450;&#35835;&#21462;&#21644;&#29983;&#25104;&#21333;&#35789;&#30340;&#26368;&#20339;&#26102;&#26426;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#34920;&#29616;&#65292;&#20294;&#29616;&#26377;&#30340;SiMT&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;&#30340;transformers&#19978;&#65292;&#37319;&#29992;&#21333;&#19968;&#27169;&#22411;&#21516;&#26102;&#30830;&#23450;&#31574;&#30053;&#21644;&#29983;&#25104;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;SiMT&#30340;&#22797;&#26434;&#24615;&#65292;&#29992;&#21333;&#19968;&#27169;&#22411;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20004;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;SiMT&#20219;&#21153;&#20998;&#35299;&#20026;&#31574;&#30053;&#20915;&#31574;&#21644;&#32763;&#35793;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SiLLM&#65292;&#23558;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#22996;&#25176;&#32473;&#29420;&#31435;&#30340;&#20195;&#29702;&#65292;&#20174;&#32780;&#23558;LLM&#32435;&#20837;SiMT&#20013;&#12290;&#31574;&#30053;&#20915;&#31574;&#20195;&#29702;&#30001;&#20256;&#32479;&#30340;SiMT&#27169;&#22411;&#31649;&#29702;&#65292;&#36127;&#36131;&#30830;&#23450;&#32763;&#35793;&#31574;&#30053;&#12290;&#32763;&#35793;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13036v1 Announce Type: new  Abstract: Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13035</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#26597;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#26657;&#27491;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13035
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#19981;&#26029;&#21162;&#21147;&#36890;&#36807;&#33258;&#25105;&#26657;&#27491;&#26469;&#23436;&#21892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#22806;&#37096;&#20934;&#30830;&#30693;&#35782;&#30340;&#33258;&#25105;&#26657;&#27491;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#38480;&#21046;&#21644;&#26377;&#25928;&#24615;&#30340;&#30097;&#38382;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLM&#30340;&#33258;&#26816;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#65292;&#31216;&#20026;&#8220;Step CoT Check&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#23558;&#21407;&#22987;CoT&#25968;&#25454;&#21644;&#26816;&#26597;&#26657;&#27491;&#25968;&#25454;&#25972;&#21512;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20854;&#33258;&#26816;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#24182;&#28040;&#38500;&#20102;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#36136;&#22270;&#30340;&#27169;&#22411;HeterFC&#65292;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#20013;&#30340;&#20107;&#23454;&#26680;&#26597;&#65292;&#21033;&#29992;&#24322;&#36136;&#35777;&#25454;&#22270;&#21644;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.13028</link><description>&lt;p&gt;
&#24322;&#36136;&#22270;&#25512;&#29702;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#36136;&#22270;&#30340;&#27169;&#22411;HeterFC&#65292;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#20013;&#30340;&#20107;&#23454;&#26680;&#26597;&#65292;&#21033;&#29992;&#24322;&#36136;&#35777;&#25454;&#22270;&#21644;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26088;&#22312;&#36890;&#36807;&#25512;&#29702;&#22810;&#20010;&#35777;&#25454;&#20197;&#39044;&#27979;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#21518;&#32773;&#65292;&#21363;&#25512;&#29702;&#20851;&#20110;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24322;&#36136;&#22270;&#30340;&#35789;&#32423;&#27169;&#22411;HeterFC&#65292;&#29992;&#20110;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24322;&#36136;&#35777;&#25454;&#22270;&#65292;&#20197;&#21333;&#35789;&#20026;&#33410;&#28857;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#20195;&#34920;&#19981;&#21516;&#35777;&#25454;&#23646;&#24615;&#30340;&#36793;&#32536;&#65292;&#36890;&#36807;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#65292;&#20419;&#36827;&#22768;&#26126;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13028v1 Announce Type: cross  Abstract: Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claim
&lt;/p&gt;</description></item><item><title>CFEVER&#26159;&#19968;&#20010;&#20026;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#32780;&#35774;&#35745;&#30340;&#27721;&#35821;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#26631;&#20934;&#21644;&#23545;&#24212;&#35777;&#25454;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20943;&#36731;&#20154;&#24037;&#26680;&#26597;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.13025</link><description>&lt;p&gt;
CFEVER&#65306;&#19968;&#20010;&#29992;&#20110;&#27721;&#35821;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CFEVER: A Chinese Fact Extraction and VERification Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13025
&lt;/p&gt;
&lt;p&gt;
CFEVER&#26159;&#19968;&#20010;&#20026;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#32780;&#35774;&#35745;&#30340;&#27721;&#35821;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#26631;&#20934;&#21644;&#23545;&#24212;&#35777;&#25454;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20943;&#36731;&#20154;&#24037;&#26680;&#26597;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CFEVER&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#32780;&#35774;&#35745;&#30340;&#27721;&#35821;&#25968;&#25454;&#38598;&#12290;CFEVER&#21253;&#25324;30,012&#20010;&#22522;&#20110;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#20869;&#23481;&#25163;&#21160;&#21019;&#24314;&#30340;&#22768;&#26126;&#12290;&#27599;&#20010;CFEVER&#20013;&#30340;&#22768;&#26126;&#37117;&#26631;&#35760;&#20026;&#8220;&#25903;&#25345;&#8221;&#12289;&#8220;&#21453;&#39539;&#8221;&#25110;&#8220;&#20449;&#24687;&#19981;&#36275;&#8221;&#65292;&#20197;&#25551;&#36848;&#20854;&#20107;&#23454;&#31243;&#24230;&#12290;&#31867;&#20284;&#20110;FEVER&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#21644;&#21453;&#39539;&#31867;&#21035;&#20013;&#30340;&#22768;&#26126;&#20063;&#26631;&#26377;&#23545;&#24212;&#30340;&#35777;&#25454;&#21477;&#65292;&#36825;&#20123;&#35777;&#25454;&#21477;&#21462;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340;&#21333;&#20010;&#25110;&#22810;&#20010;&#39029;&#38754;&#12290;&#25105;&#20204;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#22312;&#20116;&#36335;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;0.7934&#30340;Fleiss' kappa&#20540;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20197;&#21450;&#23545;CFEVER&#30340;&#31616;&#21333;&#22522;&#20934;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26032;&#30340;&#33499;&#21051;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#22522;&#20934;&#65292;&#21487;&#36827;&#19968;&#27493;&#29992;&#20110;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20943;&#36731;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#30340;&#24037;&#20316;&#37327;&#12290;CFEVER&#21487;&#22312;&#32593;&#22336;&#22788;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13025v1 Announce Type: cross  Abstract: We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as "Supports", "Refutes", or "Not Enough Info" to depict its degree of factualness. Similar to the FEVER dataset, claims in the "Supports" and "Refutes" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts. CFEVER is available at htt
&lt;/p&gt;</description></item><item><title>SoMeLVLM&#26159;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#30693;&#35782;&#29702;&#35299;&#12289;&#24212;&#29992;&#12289;&#20998;&#26512;&#12289;&#35780;&#20215;&#21644;&#21019;&#36896;&#31561;&#20116;&#22823;&#20851;&#38190;&#33021;&#21147;&#65292;&#33268;&#21147;&#20110;&#29702;&#35299;&#21644;&#29983;&#25104;&#36924;&#30495;&#30340;&#31038;&#20132;&#23186;&#20307;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.13022</link><description>&lt;p&gt;
SoMeLVLM: &#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoMeLVLM: A Large Vision Language Model for Social Media Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13022
&lt;/p&gt;
&lt;p&gt;
SoMeLVLM&#26159;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#30693;&#35782;&#29702;&#35299;&#12289;&#24212;&#29992;&#12289;&#20998;&#26512;&#12289;&#35780;&#20215;&#21644;&#21019;&#36896;&#31561;&#20116;&#22823;&#20851;&#38190;&#33021;&#21147;&#65292;&#33268;&#21147;&#20110;&#29702;&#35299;&#21644;&#29983;&#25104;&#36924;&#30495;&#30340;&#31038;&#20132;&#23186;&#20307;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#22686;&#38271;&#20197;&#20854;&#22810;&#27169;&#24335;&#29305;&#24615;&#20026;&#29305;&#24449;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#29616;&#35937;&#21644;&#25361;&#25112;&#30340;&#20986;&#29616;&#65292;&#36825;&#38656;&#35201;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#32479;&#19968;&#35299;&#20915;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#24378;&#22823;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20351;&#21516;&#26102;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#21363;&#20351;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#29992;&#39046;&#22495;&#27169;&#22411;&#22312;&#19982;&#31038;&#20132;&#23186;&#20307;&#20219;&#21153;&#30340;&#29420;&#29305;&#35328;&#35821;&#39118;&#26684;&#21644;&#35821;&#22659;&#36827;&#34892;&#23545;&#40784;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;SoMeLVLM&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#37197;&#22791;&#26377;&#30693;&#35782;&#21644;&#29702;&#35299;&#12289;&#24212;&#29992;&#12289;&#20998;&#26512;&#12289;&#35780;&#20215;&#21644;&#21019;&#36896;&#20116;&#20010;&#20851;&#38190;&#33021;&#21147;&#30340;&#35748;&#30693;&#26694;&#26550;&#12290;SoMeLVLM&#34987;&#35774;&#35745;&#20026;&#29702;&#35299;&#21644;&#29983;&#25104;&#36924;&#30495;&#30340;&#31038;&#20132;&#23186;&#20307;&#34892;&#20026;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;654k&#30340;&#22810;&#27169;&#24335;&#31038;&#20132;&#23186;&#20307;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#35748;&#30693;&#26694;&#26550;&#21644;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13022v1 Announce Type: new  Abstract: The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge &amp; comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Ou
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;transformer&#27169;&#22411;&#26102;&#65292;&#38024;&#23545;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#20010;&#21035;&#35821;&#35328;&#26631;&#31614;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20026;&#27599;&#31181;&#35821;&#35328;&#20998;&#21035;&#35745;&#31639;&#31867;&#21035;&#26435;&#37325;&#65292;&#21487;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#12289;&#35821;&#35328;&#20998;&#31163;&#26356;&#26126;&#26174;&#21644;&#26080;&#20449;&#24687;&#29305;&#24449;&#20419;&#36827;&#31561;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13016</link><description>&lt;p&gt;
&#29702;&#35299;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#29305;&#23450;&#35821;&#35328;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the effects of language-specific class imbalance in multilingual fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13016
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;transformer&#27169;&#22411;&#26102;&#65292;&#38024;&#23545;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#20010;&#21035;&#35821;&#35328;&#26631;&#31614;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20026;&#27599;&#31181;&#35821;&#35328;&#20998;&#21035;&#35745;&#31639;&#31867;&#21035;&#26435;&#37325;&#65292;&#21487;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#12289;&#35821;&#35328;&#20998;&#31163;&#26356;&#26126;&#26174;&#21644;&#26080;&#20449;&#24687;&#29305;&#24449;&#20419;&#36827;&#31561;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#23454;&#29983;&#27963;&#20013;&#22810;&#35821;&#35328;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#23384;&#22312;&#30340;&#19968;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#30340;&#24433;&#21709;&#65306;&#36328;&#35821;&#35328;&#26631;&#31614;&#30340;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20855;&#26377;&#36825;&#31181;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#35328;&#20998;&#31163;&#26356;&#21152;&#26126;&#26174;&#65292;&#24182;&#20419;&#36827;&#20102;&#26080;&#20449;&#24687;&#29305;&#24449;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#20256;&#32479;&#30340;&#31867;&#21035;&#21152;&#26435;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21035;&#35745;&#31639;&#27599;&#31181;&#35821;&#35328;&#30340;&#31867;&#21035;&#26435;&#37325;&#26469;&#32531;&#35299;&#36825;&#20123;&#19981;&#21033;&#24433;&#21709;&#12290;&#36825;&#20123;&#32467;&#26524;&#24847;&#35782;&#21040;&#20102;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#29305;&#23450;&#35821;&#35328;&#19981;&#24179;&#34913;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#20381;&#36182;&#35821;&#35328;&#20998;&#31163;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13016v1 Announce Type: new  Abstract: We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20026;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27880;&#37322;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#36807;&#28388;&#31574;&#30053;&#26469;&#25552;&#39640;&#20197;&#20195;&#30721;&#20026;&#28966;&#28857;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13013</link><description>&lt;p&gt;
&#20195;&#30721;&#38656;&#35201;&#27880;&#37322;&#65306;&#20351;&#29992;&#27880;&#37322;&#22686;&#24378;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Code Needs Comments: Enhancing Code LLMs with Comment Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13013
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20026;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27880;&#37322;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#36807;&#28388;&#31574;&#30053;&#26469;&#25552;&#39640;&#20197;&#20195;&#30721;&#20026;&#28966;&#28857;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#25216;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#33021;&#21147;&#65292;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#32534;&#31243;&#35821;&#35328;&#65288;PLs&#65289;&#21450;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;NLs&#65289;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#27880;&#37322;&#23494;&#24230;&#20316;&#20026;PL-NL&#23545;&#40784;&#30340;&#34913;&#37327;&#26631;&#20934;&#65292;&#26469;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20197;&#20195;&#30721;&#20026;&#28966;&#28857;&#30340;LLMs&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#20047;&#19982;&#20195;&#30721;&#27880;&#37322;&#23545;&#40784;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20026;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27880;&#37322;&#65292;&#21516;&#26102;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#31574;&#30053;&#26469;&#28388;&#38500;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#24615;&#36739;&#24046;&#30340;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20197;&#20195;&#30721;&#20026;&#28966;&#28857;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32534;&#31243;&#25216;&#33021;&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#29983;&#25104;&#27880;&#37322;&#30340;&#27169;&#22411;&#20197;&#21450;&#22312;&#27809;&#26377;&#22686;&#24378;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13013v1 Announce Type: new  Abstract: The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13006</link><description>&lt;p&gt;
&#25506;&#31350;&#27169;&#22411;&#19981;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Instability on Explanations and Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13006
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#65292;&#28982;&#32780;&#65292;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#12289;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#25197;&#26354;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#22312;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#34987;&#20840;&#38754;&#35780;&#20272;&#65292;&#22240;&#27492;&#24456;&#38590;&#35780;&#20272;&#29305;&#23450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#23581;&#35797;&#20026;&#35299;&#37322;&#21019;&#24314;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#20294;&#27809;&#26377;&#20154;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#20043;&#38388;&#30340;&#29616;&#26377;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25512;&#26029;&#26102;&#24341;&#20837;&#22122;&#22768;&#26469;&#20154;&#20026;&#27169;&#25311;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25554;&#20837;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#24182;&#27979;&#37327;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#28982;&#32780;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#19981;&#30830;&#23450;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#35299;&#37322;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#33655;&#20848;&#26041;&#35328;&#21644;&#38397;&#26041;&#35328;&#65292;&#21457;&#29616;&#20102;&#26041;&#35328;&#23618;&#38754;&#19978;&#35789;&#38271;&#21644;&#38899;&#20301;&#32467;&#26500;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#25299;&#23637;&#20102;&#20197;&#24448;&#20165;&#22312;&#35821;&#35328;&#23618;&#38754;&#35760;&#24405;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12998</link><description>&lt;p&gt;
&#26041;&#35328;&#20043;&#38388;&#30340;&#38899;&#20301;&#32467;&#26500;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Phonotactic Complexity across Dialects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#33655;&#20848;&#26041;&#35328;&#21644;&#38397;&#26041;&#35328;&#65292;&#21457;&#29616;&#20102;&#26041;&#35328;&#23618;&#38754;&#19978;&#35789;&#38271;&#21644;&#38899;&#20301;&#32467;&#26500;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#25299;&#23637;&#20102;&#20197;&#24448;&#20165;&#22312;&#35821;&#35328;&#23618;&#38754;&#35760;&#24405;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23398;&#31867;&#22411;&#23398;&#20013;&#30340;&#20849;&#35782;&#35748;&#20026;&#65292;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#30340;&#32467;&#26500;&#22312;&#26576;&#20010;&#32500;&#24230;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#37027;&#20040;&#22312;&#21478;&#19968;&#20010;&#32500;&#24230;&#19978;&#20250;&#21464;&#24471;&#26356;&#31616;&#21333;&#65292;&#36825;&#26159;&#24314;&#31435;&#22312;&#25152;&#26377;&#35821;&#35328;&#21516;&#31561;&#22797;&#26434;&#30340;&#20551;&#35774;&#20043;&#19978;&#30340;&#65288;Joseph and Newmeyer&#65292;2012&#65289;&#12290;&#25105;&#20204;&#22312;&#24494;&#35266;&#23618;&#38754;&#30740;&#31350;&#20102;&#36825;&#19968;&#35828;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#32452;&#20005;&#26684;&#25511;&#21046;&#30340;&#33655;&#20848;&#26041;&#35328;&#65288;&#26469;&#33258;366&#20010;&#25910;&#38598;&#28857;&#65289;&#21644;&#38397;&#26041;&#35328;&#65288;&#26469;&#33258;60&#20010;&#28857;&#65289;&#30340;&#26679;&#26412;&#65292;&#36825;&#20351;&#24471;&#22312;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#20043;&#38388;&#36827;&#34892;&#26356;&#20844;&#24179;&#30340;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#12290;&#29978;&#33267;&#22312;&#26041;&#35328;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#22522;&#20110;LSTM&#30340;&#25163;&#26426;&#32423;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35789;&#38271;&#21644;&#38899;&#20301;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#24230;&#37327;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26435;&#34913;&#8212;&#8212;&#36825;&#19968;&#32467;&#26524;&#20043;&#21069;&#20165;&#22312;&#35821;&#35328;&#23618;&#38754;&#19978;&#26377;&#35760;&#24405;&#12290;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;GAM&#65289;&#26174;&#31034;&#65292;&#38899;&#20301;&#32467;&#26500;&#22797;&#26434;&#24615;&#36739;&#20302;&#30340;&#26041;&#35328;&#38598;&#20013;&#22312;&#39318;&#37117;&#22320;&#21306;&#21608;&#22260;&#65292;&#25105;&#20204;&#20551;&#35774;&#36825;&#21487;&#33021;&#23545;&#24212;&#20110;&#20043;&#21069;&#30340;&#20551;&#35774;&#65292;&#21363;&#35268;&#27169;&#26356;&#22823;&#25110;&#26356;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#21464;&#20307;&#20250;&#26174;&#31034;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12998v1 Announce Type: new  Abstract: Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.12991</link><description>&lt;p&gt;
TRAP: &#38754;&#21521;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#30340;&#26377;&#38024;&#23545;&#24615;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#35825;&#39285;
&lt;/p&gt;
&lt;p&gt;
TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12991
&lt;/p&gt;
&lt;p&gt;
TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21644;&#27169;&#22411;&#36890;&#24120;&#20276;&#38543;&#30528;&#20851;&#20110;&#35841;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#20197;&#21450;&#20182;&#20204;&#24517;&#39035;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#30340;&#27861;&#24459;&#35268;&#23450;&#12290;&#35780;&#20272;&#21457;&#24067;&#30340;LLMs&#30340;&#21512;&#35268;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#23450;&#20445;&#25252;&#20102;LLM&#36129;&#29486;&#32773;&#30340;&#21033;&#30410;&#24182;&#38450;&#27490;&#20102;&#28389;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#65288;BBIV&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#31532;&#19977;&#26041;&#24212;&#29992;&#26159;&#21542;&#36890;&#36807;&#20854;&#32842;&#22825;&#21151;&#33021;&#20351;&#29992;&#26576;&#20010;&#29305;&#23450;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#65288;TRAP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#27491;&#22312;&#20351;&#29992;&#30340;&#20855;&#20307;LLM&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#26368;&#21021;&#29992;&#20110;&#36234;&#29425;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#20197;&#20174;&#30446;&#26631;LLM&#33719;&#24471;&#39044;&#23450;&#20041;&#30340;&#31572;&#26696;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#32473;&#20986;&#38543;&#26426;&#31572;&#26696;&#12290;TRAP&#21487;&#20197;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;&#21363;&#20351;LLM&#26377;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#30340;&#32454;&#24494;&#21464;&#21270;&#65292;TRAP&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GraphAdapter&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#39640;&#25928;&#36866;&#37197;&#22120;&#65292;&#19982;LLMs&#21327;&#21516;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.12984</link><description>&lt;p&gt;
GNN&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;LLMs&#30340;&#33391;&#22909;&#36866;&#37197;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GNN be Good Adapter for LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GraphAdapter&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#39640;&#25928;&#36866;&#37197;&#22120;&#65292;&#19982;LLMs&#21327;&#21516;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;&#25968;&#25454;&#21644;&#38646;&#27425;&#23398;&#20064;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20026;&#35768;&#22810;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#24314;&#27169;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraphAdapter&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;LLMs&#30340;&#39640;&#25928;&#36866;&#37197;&#22120;&#65292;&#20197;&#24212;&#23545;TAGs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12984v1 Announce Type: cross  Abstract: Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable param
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32500;&#20998;&#26512;&#21457;&#29616;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20854;&#20013;&#37096;&#20998;&#27169;&#22411;&#23545;&#31034;&#33539;&#36136;&#37327;&#19981;&#25935;&#24863;&#65292;&#32780;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#21487;&#20197;&#28040;&#38500;&#23545;&#31034;&#33539;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.12976</link><description>&lt;p&gt;
&#31034;&#33539;&#23545;&#22810;&#35821;&#22659;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#22810;&#32500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12976
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32500;&#20998;&#26512;&#21457;&#29616;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20854;&#20013;&#37096;&#20998;&#27169;&#22411;&#23545;&#31034;&#33539;&#36136;&#37327;&#19981;&#25935;&#24863;&#65292;&#32780;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#21487;&#20197;&#28040;&#38500;&#23545;&#31034;&#33539;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#31034;&#33539;&#32463;&#24120;&#34987;&#29992;&#20316;&#25512;&#29702;&#31574;&#30053;&#65292;&#22312;&#27492;&#31574;&#30053;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#31034;&#33539;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#19982;&#21333;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#35813;&#29615;&#22659;&#20013;&#31034;&#33539;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22810;&#35821;&#22659;&#23398;&#20064;&#36827;&#34892;&#20102;&#22810;&#32500;&#20998;&#26512;&#65292;&#23454;&#39564;&#37319;&#29992;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#30340;5&#20010;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#22312;&#20869;&#30340;9&#20010;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;56&#31181;&#31867;&#22411;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Llama 2-Chat&#12289;GPT-3.5&#21644;GPT-4&#23545;&#31034;&#33539;&#36136;&#37327;&#30340;&#25935;&#24863;&#24230;&#36739;&#20302;&#12290;&#30456;&#21453;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#24448;&#24448;&#20250;&#28040;&#38500;&#19968;&#20123;&#27169;&#22411;&#23545;&#31034;&#33539;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t
&lt;/p&gt;</description></item><item><title>Gl'orIA&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#35774;&#35745;&#30340;&#24378;&#22823;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;350&#20159;&#20010;tokens&#30340;&#20840;&#38754;PT-PT&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;CALAME-PT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.12969</link><description>&lt;p&gt;
Gl'orIA - &#19968;&#31181;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#29983;&#25104;&#24335;&#24320;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gl\'orIA - A Generative and Open Large Language Model for Portuguese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12969
&lt;/p&gt;
&lt;p&gt;
Gl'orIA&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#35774;&#35745;&#30340;&#24378;&#22823;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;350&#20159;&#20010;tokens&#30340;&#20840;&#38754;PT-PT&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;CALAME-PT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#35768;&#22810;&#39640;&#36164;&#28304;&#35821;&#35328;&#37117;&#26377;&#20016;&#23500;&#30340;LLM&#65292;&#20294;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#30340;&#36825;&#31181;&#27169;&#22411;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#35299;&#30721;&#22120;LLM&#8212;&#8212;Gl'orIA&#12290;&#20026;&#20102;&#23545;Gl'orIA&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#21508;&#20010;&#26469;&#28304;&#30340;350&#20159;&#20010;tokens&#30340;&#20840;&#38754;PT-PT&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#28982;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CALAME-PT&#65288;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12969v1 Announce Type: cross  Abstract: Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;&#21363;&#25552;&#31034;&#31363;&#21462;&#25915;&#20987;&#65292;&#30446;&#30340;&#26159;&#22522;&#20110;&#29983;&#25104;&#30340;&#31572;&#26696;&#31363;&#21462;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.12959</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompt Stealing Attacks Against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;&#21363;&#25552;&#31034;&#31363;&#21462;&#25915;&#20987;&#65292;&#30446;&#30340;&#26159;&#22522;&#20110;&#29983;&#25104;&#30340;&#31572;&#26696;&#31363;&#21462;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#39046;&#22495;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20381;&#36182;&#24378;&#35843;&#20102;&#8220;&#25552;&#31034;&#24037;&#31243;&#8221;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#25913;&#36827;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#26032;&#22411;&#25915;&#20987;&#65292;&#31216;&#20026;&#25552;&#31034;&#31363;&#21462;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25552;&#31034;&#31363;&#21462;&#25915;&#20987;&#26088;&#22312;&#22522;&#20110;&#29983;&#25104;&#30340;&#31572;&#26696;&#26469;&#31363;&#21462;&#36825;&#20123;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;&#25552;&#31034;&#31363;&#21462;&#25915;&#20987;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;&#21442;&#25968;&#25552;&#21462;&#22120;&#21644;&#25552;&#31034;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12959v1 Announce Type: cross  Abstract: The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457; GumbelSoft &#27700;&#21360;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#26696;&#34920;&#29616;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.12948</link><description>&lt;p&gt;
GumbelSoft: &#36890;&#36807;GumbelMax&#25216;&#24039;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#25968;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457; GumbelSoft &#27700;&#21360;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#26696;&#34920;&#29616;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#23398;&#26415;&#19981;&#35802;&#23454;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#35299;&#30721;&#20026;&#22522;&#30784;&#30340;&#27700;&#21360;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;GumbelMax&#25216;&#24039;&#30340;&#27700;&#21360;(GM&#27700;&#21360;)&#65292;&#26159;&#38450;&#33539;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#28389;&#29992;&#30340;&#26480;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#21487;&#26816;&#27979;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;GM&#27700;&#21360;&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#26041;&#38754;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23545;&#20110;&#30456;&#21516;&#25552;&#31034;&#22987;&#32456;&#20135;&#29983;&#30456;&#21516;&#36755;&#20986;&#65292;&#20174;&#32780;&#36127;&#38754;&#24433;&#21709;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;GM&#27700;&#21360;&#65292;&#21363;Logits-Addition&#27700;&#21360;&#65292;&#21450;&#20854;&#19977;&#20010;&#21464;&#20307;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#21464;&#20307;&#20013;&#65292;GumbelSoft&#27700;&#21360;(&#20316;&#20026;Logits-Addition&#27700;&#21360;&#30340;&#19968;&#20010;softmax&#21464;&#20307;)&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20854;AUROC&#20998;&#25968;&#36229;&#36807;&#20854;&#20182;&#20004;&#20010;&#26367;&#20195;&#21464;&#20307;0.1&#33267;0.3&#65292;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12948v1 Announce Type: new  Abstract: Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing oth
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19987;&#20026;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#32780;&#35774;&#35745;&#30340;"&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#35268;&#33539;&#27491;&#23383;&#27861;"&#65288;NOTA&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#38463;&#25289;&#20271;&#25991;&#25340;&#20889;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#30830;&#20445;&#23545;&#20854;&#29420;&#29305;&#38899;&#38901;&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#20934;&#30830;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.12940</link><description>&lt;p&gt;
&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#30340;&#35268;&#33539;&#27491;&#23383;&#27861;
&lt;/p&gt;
&lt;p&gt;
Normalized Orthography for Tunisian Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19987;&#20026;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#32780;&#35774;&#35745;&#30340;"&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#35268;&#33539;&#27491;&#23383;&#27861;"&#65288;NOTA&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#38463;&#25289;&#20271;&#25991;&#25340;&#20889;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#30830;&#20445;&#23545;&#20854;&#29420;&#29305;&#38899;&#38901;&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#20934;&#30830;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#65288;ISO 693-3&#65306;aeb&#65289;&#26159;&#31361;&#23612;&#26031;&#29305;&#26377;&#30340;&#19968;&#31181;&#35821;&#35328;&#21464;&#20307;&#65292;&#26368;&#21021;&#28304;&#33258;&#38463;&#25289;&#20271;&#35821;&#65292;&#24182;&#21463;&#21040;&#22810;&#31181;&#21382;&#21490;&#24433;&#21709;&#30340;&#20016;&#23500;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#20026;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#25340;&#20889;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;"CODA*&#25351;&#21335;"&#30340;"&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#35268;&#33539;&#27491;&#23383;&#27861;"&#65288;NOTA&#65289;&#65292;&#37325;&#28857;&#22312;&#20110;&#29992;&#25143;&#21451;&#22909;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#29992;&#20110;&#35821;&#35328;&#36164;&#28304;&#24320;&#21457;&#30446;&#30340;&#12290;&#26356;&#26032;&#21518;&#30340;&#26631;&#20934;&#26088;&#22312;&#35299;&#20915;&#19982;&#20934;&#30830;&#34920;&#29616;&#31361;&#23612;&#26031;&#35821;&#38899;&#38901;&#21644;&#24418;&#24577;&#29420;&#29305;&#29305;&#24449;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#23558;&#36890;&#36807;&#32416;&#27491;&#22522;&#20110;&#19982;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#30456;&#20284;&#24615;&#30340;&#38899;&#35793;&#25152;&#24341;&#21457;&#30340;&#38382;&#39064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12940v1 Announce Type: new  Abstract: Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences. This research introduces the "Normalized Orthography for Tunisian Arabic" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency. The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology. This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;-&#26426;&#21327;&#20316;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#27714;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;-&#26426;&#21327;&#20316;&#26041;&#27861;ReHAC&#65292;&#36890;&#36807;&#26500;&#24314;&#20154;-&#26426;&#21327;&#20316;&#25968;&#25454;&#38598;&#35757;&#32451;&#31574;&#30053;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12914</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;-&#26426;&#21327;&#20316;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-based Human-Agent Collaboration for Complex Task Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12914
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;-&#26426;&#21327;&#20316;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#27714;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;-&#26426;&#21327;&#20316;&#26041;&#27861;ReHAC&#65292;&#36890;&#36807;&#26500;&#24314;&#20154;-&#26426;&#21327;&#20316;&#25968;&#25454;&#38598;&#35757;&#32451;&#31574;&#30053;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#21019;&#24314;&#23436;&#20840;&#33258;&#20027;&#20195;&#29702;&#20154;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#32463;&#24120;&#34920;&#29616;&#20986;&#22312;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#21644;&#23436;&#20840;&#25226;&#25569;&#20154;&#31867;&#38656;&#27714;&#26041;&#38754;&#30340;&#26126;&#26174;&#32570;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#20154;-&#26426;&#21327;&#20316;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#27714;&#35299;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#23427;&#20204;&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;-&#26426;&#21327;&#20316;&#26041;&#27861;ReHAC&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#31574;&#30053;&#27169;&#22411;&#65292;&#26088;&#22312;&#30830;&#23450;&#20154;&#31867;&#20309;&#26102;&#20171;&#20837;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26368;&#21512;&#36866;&#30340;&#38454;&#27573;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;-&#26426;&#21327;&#20316;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#36825;&#20010;&#31574;&#30053;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#39564;&#35777;&#27979;&#35797;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#21644;LLM&#30340;&#21327;&#21516;&#21162;&#21147;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12914v1 Announce Type: new  Abstract: In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-
&lt;/p&gt;</description></item><item><title>&#30456;&#23545;&#36739;&#23567;&#30340;LLMs&#21487;&#20197;&#19982;&#22823;&#22411;LLM&#30456;&#27604;&#65292;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#27700;&#24179;</title><link>https://arxiv.org/abs/2402.12913</link><description>&lt;p&gt;
OPDAI&#22312;SemEval-2024&#20219;&#21153;6&#20013;&#65306;&#23567;&#22411;LLMs&#21487;&#20197;&#21152;&#36895;&#24187;&#35273;&#26816;&#27979;&#19982;&#24369;&#30417;&#30563;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12913
&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#36739;&#23567;&#30340;LLMs&#21487;&#20197;&#19982;&#22823;&#22411;LLM&#30456;&#27604;&#65292;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#25551;&#36848;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;LLMs&#30340;&#24187;&#35273;&#26816;&#27979;&#65292;&#22312;SemEval-2024&#20219;&#21153;6&#30340;&#27169;&#22411;&#26080;&#20851;&#36319;&#36394;&#20013;&#36194;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#24863;&#30693;&#36319;&#36394;&#20013;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#32467;&#26524;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#20351;&#29992;LLMs&#26816;&#27979;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#65292;&#32780;&#26080;&#38656;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#26469;&#39564;&#35777;&#19981;&#21516;LLMs&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#24615;&#33021;&#26356;&#22909;&#30340;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24369;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#19981;&#20165;&#28385;&#36275;&#20102;&#19981;&#21516;LLMs&#30340;&#19968;&#33268;&#24615;&#65292;&#36824;&#28385;&#36275;&#20102;&#26368;&#20339;LLM&#19982;&#19981;&#21516;&#25277;&#26679;&#21442;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26500;&#24314;&#30340;&#35757;&#32451;&#25968;&#25454;&#24494;&#35843;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#30456;&#23545;&#36739;&#23567;&#30340;LLM&#22312;&#19982;&#22823;&#22411;LLM&#30456;&#27604;&#65292;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12913v1 Announce Type: new  Abstract: This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35821;&#20041;&#22270;&#24179;&#28369;&#25216;&#26415;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#21462;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#26377;&#25928;&#25913;&#21892;&#25991;&#26412;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12890</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#22270;&#24179;&#28369;&#23454;&#29616;&#26356;&#20855;&#36776;&#21035;&#21147;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
More Discriminative Sentence Embeddings via Semantic Graph Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#22270;&#24179;&#28369;&#25216;&#26415;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#21462;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#26377;&#25928;&#25913;&#21892;&#25991;&#26412;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#32463;&#39564;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#26356;&#20855;&#36776;&#21035;&#24615;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#21033;&#29992;&#35821;&#20041;&#22270;&#24179;&#28369;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20843;&#20010;&#22522;&#20934;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#22270;&#24179;&#28369;&#22312;&#25913;&#21892;&#29992;&#20110;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#21477;&#23376;&#23884;&#20837;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12890v1 Announce Type: new  Abstract: This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAFFORD&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#23545;&#29289;&#20307;&#21487;&#20379;&#24615;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24403;&#21069;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#19981;&#24120;&#35265;&#29289;&#20307;&#21487;&#20379;&#24615;&#26041;&#38754;&#23384;&#22312;&#25512;&#29702;&#33021;&#21147;&#30340;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.12881</link><description>&lt;p&gt;
GRAFFORD: &#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#23545;&#29289;&#20307;&#21487;&#20379;&#24615;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAFFORD&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#23545;&#29289;&#20307;&#21487;&#20379;&#24615;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24403;&#21069;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#19981;&#24120;&#35265;&#29289;&#20307;&#21487;&#20379;&#24615;&#26041;&#38754;&#23384;&#22312;&#25512;&#29702;&#33021;&#21147;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#20851;&#20110;&#29289;&#20307;&#21487;&#20379;&#24615;&#30340;&#30693;&#35782;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#20013;&#23398;&#20064;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#22312;&#19979;&#28216;NLU&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;PTLM&#22312;&#25512;&#29702;&#21644;&#22522;&#30784;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#19988;&#19981;&#30452;&#35266;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#39318;&#27425;&#23450;&#37327;&#34913;&#37327;&#22522;&#30784;&#65288;&#25110;&#32570;&#20047;&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#20851;&#20110;&#29289;&#20307;&#21487;&#20379;&#24615;&#30340;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;-- GrAFFORD&#65292;&#21253;&#21547;15&#20010;&#21487;&#20379;&#24615;&#31867;&#21035;&#12290;&#19982;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#25910;&#38598;&#30340;&#21487;&#20379;&#24615;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#29992;&#29616;&#22330;&#21477;&#23376;&#26631;&#27880;&#20102;&#23545;&#35937;&#21644;&#21487;&#20379;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#28041;&#21450;&#19981;&#24120;&#35265;&#30340;&#29289;&#20307;&#21487;&#20379;&#24615;&#26102;&#65292;PTLM&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;PTLM&#22312;&#29702;&#35299;&#19981;&#24120;&#35265;&#29289;&#20307;&#21487;&#20379;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12881v1 Announce Type: new  Abstract: We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32508;&#21512;&#20998;&#26512;&#20102;&#33258;&#38381;&#30151;&#22312;&#35821;&#38899;&#12289;&#35821;&#35328;&#21644;&#35328;&#35821;&#20013;&#30340;&#34920;&#29616;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#25351;&#31034;&#33258;&#38381;&#30151;&#30340;&#35821;&#35328;&#12289;&#38901;&#24459;&#21644;&#22768;&#23398;&#32447;&#32034;&#30340;&#35843;&#26597;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#33258;&#38381;&#30151;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.12880</link><description>&lt;p&gt;
&#33258;&#38381;&#30151;&#35328;&#35821;&#26816;&#27979; - &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Autism Detection in Speech - A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32508;&#21512;&#20998;&#26512;&#20102;&#33258;&#38381;&#30151;&#22312;&#35821;&#38899;&#12289;&#35821;&#35328;&#21644;&#35328;&#35821;&#20013;&#30340;&#34920;&#29616;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#25351;&#31034;&#33258;&#38381;&#30151;&#30340;&#35821;&#35328;&#12289;&#38901;&#24459;&#21644;&#22768;&#23398;&#32447;&#32034;&#30340;&#35843;&#26597;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#33258;&#38381;&#30151;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#33539;&#22260;&#28085;&#30422;&#20102;&#33258;&#38381;&#30151;&#22312;&#22768;&#38899;&#12289;&#35821;&#35328;&#21644;&#35328;&#35821;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#12289;&#24515;&#29702;&#23398;&#39046;&#22495;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#20197;&#25214;&#20986;&#21487;&#33021;&#25351;&#31034;&#33258;&#38381;&#30151;&#30340;&#35821;&#35328;&#12289;&#38901;&#24459;&#21644;&#22768;&#23398;&#32447;&#32034;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#36825;&#19977;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#33258;&#38381;&#30151;&#65292;&#20197;&#21450;&#21487;&#33021;&#24433;&#21709;&#27491;&#30830;&#26816;&#27979;&#35813;&#30142;&#30149;&#30340;&#20849;&#30149;&#30151;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20102;&#35789;&#27719;&#21644;&#35821;&#20041;&#27969;&#21033;&#24615;&#12289;&#38901;&#24459;&#29305;&#24449;&#65292;&#20197;&#21450;&#19981;&#36830;&#36143;&#24615;&#21644;&#35828;&#35805;&#36895;&#24230;&#31561;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#21333;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#38899;&#39057;&#25968;&#25454;&#21644;&#25991;&#26412;&#36716;&#24405;&#19978;&#37117;&#20351;&#29992;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20294;&#22899;&#24615;&#24739;&#32773;&#20284;&#20046;&#21463;&#21040;&#20005;&#37325;&#30340;&#30740;&#31350;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#32780;&#19981;&#26159;&#21464;&#21387;&#22120;&#65292;&#22312;&#36825;&#26041;&#38754;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12880v1 Announce Type: new  Abstract: There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this conte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#38598;&#25104;&#21040;LLM&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.12869</link><description>&lt;p&gt;
&#25506;&#32034;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#23545;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#39046;&#22495;&#28151;&#21512;&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#38598;&#25104;&#21040;LLM&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#20013;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#25968;&#25454;&#36890;&#24120;&#20197;&#28151;&#21512;&#26684;&#24335;&#23384;&#22312;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#65292;&#23545;&#20449;&#24687;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#34920;&#26684;&#36716;&#25991;&#26412;&#29983;&#25104;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#23558;&#28151;&#21512;&#25968;&#25454;&#36716;&#21270;&#20026;&#32479;&#19968;&#25991;&#26412;&#26684;&#24335;&#30340;&#35821;&#26009;&#24211;&#12290;&#23613;&#31649;&#36825;&#31181;&#25216;&#26415;&#24050;&#32463;&#34987;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#20004;&#27493;&#35299;&#20915;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#34920;&#26684;&#36716;&#25991;&#26412;&#29983;&#25104;&#21019;&#26032;&#22320;&#38598;&#25104;&#21040;&#22686;&#24378;&#22522;&#20110;LLM&#30340;QA&#31995;&#32479;&#19982;&#39046;&#22495;&#28151;&#21512;&#25968;&#25454;&#26694;&#26550;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#22312;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;QA&#31995;&#32479;&#36827;&#34892;&#20102;&#27979;&#35797;&#65288;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12869v1 Announce Type: new  Abstract: Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (
&lt;/p&gt;</description></item><item><title>&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;&#65292;&#25366;&#25496;&#20449;&#24687;&#22312;LMs&#20869;&#37096;&#30340;&#27969;&#21160;&#26041;&#24335;&#65292;&#25506;&#32034;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.12865</link><description>&lt;p&gt;
&#21453;&#21521;&#38236;&#22836;&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
Backward Lens: Projecting Language Model Gradients into the Vocabulary Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12865
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;&#65292;&#25366;&#25496;&#20449;&#24687;&#22312;LMs&#20869;&#37096;&#30340;&#27969;&#21160;&#26041;&#24335;&#65292;&#25506;&#32034;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#22914;&#20309;&#23398;&#20064;&#21644;&#35760;&#24518;&#20449;&#24687;&#26159;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#23558;&#20174;&#21069;&#21521;&#20256;&#25773;&#20013;&#33719;&#24471;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#20013;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;LMs&#20869;&#37096;&#20449;&#24687;&#27969;&#21160;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;LMs&#30340;&#21518;&#21521;&#20256;&#25773;&#21644;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#26799;&#24230;&#30697;&#38453;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#20854;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#36755;&#20837;&#30340;&#20302;&#31209;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#26041;&#27861;&#23558;&#36825;&#20123;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#39033;&#20013;&#65292;&#24182;&#25506;&#35752;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12865v1 Announce Type: cross  Abstract: Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#31946;&#24773;&#32490;&#34920;&#36798;&#26816;&#27979;&#20026;&#39046;&#22495;&#22806;&#26679;&#26412;&#65292;&#24182;&#23558;&#24773;&#32490;&#34920;&#31034;&#20026;&#20998;&#24067;&#26469;&#26377;&#25928;&#22788;&#29702;&#24773;&#32490;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12862</link><description>&lt;p&gt;
&#22788;&#29702;&#24773;&#32490;&#20013;&#30340;&#27495;&#20041;&#65306;&#20174;&#39046;&#22495;&#22806;&#26816;&#27979;&#21040;&#20998;&#24067;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#31946;&#24773;&#32490;&#34920;&#36798;&#26816;&#27979;&#20026;&#39046;&#22495;&#22806;&#26679;&#26412;&#65292;&#24182;&#23558;&#24773;&#32490;&#34920;&#31034;&#20026;&#20998;&#24067;&#26469;&#26377;&#25928;&#22788;&#29702;&#24773;&#32490;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#30340;&#20027;&#35266;&#24863;&#30693;&#23548;&#33268;&#20154;&#31867;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#26631;&#31614;&#19981;&#19968;&#33268;&#12290;&#22312;&#35757;&#32451;&#24773;&#32490;&#20998;&#31867;&#22120;&#26102;&#65292;&#36890;&#24120;&#20250;&#25490;&#38500;&#32570;&#20047;&#22810;&#25968;&#19968;&#33268;&#26631;&#31614;&#30340;&#35805;&#35821;&#65292;&#36825;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#27169;&#31946;&#30340;&#24773;&#32490;&#34920;&#36798;&#26102;&#20250;&#36896;&#25104;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19977;&#31181;&#22788;&#29702;&#27169;&#31946;&#24773;&#32490;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#32570;&#20047;&#22810;&#25968;&#19968;&#33268;&#26631;&#31614;&#30340;&#35805;&#35821;&#32435;&#20837;&#20998;&#31867;&#22120;&#20316;&#20026;&#21478;&#19968;&#31867;&#20250;&#38477;&#20302;&#20854;&#20182;&#24773;&#32490;&#31867;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26469;&#37327;&#21270;&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23558;&#20855;&#26377;&#27169;&#31946;&#24773;&#32490;&#30340;&#35805;&#35821;&#26816;&#27979;&#20026;&#39046;&#22495;&#22806;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#26816;&#27979;&#27169;&#31946;&#24773;&#32490;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#27169;&#31946;&#24773;&#32490;&#20013;&#33719;&#24471;&#32454;&#31890;&#24230;&#21306;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#24773;&#32490;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12862v1 Announce Type: new  Abstract: The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24341;&#23548;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;MoELoRA&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26102;&#30340;&#28789;&#27963;&#32452;&#21512;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.12851</link><description>&lt;p&gt;
MoELoRA&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#24341;&#23548;&#19979;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12851
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24341;&#23548;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;MoELoRA&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26102;&#30340;&#28789;&#27963;&#32452;&#21512;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#32463;&#24120;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#36825;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#20316;&#20026;&#19968;&#20010;&#26480;&#20986;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#37319;&#29992;&#26377;&#38480;&#20840;&#23616;&#21442;&#25968;&#38598;&#30340;PEFT&#26041;&#27861;&#65288;&#22914;LoRA&#65292;&#23558;&#20302;&#31209;&#36924;&#36817;&#30697;&#38453;&#28155;&#21152;&#21040;&#25152;&#26377;&#26435;&#37325;&#65289;&#22312;&#28789;&#27963;&#32452;&#21512;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#19981;&#21516;&#35745;&#31639;&#27169;&#22359;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65306;MoELoRA&#12290;&#25105;&#20204;&#23558;LoRA&#35270;&#20026;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#20026;&#20102;&#20943;&#36731;MoE&#20013;&#35266;&#23519;&#21040;&#30340;&#38543;&#26426;&#36335;&#30001;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#40723;&#21169;&#19987;&#23478;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12851v1 Announce Type: new  Abstract: Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to lear
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12847</link><description>&lt;p&gt;
&#35843;&#25972;&#36807;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#30693;&#35782;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models are Better Knowledge Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21161;&#25163;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24517;&#39035;&#33021;&#22815;&#36890;&#36807;&#25345;&#32493;&#22312;&#26032;&#25968;&#25454;&#19978;&#35757;&#32451;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20256;&#32479;&#20570;&#27861;&#28041;&#21450;&#22312;&#26032;&#25991;&#26723;&#19978;&#25345;&#32493;&#39044;&#22521;&#35757;&#65292;&#28982;&#21518;&#26681;&#25454;&#38382;&#39064;-&#31572;&#26696;&#65288;QA&#65289;&#23545;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;ICON&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#65292;&#25552;&#21319;&#31995;&#32479;&#25429;&#25417;&#35821;&#20041;&#31561;&#25928;&#30149;&#21464;&#30456;&#20284;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12844</link><description>&lt;p&gt;
ICON&#65306;&#36890;&#36807;&#30149;&#21464;&#24863;&#30693;&#28151;&#21512;&#22686;&#24378;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;ICON&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#65292;&#25552;&#21319;&#31995;&#32479;&#25429;&#25417;&#35821;&#20041;&#31561;&#25928;&#30149;&#21464;&#30456;&#20284;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#20808;&#21069;&#30740;&#31350;&#22312;&#22686;&#21152;&#29983;&#25104;&#25253;&#21578;&#30340;&#20020;&#24202;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20854;&#24212;&#20855;&#22791;&#30340;&#21478;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#36136;&#65292;&#21363;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#65292;&#25351;&#30340;&#26159;&#23545;&#35821;&#20041;&#19978;&#31561;&#25928;&#30340;X&#23556;&#32447;&#29031;&#29255;&#29983;&#25104;&#19968;&#33268;&#24615;&#25253;&#21578;&#30340;&#33021;&#21147;&#12290;ICON&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12844v1 Announce Type: cross  Abstract: Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesion
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>ArabicMMLU&#26159;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30340;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#23398;&#26657;&#32771;&#35797;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;35&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#24615;&#33021;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12840</link><description>&lt;p&gt;
ArabicMMLU&#65306;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#20013;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12840
&lt;/p&gt;
&lt;p&gt;
ArabicMMLU&#26159;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30340;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#23398;&#26657;&#32771;&#35797;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;35&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#24615;&#33021;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#37325;&#28857;&#24050;&#32463;&#36716;&#21521;&#25512;&#29702;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#36825;&#24471;&#30410;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#37096;&#20998;&#22312;&#22823;&#37327;&#38463;&#25289;&#20271;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#20294;&#30001;&#20110;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ArabicMMLU&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#25968;&#25454;&#26469;&#33258;&#20110;&#36328;&#36234;&#21271;&#38750;&#12289;&#40654;&#20961;&#29305;&#21644;&#28023;&#28286;&#22320;&#21306;&#19981;&#21516;&#22269;&#23478;&#25945;&#32946;&#27700;&#24179;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21253;&#25324;40&#20010;&#20219;&#21153;&#21644;14,575&#20010;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36890;&#36807;&#19982;&#35813;&#22320;&#21306;&#30340;&#27597;&#35821;&#32773;&#21512;&#20316;&#31934;&#24515;&#26500;&#24314;&#12290;&#25105;&#20204;&#23545;35&#20010;&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#22909;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;BLOOMZ&#12289;mT0&#12289;LLama2&#21644;Fa&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12840v1 Announce Type: new  Abstract: The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Fa
&lt;/p&gt;</description></item><item><title>PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.12835</link><description>&lt;p&gt;
PANDA: &#29992;&#20110;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12835
&lt;/p&gt;
&lt;p&gt;
PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#29305;&#23450;&#39046;&#22495;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#32791;&#36153;&#36164;&#28304;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#23553;&#38381;&#28304;&#21830;&#19994;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PANDA&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;LLMs&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PANDA&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;PANDA&#30340;LLM&#29978;&#33267;&#36229;&#36807;&#20102;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12821</link><description>&lt;p&gt;
&#22312;&#25688;&#35201;&#20013;&#35782;&#21035;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65306;&#26397;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#25277;&#35937;&#24615;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21830;&#19994;&#37096;&#32626;&#26500;&#25104;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#23637;&#24320;&#65306;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#31934;&#28860;&#19968;&#20010;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21151;&#25928;&#24615;&#30340;&#26356;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;&#39318;&#20808;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#38646;&#26679;&#26412;&#33539;&#24335;&#65292;&#36328;&#36234;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65306;&#30452;&#25509;&#25512;&#29702;&#25972;&#20010;&#25688;&#35201;&#25110;&#27599;&#20010;&#25688;&#35201;&#31383;&#21475;&#65307;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#36827;&#34892;&#23454;&#20307;&#39564;&#35777;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#33021;&#22815;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#24179;&#22343;&#36229;&#36807;&#24378;&#22823;&#30340;&#35757;&#32451;&#22522;&#32447;2.8%&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#38024;&#23545;&#31934;&#28860;&#26356;&#23567;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19968;&#27425;&#24615;&#39640;&#20934;&#30830;&#22320;&#35780;&#20998;&#25972;&#20010;&#25688;&#35201;&#65292;&#32988;&#36807;&#38646;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12817</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65306;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12817
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#20197;&#22312;&#26631;&#31614;&#19981;&#36275;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20063;&#23545;&#25152;&#35859;&#30340;&#38543;&#26426;&#22240;&#32032;&#65288;&#20363;&#22914;&#25968;&#25454;&#30340;&#21464;&#21270;&#39034;&#24207;&#65289;&#24341;&#20837;&#30340;&#26080;&#27861;&#25511;&#21046;&#30340;&#38543;&#26426;&#24615;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27979;&#37327;&#21333;&#20010;&#38543;&#26426;&#22240;&#32032;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#20102;&#24615;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#21464;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;7&#20010;&#20195;&#34920;&#24615;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#26041;&#27861;&#20197;&#21450;3&#20010;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#29616;&#26377;&#20316;&#21697;&#20013;&#24573;&#30053;&#38543;&#26426;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22240;&#20026;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21542;&#23450;&#20102;&#19968;&#20123;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
&lt;/p&gt;</description></item><item><title>SymBa&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#25552;&#21319;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.12806</link><description>&lt;p&gt;
SymBa&#65306;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#29992;&#20110;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12806
&lt;/p&gt;
&lt;p&gt;
SymBa&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#25552;&#21319;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#24605;&#32500;&#25552;&#31034;&#20013;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#24544;&#23454;&#30340;&#22810;&#27493;&#25512;&#29702;&#20381;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21521;&#21518;&#25512;&#29702;&#65292;&#21363;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#36882;&#24402;&#22320;&#20998;&#35299;&#26597;&#35810;&#65292;&#30452;&#21040;&#35777;&#26126;&#20026;&#27490;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#21521;&#21518;&#25512;&#29702;&#23454;&#29616;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymBa&#65288;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#65289;&#12290;&#22312;SymBa&#20013;&#65292;&#31526;&#21495;&#21270;&#33258;&#39030;&#21521;&#19979;&#27714;&#35299;&#22120;&#25511;&#21046;&#25972;&#20010;&#35777;&#26126;&#36807;&#31243;&#65292;&#24403;&#27714;&#35299;&#22120;&#36935;&#21040;&#27515;&#32993;&#21516;&#26102;&#65292;&#25165;&#35843;&#29992;LLM&#29983;&#25104;&#21333;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;-LLM&#38598;&#25104;&#65292;SymBa&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#22522;&#20934;&#65288;ProofWriter&#65292;Birds-Electricity&#65292;GSM8k&#65292;CLUTRR-TF&#65292;ECtHR Article 6&#65289;&#20013;&#30456;&#27604;&#21521;&#21518;&#25512;&#29702;&#22522;&#32447;&#21462;&#24471;&#20102;&#24615;&#33021;&#12289;&#35777;&#26126;&#24544;&#23454;&#24615;&#21644;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12806v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning ability as in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the LLM is called to generate a single reasoning step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.
&lt;/p&gt;</description></item><item><title>&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#31181;&#35821;&#35328;&#20013;&#30340;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#32988;&#36807;LLM&#25552;&#31034;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.12801</link><description>&lt;p&gt;
&#19977;&#31181;&#35821;&#35328;&#20013;&#30340;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#65306;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#32988;&#36807;LLM&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12801
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#31181;&#35821;&#35328;&#20013;&#30340;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#32988;&#36807;LLM&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#26399;&#26395;&#23427;&#20204;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#33021;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;8&#20010;&#39046;&#22495;&#20869;&#65288;&#20020;&#24202;&#65289;&#21644;6&#20010;&#39046;&#22495;&#22806;&#30340;&#40644;&#37329;&#26631;&#20934;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;16&#20010;&#29992;&#20110;&#25991;&#26412;&#32534;&#30721;&#30340;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;BiLSTM-CRF&#30417;&#30563;&#26631;&#27880;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#38480;&#21046;&#21487;&#29992;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#37327;&#20026;100&#20010;&#21477;&#23376;&#26469;&#21019;&#24314;&#19968;&#20010;&#23569;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#26356;&#22823;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#24448;&#24448;&#22312;&#20020;&#24202;&#39046;&#22495;&#20043;&#22806;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;F-measure&#65292;&#20294;&#36825;&#31181;&#24615;&#33021;&#27700;&#24179;&#24182;&#26410;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12801v1 Announce Type: new  Abstract: Large Language Models are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their few-shot capacities are expected to yield high performance in low-resource settings. Herein, we aim to assess the performance of Large Language Models for few shot clinical entity recognition in multiple languages. We evaluate named entity recognition in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora. We assess the performance of 10 auto-regressive language models using prompting and 16 masked language models used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot set-up by limiting the amount of annotated data available to 100 sentences. Our experiments show that although larger prompt-based models tend to achieve competitive F-measure for named entity recognition outside the clinical domain, this level of performance does no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21475;&#35821;&#30340;&#19981;&#21516;&#35821;&#35328;&#39118;&#26684;&#20570;&#20986;&#24688;&#24403;&#22238;&#24212;&#65292;&#24182;&#20026;&#27492;&#25910;&#38598;&#20102;&#19968;&#32452;&#36866;&#21512;&#35757;&#32451;&#30340;&#35821;&#38899;&#23545;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.12786</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36215;&#26469;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#35821;&#35328;&#39118;&#26684;&#24182;&#22312;&#21475;&#35821;&#20132;&#27969;&#20013;&#20570;&#20986;&#24688;&#24403;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21475;&#35821;&#30340;&#19981;&#21516;&#35821;&#35328;&#39118;&#26684;&#20570;&#20986;&#24688;&#24403;&#22238;&#24212;&#65292;&#24182;&#20026;&#27492;&#25910;&#38598;&#20102;&#19968;&#32452;&#36866;&#21512;&#35757;&#32451;&#30340;&#35821;&#38899;&#23545;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#65292;&#21363;&#20351;&#20004;&#20010;&#24403;&#21069;&#23545;&#35805;&#26159;&#30456;&#21516;&#30340;&#21477;&#23376;&#65292;&#20294;&#24403;&#20197;&#19981;&#21516;&#39118;&#26684;&#21457;&#38899;&#26102;&#65292;&#23427;&#20204;&#30340;&#22238;&#24212;&#20173;&#21487;&#33021;&#19981;&#21516;&#12290;&#35821;&#38899;&#39118;&#26684;&#21253;&#21547;&#35821;&#35328;&#38468;&#21152;&#20449;&#24687;&#21644;&#38901;&#24459;&#20449;&#24687;&#65292;&#26631;&#24535;&#30528;&#25991;&#26412;&#21644;&#35821;&#38899;&#24418;&#24335;&#20043;&#38388;&#26368;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24314;&#27169;&#21475;&#35821;&#23545;&#35805;&#26102;&#65292;&#32431;&#25991;&#26412;&#30340;LLMs&#26080;&#27861;&#26681;&#25454;&#24403;&#21069;&#23545;&#35805;&#30340;&#35821;&#38899;&#39118;&#26684;&#32473;&#20986;&#19981;&#21516;&#30340;&#22238;&#24212;&#12290;&#26412;&#25991;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#20542;&#21548;&#35828;&#35805;&#39118;&#26684;&#24182;&#20316;&#20986;&#24688;&#24403;&#22238;&#24212;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25945;&#20250;LLM&#8220;&#21363;&#20351;&#21477;&#23376;&#30456;&#21516;&#65292;&#22914;&#26524;&#20197;&#19981;&#21516;&#39118;&#26684;&#35828;&#20986;&#65292;&#30456;&#24212;&#30340;&#22238;&#24212;&#21487;&#33021;&#20250;&#19981;&#21516;&#8221;&#12290;&#30001;&#20110;&#30446;&#21069;&#27809;&#26377;&#36866;&#21512;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#35821;&#38899;&#23545;&#35821;&#38899;&#30340;&#25968;&#25454;&#38598;StyleTalk&#65292;&#20855;&#26377;&#20197;&#19979;&#25152;&#38656;&#29305;&#24449;&#65306;&#24403;&#20004;&#20010;&#24403;&#21069;&#35821;&#38899;&#20855;&#26377;&#30456;&#21516;&#20869;&#23481;&#20294;&#20197;&#19981;&#21516;&#39118;&#26684;&#35828;&#20986;&#26102;&#65292;&#23427;&#20204;&#30340;&#22238;&#24212;&#23558;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12786v1 Announce Type: new  Abstract: In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses wil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#65292;&#36890;&#36807;&#23545;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#26435;&#34913;&#20851;&#38190;&#20803;&#32032;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12784</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#32531;&#35299;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#65292;&#36890;&#36807;&#23545;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#26435;&#34913;&#20851;&#38190;&#20803;&#32032;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Vec2Text&#25216;&#26415;&#65292;&#19968;&#31181;&#29992;&#20110;&#21453;&#36716;&#25991;&#26412;&#23884;&#20837;&#30340;&#25216;&#26415;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#20005;&#37325;&#38544;&#31169;&#38382;&#39064;&#30340;&#25285;&#24551;&#65292;&#21253;&#25324;&#37027;&#20123;&#20351;&#29992;OpenAI&#21644;Cohere&#25552;&#20379;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#31995;&#32479;&#12290;&#36825;&#31181;&#23041;&#32961;&#26469;&#33258;&#20110;&#19968;&#20010;&#24694;&#24847;&#25915;&#20987;&#32773;&#36890;&#36807;&#35775;&#38382;&#25991;&#26412;&#23884;&#20837;&#26469;&#37325;&#26500;&#21407;&#22987;&#25991;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20351;&#29992;Vec2Text&#24674;&#22797;&#25991;&#26412;&#30340;&#23884;&#20837;&#27169;&#22411;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#28041;&#21450;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#12289;&#21152;&#22122;&#22768;&#35757;&#32451;&#12289;&#23884;&#20837;&#37327;&#21270;&#21644;&#23884;&#20837;&#32500;&#24230;&#31561;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#21407;&#22987;Vec2Text&#35770;&#25991;&#20013;&#23578;&#26410;&#34987;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#24433;&#21709;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#20043;&#38388;&#26435;&#34913;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12784v1 Announce Type: cross  Abstract: The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text.   In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems. This a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#20849;&#24773;&#23545;&#35805;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39564;&#35777;&#21709;&#24212;&#29983;&#25104;&#23454;&#29616;&#20102;&#23545;&#24773;&#32490;&#29366;&#24577;&#30340;&#35748;&#30693;&#12290;&#27169;&#22411;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#20248;&#20110;&#38543;&#26426;&#22522;&#20934;&#32447;&#21644;ChatGPT&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12770</link><description>&lt;p&gt;
&#35748;&#30693;&#24773;&#32490;&#29366;&#24577;&#65306;&#20026;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#39564;&#35777;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#20849;&#24773;&#23545;&#35805;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39564;&#35777;&#21709;&#24212;&#29983;&#25104;&#23454;&#29616;&#20102;&#23545;&#24773;&#32490;&#29366;&#24577;&#30340;&#35748;&#30693;&#12290;&#27169;&#22411;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#20248;&#20110;&#38543;&#26426;&#22522;&#20934;&#32447;&#21644;ChatGPT&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#20013;&#65292;&#20419;&#36827;&#20849;&#24773;&#22238;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#35748;&#21487;&#26159;&#24515;&#29702;&#23398;&#20013;&#30340;&#20851;&#38190;&#27807;&#36890;&#25216;&#24039;&#20043;&#19968;&#65292;&#28041;&#21450;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#25215;&#35748;&#20182;&#20154;&#30340;&#24773;&#32490;&#29366;&#24577;&#12289;&#24605;&#24819;&#21644;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#20849;&#24773;&#23545;&#35805;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#37096;&#20998;&#27169;&#22359;&#31995;&#32479;&#65306;1)&#39564;&#35777;&#26102;&#26426;&#26816;&#27979;&#65292;2)&#29992;&#25143;&#24773;&#32490;&#29366;&#24577;&#35782;&#21035;&#65292;3)&#39564;&#35777;&#21709;&#24212;&#29983;&#25104;&#12290;&#21033;&#29992;&#26085;&#26412;EmpatheticDialogues&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;Plutchik&#24773;&#24863;&#36718;&#20013;&#36873;&#21462;&#30340;8&#20010;&#24773;&#32490;&#31867;&#21035;&#65292;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;TAPT&#65289;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#27169;&#22359;&#30340;F1&#20998;&#25968;&#26041;&#38754;&#22343;&#20248;&#20110;&#38543;&#26426;&#22522;&#20934;&#32447;&#21644;ChatGPT&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12770v1 Announce Type: new  Abstract: In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions. This study introduces the first framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules. Further validation of our model's efficacy is confirmed in its application to the TUT Emotional S
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12750</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Model Composition for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#31034;&#20986;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#26397;&#30528;&#21019;&#24314;&#33021;&#22815;&#29702;&#35299;&#21508;&#31181;&#27169;&#24577;&#36755;&#20837;&#30340;&#22810;&#21151;&#33021;MLLMs&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#37197;&#23545;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36825;&#23545;&#36164;&#28304;&#35201;&#27714;&#39640;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29616;&#26377;MLLMs&#30340;&#27169;&#22411;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#26032;&#27169;&#22411;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#23454;&#29616;NaiveMC&#36890;&#36807;&#37325;&#29992;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#21512;&#24182;LLM&#21442;&#25968;&#23637;&#31034;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAMC&#26469;&#35299;&#20915;&#22312;&#21512;&#24182;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCUB&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;MLLMs&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12750v1 Announce Type: cross  Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to unders
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#19987;&#23478;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#24515;&#29702;&#36741;&#23548;&#24773;&#22659;&#20013;&#19982;&#20154;&#31867;&#22238;&#22797;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.12738</link><description>&lt;p&gt;
&#33021;&#21542;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#24515;&#29702;&#21672;&#35810;&#65311;&#23545;GPT-4&#29983;&#25104;&#30340;&#23545;&#35805;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12738
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19987;&#23478;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#24515;&#29702;&#36741;&#23548;&#24773;&#22659;&#20013;&#19982;&#20154;&#31867;&#22238;&#22797;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12738v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#23545;&#29616;&#20195;&#31038;&#20250;&#26500;&#25104;&#26085;&#30410;&#20005;&#23803;&#25361;&#25112;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#20449;&#24687;&#25216;&#26415;&#35299;&#20915;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#21253;&#25324;&#26088;&#22312;&#24320;&#21457;&#21672;&#35810;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#26377;&#24517;&#35201;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21672;&#35810;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#26356;&#22810;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#28041;&#21450;&#19987;&#23478;&#36741;&#23548;&#21592;&#30340;&#35282;&#33394;&#25198;&#28436;&#24773;&#26223;&#25910;&#38598;&#20102;&#36741;&#23548;&#23545;&#35805;&#25968;&#25454;&#65292;&#24182;&#38024;&#23545;&#36741;&#23548;&#21592;&#30340;&#24847;&#22270;&#23545;&#35805;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#20026;&#20102;&#30830;&#23450;&#23545;&#35805;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#36741;&#23548;&#24773;&#26223;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#31532;&#19977;&#26041;&#36741;&#23548;&#21592;&#22312;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#25968;&#25454;&#20013;&#23545;&#20154;&#31867;&#36741;&#23548;&#21592;&#21644;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#30456;&#21516;&#24773;&#22659;&#19979;&#30340;&#24688;&#24403;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#19982;&#20154;&#31867;&#30340;&#22238;&#22797;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12738v1 Announce Type: cross  Abstract: Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12730</link><description>&lt;p&gt;
UMBCLU&#22312;SemEval-2024&#20219;&#21153;1A&#21644;1C&#20013;&#30340;&#34920;&#29616;&#65306;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12730
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#8220;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#8221;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#30446;&#26631;&#35821;&#35328;&#20013;&#23646;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#38598;&#21512;&#30340;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A&#21644;C&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#35757;&#32451;&#12290; &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290; &#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#21477;&#23376;&#23884;&#20837;LLMs&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#23376;&#20219;&#21153;A&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;STR&#27169;&#22411;&#65292;TranSem&#65292;&#24182;&#23545;STR&#25968;&#25454;&#19978;&#30340;T5&#31995;&#21015;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#29992;&#20110;&#23376;&#20219;&#21153;C&#30340;FineSem&#12290; &#25105;&#20204;&#22312;&#23376;&#20219;&#21153;A&#20013;7&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#32467;&#26524;&#27604;3&#31181;&#35821;&#35328;&#30340;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#65292;&#32780;&#19982;&#20854;&#20182;4&#31181;&#35821;&#35328;&#30340;&#22522;&#20934;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#37329;&#34701;&#20559;&#35265;&#25351;&#26631;&#65288;FBI&#65289;&#26694;&#26550;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37329;&#34701;&#21512;&#29702;&#24615;&#65292;&#30528;&#37325;&#26816;&#39564;&#23427;&#20204;&#23545;&#37329;&#34701;&#20449;&#24687;&#30340;&#36776;&#21035;&#21644;&#24066;&#22330;&#20998;&#26512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38750;&#29702;&#24615;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.12713</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29702;&#24615;&#25237;&#36164;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Rational Investors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#37329;&#34701;&#20559;&#35265;&#25351;&#26631;&#65288;FBI&#65289;&#26694;&#26550;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37329;&#34701;&#21512;&#29702;&#24615;&#65292;&#30528;&#37325;&#26816;&#39564;&#23427;&#20204;&#23545;&#37329;&#34701;&#20449;&#24687;&#30340;&#36776;&#21035;&#21644;&#24066;&#22330;&#20998;&#26512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38750;&#29702;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#36880;&#28176;&#34987;&#24341;&#20837;&#37329;&#34701;&#20998;&#26512;&#39046;&#22495;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#30693;&#35782;&#24211;&#26469;&#35299;&#37322;&#22797;&#26434;&#30340;&#24066;&#22330;&#25968;&#25454;&#21644;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#22266;&#26377;&#20559;&#35265;&#65288;&#21363;&#39118;&#38505;&#20559;&#22909;&#20559;&#35265;&#65289;&#21644;&#23545;&#24066;&#22330;&#22797;&#26434;&#24615;&#30340;&#32932;&#27973;&#29702;&#35299;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#23545;&#23427;&#20204;&#30340;&#37329;&#34701;&#27934;&#23519;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#37329;&#34701;&#20559;&#35265;&#25351;&#26631;&#65288;FBI&#65289;&#65292;&#20197;&#23545;LLMs&#30340;&#37329;&#34701;&#21512;&#29702;&#24615;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#36776;&#21035;&#21644;&#23548;&#33322;&#37329;&#34701;&#20449;&#24687;&#30340;&#24494;&#22937;&#20043;&#22788;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#21487;&#33021;&#25197;&#26354;&#24066;&#22330;&#20998;&#26512;&#30340;&#20219;&#20309;&#38750;&#29702;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12713v1 Announce Type: new  Abstract: Large Language Models (LLMs) are progressively being adopted in financial analysis to harness their extensive knowledge base for interpreting complex market data and trends. However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight. This study introduces a novel framework, Financial Bias Indicators (FBI), to critically evaluate the financial rationality of LLMs, focusing on their ability to discern and navigate the subtleties of financial information and to identify any irrational biases that might skew market analysis.   Our research adopts an innovative methodology to measure financial rationality, integrating principles of behavioral finance to scrutinize the biases and decision-making patterns of LLMs. We conduct a comprehensive evaluation of 19 leading LLMs,
&lt;/p&gt;</description></item><item><title>FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.12692</link><description>&lt;p&gt;
FormulaQA&#65306;&#19968;&#20010;&#22522;&#20110;&#20844;&#24335;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12692
&lt;/p&gt;
&lt;p&gt;
FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20844;&#24335;&#26159;&#20154;&#31867;&#22312;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#25512;&#29702;&#25968;&#25454;&#38598;&#24456;&#23569;&#26126;&#30830;&#25351;&#20986;&#25512;&#29702;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#20844;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;FormulaQA&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#23567;&#20174;7B&#21040;&#36229;&#36807;100B&#21442;&#25968;&#30340;LLMs&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#22806;&#37096;&#20844;&#24335;&#25968;&#25454;&#24211;&#26102;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#22823;&#23567;&#19981;&#36229;&#36807;2B&#30340;&#36739;&#23567;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#24378;&#35843;&#20102;&#24403;&#24212;&#29992;&#20110;&#25105;&#20204;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25913;&#36827;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12692v1 Announce Type: new  Abstract: The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a question answering dataset for formula-based numerical reasoning called FormulaQA, from junior high school physics examinations. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We also fine-tune on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; Tree-Planted Transformers (TPT)&#65292;&#36890;&#36807;&#22312; Transformer LMs &#30340;&#27880;&#24847;&#26435;&#37325;&#20013;&#38544;&#24335;&#22320;&#8220;&#31181;&#26893;&#8221;&#26641;&#26408;&#26469;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#21477;&#27861;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLM&#65289;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.12691</link><description>&lt;p&gt;
&#26641;&#31181;&#26893;&#21464;&#21387;&#22120;&#65306;&#20855;&#26377;&#38544;&#24335;&#21477;&#27861;&#30417;&#30563;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12691
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; Tree-Planted Transformers (TPT)&#65292;&#36890;&#36807;&#22312; Transformer LMs &#30340;&#27880;&#24847;&#26435;&#37325;&#20013;&#38544;&#24335;&#22320;&#8220;&#31181;&#26893;&#8221;&#26641;&#26408;&#26469;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#21477;&#27861;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLM&#65289;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#22312;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#21477;&#27861;&#30417;&#30563;&#39640;&#25928;&#35757;&#32451;&#65292;&#36798;&#21040;&#30456;&#23545;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;LLMs&#21644;SLMs&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#31181;&#23558;LLMs&#30340;&#21487;&#25193;&#23637;&#24615;&#19982;SLMs&#30340;&#35757;&#32451;&#25928;&#29575;&#32467;&#21512;&#36215;&#26469;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#21477;&#27861;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLM&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#26641;&#31181;&#26893;&#8221;&#30340;&#26032;&#26041;&#27861;&#65306;&#22312;Transformer LMs&#30340;&#27880;&#24847;&#26435;&#37325;&#20013;&#26263;&#31034;&#22320;&#8220;&#31181;&#26893;&#8221;&#26641;&#26408;&#65292;&#20197;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#26641;&#31181;&#26893;&#35757;&#32451;&#30340;Transformer LMs&#23558;&#34987;&#31216;&#20026;&#26641;&#31181;&#26893;&#21464;&#21387;&#22120;&#65288;TPT&#65289;&#65292;&#23427;&#20204;&#36890;&#36807;&#26641;&#31181;&#26893;&#22312;&#23567;&#22411;&#26641;&#24211;&#19978;&#23398;&#20064;&#35821;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#32487;&#32493;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#19978;&#36827;&#34892;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12691v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly "plant" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via conti
&lt;/p&gt;</description></item><item><title>&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20026;&#27491;&#30456;&#20851;&#30340;&#24726;&#35770;&#65292;&#36825;&#26159;&#36763;&#26222;&#26862;&#24726;&#35770;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#20108;&#32773;&#21576;&#27491;&#30456;&#20851;&#65292;&#22312;&#21333;&#20010;&#28304;&#27573;&#32423;&#21035;&#19978;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.12690</link><description>&lt;p&gt;
&#36763;&#26222;&#26862;&#24726;&#35770;&#19982;&#32763;&#35793;&#20013;&#30340;&#20934;&#30830;&#24615;-&#27969;&#30021;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12690
&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20026;&#27491;&#30456;&#20851;&#30340;&#24726;&#35770;&#65292;&#36825;&#26159;&#36763;&#26222;&#26862;&#24726;&#35770;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#20108;&#32773;&#21576;&#27491;&#30456;&#20851;&#65292;&#22312;&#21333;&#20010;&#28304;&#27573;&#32423;&#21035;&#19978;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#22909;&#30340;&#32763;&#35793;&#24212;&#35813;&#24544;&#23454;&#20110;&#21407;&#25991;&#24182;&#36981;&#23432;&#30446;&#26631;&#35821;&#35328;&#30340;&#35268;&#33539;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35770;&#38590;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#36763;&#26222;&#26862;&#24726;&#35770;&#30340;&#19968;&#31181;&#23454;&#20363;&#65292;&#34920;&#26126;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#65292;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#21576;&#27491;&#30456;&#20851;&#20294;&#22312;&#21333;&#20010;&#28304;&#27573;&#30340;&#32423;&#21035;&#19978;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12690v1 Announce Type: new  Abstract: A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al. 2007). We show that the tension between these views is an instance of Simpson's paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off be
&lt;/p&gt;</description></item><item><title>SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;&#39046;&#22495;&#22806;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.12663</link><description>&lt;p&gt;
SoftQE: LLM&#25193;&#23637;&#30340;&#26597;&#35810;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SoftQE: Learned Representations of Queries Expanded by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12663
&lt;/p&gt;
&lt;p&gt;
SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;&#39046;&#22495;&#22806;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#26597;&#35810;&#32534;&#30721;&#22120;&#20013;&#65292;&#20197;&#25913;&#21892;&#23494;&#38598;&#26816;&#32034;&#65292;&#21516;&#26102;&#36991;&#20813;&#22312;&#25512;&#26029;&#26102;&#20381;&#36182;LLMs&#22686;&#21152;&#24310;&#36831;&#21644;&#25104;&#26412;&#12290;SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#26469;&#25972;&#21512;LLMs&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#23545;&#20110;&#39046;&#22495;&#20869;MS-MARCO&#25351;&#26631;&#65292;SoftQE&#30456;&#23545;&#20110;&#21508;&#31181;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#25913;&#21892;&#26377;&#38480;&#65292;&#20294;&#22312;&#20116;&#20010;&#39046;&#22495;&#22806;BEIR&#20219;&#21153;&#19978;&#65292;SoftQE&#22312;&#24179;&#22343;&#24615;&#33021;&#19978;&#25552;&#39640;&#20102;2.83&#20010;&#32477;&#23545;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12663v1 Announce Type: new  Abstract: We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12659</link><description>&lt;p&gt;
FinBen&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#36130;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
The FinBen: An Holistic Financial Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#24443;&#24213;&#30340;&#35780;&#20272;&#21644;&#37329;&#34701;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FinBen&#65292;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;FinBen&#21253;&#25324;23&#31181;&#37329;&#34701;&#20219;&#21153;&#30340;35&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#26681;&#25454;&#21345;&#29305;&#23572;-&#38669;&#24681;-&#21345;&#32599;&#23572;&#29702;&#35770;&#30340;&#28789;&#24863;&#32452;&#32455;&#25104;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#35889;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24402;&#32435;&#25512;&#29702;&#12289;&#32852;&#24819;&#35760;&#24518;&#12289;&#25968;&#37327;&#25512;&#29702;&#12289;&#26230;&#20307;&#26234;&#21147;&#31561;&#26041;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#65288;&#21253;&#25324;GPT-4&#12289;ChatGPT&#21644;&#26368;&#26032;&#30340;Gemini&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12654</link><description>&lt;p&gt;
OWSM-CTC:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#35821;&#35328;&#35782;&#21035;&#30340;&#24320;&#25918;&#32534;&#30721;&#22120;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#23545;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#38750;&#24120;&#27969;&#34892;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#20250;&#27604;&#36739;&#24930;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#24187;&#35273;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25193;&#23637;&#21040;&#19981;&#21516;&#35821;&#35328;&#21644;&#20219;&#21153;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#29983;&#25104;&#20013;&#12290;&#21463;Open Whisper-style Speech Model (OWSM)&#39033;&#30446;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification (CTC)&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;18&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12654v1 Announce Type: new  Abstract: There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and languag
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;"&#25216;&#24039;&#27979;&#35797;"&#19982;&#26356;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#30340;RUTEd&#35780;&#20272;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#35780;&#20272;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.12649</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65306;&#36229;&#36234;&#25216;&#24039;&#27979;&#35797;&#65292;&#36208;&#21521;RUTEd&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12649
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;"&#25216;&#24039;&#27979;&#35797;"&#19982;&#26356;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#30340;RUTEd&#35780;&#20272;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#35780;&#20272;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized "trick tests" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no corres
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12649v1 Announce Type: new  Abstract: Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized "trick tests" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no corres
&lt;/p&gt;</description></item><item><title>StyleDubber&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38899;&#32032;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069; V2C &#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12636</link><description>&lt;p&gt;
StyleDubber: &#38754;&#21521;&#30005;&#24433;&#37197;&#38899;&#30340;&#22810;&#23610;&#24230;&#39118;&#26684;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12636
&lt;/p&gt;
&lt;p&gt;
StyleDubber&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38899;&#32032;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069; V2C &#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20221;&#21095;&#26412;&#65292;&#22312;&#30005;&#24433;&#37197;&#38899;&#65288;&#35270;&#35273;&#35821;&#38899;&#20811;&#38534;&#65292;V2C&#65289;&#20013;&#30340;&#25361;&#25112;&#26159;&#26681;&#25454;&#21442;&#32771;&#38899;&#36712;&#30340;&#35821;&#27668;&#65292;&#29983;&#25104;&#19982;&#35270;&#39057;&#22312;&#26102;&#38388;&#21644;&#24773;&#32490;&#19978;&#37117;&#33391;&#22909;&#23545;&#40784;&#30340;&#35821;&#38899;&#12290;&#29616;&#26377;&#30340; V2C &#27169;&#22411;&#26681;&#25454;&#35270;&#39057;&#24103;&#38388;&#30340;&#38388;&#38548;&#23383;&#26029;&#20998;&#21106;&#21095;&#26412;&#30340;&#38899;&#32032;&#65292;&#36825;&#35299;&#20915;&#20102;&#26102;&#38388;&#23545;&#40784;&#38382;&#39064;&#65292;&#20294;&#23548;&#33268;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986; StyleDubber&#65292;&#23427;&#23558;&#37197;&#38899;&#23398;&#20064;&#20174;&#24103;&#32423;&#21035;&#36716;&#20026;&#38899;&#32032;&#32423;&#21035;&#12290;&#23427;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22810;&#27169;&#24577;&#39118;&#26684;&#36866;&#37197;&#22120;&#65292;&#20197;&#38899;&#32032;&#32423;&#21035;&#25805;&#20316;&#65292;&#20174;&#21442;&#32771;&#38899;&#39057;&#20013;&#23398;&#20064;&#21457;&#38899;&#39118;&#26684;&#65292;&#24182;&#29983;&#25104;&#21463;&#35270;&#39057;&#20013;&#21576;&#29616;&#30340;&#38754;&#37096;&#24773;&#32490;&#24433;&#21709;&#30340;&#20013;&#38388;&#34920;&#31034;&#65307;&#65288;2&#65289;&#19968;&#20010;&#20197;&#35821;&#21477;&#32423;&#21035;&#39118;&#26684;&#23398;&#20064;&#27169;&#22359;&#65292;&#24341;&#23548;&#20013;&#38388;&#34920;&#29616;&#30340; mel-spectrogram &#35299;&#30721;&#21644;&#32454;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12621</link><description>&lt;p&gt;
Reflect-RL&#65306;&#20004;&#20010;&#29609;&#23478;&#22312;&#32447;RL&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reflect-RL: Two-Player Online RL Fine-Tuning for LMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12621
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20854;&#33021;&#21147;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#38656;&#35201;&#22810;&#36718;&#20132;&#20114;&#30340;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#24615;&#65292;&#22240;&#27492;&#20165;&#22312;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#29615;&#22659;&#20869;&#30452;&#25509;&#23545;LM&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;LM&#36827;&#34892;&#24494;&#35843;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Reflect-RL&#65292;&#19968;&#20010;&#20004;&#20010;&#29609;&#23478;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22312;&#32447;RL&#23545;LM&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#20923;&#32467;&#30340;&#21453;&#23556;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#27169;&#22411;&#12290;&#20026;&#20102;&#20026;&#28909;&#36523;SFT&#38454;&#27573;&#29983;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36127;&#20363;&#29983;&#25104;&#26469;&#22686;&#24378;&#21453;&#23556;&#27169;&#22411;&#30340;&#32416;&#38169;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#65292;&#24182;&#24212;&#29992;&#20102;&#35838;&#31243;&#23398;&#20064;&#35753;&#31574;&#30053;&#27169;&#22411;&#23398;&#20064;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12617</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#65306;&#25361;&#25112;&#19982;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Generative AI Security: Challenges and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12617
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#34892;&#19994;&#30340;&#19981;&#26029;&#25193;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#21644;&#22686;&#21152;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25506;&#35752;&#20102;&#35789;&#27719;&#26159;&#20160;&#20040;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#29702;&#35770;&#23545;&#20110;&#35789;&#27719;&#30340;&#35299;&#37322;&#21450;&#23454;&#39564;&#35774;&#35745;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#24433;&#21709;</title><link>https://arxiv.org/abs/2402.12605</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19968;&#20010;&#35789;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is a word?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12605
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25506;&#35752;&#20102;&#35789;&#27719;&#26159;&#20160;&#20040;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#29702;&#35770;&#23545;&#20110;&#35789;&#27719;&#30340;&#35299;&#37322;&#21450;&#23454;&#39564;&#35774;&#35745;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35774;&#35745;&#20986;&#33021;&#22815;&#26377;&#25928;&#38548;&#31163;&#35789;&#27719;&#35775;&#38382;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#33539;&#24335;&#65292;&#25105;&#20204;&#38656;&#35201;&#30693;&#36947;&#19968;&#20010;&#35789;&#26159;&#20160;&#20040;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#35821;&#35328;&#23398;&#23478;&#21644;&#21746;&#23398;&#23478;&#23545;&#19968;&#20010;&#35789;&#26377;&#30528;&#28165;&#26224;&#30340;&#27169;&#22411;&#65292;&#23613;&#31649;&#35789;&#27719;&#22522;&#26412;&#24433;&#21709;&#30528;&#20154;&#31867;&#29983;&#27963;&#30340;&#26041;&#26041;&#38754;&#38754;&#12290;&#32463;&#24120;&#21457;&#34920;&#20851;&#20110;&#35821;&#35328;&#30340;&#23398;&#26415;&#35770;&#25991;&#30340;&#30740;&#31350;&#20154;&#21592;&#24448;&#24448;&#20381;&#36182;&#20110;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#20851;&#20110;&#35789;&#24615;&#30340;&#20551;&#35774;&#12290;&#36825;&#31687;&#31616;&#30701;&#30340;&#25945;&#32946;&#24615;&#25991;&#20214;&#27010;&#36848;&#20102;&#35789;&#27719;&#32477;&#23545;&#19981;&#26159;&#20160;&#20040;&#65288;&#23613;&#31649;&#32463;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#26159;&#20160;&#20040;&#65289;&#12289;&#23427;&#21487;&#33021;&#26159;&#20160;&#20040;&#65288;&#22522;&#20110;&#24403;&#21069;&#30340;&#22909;&#29702;&#35770;&#65289;&#12289;&#20197;&#21450;&#23454;&#39564;&#35774;&#35745;&#30340;&#19968;&#20123;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12605v1 Announce Type: new  Abstract: In order to design strong paradigms for isolating lexical access and semantics, we need to know what a word is. Surprisingly few linguists and philosophers have a clear model of what a word is, even though words impact basically every aspect of human life. Researchers that regularly publish academic papers about language often rely on outdated, or inaccurate, assumptions about wordhood. This short pedagogical document outlines what the lexicon is most certainly not (though is often mistakenly taken to be), what it might be (based on current good theories), and what some implications for experimental design are.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Standardize&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19987;&#23478;&#23450;&#20041;&#30340;&#26631;&#20934;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#20869;&#23481;&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12593</link><description>&lt;p&gt;
&#26631;&#20934;&#21270;: &#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#19987;&#23478;&#23450;&#20041;&#30340;&#26631;&#20934;&#23545;&#40784;&#65292;&#29992;&#20110;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Standardize&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19987;&#23478;&#23450;&#20041;&#30340;&#26631;&#20934;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#20869;&#23481;&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#25945;&#32946;&#39046;&#22495;&#65292;&#39046;&#22495;&#19987;&#23478;&#36981;&#24490;&#20005;&#26684;&#30340;&#26631;&#20934;&#26469;&#21046;&#20316;&#36136;&#37327;&#20869;&#23481;&#65292;&#22914;&#25216;&#26415;&#25163;&#20876;&#12289;&#33647;&#29289;&#35828;&#26126;&#21644;&#20799;&#31461;&#35835;&#29289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#25506;&#35752;&#20351;&#29992;&#36825;&#20123;&#26631;&#20934;&#20316;&#20026;&#25511;&#21046;&#30340;&#21442;&#32771;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Standardize&#30340;&#26816;&#32034;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19987;&#23478;&#23450;&#20041;&#30340;&#26631;&#20934;&#23545;&#40784;&#12290;&#20197;&#33521;&#35821;&#35821;&#35328;&#26631;&#20934;&#22312;&#25945;&#32946;&#39046;&#22495;&#20316;&#20026;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27431;&#27954;&#20849;&#21516;&#35821;&#35328;&#21442;&#32771;&#26694;&#26550;&#65288;CEFR&#65289;&#21644;&#36890;&#29992;&#26680;&#24515;&#26631;&#20934;&#65288;CCS&#65289;&#29992;&#20110;&#24320;&#25918;&#24615;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#31934;&#30830;&#24615;&#23545;&#20110;Llama2&#21644;GPT-4&#20998;&#21035;&#21487;&#20197;&#25552;&#39640;40%&#21040;100%&#65292;&#35777;&#26126;&#20102;&#20174;&#26631;&#20934;&#20013;&#25552;&#21462;&#30693;&#35782;&#24037;&#20214;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#29983;&#25104;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12593v1 Announce Type: new  Abstract: Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children's reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 40% to 100% increase in precise accuracy for Llama2 and GPT-4, respectively, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the gener
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#20114;&#21160;&#30340;&#8220;&#31867;&#31038;&#20250;&#8221;&#23646;&#24615;&#65292;&#20197;&#22686;&#21152;&#22870;&#21169;&#24182;&#20943;&#23569;&#39118;&#38505;&#65292;&#23637;&#31034;&#26032;&#20852;&#30340;&#21435;&#20013;&#24515;&#21270;AI&#38598;&#20307;&#22914;&#20309;&#25193;&#22823;&#20154;&#31867;&#22810;&#26679;&#24615;&#33539;&#22260;&#21644;&#38477;&#20302;&#22312;&#32447;&#26377;&#27602;&#34892;&#20026;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.12590</link><description>&lt;p&gt;
&#23558;AI&#38598;&#20307;&#36827;&#21270;&#65292;&#22686;&#24378;&#20154;&#31867;&#22810;&#26679;&#24615;&#24182;&#23454;&#29616;&#33258;&#25105;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12590
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#20114;&#21160;&#30340;&#8220;&#31867;&#31038;&#20250;&#8221;&#23646;&#24615;&#65292;&#20197;&#22686;&#21152;&#22870;&#21169;&#24182;&#20943;&#23569;&#39118;&#38505;&#65292;&#23637;&#31034;&#26032;&#20852;&#30340;&#21435;&#20013;&#24515;&#21270;AI&#38598;&#20307;&#22914;&#20309;&#25193;&#22823;&#20154;&#31867;&#22810;&#26679;&#24615;&#33539;&#22260;&#21644;&#38477;&#20302;&#22312;&#32447;&#26377;&#27602;&#34892;&#20026;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#20182;&#20154;&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#24341;&#23548;&#20854;&#34892;&#20026;&#12290;&#36825;&#31181;&#33021;&#21147;&#21450;&#20854;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#26085;&#30410;&#26222;&#21450;&#30340;&#36235;&#21183;&#39044;&#31034;&#30528;&#23427;&#20204;&#23558;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#8220;&#32534;&#31243;&#8221;&#24444;&#27492;&#24182;&#24418;&#25104;&#26032;&#20852;&#30340;AI&#20027;&#20307;&#24615;&#12289;&#20851;&#31995;&#21644;&#38598;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#25506;&#31350;&#36825;&#20123;&#20114;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#8220;&#31867;&#31038;&#20250;&#8221;&#23646;&#24615;&#65292;&#20197;&#22686;&#21152;&#20854;&#22870;&#21169;&#24182;&#20943;&#23569;&#23545;&#20154;&#31867;&#31038;&#20250;&#21644;&#22312;&#32447;&#29615;&#22659;&#20581;&#24247;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#21450;&#20854;&#36755;&#20986;&#26469;&#35828;&#26126;&#36825;&#31181;&#26032;&#20852;&#30340;&#12289;&#21435;&#20013;&#24515;&#21270;&#30340;AI&#38598;&#20307;&#22914;&#20309;&#25193;&#22823;&#20154;&#31867;&#22810;&#26679;&#24615;&#33539;&#22260;&#24182;&#38477;&#20302;&#22312;&#32447;&#26377;&#27602;&#12289;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;AI&#33258;&#25105;&#35843;&#33410;&#30340;&#26426;&#20250;&#65292;&#24182;&#35299;&#20915;&#20102;&#28041;&#21450;&#21019;&#24314;&#21644;&#32500;&#25252;&#21435;&#20013;&#24515;&#21270;AI&#38598;&#20307;&#30340;&#20262;&#29702;&#38382;&#39064;&#21644;&#35774;&#35745;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12590v1 Announce Type: new  Abstract: Large language models steer their behaviors based on texts generated by others. This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally "program" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these "society-like" properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives.
&lt;/p&gt;</description></item><item><title>GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.12566</link><description>&lt;p&gt;
GenAudit&#65306;&#21033;&#29992;&#35777;&#25454;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12566
&lt;/p&gt;
&lt;p&gt;
GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25991;&#26723;&#65292;&#20063;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#38472;&#36848;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#21307;&#30103;&#20445;&#20581;&#25110;&#37329;&#34701;&#38382;&#31572;&#65289;&#65292;&#36825;&#26679;&#30340;&#38169;&#35823;&#21487;&#33021;&#20855;&#26377;&#21361;&#38505;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenAudit -- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#26816;&#26597;&#22522;&#20110;&#25991;&#26723;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#30340;&#24037;&#20855;&#12290;GenAudit&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#26723;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#21516;&#26102;&#20026;&#30475;&#20284;&#34987;&#35777;&#25454;&#25903;&#25345;&#30340;&#20107;&#23454;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#26469;&#24314;&#35758;&#20462;&#25913;LLM&#21709;&#24212;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#24314;&#35758;&#30340;&#20462;&#25913;&#21644;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20998;&#21592;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#65292;GenAudit&#22312;&#24635;&#32467;&#19981;&#21516;&#39046;&#22495;&#25991;&#26723;&#26102;&#33021;&#22815;&#26816;&#27979;&#20986;8&#31181;&#19981;&#21516;&#30340;LLM&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#26631;&#35760;&#22823;&#22810;&#25968;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#21484;&#22238;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#39044;&#22788;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.12563</link><description>&lt;p&gt;
&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#65306;&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#28608;&#21457;&#20102;&#23545;&#23427;&#20204;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35797;&#22270;&#35299;&#20915;&#20851;&#20110;&#20854;&#21487;&#34892;&#24615;&#30340;&#25345;&#32493;&#20105;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28508;&#22312;&#22240;&#32032; - LLMs&#30340;&#8220;&#20449;&#24515;&#8221; - &#22312;&#33258;&#25105;&#26657;&#27491;&#36807;&#31243;&#20013;&#12290;&#24573;&#35270;&#36825;&#19968;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25209;&#35780;&#33258;&#24049;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#33258;&#26657;&#27491;&#25928;&#26524;&#30340;&#21487;&#38752;&#32467;&#35770;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23454;&#39564;&#35266;&#23519;&#21040;LLMs&#20855;&#26377;&#29702;&#35299;&#20854;&#33258;&#36523;&#22238;&#24212;&#8220;&#20449;&#24515;&#8221;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#23548;LLMs&#35780;&#20272;&#20854;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#65292;&#20419;&#36827;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;IoE&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
&lt;/p&gt;</description></item><item><title>CausalGym&#20171;&#32461;&#20102;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22522;&#20934;&#27979;&#35797;&#35299;&#37322;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;DAS&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#29992;&#23427;&#26469;&#30740;&#31350;&#20102;pythia-1b&#20013;&#30340;&#20004;&#20010;&#22256;&#38590;&#35821;&#35328;&#29616;&#35937;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.12560</link><description>&lt;p&gt;
CausalGym&#65306;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23545;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CausalGym: Benchmarking causal interpretability methods on linguistic tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12560
&lt;/p&gt;
&lt;p&gt;
CausalGym&#20171;&#32461;&#20102;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22522;&#20934;&#27979;&#35797;&#35299;&#37322;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;DAS&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#29992;&#23427;&#26469;&#30740;&#31350;&#20102;pythia-1b&#20013;&#30340;&#20004;&#20010;&#22256;&#38590;&#35821;&#35328;&#29616;&#35937;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#32431;&#34892;&#20026;&#27979;&#37327;&#65288;&#20363;&#22914;&#65292;&#24778;&#22855;&#27604;&#36739;&#65289;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#24050;&#24320;&#22987;&#38416;&#26126;&#22609;&#36896;LM&#34892;&#20026;&#30340;&#25277;&#35937;&#22240;&#26524;&#26426;&#21046;&#12290;&#20026;&#20102;&#24110;&#21161;&#23558;&#36825;&#20123;&#30740;&#31350;&#39046;&#22495;&#26356;&#32039;&#23494;&#22320;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;CausalGym&#12290;&#25105;&#20204;&#25913;&#32534;&#24182;&#25193;&#23637;&#20102;SyntaxGym&#20219;&#21153;&#22871;&#20214;&#65292;&#20197;&#22522;&#20934;&#27979;&#35797;&#35299;&#37322;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35828;&#26126;CausalGym&#30340;&#29992;&#36884;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;pythia&#27169;&#22411;&#65288;&#20174;14M&#21040;6.9B&#65289;&#24182;&#35780;&#20272;&#20102;&#24191;&#27867;&#35299;&#37322;&#26041;&#27861;&#30340;&#22240;&#26524;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#32447;&#24615;&#25506;&#38024;&#21644;&#20998;&#24067;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;DAS&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;pythia-1b&#20013;&#20004;&#20010;&#22256;&#38590;&#30340;&#35821;&#35328;&#29616;&#35937;&#30340;&#23398;&#20064;&#36712;&#36857;&#65306;&#36127;&#26497;&#24615;&#39033;&#35768;&#21487;&#21644;&#22635;&#20805;-&#38388;&#38553;d&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12560v1 Announce Type: cross  Abstract: Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;GPT-4&#21450;&#20854;&#36827;&#38454;&#29256;GPT-4 Turbo&#33258;&#20027;&#24320;&#21457;&#20102;&#28085;&#30422;5000&#22810;&#31181;&#24494;&#22937;&#23454;&#20307;&#31867;&#22411;&#30340;&#35814;&#32454;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#25216;&#26415;&#19981;&#26029;&#25913;&#36827;&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.12557</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21019;&#24314;&#31934;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Creating a Fine Grained Entity Type Taxonomy Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;GPT-4&#21450;&#20854;&#36827;&#38454;&#29256;GPT-4 Turbo&#33258;&#20027;&#24320;&#21457;&#20102;&#28085;&#30422;5000&#22810;&#31181;&#24494;&#22937;&#23454;&#20307;&#31867;&#22411;&#30340;&#35814;&#32454;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#25216;&#26415;&#19981;&#26029;&#25913;&#36827;&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;GPT-4&#21450;&#20854;&#36827;&#38454;&#29256;GPT-4 Turbo&#22312;&#33258;&#20027;&#24320;&#21457;&#35814;&#32454;&#23454;&#20307;&#31867;&#22411;&#20998;&#31867;&#27861;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#22871;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#24191;&#27867;&#30340;&#23454;&#20307;&#31867;&#22411;&#20998;&#31867;&#24320;&#22987; - &#21253;&#25324;&#23545;&#35937;&#12289;&#26102;&#38388;&#12289;&#22320;&#28857;&#12289;&#32452;&#32455;&#12289;&#20107;&#20214;&#12289;&#34892;&#21160;&#21644;&#20027;&#39064; - &#31867;&#20284;&#20110;&#29616;&#26377;&#30340;&#25163;&#21160;&#31574;&#21010;&#20998;&#31867;&#27861;&#12290;&#36825;&#19968;&#20998;&#31867;&#27861;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#25216;&#26415;&#36880;&#28176;&#32454;&#21270;&#65292;&#21033;&#29992;GPT-4&#20869;&#37096;&#30693;&#35782;&#24211;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;5000&#31181;&#24494;&#22937;&#23454;&#20307;&#31867;&#22411;&#30340;&#24191;&#27867;&#20998;&#31867;&#27861;&#65292;&#22312;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12557v1 Announce Type: new  Abstract: In this study, we investigate the potential of GPT-4 and its advanced iteration, GPT-4 Turbo, in autonomously developing a detailed entity type taxonomy. Our objective is to construct a comprehensive taxonomy, starting from a broad classification of entity types - including objects, time, locations, organizations, events, actions, and subjects - similar to existing manually curated taxonomies. This classification is then progressively refined through iterative prompting techniques, leveraging GPT-4's internal knowledge base. The result is an extensive taxonomy comprising over 5000 nuanced entity types, which demonstrates remarkable quality upon subjective evaluation.   We employed a straightforward yet effective prompting strategy, enabling the taxonomy to be dynamically expanded. The practical applications of this detailed taxonomy are diverse and significant. It facilitates the creation of new, more intricate branches through pattern-b
&lt;/p&gt;</description></item><item><title>IMBUE&#26159;&#39318;&#20010;&#21516;&#26102;&#19987;&#27880;&#20110;&#27807;&#36890;&#25216;&#33021;&#21644;&#24773;&#32490;&#31649;&#29702;&#12289;&#22312;&#25552;&#20379;&#21453;&#39304;&#20013;&#25972;&#21512;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#20114;&#21160;&#24335;&#22521;&#35757;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.12556</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#21644;&#21363;&#26102;&#21453;&#39304;&#20197;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20114;&#21160;&#25552;&#39640;&#20154;&#38469;&#26377;&#25928;&#24615;&#65306;IMBUE
&lt;/p&gt;
&lt;p&gt;
IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12556
&lt;/p&gt;
&lt;p&gt;
IMBUE&#26159;&#39318;&#20010;&#21516;&#26102;&#19987;&#27880;&#20110;&#27807;&#36890;&#25216;&#33021;&#21644;&#24773;&#32490;&#31649;&#29702;&#12289;&#22312;&#25552;&#20379;&#21453;&#39304;&#20013;&#25972;&#21512;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#20114;&#21160;&#24335;&#22521;&#35757;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20010;&#20307;&#32570;&#20047;&#25216;&#33021;&#21644;&#24378;&#28872;&#24773;&#32490;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#27807;&#36890;&#24773;&#22659;&#19979;&#30340;&#23548;&#33322;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#30340;&#23398;&#20064;&#26426;&#20250;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#23450;&#21046;&#27807;&#36890;&#22521;&#35757;&#65292;&#24182;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#65292;&#20197;&#25903;&#25345;&#20154;&#38469;&#26377;&#25928;&#24615;&#25216;&#33021;&#30340;&#23454;&#36341;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#26041;&#35328;&#34892;&#20026;&#27835;&#30103;&#65288;DBT&#65289;&#30340;&#20154;&#38469;&#26377;&#25928;&#24615;&#26694;&#26550;DEAR MAN&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;&#35805;&#21644;&#24773;&#32490;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IMBUE&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22521;&#35757;&#31995;&#32479;&#65292;&#20854;&#25552;&#20379;&#30340;&#21453;&#39304;&#19982;&#19987;&#19994;&#20154;&#22763;&#21453;&#39304;&#26356;&#30456;&#20284;&#65292;&#27604;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#25552;&#39640;&#20102;25%&#12290;IMBUE&#39318;&#27425;&#21516;&#26102;&#19987;&#27880;&#20110;&#27807;&#36890;&#25216;&#33021;&#21644;&#24773;&#32490;&#31649;&#29702;&#65292;&#23558;&#19987;&#23478;&#30340;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#12290;&#36890;&#36807;&#38543;&#26426; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12556v1 Announce Type: cross  Abstract: Navigating certain communication situations can be challenging due to individuals' lack of skills and the interference of strong emotions. However, effective learning opportunities are rarely accessible. In this work, we conduct a human-centered study that uses language models to simulate bespoke communication training and provide just-in-time feedback to support the practice and learning of interpersonal effectiveness skills. We apply the interpersonal effectiveness framework from Dialectical Behavioral Therapy (DBT), DEAR MAN, which focuses on both conversational and emotional skills. We present IMBUE, an interactive training system that provides feedback 25% more similar to experts' feedback, compared to that generated by GPT-4. IMBUE is the first to focus on communication skills and emotion management simultaneously, incorporate experts' domain knowledge in providing feedback, and be grounded in psychology theory. Through a randomi
&lt;/p&gt;</description></item><item><title>Archer&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#35821;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12554</link><description>&lt;p&gt;
Archer: &#19968;&#20010;&#20855;&#26377;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#30340;&#20154;&#24037;&#26631;&#35760;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12554
&lt;/p&gt;
&lt;p&gt;
Archer&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#35821;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Archer&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#35821;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;&#21253;&#25324;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#22312;&#20869;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#23427;&#21253;&#21547;1,042&#20010;&#33521;&#25991;&#38382;&#39064;&#21644;1,042&#20010;&#20013;&#25991;&#38382;&#39064;&#65292;&#20197;&#21450;521&#20010;&#21807;&#19968;&#30340;SQL&#26597;&#35810;&#65292;&#28085;&#30422;&#20102;20&#20010;&#39046;&#22495;&#20013;&#30340;20&#20010;&#33521;&#35821;&#25968;&#25454;&#24211;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#20844;&#24320;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Archer&#25361;&#25112;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;Spider&#25490;&#34892;&#27036;&#19978;&#30340;&#25490;&#21517;&#38752;&#21069;&#30340;&#27169;&#22411;&#22312;Archer&#27979;&#35797;&#38598;&#19978;&#20165;&#36798;&#21040;6.73%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#22240;&#27492;&#65292;Archer&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12554v1 Announce Type: new  Abstract: We present Archer, a challenging bilingual text-to-SQL dataset specific to complex reasoning, including arithmetic, commonsense and hypothetical reasoning. It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.
&lt;/p&gt;</description></item><item><title>TrustScore &#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#19968;&#33268;&#24615;&#27010;&#24565;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#26159;&#21542;&#19982;&#20854;&#20869;&#22312;&#30693;&#35782;&#30456;&#19968;&#33268;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.12545</link><description>&lt;p&gt;
TrustScore: &#26080;&#21442;&#32771;&#35780;&#20272;LLM&#21709;&#24212;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12545
&lt;/p&gt;
&lt;p&gt;
TrustScore &#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#19968;&#33268;&#24615;&#27010;&#24565;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#26159;&#21542;&#19982;&#20854;&#20869;&#22312;&#30693;&#35782;&#30456;&#19968;&#33268;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;LLMs&#36755;&#20986;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23553;&#38381;&#20070;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#65292;&#38750;&#19987;&#23478;&#21487;&#33021;&#22240;&#32570;&#23569;&#19978;&#19979;&#25991;&#25110;&#22522;&#20934;&#20449;&#24687;&#32780;&#38590;&#20197;&#35782;&#21035;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustScore&#65292;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#19968;&#33268;&#24615;&#27010;&#24565;&#30340;&#26694;&#26550;&#65292;&#35780;&#20272;LLMs&#30340;&#21709;&#24212;&#26159;&#21542;&#19982;&#20854;&#20869;&#22312;&#30693;&#35782;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;TrustScore&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#38598;&#25104;&#65292;&#35780;&#20272;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;TrustScore&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24378;&#22823;&#30340;&#30456;&#20851;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25351;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#22522;&#20934;&#25351;&#26631;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12545v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, prompting a surge in their practical applications. However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information. This paper introduces TrustScore, a framework based on the concept of Behavioral Consistency, which evaluates whether an LLMs response aligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly integrate with fact-checking methods, which assesses alignment with external knowledge sources. The experimental results show that TrustScore achieves strong correlations with human judgments, surpassing existing reference-free metrics, and achieving results on par with reference-based metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12530</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel Structures in Pre-training Data Yield In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20855;&#22791;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#22312;&#21482;&#32473;&#20986;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#33021;&#21147;&#26469;&#33258;&#20309;&#22788;&#65292;&#22240;&#20026;&#39044;&#35757;&#32451;&#25991;&#26412;&#19982;ICL&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21738;&#20123;&#27169;&#24335;&#26377;&#21161;&#20110;ICL&#12290;&#25105;&#20204;&#21457;&#29616;LMs&#30340;ICL&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#8220;&#24179;&#34892;&#32467;&#26500;&#8221;&#8212;&#8212;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#36981;&#24490;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#19968;&#20010;&#30701;&#35821;&#26159;&#21542;&#25552;&#39640;&#20102;&#23545;&#21478;&#19968;&#20010;&#30701;&#35821;&#30340;&#39044;&#27979;&#26469;&#26816;&#27979;&#24179;&#34892;&#32467;&#26500;&#65292;&#24182;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#20197;&#30740;&#31350;&#20854;&#23545;ICL&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21435;&#38500;&#24179;&#34892;&#32467;&#26500;&#20250;&#23548;&#33268;LMs&#30340;ICL&#20934;&#30830;&#24230;&#19979;&#38477;51&#65285;&#65288;&#19982;&#38543;&#26426;&#20999;&#38500;&#30340;2&#65285;&#30456;&#27604;&#65289;&#12290;&#21363;&#20351;&#25490;&#38500;&#24120;&#35265;&#27169;&#24335;&#22914; n-gram
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Self-Filter&#36825;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;VLM&#26412;&#36523;&#20316;&#20026;&#19968;&#20010;&#36807;&#28388;&#22120;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65292;&#20197;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#24182;&#35757;&#32451;&#25351;&#20196;-following&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12501</link><description>&lt;p&gt;
&#20320;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#36807;&#28388;&#22120;&#65306;&#26397;&#21521;&#20351;&#29992;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#39640;&#36136;&#37327;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12501
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Self-Filter&#36825;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;VLM&#26412;&#36523;&#20316;&#20026;&#19968;&#20010;&#36807;&#28388;&#22120;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65292;&#20197;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#24182;&#35757;&#32451;&#25351;&#20196;-following&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25351;&#20196;&#35843;&#25972;&#20013;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#25104;&#20026;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#24182;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#20294;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#32780;&#35328;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#26410;&#34987;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;LLMs&#19978;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21333;&#19968;&#19981;&#21487;&#38752;&#30340;&#20998;&#25968;&#65292;&#35201;&#20040;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36873;&#25321;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#25152;&#36873;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#36873;&#25321;&#26041;&#27861;&#65292;Self-Filter&#65292;&#23427;&#21033;&#29992;VLM&#26412;&#36523;&#20316;&#20026;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12501v1 Announce Type: new  Abstract: Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs). Existing data selection approaches on LLMs either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging instructions. Self-Filter operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training instructions, which is co-trained with the VLM. In the second stage, we use the trained score net to measure the
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26126;&#30830;&#25552;&#31034;&#26102;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#35782;&#21035;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#20294;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#21017;&#26356;&#20026;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.12486</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#26816;&#27979;&#21644;&#29702;&#35299;&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#65311;&#38382;&#38382;DUST&#65281;
&lt;/p&gt;
&lt;p&gt;
Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12486
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26126;&#30830;&#25552;&#31034;&#26102;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#35782;&#21035;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#20294;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#21017;&#26356;&#20026;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#35821;&#35328;&#20351;&#29992;&#20013;&#65292;&#35828;&#35805;&#32773;&#32463;&#24120;&#20351;&#29992;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#21363;&#20869;&#23481;&#19981;&#36275;&#20197;&#23436;&#20840;&#20256;&#36798;&#20854;&#20449;&#24687;&#25110;&#20197;&#21807;&#19968;&#26041;&#24335;&#35299;&#37322;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35821;&#20041;&#19981;&#30830;&#23450;&#21477;&#23376;&#31867;&#22411;&#20998;&#32452;&#25968;&#25454;&#38598;&#8221;&#65288;DUST&#65289;&#65292;&#29992;&#26469;&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#27491;&#30830;&#35782;&#21035;&#21644;&#35299;&#37322;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26126;&#30830;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#35782;&#21035;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#20294;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#21017;&#26356;&#20026;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#35299;&#37322;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#23567;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19982;&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#25551;&#36848;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12486v1 Announce Type: new  Abstract: In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence "Don't spend too much", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecificati
&lt;/p&gt;</description></item><item><title>LLMs&#33021;&#22815;&#22312;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20165;&#20174;&#36873;&#39033;&#20013;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36890;&#36807;&#35760;&#24518;&#12289;&#36873;&#25321;&#21160;&#24577;&#21644;&#38382;&#39064;&#25512;&#29702;&#36827;&#34892;&#40657;&#30418;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36873;&#25321;&#24615;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12483</link><description>&lt;p&gt;
&#25991;&#29289;&#36824;&#26159;&#32465;&#26550;&#65306;LLMs&#22914;&#20309;&#22312;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12483
&lt;/p&gt;
&lt;p&gt;
LLMs&#33021;&#22815;&#22312;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20165;&#20174;&#36873;&#39033;&#20013;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36890;&#36807;&#35760;&#24518;&#12289;&#36873;&#25321;&#21160;&#24577;&#21644;&#38382;&#39064;&#25512;&#29702;&#36827;&#34892;&#40657;&#30418;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36873;&#25321;&#24615;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#36890;&#24120;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;MCQA&#26159;&#21542;&#25353;&#39044;&#26399;&#35780;&#20272;LLMs&#65292;&#25105;&#20204;&#25506;&#31350;LLMs&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#21482;&#26377;&#36873;&#39033;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;MCQA&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#20165;&#20174;&#36873;&#39033;&#20013;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#12290;&#22312;&#19977;&#20010;MCQA&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;LLMs&#20013;&#65292;&#36825;&#20010;&#25552;&#31034;&#22312;12&#20010;&#26696;&#20363;&#20013;&#30340;11&#20010;&#20013;&#20248;&#20110;&#22810;&#25968;&#22522;&#32447;&#65292;&#24182;&#21487;&#33719;&#24471;&#39640;&#36798;0.33&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#65292;&#25105;&#20204;&#23545;&#35760;&#24518;&#12289;&#36873;&#25321;&#21160;&#24577;&#21644;&#38382;&#39064;&#25512;&#29702;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#40657;&#30418;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26377;&#19977;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#21482;&#26377;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#20165;&#28304;&#33258;&#35760;&#24518;&#12290;&#20854;&#27425;&#65292;&#23545;&#21333;&#20010;&#36873;&#25321;&#30340;&#20808;&#39564;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#21482;&#26377;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#65292;&#26263;&#31034;LLMs&#20351;&#29992;&#36873;&#25321;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;&#31532;&#19977;&#65292;LLMs&#26377;&#19968;&#23450;&#33021;&#21147;&#20174;&#36873;&#25321;&#20013;&#25512;&#26029;&#20986;&#30456;&#20851;&#38382;&#39064;&#65292;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#21305;&#37197;&#21407;&#22987;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#40723;&#21169;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12483v1 Announce Type: new  Abstract: Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of st
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12451</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The (R)Evolution of Multimodal Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12451
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#22312;&#29983;&#25104;&#26234;&#33021;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30446;&#21069;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#26080;&#32541;&#22320;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#21516;&#26102;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#25552;&#20379;&#22522;&#20110;&#23545;&#35805;&#30340;&#25509;&#21475;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;MLLMs&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#36873;&#25321;&#12289;&#22810;&#27169;&#24577;&#23545;&#40784;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#35270;&#35273;&#23450;&#20301;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#12289;&#35270;&#35273;&#29702;&#35299;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32534;&#21046;&#24182;&#25551;&#36848;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#31185;&#23398;&#21457;&#29616;&#25253;&#36947;&#20013;&#30340;&#32454;&#24494;&#22833;&#30495;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#36890;&#36807;&#26631;&#27880;&#21644;&#33258;&#21160;&#26816;&#27979;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#22312;&#26032;&#38395;&#25253;&#36947;&#21644;&#25512;&#25991;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#22833;&#30495;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.12431</link><description>&lt;p&gt;
&#20102;&#35299;&#31185;&#23398;&#21457;&#29616;&#25253;&#36947;&#20013;&#30340;&#32454;&#24494;&#22833;&#30495;
&lt;/p&gt;
&lt;p&gt;
Understanding Fine-grained Distortions in Reports of Scientific Findings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#31185;&#23398;&#21457;&#29616;&#25253;&#36947;&#20013;&#30340;&#32454;&#24494;&#22833;&#30495;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#36890;&#36807;&#26631;&#27880;&#21644;&#33258;&#21160;&#26816;&#27979;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#22312;&#26032;&#38395;&#25253;&#36947;&#21644;&#25512;&#25991;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#22833;&#30495;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#30495;&#30340;&#31185;&#23398;&#20256;&#25773;&#25439;&#23475;&#20010;&#20154;&#21644;&#31038;&#20250;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#20581;&#24247;&#30340;&#34892;&#20026;&#25913;&#21464;&#65292;&#24182;&#38477;&#20302;&#23545;&#31185;&#23398;&#26426;&#26500;&#30340;&#20449;&#20219;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#31185;&#23398;&#20256;&#25773;&#37327;&#30340;&#24555;&#36895;&#22686;&#21152;&#65292;&#23545;&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#30740;&#31350;&#32467;&#26524;&#22914;&#20309;&#25253;&#36947;&#32473;&#20844;&#20247;&#20197;&#21450;&#33258;&#21160;&#26816;&#27979;&#21407;&#22987;&#24037;&#20316;&#20013;&#30340;&#22833;&#30495;&#30340;&#32454;&#33268;&#20102;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#22833;&#30495;&#30340;&#20010;&#21035;&#26041;&#38754;&#25110;&#20351;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#20570;&#20986;&#20102;&#19977;&#39033;&#22522;&#30784;&#24615;&#36129;&#29486;&#65306;(1)&#27880;&#37322;&#20102;&#26469;&#33258;&#23398;&#26415;&#35770;&#25991;&#30340;1,600&#20010;&#31185;&#23398;&#21457;&#29616;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#19982;&#26032;&#38395;&#25991;&#31456;&#21644;&#25512;&#25991;&#20013;&#25253;&#36947;&#30340;&#30456;&#24212;&#21457;&#29616;&#26377;&#20851;&#65292;&#28041;&#21450;&#22235;&#20010;&#29305;&#24449;&#65306;&#22240;&#26524;&#20851;&#31995;&#12289;&#30830;&#23450;&#24615;&#12289;&#19968;&#33324;&#24615;&#21644;&#36720;&#21160;&#24615;&#65307;(2)&#24314;&#31435;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#29305;&#24449;&#30340;&#22522;&#20934;&#65307;&#20197;&#21450;(3)&#20998;&#26512;&#20102;&#21464;&#21270;&#30340;&#26222;&#36941;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12431v1 Announce Type: new  Abstract: Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.12423</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#39046;&#22495;&#65292;Denoising Diffusion Models (DDMs) &#30340;&#24341;&#20837;&#26085;&#30410;&#22686;&#22810;&#65292;&#20026;&#21512;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#25552;&#20379;&#20102;&#24040;&#22823;&#20215;&#20540;&#12290;&#23613;&#31649;&#23427;&#20204;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#30340;&#35821;&#20041;&#33021;&#21147;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#25511;&#21046;&#21512;&#25104;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21463;&#22270;&#20687;&#21512;&#25104;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20923;&#32467;&#30340;TTS&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#30001;DDM&#21435;&#22122;&#22120;&#30340;&#28508;&#31354;&#38388;&#28608;&#27963;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#31354;&#38388;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#27010;&#36848;&#20102;&#33509;&#24178;&#26597;&#25214;&#20854;&#20013;&#35821;&#20041;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#29616;&#25104;&#38899;&#39057;&#32534;&#36753;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#32534;&#36753;&#21518;&#38899;&#39057;&#30340;&#35821;&#20041;&#21644;&#22768;&#23398;&#29305;&#36136;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#34917;&#20805;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;BTM&#24037;&#20855;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#30340;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;&#36328;&#35821;&#26009;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24402;&#19968;&#21270;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12372</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#26009;&#35780;&#20272;&#20013;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24402;&#19968;&#21270;&#24037;&#20855;&#30340;HunFlair2
&lt;/p&gt;
&lt;p&gt;
HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12372
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;BTM&#24037;&#20855;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#30340;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;&#36328;&#35821;&#26009;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24402;&#19968;&#21270;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65288;BTM&#65289;&#24050;&#25104;&#20026;&#21152;&#36895;&#20174;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#35265;&#35299;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#22312;BTM&#27969;&#31243;&#20013;&#65292;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#65288;&#20363;&#22914;&#30142;&#30149;&#12289;&#33647;&#29289;&#25110;&#22522;&#22240;&#65289;&#20197;&#21450;&#23558;&#20854;&#38142;&#25509;&#21040;&#21442;&#32771;&#30693;&#35782;&#24211;&#26159;&#20851;&#38190;&#27493;&#39588;&#65292;&#20197;&#20415;&#20174;&#19981;&#21516;&#25991;&#26723;&#20013;&#21551;&#29992;&#20449;&#24687;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#36825;&#20004;&#20010;&#27493;&#39588;&#30340;&#24037;&#20855;&#24456;&#23569;&#22312;&#24320;&#21457;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#34987;&#24212;&#29992;&#22312;&#37326;&#22806;&#65292;&#21363;&#22312;&#24212;&#29992;&#30456;&#20851;&#30340;&#25991;&#26412;&#38598;&#21512;&#19978;&#65292;&#19981;&#21516;&#20110;&#29992;&#20110;&#24037;&#20855;&#35757;&#32451;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#22312;&#28966;&#28857;&#12289;&#20307;&#35009;&#12289;&#39118;&#26684;&#21644;&#25991;&#26412;&#31867;&#22411;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#20449;&#20219;&#25253;&#21578;&#30340;BTM&#24037;&#20855;&#24615;&#33021;&#65292;&#29992;&#20110;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#36328;&#35821;&#26009;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#24037;&#20855;&#34987;&#24212;&#29992;&#21040;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12372v1 Announce Type: new  Abstract: With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied system
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12243</link><description>&lt;p&gt;
&#29702;&#35299;&#25991;&#26412;&#21040;SQL&#20013;&#22122;&#22768;&#30340;&#24433;&#21709;&#65306;&#23545;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12243
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Text-to-SQL&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#65292;&#23545;&#20110;&#20351;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21487;&#20197;&#22312;&#27809;&#26377;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24191;&#27867;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21407;&#22240;&#21253;&#25324;&#23384;&#22312;&#8220;&#22122;&#22768;&#8221;&#65292;&#22914;&#27169;&#31946;&#38382;&#39064;&#21644;&#35821;&#27861;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#22122;&#22768;&#30340;&#20998;&#24067;&#21644;&#31867;&#22411;&#20197;&#21450;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#34429;&#28982;BIRD-Bench&#26088;&#22312;&#27169;&#25311;&#33039;&#20081;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24211;&#20540;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#21644;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#39046;&#22495;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22122;&#22768;&#65292;&#24182;&#19988;&#22122;&#22768;&#31867;&#22411;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#23384;&#22312;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;SQL&#26597;&#35810;&#65292;&#36827;&#32780;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;&#31572;&#26696;&#65292;&#23545;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
&lt;/p&gt;</description></item><item><title>PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.12168</link><description>&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12168
&lt;/p&gt;
&lt;p&gt;
PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#25552;&#20986;&#24182;&#25104;&#21151;&#23454;&#26045;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#38754;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#20165;&#26356;&#26032;&#26377;&#38480;&#27169;&#22411;&#21442;&#25968;&#30340;PEFT&#26159;&#21542;&#26500;&#25104;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#20173;&#28982;&#26131;&#21463;&#21033;&#29992;&#65292;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#22312;&#24494;&#35843;&#21518;&#20381;&#28982;&#20445;&#25345;&#39640;&#32622;&#20449;&#24230;&#12290;&#21463;&#21040;&#36825;&#19968;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;PEFT&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#65292;&#25552;&#20379;&#38024;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31283;&#20581;&#38450;&#24481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;PEFT&#35757;&#32451;PSIM&#65292;&#24102;&#26377;&#38543;&#26426;&#37325;&#32622;&#26679;&#26412;&#26631;&#31614;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12065</link><description>&lt;p&gt;
WKVQuant&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30528;&#37096;&#32626;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#30340;&#37327;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#28608;&#27963;&#36716;&#25442;&#20026;&#20302;&#27604;&#29305;&#25972;&#25968;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23427;&#20204;&#22312;&#24179;&#34913;&#37327;&#21270;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WKVQuant&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#37327;&#21270;LLMs&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#32780;&#35774;&#35745;&#30340;PTQ&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20165;&#32771;&#34385;&#36807;&#21435;&#30340;&#37327;&#21270;&#20197;&#25913;&#21892;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20108;&#32500;&#37327;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;KV&#32531;&#23384;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#22359;&#37325;&#24314;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#24110;&#21161;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;logit&#33976;&#39311; (ULD) &#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#27169;&#22411;&#20043;&#38388;&#33976;&#39311;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.12030</link><description>&lt;p&gt;
&#36328;&#20998;&#35789;&#22120;&#33976;&#39311;&#65306;&#29992;&#20110;LLM&#30340;&#36890;&#29992;logit&#33976;&#39311;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12030
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;logit&#33976;&#39311; (ULD) &#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#27169;&#22411;&#20043;&#38388;&#33976;&#39311;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#20960;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#22823;&#22810;&#25968;&#24037;&#19994;&#24212;&#29992;&#20013;&#21487;&#33021;&#24182;&#19981;&#20999;&#23454;&#38469;&#65292;&#21407;&#22240;&#26159;&#35832;&#22914;&#25104;&#26412;&#12289;&#24310;&#36831;&#38480;&#21046;&#21644;&#30828;&#20214;&#21487;&#35775;&#38382;&#24615;&#31561;&#32422;&#26463;&#12290;&#30693;&#35782;&#33976;&#39311; (KD) &#36890;&#36807;&#23558;&#36164;&#28304;&#23494;&#38598;&#22411;&#22823;&#27169;&#22411;&#30340;&#30693;&#35782;&#21387;&#32553;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#23384;&#22312;&#22810;&#31181;&#31574;&#30053;&#65292;&#19968;&#20123;&#20381;&#36182;&#20110;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#21487;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#20854;logits&#26469;&#22686;&#24378;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;logits&#30340;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#35789;&#22120;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;LLM&#31995;&#21015;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;logit&#33976;&#39311; (ULD) &#25439;&#22833;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;ULD&#25439;&#22833;&#22312;&#21551;&#29992;&#19981;&#21516;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#33976;&#39311;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12030v1 Announce Type: new  Abstract: Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#22242;&#20307;&#20114;&#21160;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#30340;&#24433;&#21709;&#65292;&#22312;&#21360;&#24230; Twitter &#29992;&#25143;&#20013;&#21457;&#29616;&#65292;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#21487;&#20197;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11895</link><description>&lt;p&gt;
&#19982;&#22242;&#20307;&#20114;&#21160;&#65306;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#24433;&#21709;&#30340;&#32852;&#31995;&#19982;&#30772;&#35010;
&lt;/p&gt;
&lt;p&gt;
Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11895
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#22242;&#20307;&#20114;&#21160;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#30340;&#24433;&#21709;&#65292;&#22312;&#21360;&#24230; Twitter &#29992;&#25143;&#20013;&#21457;&#29616;&#65292;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#21487;&#20197;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25509;&#35302;&#19981;&#21516;&#35266;&#28857;&#21487;&#33021;&#20250;&#20943;&#23569;&#26497;&#31471;&#21270;&#65292;&#20294;&#24403;&#35752;&#35770;&#23545;&#25239;&#24615;&#26102;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#21453;&#25928;&#24212;&#24182;&#21152;&#21095;&#26497;&#31471;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22260;&#32469;&#37325;&#35201;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#26159;&#21542;&#24433;&#21709;&#31038;&#20132;&#32593;&#32476;&#20013;&#22810;&#25968;&#32676;&#20307;&#21644;&#23569;&#25968;&#32676;&#20307;&#20043;&#38388;&#30340;&#26497;&#31471;&#21270;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422; 70 &#19975;&#21517;&#21360;&#24230; Twitter &#29992;&#25143;&#22312; 2020 &#24180;&#21442;&#19982;&#19982; COVID-19 &#30456;&#20851;&#35805;&#39064;&#35752;&#35770;&#26102;&#30340;&#23447;&#25945;&#36523;&#20221;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25512;&#25991;&#25991;&#26412;&#30340;&#24773;&#22659;&#23884;&#20837;&#30340;&#26032;&#37327;&#24230;&#65292;&#29992;&#20110;&#24110;&#21161;&#25105;&#20204;&#35780;&#20272;&#23447;&#25945;&#32676;&#20307;&#20043;&#38388;&#30340;&#26497;&#31471;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#30740;&#31350;&#22260;&#32469;&#20849;&#21516;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#32463;&#27982;&#20107;&#20214;&#30340;&#24322;&#36136;&#22788;&#29702;&#25928;&#26524;&#23545;&#20010;&#20307;&#32676;&#20307;&#31526;&#21512;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#26041;&#38754;&#65292;&#22242;&#20307;&#38388;&#20114;&#21160;&#20250;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11895v1 Announce Type: cross  Abstract: While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#38382;&#31572;&#23545;&#35805;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#31243;&#24207;&#23545;&#22522;&#30784;&#25991;&#26723;&#30340;&#24544;&#35802;&#24230;&#65292;&#35757;&#32451;&#24378;&#22823;&#30340;&#23545;&#35805;&#38382;&#31572;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11770</link><description>&lt;p&gt;
&#38754;&#21521;&#23569;&#26679;&#26412;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#20851;&#38382;&#31572;&#23545;&#35805;&#32467;&#26500;&#21270;&#24605;&#32500;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11770
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#38382;&#31572;&#23545;&#35805;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#31243;&#24207;&#23545;&#22522;&#30784;&#25991;&#26723;&#30340;&#24544;&#35802;&#24230;&#65292;&#35757;&#32451;&#24378;&#22823;&#30340;&#23545;&#35805;&#38382;&#31572;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#65288;SCoT&#65289;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#22522;&#20110;&#20869;&#23481;&#30340;&#22810;&#36718;&#38382;&#31572;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#26680;&#24515;&#26159;&#23558;&#22797;&#26434;&#20219;&#21153;&#32467;&#26500;&#21270;&#20998;&#35299;&#20026;&#29366;&#24577;&#26426;&#20013;&#30340;&#22810;&#20010;&#29366;&#24577;&#65292;&#20197;&#20415;&#25191;&#34892;&#23545;&#24212;&#20110;&#21508;&#31181;&#23376;&#20219;&#21153;&#65288;&#20363;&#22914;&#20869;&#23481;&#38405;&#35835;&#21644;&#35805;&#35821;&#29983;&#25104;&#65289;&#30340;&#21160;&#20316;&#12290;&#27599;&#20010;&#29366;&#24577;&#21033;&#29992;&#19968;&#32452;&#29420;&#29305;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#25552;&#31034;&#21644;&#65288;&#21487;&#36873;&#65289;&#39069;&#22806;&#24037;&#20855;&#20197;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24187;&#35273;&#20943;&#36731;&#65292;&#20351;&#29992;&#20855;&#26377;&#25351;&#23450;&#29366;&#24577;&#30340;SCoT&#25552;&#31034;&#21487;&#20197;&#20351;&#23545;&#25509;&#22320;&#25991;&#26723;&#30340;&#20195;&#29702;&#24544;&#35802;&#24230;&#25552;&#39640;&#39640;&#36798;16.8&#65285;&#12290;&#24403;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20165;&#20174;6&#20010;&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#30340;&#31181;&#23376;&#31034;&#33539;&#21512;&#25104;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#35757;&#32451;&#20986;&#24378;&#22823;&#30340;&#23545;&#35805;&#38382;&#31572;&#20195;&#29702;&#31243;&#24207;&#65307;&#22312;&#39046;&#22495;&#22806;&#35780;&#20272;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11770v1 Announce Type: new  Abstract: We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for exam
&lt;/p&gt;</description></item><item><title>MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11756</link><description>&lt;p&gt;
MARS&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;LLMs&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11756
&lt;/p&gt;
&lt;p&gt;
MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#19981;&#20934;&#30830;&#25110;&#35823;&#23548;&#24615;&#36755;&#20986;&#30340;&#20542;&#21521;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#29983;&#25104;&#24335;LLM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#26159;&#22686;&#24378;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;SOTA&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;&#65288;MARS&#65289;&#30340;&#26367;&#20195;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#30340;UE&#26041;&#27861;&#12290;MARS&#26159;&#19968;&#31181;&#32771;&#34385;&#22312;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;MARS&#25972;&#21512;&#21040;UE&#26041;&#27861;&#20013;&#20250;&#22312;UE&#24615;&#33021;&#19978;&#24102;&#26469;&#26222;&#36941;&#21644;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#38381;&#21367;&#24335;&#38382;&#31572;&#26469;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11512</link><description>&lt;p&gt;
&#20174;&#20559;&#35265;&#21040;&#24179;&#31561;&#65306;&#21435;&#20559;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#35789;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#23427;&#20204;&#26159;&#36825;&#20123;&#27169;&#22411;&#25226;&#25569;&#19978;&#19979;&#25991;&#20851;&#31995;&#12289;&#20419;&#36827;&#26356;&#32454;&#33268;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#22312;&#35768;&#22810;&#38656;&#35201;&#23545;&#20154;&#31867;&#35821;&#35328;&#26377;&#22522;&#26412;&#29702;&#35299;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#22522;&#30707;&#12290;&#37492;&#20110;&#36825;&#20123;&#23884;&#20837;&#24448;&#24448;&#33258;&#36523;&#21453;&#26144;&#25110;&#23637;&#31034;&#20559;&#35265;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20063;&#20250;&#26080;&#24847;&#20013;&#23398;&#20064;&#36825;&#31181;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#24320;&#21019;&#24615;&#21069;&#20154;&#30740;&#31350;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;DeepSoftDebias&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#8220;&#36719;&#21435;&#20559;&#8221;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31867;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#20855;&#26377;&#25361;&#25112;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;DeepSoftDebias&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
&lt;/p&gt;</description></item><item><title>&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10949</link><description>&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Eccentric Automatic Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10949
&lt;/p&gt;
&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#21046;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#37327;&#21270;&#23558;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#32435;&#20837;&#31995;&#32479;&#25552;&#31034;&#28040;&#24687;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#31995;&#32479;&#21270;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;60&#31181;&#31995;&#32479;&#28040;&#24687;&#29255;&#27573;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Chain of Thought&#25552;&#31034;&#65292;&#36328;&#19977;&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;70&#20159;&#21040;70&#20159;&#20010;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#26524;&#24182;&#19981;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#26222;&#36941;&#36866;&#29992;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#31215;&#26497;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Llama2-70B&#22312;&#19981;&#20351;&#29992;Chain of Thought&#26102;&#26159;&#20010;&#20363;&#22806;&#65292;&#22240;&#20026;&#21457;&#29616;&#26368;&#20339;&#31995;&#32479;&#28040;&#24687;&#23454;&#38469;&#19978;&#26159;&#27809;&#26377;&#28040;&#24687;&#12290;&#32771;&#34385;&#21040;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23548;&#33267;&#30340;&#21152;# Truncated due to exceeding character limit.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10669</link><description>&lt;p&gt;
&#20154;&#31867;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35009;&#21028;&#65311;&#19968;&#39033;&#20851;&#20110;&#21028;&#20915;&#20559;&#35265;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Humans or LLMs as the Judge? A Study on Judgement Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35009;&#21028;&#65288;&#21363;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#65289;&#26469;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#20570;&#27861;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#21487;&#33021;&#24341;&#20837;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#36136;&#30097;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;5&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;142&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#20462;&#35746;&#30340;&#24067;&#21346;&#22982;&#20998;&#31867;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21315;&#19978;&#19975;&#27425;&#30340;&#20154;&#31867;&#21644;LLM&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#37117;&#23481;&#26131;&#21463;&#21040;&#24178;&#25200;&#65292;&#21363;&#20351;&#26368;&#23574;&#31471;&#30340;&#35009;&#21028;&#20063;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20182;&#20204;&#30340;&#24369;&#28857;&#23545;LLM&#35009;&#21028;&#36827;&#34892;&#25915;&#20987;&#12290;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#25552;&#37266;&#31038;&#32676;&#20851;&#20110;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#20197;&#21450;&#21457;&#23637;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10669v1 Announce Type: new  Abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of develop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10311</link><description>&lt;p&gt;
&#21517;&#35789;&#30701;&#35821;&#20013;&#22836;&#37096;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21477;&#35805;&#30340;&#35789;&#24207;&#30001;&#22810;&#31181;&#21407;&#21017;&#22609;&#36896;&#12290;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#21407;&#21017;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#22312;&#21333;&#19968;&#22836;&#37096;&#30340;&#21477;&#27861;&#20381;&#36182;&#32467;&#26500;&#20013;&#23384;&#22312;&#20914;&#31361;&#65306;&#21069;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#32447;&#24615;&#25490;&#21015;&#30340;&#20013;&#24515;&#65292;&#21518;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#20004;&#31471;&#20043;&#19968;&#65288;&#35201;&#20040;&#22312;&#39318;&#20301;&#65292;&#35201;&#20040;&#22312;&#26411;&#20301;&#65289;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20309;&#26102;&#24847;&#22806;&#20943;&#23569;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#24212;&#35813;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#22312;&#21333;&#19968;&#22836;&#37096;&#32467;&#26500;&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#27979;&#22312;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#26102;&#26356;&#26377;&#21487;&#33021;&#21457;&#29983;&#65292;&#21363;&#65288;a&#65289;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#65292;&#24182;&#19988;&#65288;b&#65289;&#21333;&#35789;&#36739;&#30701;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#30001;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#32452;&#25104;&#30340;&#21517;&#35789;&#30701;&#35821;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39318;&#36873;&#39034;&#24207;&#20013;...&#65288;&#32570;&#22833;&#37096;&#20998;&#26080;&#27861;&#25552;&#20379;&#23436;&#25972;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07859</link><description>&lt;p&gt;
Lissard&#65306;&#38271;&#32780;&#31616;&#21333;&#30340;&#39034;&#24207;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Lissard: Long and Simple Sequential Reasoning Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07859
&lt;/p&gt;
&lt;p&gt;
Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#33021;&#22815;&#35299;&#20915;&#38656;&#35201;&#22788;&#29702;&#25968;&#21313;&#19975;&#20010;&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#37325;&#22797;&#20351;&#29992;&#31616;&#21333;&#35268;&#21017;&#30340;&#20219;&#21153;&#19978;&#24120;&#24120;&#22833;&#36133;&#65292;&#29978;&#33267;&#22312;&#27604;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#24207;&#21015;&#35201;&#30701;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20363;&#22914;&#65292;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22312;&#20004;&#20010;&#21015;&#34920;&#20013;&#25214;&#21040;&#20849;&#21516;&#39033;&#65292;&#21015;&#34920;&#20013;&#30340;&#39033;&#26368;&#22810;&#21487;&#36798;20&#20010;&#65292;&#20294;&#26159;&#24403;&#21015;&#34920;&#20013;&#30340;&#39033;&#36798;&#21040;80&#20010;&#26102;&#65292;&#23427;&#20204;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Lissard&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24320;&#28304;&#27169;&#22411;&#65288;Mistral-7B&#21644;Mixtral-8x7B&#65289;&#21644;&#19987;&#26377;&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/unicamp-dl/Lissard&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.06196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06196
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#33258;2022&#24180;11&#26376;ChatGPT&#21457;&#24067;&#20197;&#26469;&#12290;LLMs&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#21313;&#20159;&#21442;&#25968;&#26469;&#33719;&#24471;&#24191;&#27867;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#31526;&#21512;&#32553;&#25918;&#23450;&#24459;&#30340;&#39044;&#27979;&#12290;LLMs&#30340;&#30740;&#31350;&#39046;&#22495;&#23613;&#31649;&#38750;&#24120;&#26032;&#65292;&#20294;&#22312;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#26368;&#33879;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;&#19977;&#20010;&#27969;&#34892;&#30340;LLM&#31995;&#21015;&#65288;GPT&#12289;LLaMA&#12289;PaLM&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#36129;&#29486;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;LLM&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#20934;&#22791;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#65292;&#23457;&#26597;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#27969;&#34892;LLM&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05926</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#38454;&#32852;&#37030;&#35843;&#25972;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05926
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34701;&#21512;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;LLM&#25152;&#38656;&#30340;&#24378;&#22823;&#20869;&#23384;&#35201;&#27714;&#22312;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#30340;&#20840;&#26032;&#25972;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FedMeZO&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22312;LLM&#32972;&#26223;&#19979;&#32771;&#23519;FedMeZO&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#22823;&#21442;&#25968;&#31354;&#38388;&#23545;&#20248;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12289;&#25910;&#25947;&#24615;&#30340;&#24314;&#31435;&#20197;&#21450;&#20026;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#30830;&#23450;&#20851;&#38190;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#34920;&#26126;FedMeZO&#19981;&#20165;&#27604;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;SGD&#65289;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#26126;&#26174;...
&lt;/p&gt;
&lt;p&gt;
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.</title><link>https://arxiv.org/abs/2402.01697</link><description>&lt;p&gt;
APT-Pipe: &#29992;&#20110;&#31038;&#20132;&#35745;&#31639;&#25968;&#25454;&#26631;&#27880;&#30340;&#33258;&#21160;&#25552;&#31034;&#35843;&#25972;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01697
&lt;/p&gt;
&lt;p&gt;
APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#24212;&#29992;&#22312;&#31038;&#20132;&#35745;&#31639;&#25991;&#26412;&#26631;&#27880;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#24615;&#33021;&#21462;&#20915;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#25552;&#31034;&#35843;&#25972;&#30340;&#25216;&#26415;&#21644;&#25351;&#21335;&#65292;&#35797;&#22270;&#25913;&#21892;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#25163;&#24037;&#21162;&#21147;&#21644;&#23545;&#27491;&#22312;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#25552;&#31034;&#35843;&#25972;&#27969;&#27700;&#32447;APT-Pipe&#12290;APT-Pipe&#26088;&#22312;&#33258;&#21160;&#35843;&#25972;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;ChatGPT&#22312;&#20219;&#20309;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;APT-Pipe&#65292;&#24182;&#22312;12&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;APT-Pipe&#35843;&#25972;&#30340;&#25552;&#31034;&#26377;&#21161;&#20110;ChatGPT&#22312;12&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#20013;&#26377;9&#20010;&#33719;&#24471;&#26356;&#39640;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#65292;&#24179;&#22343;&#25913;&#36827;&#20102;7.01&#65285;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;APT-Pipe&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#31070;&#32463;&#12289;&#30417;&#30563;&#21644;&#32463;&#20856;&#20027;&#39064;&#27169;&#22411;&#22312;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#26174;&#31034;&#19978;&#19979;&#25991;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#32858;&#31867;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2401.16348</link><description>&lt;p&gt;
&#25913;&#36827;&#26631;&#31614;&#30340;TENOR&#65306;&#37325;&#26032;&#35780;&#20272;&#29992;&#20110;&#20869;&#23481;&#20998;&#26512;&#30340;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#31070;&#32463;&#12289;&#30417;&#30563;&#21644;&#32463;&#20856;&#20027;&#39064;&#27169;&#22411;&#22312;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#26174;&#31034;&#19978;&#19979;&#25991;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#32858;&#31867;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#26159;&#29702;&#35299;&#25991;&#26412;&#38598;&#21512;&#30340;&#19968;&#31181;&#27969;&#34892;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#35780;&#20272;&#19968;&#30452;&#26159;&#19968;&#20010;&#20105;&#35770;&#28857;&#12290;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#22914;&#36830;&#36143;&#24615;&#32463;&#24120;&#34987;&#20351;&#29992;&#65292;&#28982;&#32780;&#23545;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#20182;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36136;&#30097;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#30410;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#20132;&#20114;&#24335;&#20219;&#21153;&#30340;&#29615;&#22659;&#20013;&#39318;&#27425;&#35780;&#20272;&#20102;&#31070;&#32463;&#12289;&#30417;&#30563;&#21644;&#32463;&#20856;&#20027;&#39064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20027;&#39064;&#27169;&#22411;&#19982;&#20998;&#31867;&#22120;&#32467;&#21512;&#65292;&#24182;&#27979;&#35797;&#23427;&#20204;&#24110;&#21161;&#20154;&#31867;&#36827;&#34892;&#20869;&#23481;&#20998;&#26512;&#21644;&#25991;&#26723;&#27880;&#37322;&#30340;&#33021;&#21147;&#12290;&#20174;&#27169;&#25311;&#12289;&#30495;&#23454;&#29992;&#25143;&#21644;&#19987;&#23478;&#30340;&#35797;&#28857;&#30740;&#31350;&#20013;&#65292;&#19978;&#19979;&#25991;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65307;&#28982;&#32780;&#65292;LDA&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#20013;&#19982;&#21478;&#22806;&#20004;&#31181;NTMs&#31454;&#20105;&#28608;&#28872;&#65292;&#19982;&#36830;&#36143;&#24615;&#20998;&#25968;&#25152;&#26263;&#31034;&#30340;&#24773;&#20917;&#30456;&#21453;&#12290;&#25105;&#20204;&#34920;&#26126;&#24403;&#21069;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#24182;&#19981;&#25552;&#20379;&#23436;&#25972;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16348v2 Announce Type: replace  Abstract: Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook a models benefits in real world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>MAPLE&#36890;&#36807;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#23545;LLama-2-7B&#21644;Mistral-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20845;&#39033;&#28085;&#30422;40&#31181;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#24494;&#35843;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.07598</link><description>&lt;p&gt;
MAPLE: &#22810;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07598
&lt;/p&gt;
&lt;p&gt;
MAPLE&#36890;&#36807;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#23545;LLama-2-7B&#21644;Mistral-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20845;&#39033;&#28085;&#30422;40&#31181;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#24494;&#35843;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Finetuning (PEFT)&#24050;&#32463;&#25104;&#20026;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#36164;&#28304;&#21644;&#35745;&#31639;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#21512;&#25104;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLama-2-7B&#21644;Mistral-7B&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20854;&#23545;&#20845;&#20010;&#28085;&#30422;&#22235;&#21313;&#31181;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#19981;&#21516;&#30340;&#21442;&#25968;&#65292;&#20363;&#22914;&#29992;&#20110;&#20302;&#31209;&#36866;&#24212;&#30340;&#31209;&#21644;&#37327;&#21270;&#20540;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26356;&#39640;&#30340;&#31209;&#21644;&#26356;&#39640;&#30340;hig
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07598v2 Announce Type: replace  Abstract: Parameter Efficient Finetuning (PEFT) has emerged as a viable solution for improving the performance of Large Language Models (LLMs) without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the LLama-2-7B and Mistral-7B models on two synthetic multilingual instruction tuning datasets to determine its effect on model performance on six downstream tasks covering forty languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and hig
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>EHRAgent&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#20195;&#30721;&#65292;&#36890;&#36807;&#38169;&#35823;&#20449;&#24687;&#23398;&#20064;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;</title><link>https://arxiv.org/abs/2401.07128</link><description>&lt;p&gt;
EHRAgent&#65306;&#20195;&#30721;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07128
&lt;/p&gt;
&lt;p&gt;
EHRAgent&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#20195;&#30721;&#65292;&#36890;&#36807;&#38169;&#35823;&#20449;&#24687;&#23398;&#20064;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21307;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#23578;&#26410;&#26377;&#22826;&#22810;&#24320;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;EHRAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;LLM&#20195;&#29702;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#20013;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#30340;&#20195;&#30721;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;EHR&#38382;&#31572;&#20219;&#21153;&#21046;&#23450;&#20026;&#24037;&#20855;&#20351;&#29992;&#35268;&#21010;&#36807;&#31243;&#65292;&#23558;&#19968;&#20010;&#22797;&#26434;&#20219;&#21153;&#39640;&#25928;&#22320;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21487;&#31649;&#29702;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#38598;&#25104;&#20132;&#20114;&#24335;&#32534;&#30721;&#21644;&#25191;&#34892;&#21453;&#39304;&#65292;EHRAgent&#20174;&#38169;&#35823;&#28040;&#24687;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26368;&#21021;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#26469;&#22686;&#24378;LLM&#20195;&#29702;&#65292;&#20351;EHRAgent&#33021;&#22815;&#26377;&#25928;&#22320;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#26368;&#30456;&#20851;&#30340;&#25104;&#21151;&#26696;&#20363;&#19978;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#34920;&#26684;EHR&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07128v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Learn Temporal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#21644;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#26102;&#38388;&#25512;&#29702;&#65288;TR&#65289;&#23545;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#22810;&#26679;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TG-LLM&#65292;&#19968;&#20010;&#33268;&#21147;&#20110;&#22522;&#20110;&#35821;&#35328;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65288;TG&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#21487;&#25511;&#19988;&#38656;&#35201;&#26368;&#23569;&#30417;&#30563;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#36825;&#20010;&#22270;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#23454;&#65292;&#23398;&#20064;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#30340;TG&#25552;&#21462;&#33021;&#21147;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;TR&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;CoTs&#24341;&#23548;LLM&#36890;&#36807;TG&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin
&lt;/p&gt;</description></item><item><title>&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#24182;&#26410;&#20135;&#29983;&#19968;&#36143;&#24615;&#25913;&#36827;&#25928;&#26524;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;</title><link>https://arxiv.org/abs/2312.15006</link><description>&lt;p&gt;
&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15006
&lt;/p&gt;
&lt;p&gt;
&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#24182;&#26410;&#20135;&#29983;&#19968;&#36143;&#24615;&#25913;&#36827;&#25928;&#26524;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#25552;&#31034;&#26041;&#27861;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#19977;&#31181;&#35268;&#23450;&#24615;&#25552;&#31034;&#26041;&#27861; - &#31616;&#21333;&#25552;&#31034;&#12289;&#20010;&#20154;&#25552;&#31034;&#21644;&#23545;&#35805;&#25552;&#31034; - &#36825;&#20123;&#26041;&#27861;&#20197;&#25552;&#21319;LLMs&#35821;&#35328;&#20219;&#21153;&#25928;&#26524;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#22312;OpenAI&#30340;LLM&#38386;&#32842;&#26426;&#22120;&#20154;ChatGPT-3.5&#19978;&#36827;&#34892;&#27492;&#20998;&#26512;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;MATH&#12289;GSM8K&#21644;MMLU&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38382;&#39064;&#38598;&#21512;&#65292;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#23398;&#25361;&#25112;&#12290;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#35843;&#25972;&#30340;&#35780;&#20998;&#33050;&#26412;&#29992;&#20110;&#30830;&#23450;&#36825;&#20123;&#25552;&#31034;&#24178;&#39044;&#22312;&#22686;&#24378;&#27169;&#22411;&#25968;&#23398;&#20998;&#26512;&#33021;&#21147;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#25152;&#26816;&#39564;&#30340;&#26041;&#27861;&#22343;&#26410;&#22312;&#25345;&#32493;&#25913;&#36827;ChatGPT-3.5&#22522;&#20934;&#34920;&#29616;&#19978;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#26126;&#26174;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#31574;&#30053;&#26410;&#24517;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02073</link><description>&lt;p&gt;
&#19968;&#21058;&#30149;&#27602;&#65311;&#20351;&#29992;Fakepedia&#23450;&#20301;&#21644;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20174;&#20854;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#26032;&#39062;&#20449;&#24687;&#20013;&#33719;&#24471;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24773;&#20917;&#19979;&#65292;&#25903;&#25745;&#36825;&#31181;&#19978;&#19979;&#25991;&#22522;&#30784;&#30340;&#26426;&#21046;&#65292;LLMs&#22312;&#22238;&#24518;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#20559;&#22909;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#19982;&#26368;&#26032;&#20449;&#24687;&#20016;&#23500;&#65292;&#24076;&#26395;&#22522;&#30784;&#21487;&#20197;&#32416;&#27491;&#36807;&#26102;&#25110;&#26377;&#22122;&#22768;&#30340;&#23384;&#20648;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Fakepedia&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#30740;&#31350;&#22522;&#30784;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#27169;&#22411;&#20869;&#37096;&#21442;&#25968;&#30693;&#35782;&#20914;&#31361;&#30340;&#21453;&#20107;&#23454;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;Fakepedia&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36974;&#34109;&#20998;&#32452;&#22240;&#26524;&#36861;&#36394;&#65288;MGCT&#65289;&#65292;&#23545;&#22238;&#31572;Fakepedia&#26597;&#35810;&#26102;&#30340;LLM&#32452;&#20214;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#37492;&#21035;&#20986;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#27010;&#36848;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#21464;&#38761;&#24615;&#20316;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#35786;&#26029;&#21644;&#27835;&#30103;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#30284;&#30151;&#25252;&#29702;&#12289;&#30382;&#32932;&#31185;&#12289;&#29273;&#31185;&#21644;&#24515;&#29702;&#20581;&#24247;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.12882</link><description>&lt;p&gt;
&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#20013;&#30340;&#24212;&#29992;&#27010;&#35272;
&lt;/p&gt;
&lt;p&gt;
Overview of Current Applications of Large Language Models in Various Medical Specialities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12882
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#27010;&#36848;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#21464;&#38761;&#24615;&#20316;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#35786;&#26029;&#21644;&#27835;&#30103;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#30284;&#30151;&#25252;&#29702;&#12289;&#30382;&#32932;&#31185;&#12289;&#29273;&#31185;&#21644;&#24515;&#29702;&#20581;&#24247;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#27010;&#36848;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#26368;&#26032;&#24212;&#29992;&#65292;&#31361;&#20986;&#23427;&#20204;&#22312;&#25552;&#21319;&#21307;&#30103;&#36136;&#37327;&#26041;&#38754;&#30340;&#21464;&#38761;&#24615;&#20316;&#29992;&#12290;&#36890;&#36807;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;LLMs&#24050;&#25104;&#20026;&#21327;&#21161;&#21307;&#29983;&#12289;&#21307;&#30103;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#37325;&#28857;&#23457;&#35270;&#20102;LLMs&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#30456;&#20851;&#24212;&#29992;&#19978;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;LLMs&#22312;&#30284;&#30151;&#25252;&#29702;&#12289;&#30382;&#32932;&#31185;&#12289;&#29273;&#31185;&#21644;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20026;&#21307;&#23398;&#35786;&#26029;&#21644;&#24739;&#32773;&#25252;&#29702;&#24102;&#26469;&#30340;&#21019;&#26032;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;&#23558;LLMs&#25972;&#21512;&#21040;&#21307;&#30103;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25351;&#20986;&#20102;&#23613;&#31649;&#23384;&#22312;&#24403;&#21069;&#23616;&#38480;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12882v2 Announce Type: replace  Abstract: We aim to provide an overview of the latest applications of Large Language Models (LLMs) in the healthcare sector, highlighting their transformative role in enhancing medical care quality. By processing vast amounts of data from diverse medical domains, LLMs have become pivotal in assisting doctors, healthcare providers, and patients. We review the application of Large Language Models (LLMs) in healthcare, focusing on diagnostics and treatment related applications. We highlight the use of LLMs in cancer care, dermatology, dental, and mental health, emphasizing the innovation they bring to medical diagnostics and patient care. The analysis addresses the challenges and opportunities of integrating LLMs in healthcare, noting their potential in various medical specialties despite current limitations. Further, we provide an overview of handling various data types in the medical field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.10537</link><description>&lt;p&gt;
MedAgents: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#21307;&#23398;&#25512;&#29702;&#30340;&#21512;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23613;&#31649;&#22312;&#21508;&#31181;&#36890;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#21253;&#25324;&#20116;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#25910;&#38598;&#39046;&#22495;&#19987;&#23478;&#12289;&#25552;&#20986;&#20010;&#21035;&#20998;&#26512;&#12289;&#23558;&#36825;&#20123;&#20998;&#26512;&#24635;&#32467;&#25104;&#25253;&#21578;&#12289;&#22312;&#35752;&#35770;&#20013;&#21453;&#22797;&#36845;&#20195;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#65292;&#26368;&#32456;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#38646;-shot&#24773;&#26223;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#20061;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#26159;&#21542;&#20250;&#23545;&#30456;&#21516;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22312;&#26080;&#21442;&#32771;&#24773;&#20917;&#19979;&#20351;&#29992;&#26102;&#20559;&#35265;&#23588;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2311.09766</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#33258;&#24651;&#35780;&#20272;&#32773;&#65306;&#24403;&#33258;&#25105;&#33192;&#32960;&#24433;&#21709;&#35780;&#20272;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#26159;&#21542;&#20250;&#23545;&#30456;&#21516;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22312;&#26080;&#21442;&#32771;&#24773;&#20917;&#19979;&#20351;&#29992;&#26102;&#20559;&#35265;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09766v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#29983;&#25104;&#25991;&#26412;&#20869;&#23481;&#30340;&#33258;&#21160;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#37492;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20542;&#21521;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#21019;&#36896;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#21542;&#20250;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#20559;&#21521;&#20110;&#30001;&#30456;&#21516;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20559;&#35265;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#30693;&#21517;&#30340;&#22522;&#20110;LM&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;&#20363;&#22914;BARTScore&#12289;T5Score&#21644;GPTScore&#65289;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#26159;&#21542;&#23545;&#20854;&#21508;&#33258;&#30340;&#22522;&#30784;LM&#34920;&#29616;&#20986;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#28508;&#22312;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#35780;&#20272;&#25351;&#26631;&#22312;&#26080;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#19988;&#19981;&#21033;&#29992;&#40644;&#37329;&#25688;&#35201;&#26102;&#65292;&#36825;&#31181;&#20559;&#35265;&#23588;&#20026;&#26174;&#33879;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#33719;&#24471;&#30340;&#35780;&#20272;&#32467;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#33258;&#25105;&#20559;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09766v2 Announce Type: replace  Abstract: Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#21644;&#25277;&#35937;&#65292;&#26412;&#30740;&#31350;&#38477;&#20302;&#20102;&#22312;&#32447;&#33258;&#25105;&#25259;&#38706;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#33258;&#25105;&#25259;&#38706;&#25277;&#35937;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#24494;&#35843;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2311.09538</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#38477;&#20302;&#22312;&#32447;&#33258;&#25105;&#25259;&#38706;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Reducing Privacy Risks in Online Self-Disclosures with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09538
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#21644;&#25277;&#35937;&#65292;&#26412;&#30740;&#31350;&#38477;&#20302;&#20102;&#22312;&#32447;&#33258;&#25105;&#25259;&#38706;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#33258;&#25105;&#25259;&#38706;&#25277;&#35937;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#25259;&#38706;&#22312;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#20013;&#26082;&#26222;&#36941;&#21448;&#26377;&#22238;&#25253;&#65292;&#20294;&#20063;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#27979;&#21644;&#25277;&#35937;&#20027;&#21160;&#20445;&#25252;&#19982;&#22312;&#32447;&#33258;&#25105;&#25259;&#38706;&#30456;&#20851;&#30340;&#29992;&#25143;&#38544;&#31169;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;4.8K&#20010;&#26631;&#27880;&#25259;&#38706;&#27573;&#30340;19&#31181;&#33258;&#25105;&#25259;&#38706;&#31867;&#21035;&#30340;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#20026;&#26816;&#27979;&#24494;&#35843;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;65%&#20197;&#19978;&#30340;&#23616;&#37096;&#36328;&#24230;F$_1$&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#26426;&#20132;&#20114;&#29992;&#25143;&#30740;&#31350;&#65292;82%&#30340;&#21442;&#19982;&#32773;&#23545;&#35813;&#27169;&#22411;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#31361;&#20986;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#22312;&#29992;&#25143;&#21453;&#39304;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#25105;&#25259;&#38706;&#25277;&#35937;&#30340;&#20219;&#21153;&#65292;&#21363;&#23558;&#25259;&#38706;&#37325;&#36848;&#20026;&#19981;&#22826;&#20855;&#20307;&#30340;&#26415;&#35821;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#23454;&#29992;&#24615;&#65292;&#20363;&#22914;&#23558;"Im 16F"&#37325;&#36848;&#20026;"I'm a teenage girl"&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#24494;&#35843;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#25277;&#35937;&#65292;&#20174;&#32780;&#36866;&#24230;&#20943;&#23569;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09538v2 Announce Type: replace  Abstract: Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65% partial span F$_1$. We further conduct an HCI user study, with 82% of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is paraphrasing disclosures into less specific terms while preserving their utility, e.g., "Im 16F" to "I'm a teenage girl". We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21551;&#21457;&#24335;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22788;&#29702;&#26032;&#24773;&#20917;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.06555</link><description>&lt;p&gt;
&#21551;&#21457;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06555
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21551;&#21457;&#24335;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22788;&#29702;&#26032;&#24773;&#20917;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#20943;&#36731;&#36825;&#19968;&#20219;&#21153;&#23545;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21551;&#21457;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#65288;HD-LoA&#65289;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#31034;&#20363;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20026;EAE&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#24182;&#39564;&#35777;&#20102;LLMs&#36890;&#36807;ICL&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#21551;&#21457;&#24335;&#39537;&#21160;&#31034;&#33539;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#26434;&#20081;&#30340;&#31034;&#20363;&#36873;&#25321;&#36807;&#31243;&#36716;&#21270;&#20026;&#24378;&#35843;&#20219;&#21153;&#21551;&#21457;&#24335;&#30340;&#26377;&#26465;&#19981;&#32010;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#21463;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#27604;&#38142;&#25509;&#25552;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#23558;&#26032;&#24773;&#20917;&#31867;&#27604;&#20110;&#24050;&#30693;&#24773;&#20917;&#26469;&#22788;&#29702;&#26032;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#22312;&#26377;&#38480;ICL&#31034;&#20363;&#20197;&#22806;&#30340;&#26410;&#35265;&#31867;&#21035;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06555v2 Announce Type: replace-cross  Abstract: In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL exa
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.05661</link><description>&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#26816;&#26597;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#20551;&#35774;&#24403;&#21069;&#25552;&#31034;&#20013;&#32570;&#23569;&#25110;&#35823;&#23548;&#20102;&#20160;&#20040;&#65292;&#24182;&#28165;&#26224;&#22320;&#20256;&#36798;&#20219;&#21153;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#25552;&#31034;&#26469;&#25191;&#34892;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30001;&#20110;&#20803;&#25552;&#31034;&#20013;&#32570;&#20047;&#22797;&#26434;&#25512;&#29702;&#30340;&#20805;&#20998;&#25351;&#23548;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#27880;&#20837;&#21040;&#20803;&#25552;&#31034;&#20013;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;PE2&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#20013;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#25214;&#21040;&#30340;&#25552;&#31034;&#22312;MultiArith&#19978;&#27604;&#8220;&#25353;&#27493;&#39588;&#24605;&#32771;&#8221;&#39640;&#20986;6.3%&#65292;&#22312;GSM8K&#19978;&#39640;&#20986;3.1%&#65292;&#24182;&#22312;&#23545;&#31435;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#25688;&#35201;&#20219;&#21153;&#20013;&#20851;&#20110;&#36755;&#20837;&#20301;&#32622;&#30340;&#24615;&#33021;&#27169;&#24335;&#20197;&#21450;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20869;&#23481;&#26144;&#23556;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10570</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Context Utilization in Summarization with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#25688;&#35201;&#20219;&#21153;&#20013;&#20851;&#20110;&#36755;&#20837;&#20301;&#32622;&#30340;&#24615;&#33021;&#27169;&#24335;&#20197;&#21450;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20869;&#23481;&#26144;&#23556;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#27969;&#30021;&#19988;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#25193;&#23637;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#36229;&#36807;&#20102;100k&#20010;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#38382;&#31572;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#19981;&#22343;&#21248;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20559;&#29233;&#21021;&#22987;&#21644;&#26368;&#32456;&#27573;&#33853;&#65292;&#23548;&#33268;&#20102;&#20851;&#20110;&#31572;&#26696;&#22312;&#36755;&#20837;&#20013;&#20301;&#32622;&#30340;U&#24418;&#24615;&#33021;&#27169;&#24335;&#12290;&#36825;&#31181;&#20559;&#35265;&#24341;&#21457;&#20102;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#25688;&#35201;&#20013;&#65292;&#20851;&#38190;&#20869;&#23481;&#21487;&#33021;&#20998;&#25955;&#22312;&#28304;&#25991;&#20214;&#20013;&#12290;&#27492;&#22806;&#65292;&#22312;&#25688;&#35201;&#20013;&#65292;&#20174;&#28304;&#25991;&#20214;&#21040;&#25688;&#35201;&#30340;&#20107;&#23454;&#26144;&#23556;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#26174;&#33879;&#20869;&#23481;&#36890;&#24120;&#20250;&#34987;&#37325;&#26032;&#34920;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25688;&#35201;&#20013;&#19978;&#19979;&#25991;&#21033;&#29992;&#21644;&#20301;&#32622;&#20559;&#35265;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;5&#20010;LLMs&#65292;10&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10570v3 Announce Type: replace  Abstract: Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluatio
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2310.03976</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#33258;&#25105;&#65306;&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#20154;&#38469;&#20132;&#27969;&#21644;&#33258;&#25105;&#26041;&#38754;&#28508;&#21147;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03976
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;AI&#20013;&#20171;&#20132;&#27969;&#65288;AIMC&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#24037;&#20855;&#27491;&#25104;&#20026;&#20154;&#38469;&#20132;&#27969;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;&#19968;&#21608;&#30340;&#26085;&#35760;&#21644;&#35775;&#35848;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#29992;&#25143;&#23545;&#36825;&#20123;&#24037;&#20855;&#22312;&#30701;&#26399;&#20869;&#25903;&#25345;&#20154;&#38469;&#20132;&#27969;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#23548;&#33268;&#30340;&#38271;&#26399;&#25928;&#26524;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#23545;AIMC&#25903;&#25345;&#25345;&#26377;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#20854;&#33021;&#22815;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#65292;&#24110;&#21161;&#25214;&#21040;&#20934;&#30830;&#30340;&#35821;&#35328;&#34920;&#36798;&#24819;&#27861;&#65292;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;AIMC&#24037;&#20855;&#30446;&#21069;&#23384;&#22312;&#30340;&#23616;&#38480;&#65292;&#21253;&#25324;&#21872;&#21990;&#30340;&#22238;&#22797;&#12289;&#19981;&#33258;&#28982;&#30340;&#22238;&#24212;&#20197;&#21450;&#36807;&#24230;&#24773;&#32490;&#21270;&#12290;&#36825;&#20123;&#32570;&#38519;&#36827;&#19968;&#27493;&#21463;&#21040;&#29992;&#25143;&#23545;&#19981;&#30495;&#23454;&#24615;&#21644;&#23545;&#25216;&#26415;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#25152;&#21152;&#21095;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03976v2 Announce Type: cross  Abstract: In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users' perceptions of these tools' ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, and finding precise language to express their thoughts, navigating linguistic and cultural barriers. However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. Furthermore, we identified fou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21482;&#38656;&#35201;&#20960;&#21313;&#23567;&#26102;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#30340;&#26080;&#25991;&#26412;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#21333;&#20803;&#21040;&#21333;&#20803;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#32763;&#35793;&#20219;&#21153;&#21644;&#26080;&#30417;&#30563;&#21453;&#21521;&#32763;&#35793;&#30446;&#26631;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2305.15405</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#20803;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25991;&#26412;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Textless Low-Resource Speech-to-Speech Translation With Unit Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15405
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21482;&#38656;&#35201;&#20960;&#21313;&#23567;&#26102;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#30340;&#26080;&#25991;&#26412;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#21333;&#20803;&#21040;&#21333;&#20803;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#32763;&#35793;&#20219;&#21153;&#21644;&#26080;&#30417;&#30563;&#21453;&#21521;&#32763;&#35793;&#30446;&#26631;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#22823;&#33268;&#20998;&#20026;&#20004;&#31867;&#65306;&#20351;&#29992;&#25968;&#30334;&#23567;&#26102;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#35757;&#32451;&#30340;&#26080;&#25991;&#26412;&#27169;&#22411;&#65292;&#25110;&#32773;&#23558;&#25991;&#26412;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#38480;&#21046;&#20102;&#20026;&#24191;&#27867;&#35821;&#35328;&#26500;&#24314;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#25490;&#38500;&#20102;&#20027;&#35201;&#21475;&#35821;&#30340;&#35821;&#35328;&#20197;&#21450;&#32570;&#20047;&#22823;&#35268;&#27169;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#30340;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21482;&#38656;&#35201;&#20960;&#21313;&#23567;&#26102;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#30340;&#26080;&#25991;&#26412;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;S2ST&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#21333;&#20803;&#21040;&#21333;&#20803;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#32763;&#35793;&#20219;&#21153;&#65292;&#24182;&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#21333;&#35821;&#35328;&#35821;&#38899;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#24179;&#34892;&#35821;&#38899;&#25968;&#25454;&#65288;$20-60$&#23567;&#26102;&#65289;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26080;&#30417;&#30563;&#21453;&#21521;&#32763;&#35793;&#30446;&#26631;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;&#33521;&#35821;&#21040;&#24503;&#35821;&#65292;&#24503;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15405v2 Announce Type: replace  Abstract: Existing speech-to-speech translation models fall into two camps: textless models trained with hundreds of hours of parallel speech data or unsupervised models that leverage text as an intermediate step. Both approaches limit building speech-to-speech translation models for a wide range of languages, as they exclude languages that are primarily spoken and language pairs that lack large-scale parallel speech data. We present a new framework for training textless low-resource speech-to-speech translation (S2ST) systems that only need dozens of hours of parallel speech data. We reformulate S2ST as a unit-to-unit seq2seq translation task, and start by pretraining a model on large-scale monolingual speech data. Then, we finetune it with a small amount of parallel speech data ($20-60$ hours). Lastly, we improve model performance through an unsupervised backtranslation objective. We train and evaluate our models for English-to-German, Germa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; GaussCSE&#65292;&#29992;&#20110;&#22788;&#29702;&#21477;&#23376;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#33021;&#22815;&#20272;&#35745;&#34164;&#28085;&#20851;&#31995;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2305.12990</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#23884;&#20837;&#34920;&#31034;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Sentence Representations via Gaussian Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; GaussCSE&#65292;&#29992;&#20110;&#22788;&#29702;&#21477;&#23376;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#33021;&#22815;&#20272;&#35745;&#34164;&#28085;&#20851;&#31995;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23558;&#21477;&#23376;&#30340;&#21547;&#20041;&#34920;&#31034;&#20026;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#28857;&#65292;&#24050;&#22312;&#35832;&#22914;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#28857;&#30340;&#21477;&#23376;&#34920;&#31034;&#21482;&#33021;&#34920;&#36798;&#21477;&#23376;&#20855;&#26377;&#30340;&#22810;&#26679;&#20449;&#24687;&#30340;&#19968;&#37096;&#20998;&#65292;&#27604;&#22914;&#21477;&#23376;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GaussCSE&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#65292;&#21487;&#20197;&#22788;&#29702;&#21477;&#23376;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#20851;&#31995;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#29992;&#20110;&#35782;&#21035;&#21253;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GaussCSE&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20272;&#35745;&#34164;&#28085;&#20851;&#31995;&#30340;&#26041;&#21521;&#65292;&#36825;&#22312;&#28857;&#34920;&#31034;&#20013;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12990v2 Announce Type: replace  Abstract: Recent progress in sentence embedding, which represents the meaning of a sentence as a point in a vector space, has achieved high performance on tasks such as a semantic textual similarity (STS) task. However, sentence representations as a point in a vector space can express only a part of the diverse information that sentences have, such as asymmetrical relationships between sentences. This paper proposes GaussCSE, a Gaussian distribution-based contrastive learning framework for sentence embedding that can handle asymmetric relationships between sentences, along with a similarity measure for identifying inclusion relations. Our experiments show that GaussCSE achieves the same performance as previous methods in natural language inference tasks, and is able to estimate the direction of entailment relations, which is difficult with point representations.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#31934;&#30830;&#30828;&#21333;&#35843;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#23383;&#31526;&#32423;&#36716;&#23548;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/1905.06319</link><description>&lt;p&gt;
&#23383;&#31526;&#32423;&#36716;&#23548;&#30340;&#31934;&#30830;&#30828;&#21333;&#35843;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exact Hard Monotonic Attention for Character-Level Transduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1905.06319
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#31934;&#30830;&#30828;&#21333;&#35843;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#23383;&#31526;&#32423;&#36716;&#23548;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24120;&#35265;&#30340;&#23383;&#31526;&#32423;&#23383;&#31526;&#20018;&#36716;&#23548;&#20219;&#21153;&#65292;&#20363;&#22914;&#38899;&#32032;&#21040;&#23383;&#38899;&#36716;&#25442;&#21644;&#24418;&#24577;&#23624;&#25240;&#65292;&#20960;&#20046;&#23436;&#20840;&#30001;&#21333;&#35843;&#36716;&#23548;&#32452;&#25104;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#38750;&#21333;&#35843;&#36719;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#36890;&#24120;&#20248;&#20110;&#27969;&#34892;&#30340;&#21333;&#35843;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#65292;&#21333;&#35843;&#24615;&#30495;&#30340;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#32422;&#26463;&#21527;&#65311;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#30828;&#27880;&#24847;&#21147;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#24378;&#21046;&#25191;&#34892;&#20005;&#26684;&#30340;&#21333;&#35843;&#24615;&#65292;&#24182;&#22312;&#23398;&#20064;&#36716;&#23548;&#30340;&#21516;&#26102;&#23398;&#20064;&#28508;&#22312;&#30340;&#23545;&#40784;&#12290;&#20511;&#21161;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#20986;&#25152;&#26377;&#21333;&#35843;&#23545;&#40784;&#30340;&#31934;&#30830;&#36793;&#32536;&#21270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24418;&#24577;&#23624;&#25240;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21478;&#22806;&#20004;&#20010;&#23383;&#31526;&#32423;&#36716;&#23548;&#20219;&#21153;&#19978;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/shijie-wu/neural-transducer &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1905.06319v3 Announce Type: replace  Abstract: Many common character-level, string-to string transduction tasks, e.g., grapheme-tophoneme conversion and morphological inflection, consist almost exclusively of monotonic transductions. However, neural sequence-to sequence models that use non-monotonic soft attention often outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias for these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns a latent alignment jointly while learning to transduce. With the help of dynamic programming, we are able to compute the exact marginalization over all monotonic alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23383;&#31526;&#32423;&#36716;&#24405;&#30340;&#30828;&#24615;&#38750;&#21333;&#35843;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#12289;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#22788;&#29702;&#20004;&#20010;&#23383;&#31526;&#20018;&#20043;&#38388;&#30340;&#38750;&#21333;&#35843;&#23545;&#40784;&#65292;&#34920;&#26126;&#30828;&#24615;&#27880;&#24847;&#21147;&#27169;&#22411;&#26159;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/1808.10024</link><description>&lt;p&gt;
&#30828;&#24615;&#38750;&#21333;&#35843;&#27880;&#24847;&#21147;&#29992;&#20110;&#23383;&#31526;&#32423;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Hard Non-Monotonic Attention for Character-Level Transduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1808.10024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23383;&#31526;&#32423;&#36716;&#24405;&#30340;&#30828;&#24615;&#38750;&#21333;&#35843;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#12289;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#22788;&#29702;&#20004;&#20010;&#23383;&#31526;&#20018;&#20043;&#38388;&#30340;&#38750;&#21333;&#35843;&#23545;&#40784;&#65292;&#34920;&#26126;&#30828;&#24615;&#27880;&#24847;&#21147;&#27169;&#22411;&#26159;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#32423;&#23383;&#31526;&#20018;&#21040;&#23383;&#31526;&#20018;&#30340;&#36716;&#24405;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#26469;&#23398;&#20064;&#27169;&#22411;&#22312;&#29983;&#25104;&#36755;&#20986;&#23383;&#31526;&#20018;&#26102;&#24212;&#35813;&#20851;&#27880;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#20256;&#32479;&#30340;&#36719;&#24615;&#27880;&#24847;&#21147;&#21644;&#30828;&#24615;&#21333;&#35843;&#27880;&#24847;&#21147;&#24050;&#32463;&#34987;&#20351;&#29992;&#65292;&#20294;&#30828;&#24615;&#38750;&#21333;&#35843;&#27880;&#24847;&#21147;&#20165;&#29992;&#20110;&#20854;&#20182;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65289;&#65292;&#38656;&#35201;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#26469;&#35745;&#31639;&#26799;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#12289;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23545;&#20004;&#20010;&#23383;&#31526;&#20018;&#20043;&#38388;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#38750;&#21333;&#35843;&#23545;&#40784;&#36827;&#34892;&#36793;&#32536;&#21270;&#65292;&#34920;&#26126;&#30828;&#24615;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1808.10024v3 Announce Type: replace  Abstract: Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks such as image captioning (Xu et al., 2015), and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13601</link><description>&lt;p&gt;
MM-LLMs: &#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13601
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;LLMs&#23545;&#22810;&#27169;&#36755;&#20837;&#25110;&#36755;&#20986;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#32467;&#26524;&#27169;&#22411;&#19981;&#20165;&#20445;&#30041;&#20102;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#36824;&#36171;&#20104;&#20102;&#21508;&#31181;&#22810;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#27969;&#31243;&#30340;&#19968;&#33324;&#35774;&#35745;&#26041;&#26696;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;26&#31181;&#29616;&#26377;&#30340;MM-LLMs&#65292;&#27599;&#31181;&#37117;&#20197;&#20854;&#20855;&#20307;&#30340;&#20844;&#24335;&#20026;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;MM-LLMs&#22312;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#25552;&#39640;MM-LLMs&#25928;&#21147;&#30340;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MM-LLMs&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#36824;&#20026;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20379;&#20102;&#23454;&#26102;&#36861;&#36394;&#32593;&#31449;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#22815;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#21010;&#20998;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#29992;&#26080;&#26631;&#27880;&#35821;&#26009;&#24211;&#25214;&#21040;&#26356;&#22810;&#23454;&#20307;&#26469;&#20016;&#23500;&#30417;&#30563;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21010;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.13129</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains. (arXiv:2401.13129v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#21010;&#20998;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#29992;&#26080;&#26631;&#27880;&#35821;&#26009;&#24211;&#25214;&#21040;&#26356;&#22810;&#23454;&#20307;&#26469;&#20016;&#23500;&#30417;&#30563;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#23545;&#25991;&#26412;&#29255;&#27573;&#20013;&#30340;&#23454;&#20307;&#25552;&#20379;&#31867;&#22411;&#21010;&#20998;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#26469;&#25191;&#34892;&#23454;&#20307;&#31867;&#22411;&#21010;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65288;&#20363;&#22914;&#36719;&#20214;&#24037;&#31243;&#21644;&#23433;&#20840;&#39046;&#22495;&#65289;&#20013;&#25910;&#38598;&#27492;&#31867;&#25968;&#25454;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#26356;&#19981;&#29992;&#25552;&#36825;&#20123;&#27169;&#22411;&#22914;&#26524;&#38656;&#35201;&#24212;&#29992;&#20110;&#20445;&#23494;&#25968;&#25454;&#38598;&#26102;&#65292;&#35757;&#32451;&#21644;&#25512;&#26029;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#31181;&#23376;&#24341;&#23548;&#19979;&#23545;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#36827;&#34892;&#21010;&#20998;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#20197;&#23454;&#20307;&#30340;&#21517;&#31216;&#21644;&#19968;&#20123;&#31181;&#23376;&#23454;&#20307;&#20316;&#20026;&#21807;&#19968;&#30340;&#30417;&#30563;&#65292;&#24182;&#26088;&#22312;&#23558;&#26032;&#30340;&#23454;&#20307;&#25552;&#21450;&#20998;&#31867;&#20026;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#22411;&#65288;&#21363;&#27809;&#26377;&#31181;&#23376;&#23454;&#20307;&#30340;&#31867;&#22411;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEType&#65292;&#39318;&#20808;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#26080;&#26631;&#27880;&#35821;&#26009;&#24211;&#25214;&#21040;&#27599;&#20010;&#24050;&#30693;&#31867;&#22411;&#30340;&#26356;&#22810;&#23454;&#20307;&#26469;&#20016;&#23500;&#24369;&#30417;&#30563;&#20449;&#24687;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#24102;&#26377;&#20004;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#23545;&#23454;&#20307;&#36827;&#34892;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately typing entity mentions from text segments is a fundamental task for various natural language processing applications. Many previous approaches rely on massive human-annotated data to perform entity typing. Nevertheless, collecting such data in highly specialized science and engineering domains (e.g., software engineering and security) can be time-consuming and costly, without mentioning the domain gaps between training and inference data if the model needs to be applied to confidential datasets. In this paper, we study the task of seed-guided fine-grained entity typing in science and engineering domains, which takes the name and a few seed entities for each entity type as the only supervision and aims to classify new entity mentions into both seen and unseen types (i.e., those without seed entities). To solve this problem, we propose SEType which first enriches the weak supervision by finding more entities for each seen type from an unlabeled corpus using the contextualized 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12585</link><description>&lt;p&gt;
SLANG: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#21160;&#24577;&#24615;&#65292;&#23588;&#20854;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#20442;&#35821;&#21644;&#34920;&#24773;&#21253;&#31561;&#26041;&#38754;&#30340;&#20307;&#29616;&#65292;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36866;&#24212;&#24615;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20165;&#32465;&#23450;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#65292;&#24456;&#38590;&#36319;&#19978;&#22312;&#32447;&#31038;&#21306;&#20013;&#24555;&#36895;&#35821;&#35328;&#36827;&#21270;&#30340;&#27493;&#20240;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#39640;&#25104;&#26412;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#25345;&#32493;&#37325;&#35757;&#32451;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#26032;&#20852;&#35821;&#35328;&#36235;&#21183;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934; - SLANG&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861; FOCUS&#65292;&#23427;&#33021;&#22686;&#24378;LLMs&#23545;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#35821;&#35328;&#36716;&#21464;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#65292;&#20316;&#20026;&#32972;&#26223;&#20381;&#25454;&#65292;&#20197;&#24418;&#25104;&#26356;&#31934;&#30830;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#26032;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26234;&#24935;M&#30340;&#25554;&#20214;&#26694;&#26550;&#65292;&#21033;&#29992;&#20174;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20135;&#29983;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#26469;&#25913;&#36827;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.06659</link><description>&lt;p&gt;
&#25913;&#21892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#26234;&#24935;M&#65306;&#34701;&#21512;&#32972;&#26223;&#30693;&#35782;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26234;&#24935;M&#30340;&#25554;&#20214;&#26694;&#26550;&#65292;&#21033;&#29992;&#20174;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20135;&#29983;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#26469;&#25913;&#36827;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#34920;&#38754;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#65288;&#20363;&#22914;&#20174;&#32473;&#23450;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#20043;&#22806;&#33719;&#21462;&#30340;&#32972;&#26223;&#20449;&#24687;&#65289;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26234;&#24935;M&#30340;&#25554;&#20214;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#20174;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#20135;&#29983;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#26469;&#25913;&#36827;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#26234;&#24935;M&#21033;&#29992;LVLM&#26469;&#20840;&#38754;&#20998;&#26512;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#21477;&#23376;&#65292;&#21516;&#26102;&#29983;&#25104;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#20943;&#23569;&#19978;&#19979;&#25991;&#20013;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#34701;&#21512;&#26426;&#21046;&#12290;&#22312;&#22810;&#26679;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (br
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2311.00967</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#35270;&#35273;&#35821;&#35328;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#21152;&#36895;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#30340;&#21457;&#23637;&#12290;&#21516;&#26102;&#65292;&#31526;&#21495;&#35268;&#21010;&#22120;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#23558;&#36825;&#20004;&#31181;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#21363;&#22810;&#27169;&#24577;&#35268;&#21010;&#38382;&#39064;&#35268;&#33539;&#12290;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#38382;&#39064;&#25551;&#36848;&#65288;PD&#65289;&#65292;&#36825;&#26159;&#35268;&#21010;&#22120;&#29992;&#26469;&#26597;&#25214;&#35745;&#21010;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#20214;&#12290;&#36890;&#36807;&#20174;&#35821;&#35328;&#25351;&#20196;&#21644;&#22330;&#26223;&#35266;&#27979;&#20013;&#29983;&#25104;PD&#65292;&#25105;&#20204;&#21487;&#20197;&#39537;&#21160;&#31526;&#21495;&#35268;&#21010;&#22120;&#22312;&#35821;&#35328;&#24341;&#23548;&#26694;&#26550;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20808;&#36827;&#30340;LLM&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;PD&#12290;ViLaIn&#21487;&#20197;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;PD&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#22320;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#21527;&#65311;&#20026;&#20102;&#35780;&#20272;ViLaIn&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#38382;&#39064;&#25551;&#36848;&#29983;&#25104;&#65288;ProDG&#65289;&#25968;&#25454;&#38598;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#23558;&#22312;&#35780;&#20272;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2310.19531</link><description>&lt;p&gt;
&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19531
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#39044;&#27979;&#19978;&#19968;&#20010;&#26631;&#35760;&#65288;&#23376;&#35789;/&#35789;/&#30701;&#35821;&#65289;&#32473;&#20986;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#24120;&#24573;&#35270;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#21363;&#39057;&#32321;&#26631;&#35760;&#21644;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#34987;&#24120;&#35265;&#19988;&#26131;&#23398;&#30340;&#26631;&#35760;&#25152;&#20027;&#23548;&#65292;&#20174;&#32780;&#24573;&#35270;&#19981;&#32463;&#24120;&#20986;&#29616;&#19988;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#65288;InfoEntropy Loss&#65289;&#20989;&#25968;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#30456;&#24212;&#30340;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#20449;&#24687;&#29109;&#21160;&#24577;&#35780;&#20272;&#24453;&#23398;&#20064;&#26631;&#35760;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#28982;&#21518;&#65292;&#23427;&#36866;&#24212;&#22320;&#35843;&#25972;&#35757;&#32451;&#25439;&#22833;&#65292;&#35797;&#22270;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Variator&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#30340;&#21387;&#32553;&#25554;&#20214;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#24037;&#20316;&#36127;&#36733;&#21160;&#24577;&#36873;&#25321;&#19981;&#21516;&#21152;&#36895;&#27604;&#30340;&#25554;&#20214;&#12290;&#25554;&#20214;&#37319;&#29992;&#20102;&#21387;&#32553;&#38544;&#34255;&#21521;&#37327;&#30340;&#26041;&#27861;&#26469;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19988;&#30001;&#20110;&#21442;&#25968;&#23569;&#65292;&#21487;&#20197;&#33410;&#30465;&#23384;&#20648;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.15724</link><description>&lt;p&gt;
Variator: &#20351;&#29992;&#21363;&#25554;&#21363;&#29992;&#21387;&#32553;&#27169;&#22359;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Variator&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#30340;&#21387;&#32553;&#25554;&#20214;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#24037;&#20316;&#36127;&#36733;&#21160;&#24577;&#36873;&#25321;&#19981;&#21516;&#21152;&#36895;&#27604;&#30340;&#25554;&#20214;&#12290;&#25554;&#20214;&#37319;&#29992;&#20102;&#21387;&#32553;&#38544;&#34255;&#21521;&#37327;&#30340;&#26041;&#27861;&#26469;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19988;&#30001;&#20110;&#21442;&#25968;&#23569;&#65292;&#21487;&#20197;&#33410;&#30465;&#23384;&#20648;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#38543;&#20043;&#32780;&#26469;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Variator&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#30340;&#21387;&#32553;&#25554;&#20214;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#21387;&#32553;&#25554;&#20214;&#36890;&#36807;&#23558;&#22810;&#20010;&#38544;&#34255;&#21521;&#37327;&#21387;&#32553;&#21040;&#19968;&#20010;&#21521;&#37327;&#26469;&#32553;&#20943;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19982;&#21407;&#22987;PLMs&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;&#20256;&#32479;&#30340;&#27169;&#22411;&#21152;&#36895;&#26041;&#27861;&#19981;&#21516;&#65292;Variator&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#20248;&#28857;&#65306;&#65288;1&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#21387;&#32553;&#25554;&#20214;&#30340;&#21363;&#25554;&#21363;&#29992;&#24615;&#36136;&#20351;&#24471;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#24037;&#20316;&#36127;&#36733;&#21160;&#24577;&#36873;&#25321;&#20855;&#26377;&#19981;&#21516;&#21152;&#36895;&#27604;&#30340;&#21387;&#32553;&#25554;&#20214;&#12290;&#65288;2&#65289;&#21387;&#32553;&#25554;&#20214;&#30001;&#20960;&#20010;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#32452;&#25104;&#65292;&#21442;&#25968;&#24456;&#23569;&#65292;&#22823;&#22823;&#33410;&#30465;&#20102;&#23384;&#20648;&#21644;&#20869;&#23384;&#24320;&#38144;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#36739;&#22823;&#23384;&#20648;&#38656;&#27714;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#22330;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a gro
&lt;/p&gt;</description></item><item><title>EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.08949</link><description>&lt;p&gt;
Easier Multimodal Generation: Diffusion Models Meet LLMs
&lt;/p&gt;
&lt;p&gt;
Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08949
&lt;/p&gt;
&lt;p&gt;
EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EasyGen&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20027;&#35201;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#22914;CLIP&#25110;ImageBind&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#26725;&#25509;&#27169;&#24577;&#20043;&#38388;&#24046;&#36317;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;EasyGen&#22522;&#20110;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#12290;EasyGen&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#23618;&#23558;BiDiffuser&#21644;LLM&#36827;&#34892;&#38598;&#25104;&#65292;&#22788;&#29702;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38480;&#20110;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#19981;&#21516;&#65292;EasyGen&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;LLM&#21019;&#24314;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30001;BiDiffuser&#35299;&#37322;&#29983;&#25104;&#36866;&#24403;&#30340;&#35270;&#35273;&#22238;&#22797;&#26469;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;EasyGen&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#35757;&#32451;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
&lt;/p&gt;</description></item><item><title>Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06692</link><description>&lt;p&gt;
Meta-CoT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#30340;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06692
&lt;/p&gt;
&lt;p&gt;
Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#31181;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#38142;&#20197;&#20316;&#20026;&#24471;&#20986;&#31572;&#26696;&#30340;&#22522;&#26412;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;CoT&#26041;&#27861;&#35201;&#20040;&#20165;&#20165;&#20351;&#29992;&#31867;&#20284;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#30340;&#36890;&#29992;&#25552;&#31034;&#65292;&#35201;&#20040;&#36807;&#20110;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#26469;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#40511;&#27807;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-CoT&#65292;&#19968;&#31181;&#22312;&#26410;&#30693;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#30340;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;CoT&#25552;&#31034;&#26041;&#27861;&#12290;Meta-CoT&#39318;&#20808;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#23545;&#22330;&#26223;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#20197;&#33258;&#21160;&#27169;&#24335;&#20174;&#30456;&#24212;&#30340;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#22810;&#26679;&#30340;&#28436;&#31034;&#12290;Meta-CoT&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Meta-CoT&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04270</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;6&#20010;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;26&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#28909;&#38376;LLMs&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#38646;&#26679;&#26412;LLMs&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#27809;&#26377;&#19968;&#20010;LLM&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#20914;&#31361;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35823;&#23548;&#24615;&#25552;&#31034;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.17415</link><description>&lt;p&gt;
&#30452;&#35273;&#36824;&#26159;&#20381;&#36182;&#65311;&#30740;&#31350;LLMs&#23545;&#20914;&#31361;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intuitive or Dependent? Investigating LLMs' Robustness to Conflicting Prompts. (arXiv:2309.17415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#20914;&#31361;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35823;&#23548;&#24615;&#25552;&#31034;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#23545;&#20854;&#20869;&#37096;&#35760;&#24518;&#25110;&#32473;&#23450;&#25552;&#31034;&#30340;&#20559;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#30001;&#20110;&#22122;&#22768;&#25110;&#20219;&#21153;&#35774;&#32622;&#65292;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#23545;&#31435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#24178;&#39044;&#26469;&#25511;&#21046;LLMs&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#40065;&#26834;&#24615;&#65292;&#21363;&#20107;&#23454;&#40065;&#26834;&#24615;&#21644;&#20915;&#31574;&#39118;&#26684;&#65292;&#20107;&#23454;&#40065;&#26834;&#24615;&#26159;&#25351;&#20174;&#25552;&#31034;&#25110;&#35760;&#24518;&#20013;&#35782;&#21035;&#27491;&#30830;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#32780;&#20915;&#31574;&#39118;&#26684;&#26159;&#22522;&#20110;&#35748;&#30693;&#29702;&#35770;&#23545;LLMs&#22312;&#36827;&#34892;&#19968;&#33268;&#36873;&#25321;&#36807;&#31243;&#20013;&#34892;&#20026;&#30340;&#20998;&#31867; - &#30452;&#35273;&#22411;&#12289;&#20381;&#36182;&#22411;&#25110;&#29702;&#24615;&#22411;&#65292;&#36825;&#37324;&#20551;&#35774;&#27809;&#26377;&#26126;&#30830;&#30340;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35823;&#23548;&#24615;&#25552;&#31034;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#12290;&#23613;&#31649;&#35814;&#32454;&#30340;&#35828;&#26126;&#21487;&#20197;&#20943;&#36731;&#36873;&#25321;&#35823;&#23548;&#24615;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#20294;&#20063;&#20250;&#22686;&#21152;&#20986;&#29616;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive "right" answer -intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#31614;&#19978;&#19979;&#25991;&#30340;&#21322;&#33258;&#22238;&#24402;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23376;&#32593;&#32476;&#65292;&#23558;&#20808;&#21069;&#22359;&#20013;&#30340;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10926</link><description>&lt;p&gt;
&#24102;&#26631;&#31614;&#19978;&#19979;&#25991;&#30340;&#21322;&#33258;&#22238;&#24402;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#31614;&#19978;&#19979;&#25991;&#30340;&#21322;&#33258;&#22238;&#24402;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23376;&#32593;&#32476;&#65292;&#23558;&#20808;&#21069;&#22359;&#20013;&#30340;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;(NAR)&#24314;&#27169;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#38388;&#26041;&#38754;&#27604;&#33258;&#22238;&#24402;(AR)&#27169;&#22411;&#22823;&#22823;&#38477;&#20302;&#65292;&#21516;&#26102;&#20063;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#36716;&#24405;&#20934;&#30830;&#29575;&#12290;&#30001;&#20110;NAR&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#27169;&#22411;&#24517;&#39035;&#31561;&#24453;&#25972;&#20010;&#35805;&#35821;&#30340;&#23436;&#25972;&#23436;&#25104;&#25165;&#33021;&#36827;&#34892;&#22788;&#29702;&#65292;&#22240;&#27492;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#22359;&#29366;&#27880;&#24847;&#21147;&#30340;&#27969;&#24335;NAR&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#20302;&#24310;&#36831;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#27969;&#24335;AR&#21644;&#38750;&#27969;&#24335;NAR&#27169;&#22411;&#30456;&#27604;&#65292;&#27969;&#24335;NAR&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#30340;&#8220;&#21322;&#33258;&#22238;&#24402;&#8221;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;(LM)&#23376;&#32593;&#32476;&#23558;&#20808;&#21069;&#22359;&#20013;&#21457;&#20986;&#30340;&#26631;&#31614;&#20316;&#20026;&#38468;&#21152;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22359;&#20043;&#38388;&#38468;&#36817;&#22788;&#29702;&#25554;&#20837;&#21644;&#21024;&#38500;&#38169;&#35823;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#22686;&#21152;&#25512;&#26029;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#27969;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming "semi-autoregressive" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20316;&#23478;&#23450;&#20041;&#30340;AI&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#21363;&#26102;&#21453;&#39304;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#27010;&#24565;&#21463;&#21040;&#20102;&#20316;&#23478;&#30340;&#27426;&#36814;&#24182;&#24110;&#21161;&#20182;&#20204;&#33719;&#24471;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;AI&#24037;&#20855;&#35774;&#35745;&#20013;&#30340;&#31038;&#20250;&#25216;&#26415;&#35270;&#35282;&#65292;&#20026;&#25903;&#25345;&#20316;&#23478;&#19982;AI&#30340;&#24895;&#26223;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.10433</link><description>&lt;p&gt;
&#20026;&#21363;&#26102;&#21453;&#39304;&#29983;&#25104;&#23450;&#20041;AI&#20154;&#29289;&#24418;&#35937;
&lt;/p&gt;
&lt;p&gt;
Writer-Defined AI Personas for On-Demand Feedback Generation. (arXiv:2309.10433v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20316;&#23478;&#23450;&#20041;&#30340;AI&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#21363;&#26102;&#21453;&#39304;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#27010;&#24565;&#21463;&#21040;&#20102;&#20316;&#23478;&#30340;&#27426;&#36814;&#24182;&#24110;&#21161;&#20182;&#20204;&#33719;&#24471;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;AI&#24037;&#20855;&#35774;&#35745;&#20013;&#30340;&#31038;&#20250;&#25216;&#26415;&#35270;&#35282;&#65292;&#20026;&#25903;&#25345;&#20316;&#23478;&#19982;AI&#30340;&#24895;&#26223;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#31168;&#30340;&#20889;&#20316;&#24212;&#35813;&#26681;&#25454;&#21463;&#20247;&#36827;&#34892;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#20316;&#23478;&#21487;&#33021;&#38590;&#20197;&#21516;&#35835;&#32773;&#20135;&#29983;&#20849;&#40483;&#65292;&#38590;&#20197;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#25110;&#32773;&#38590;&#20197;&#33719;&#24471;&#30446;&#26631;&#32676;&#20307;&#30340;&#20449;&#24687;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#65292;&#21363;&#22522;&#20110;&#20316;&#23478;&#23450;&#20041;&#30340;&#20219;&#20309;&#30446;&#26631;&#21463;&#20247;&#30340;AI&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#21363;&#26102;&#21453;&#39304;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21407;&#22411;&#65288;&#20351;&#29992;GPT-3.5&#65289;&#22312;&#20004;&#39033;&#29992;&#25143;&#30740;&#31350;&#65288;N=5&#21644;N=11&#65289;&#20013;&#25506;&#32034;&#20102;&#36825;&#19968;&#27010;&#24565;&#65306;&#20316;&#23478;&#20204;&#36190;&#36175;&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#19988;&#25112;&#30053;&#24615;&#22320;&#20351;&#29992;&#20154;&#29289;&#24418;&#35937;&#26469;&#33719;&#24471;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#21453;&#39304;&#34987;&#35748;&#20026;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#28608;&#21457;&#20102;&#25991;&#26412;&#21644;&#20154;&#29289;&#24418;&#35937;&#30340;&#20462;&#35746;&#65292;&#23613;&#31649;&#35813;&#21453;&#39304;&#36890;&#24120;&#20887;&#38271;&#32780;&#19981;&#20855;&#20307;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21363;&#26102;&#21453;&#39304;&#30340;&#24433;&#21709;&#12289;&#24403;&#20195;AI&#31995;&#32479;&#30340;&#26377;&#38480;&#20195;&#34920;&#24615;&#20197;&#21450;&#36827;&#19968;&#27493;&#23450;&#20041;AI&#20154;&#29289;&#30340;&#24819;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#25903;&#25345;&#20316;&#23478;&#19982;AI&#30340;&#24895;&#26223;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25193;&#23637;&#20102;AI&#24037;&#20855;&#35774;&#35745;&#20013;&#30340;&#31038;&#20250;&#25216;&#26415;&#35270;&#35282;&#65306;&#20026;&#20102;&#36171;&#20104;&#21019;&#20316;&#32773;&#26435;&#21147;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#32771;&#34385;&#20182;&#20204;&#19982;&#35266;&#20247;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25991;&#26412;&#21069;&#23548;&#35821;&#21644;&#32844;&#19994;&#25551;&#36848;&#21477;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#21629;&#39064;&#27169;&#26495;&#21487;&#20197;&#26377;&#25928;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.07251</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32972;&#26223;&#20559;&#35265;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
In-Contextual Bias Suppression for Large Language Models. (arXiv:2309.07251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07251
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#21069;&#23548;&#35821;&#21644;&#32844;&#19994;&#25551;&#36848;&#21477;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#21629;&#39064;&#27169;&#26495;&#21487;&#20197;&#26377;&#25928;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24050;&#26377;&#30740;&#31350;&#25253;&#21578;&#31216;&#20854;&#23384;&#22312;&#20196;&#20154;&#25285;&#24551;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#31034;&#20363;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;LLM&#30340;&#24494;&#35843;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36827;&#34892;&#21435;&#20559;&#25152;&#38656;&#30340;&#20869;&#37096;&#21442;&#25968;&#65292;&#22914;&#21830;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#20559;&#35265;&#25233;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#21453;&#20107;&#23454;&#21629;&#39064;&#27169;&#26495;&#29983;&#25104;&#30340;&#25991;&#26412;&#21069;&#23548;&#35821;&#21487;&#20197;&#20934;&#30830;&#22320;&#25233;&#21046;LLM&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#32844;&#19994;&#30340;&#25551;&#36848;&#21477;&#21487;&#20197;&#36827;&#19968;&#27493;&#25233;&#21046;&#24615;&#21035;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#25233;&#21046;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#19981;&#21033;&#24433;&#21709;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender bias. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of the LLMs, which are computationally costly. Moreover, one might not even have access to the internal parameters for performing debiasing such as in the case of commercially available LLMs such as GPT-4. To address this challenge we propose bias suppression, a novel alternative to debiasing that does not require access to model parameters. We show that text-based preambles, generated from manually designed templates covering counterfactual statements, can accurately suppress gender biases in LLMs. Moreover, we find that descriptive sentences for occupations can further suppress gender biases. Interestingly, we find that bias suppression has a minimal adverse effect on downstream task performance, while effectively mit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12032</link><description>&lt;p&gt;
&#20174;&#25968;&#37327;&#21040;&#36136;&#37327;&#65306;&#21033;&#29992;&#33258;&#25105;&#24341;&#23548;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#25552;&#21319;LLM&#24615;&#33021;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#24050;&#25104;&#20026;&#19968;&#20010;&#28966;&#28857;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#35782;&#21035;&#21644;&#36873;&#25321;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#31934;&#36873;&#26679;&#26412;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#25163;&#21160;&#31579;&#36873;&#21644;&#28508;&#22312;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#65288;IFD&#65289;&#25351;&#26631;&#65292;&#23427;&#25104;&#20026;&#20102;&#19968;&#20010;&#20915;&#23450;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#27169;&#22411;&#26399;&#26395;&#21709;&#24212;&#21644;&#33258;&#20027;&#29983;&#25104;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#28789;&#27963;&#24212;&#29992;IFD&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#31934;&#36873;&#26679;&#26412;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#21319;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;Alpaca&#21644;WizardLM&#31561;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65307;&#20165;&#20351;&#29992;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;10%&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33258;&#25105;&#24341;&#23548;&#25361;&#36873;&#21644;IFD&#25351;&#26631;&#30340;&#32508;&#21512;&#24847;&#21619;&#30528;LLM&#20248;&#21270;&#30340;&#19968;&#20010;&#21464;&#38761;&#24615;&#39134;&#36291;&#65292;&#26377;&#26395;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.16200</link><description>&lt;p&gt;
&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;-&#29366;&#24577;&#23545;&#25552;&#21462;&#65288;MD-TSPE&#65289;&#65292;&#36825;&#22312;&#35786;&#26029;&#23545;&#35805;&#31995;&#32479;&#21644;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#30340;&#33258;&#21160;&#25220;&#20889;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;MD-TSPE&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#20043;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#26041;&#27861;&#22312;&#19968;&#38454;&#27573;&#36755;&#20986;&#25972;&#20010;&#30001;&#26415;&#35821;-&#29366;&#24577;&#23545;&#32452;&#25104;&#30340;&#24207;&#21015;&#26102;&#24573;&#30053;&#20102;&#38598;&#25104;&#20808;&#21069;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#26469;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25512;&#26029;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#37319;&#29992;&#21333;&#19968;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#29983;&#25104;&#24418;&#24335;&#23436;&#25104;MD-TSPE&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#29983;&#25104;&#25152;&#26377;&#30340;&#26415;&#35821;&#65292;&#28982;&#21518;&#29983;&#25104;&#27599;&#20010;&#29983;&#25104;&#30340;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effect
&lt;/p&gt;</description></item><item><title>DIALGEN&#26159;&#19968;&#20010;&#20154;&#26426;&#21322;&#33258;&#21160;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#23376;&#23545;&#35805;&#21644;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#29702;&#35299;&#20154;&#38469;&#23545;&#35805;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07047</link><description>&lt;p&gt;
DIALGEN: &#36890;&#36807;&#20154;&#24037;&#29983;&#25104;&#23545;&#35805;&#25913;&#21892;&#23545;&#20154;&#38469;&#23545;&#35805;&#30340;&#29702;&#35299;&#30340;&#21327;&#21516;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07047
&lt;/p&gt;
&lt;p&gt;
DIALGEN&#26159;&#19968;&#20010;&#20154;&#26426;&#21322;&#33258;&#21160;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#23376;&#23545;&#35805;&#21644;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#29702;&#35299;&#20154;&#38469;&#23545;&#35805;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#33258;&#21160;&#29702;&#35299;&#20154;&#38469;&#23545;&#35805;&#30340;&#24212;&#29992;&#36890;&#24120;&#38754;&#20020;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#21628;&#21483;&#20013;&#24515;&#25110;&#20020;&#24202;&#23545;&#35805;&#65292;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#22788;&#29702;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#20063;&#20250;&#22686;&#21152;&#27880;&#37322;&#25104;&#26412;&#65292;&#20174;&#32780;&#38480;&#21046;&#25216;&#26415;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIALGEN&#65292;&#19968;&#31181;&#20154;&#26426;&#21322;&#33258;&#21160;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#12290;DIALGEN&#20351;&#29992;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGPT&#65289;&#65292;&#21487;&#20197;&#36981;&#24490;&#26550;&#26500;&#21644;&#39118;&#26684;&#35268;&#33539;&#65292;&#29983;&#25104;&#27969;&#21033;&#30340;&#23545;&#35805;&#25991;&#26412;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#23376;&#23545;&#35805;&#24182;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#32416;&#27491;&#19981;&#19968;&#33268;&#25110;&#37325;&#23450;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#22312;&#23558;&#20195;&#29702;-&#23458;&#25143;&#20449;&#24687;&#25910;&#38598;&#21628;&#21483;&#24402;&#32435;&#20026;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32467;&#26500;&#21270;&#27010;&#25324;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DIALGEN&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications that could benefit from automatic understanding of human-human conversations often come with challenges associated with private information in real-world data such as call center or clinical conversations. Working with protected data also increases costs of annotation, which limits technology development. To address these challenges, we propose DIALGEN, a human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a language model (ChatGPT) that can follow schema and style specifications to produce fluent conversational text, generating a complex conversation through iteratively generating subdialogues and using human feedback to correct inconsistencies or redirect the flow. In experiments on structured summarization of agent-client information gathering calls, framed as dialogue state tracking, we show that DIALGEN data enables significant improvement in model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#21644;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26089;&#26399;ArXiving&#35770;&#25991;&#23545;&#20854;&#34987;ICLR&#20250;&#35758;&#25509;&#21463;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26089;&#26399;ArXiving&#21487;&#33021;&#20250;&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#26426;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#65292;&#24182;&#19988;&#19981;&#22240;&#20316;&#32773;&#24341;&#29992;&#27425;&#25968;&#21644;&#26426;&#26500;&#25490;&#21517;&#31561;&#22240;&#32032;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.13891</link><description>&lt;p&gt;
&#26089;&#26399;ArXiving&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#22240;&#26524;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating the Causal Effect of Early ArXiving on Paper Acceptance. (arXiv:2306.13891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#21644;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26089;&#26399;ArXiving&#35770;&#25991;&#23545;&#20854;&#34987;ICLR&#20250;&#35758;&#25509;&#21463;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26089;&#26399;ArXiving&#21487;&#33021;&#20250;&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#26426;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#65292;&#24182;&#19988;&#19981;&#22240;&#20316;&#32773;&#24341;&#29992;&#27425;&#25968;&#21644;&#26426;&#26500;&#25490;&#21517;&#31561;&#22240;&#32032;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35770;&#25991;&#25552;&#20132;&#21516;&#34892;&#23457;&#26597;&#21069;&#21457;&#24067;&#39044;&#21360;&#26412;&#20250;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#65311;&#30001;&#20110;&#27809;&#26377;&#36827;&#34892;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#35266;&#23519;&#25968;&#25454;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;ICLR&#20250;&#35758;&#65288;2018-2022&#65289;&#30340;&#25968;&#25454;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#26469;&#20272;&#35745;&#22312;&#23457;&#38405;&#26399;&#21069;&#21024;&#38500;&#35770;&#25991;&#65288;&#26089;&#26399;arXiving&#65289;&#23545;&#35770;&#25991;&#34987;&#20250;&#35758;&#25509;&#21463;&#30340;&#24433;&#21709;&#12290;&#35843;&#25972;&#20102;18&#20010;&#28151;&#26434;&#22240;&#32032;&#65292;&#22914;&#20027;&#39064;&#12289;&#20316;&#32773;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#22240;&#26524;&#25928;&#24212;&#30340;&#20272;&#35745;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36136;&#37327;&#26159;&#19968;&#31181;&#38590;&#20197;&#20272;&#35745;&#30340;&#26500;&#24314;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#36127;&#38754;&#32467;&#26524;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#20316;&#20026;&#23545;&#29031;&#21464;&#37327;&#65292;&#20197;&#28040;&#38500;&#36136;&#37327;&#28151;&#26434;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;arXiving&#21487;&#33021;&#20250;&#23545;&#35770;&#25991;&#34987;&#25509;&#21463;&#30340;&#26426;&#20250;&#20135;&#29983;&#19968;&#23450;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#24433;&#21709;&#26102;&#65292;&#36825;&#31181;&#24433;&#21709;&#22312;&#20316;&#32773;&#24341;&#29992;&#27425;&#25968;&#21644;&#26426;&#26500;&#25490;&#21517;&#20998;&#32452;&#21518;&#24182;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#34920;&#26126;&#26089;&#26399;arXiving&#23545;ICLR&#20250;&#35758;&#25509;&#21463;&#35770;&#25991;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the effect of releasing a preprint of a paper before it is submitted for peer review? No randomized controlled trial has been conducted, so we turn to observational data to answer this question. We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (early arXiving) on its acceptance to the conference. Adjusting for 18 confounders such as topic, authors, and quality, we may estimate the causal effect. However, since quality is a challenging construct to estimate, we use the negative outcome control method, using paper citation count as a control variable to debias the quality confounding effect. Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02541</link><description>&lt;p&gt;
PWESuite&#65306;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#21521;&#37327;&#31354;&#38388;&#30340;&#21333;&#35789;&#23884;&#20837;&#26159;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#12290;&#22823;&#22810;&#25968;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#32534;&#30721;&#35821;&#20041;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#30340;&#35821;&#38899;&#20449;&#24687;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21457;&#22768;&#29305;&#24449;&#26500;&#24314;&#35821;&#38899;&#30693;&#24773;&#21333;&#35789;&#23884;&#20837;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#20197;&#40723;&#21169;&#20854;&#31038;&#21306;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#20351;&#29992;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#23398;&#20064;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26041;&#38754;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#20869;&#22312;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#22914;&#21333;&#35789;&#26816;&#32034;&#21644;&#19982;&#22768;&#38899;&#30456;&#20284;&#24615;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22806;&#22312;&#34920;&#29616;&#65292;&#22914;&#38901;&#24459;&#21644;&#21516;&#28304;&#26816;&#27979;&#21644;&#22768;&#38899;&#31867;&#27604;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20219;&#21153;&#22871;&#20214;&#23558;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#24182;&#25552;&#20379;&#26410;&#26469;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddi
&lt;/p&gt;</description></item></channel></rss>