<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Proto-lm&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#21019;&#24314;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01732</link><description>&lt;p&gt;
Proto-lm&#65306;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01732
&lt;/p&gt;
&lt;p&gt;
Proto-lm&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#21019;&#24314;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#26377;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;LLMs&#30340;&#26041;&#27861;&#26159;&#20107;&#21518;&#30340;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20043;&#21518;&#24212;&#29992;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#23427;&#20204;&#20851;&#27880;&#20302;&#32423;&#29305;&#24449;&#24182;&#19988;&#22312;&#26356;&#39640;&#32423;&#25991;&#26412;&#21333;&#20301;&#19978;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;proto-lm&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#30333;&#30418;&#23376;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#22312;&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#21363;&#26102;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#22312;LLMs&#20013;&#30340;&#26032;&#39062;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#20197;&#20026;&#26080;&#38656;&#29306;&#29298;&#24615;&#33021;&#30340;&#26356;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01544</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65306;&#36890;&#36807;&#27979;&#37327;&#34928;&#20943;&#26469;&#20462;&#21098;LLM&#32452;&#20214;&#24182;&#20248;&#21270;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#21644;LLM&#21387;&#32553;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21387;&#32553;LLM&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25351;&#26631;&#22914;&#22256;&#24785;&#24230;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;DTM&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;DTM&#36824;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#31532;&#19968;&#20010;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;FDTM&#65289;&#22312;&#27169;&#22411;&#31232;&#30095;&#21270;&#20013;&#26174;&#31034;&#65292;&#36229;&#36807;90%&#30340;&#25152;&#26377;&#32452;&#20214;&#21487;&#20197;&#20462;&#21098;&#25481;&#12290;&#23545;&#20110;&#37327;&#21270;&#65292;FDTM&#34920;&#26126;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#21487;&#20197;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>COPAL-ID&#26159;&#19968;&#20010;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#26368;&#26032;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20165;&#20026;65.47%&#12290;</title><link>http://arxiv.org/abs/2311.01012</link><description>&lt;p&gt;
COPAL-ID: &#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#25512;&#29702;&#19982;&#26412;&#22303;&#25991;&#21270;&#21644;&#32454;&#24494;&#24046;&#21035;
&lt;/p&gt;
&lt;p&gt;
COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01012
&lt;/p&gt;
&lt;p&gt;
COPAL-ID&#26159;&#19968;&#20010;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#26368;&#26032;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20165;&#20026;65.47%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;COPAL-ID&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#12290;&#19982;&#20197;&#21069;&#30340;&#21360;&#23612;COPA&#25968;&#25454;&#38598;&#65288;XCOPA-ID&#65289;&#19981;&#21516;&#65292;COPAL-ID&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#22240;&#27492;&#22312;&#21360;&#23612;&#25991;&#21270;&#39046;&#22495;&#20869;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;COPAL-ID&#30001;&#26412;&#22303;&#20154;&#20174;&#22836;&#24320;&#22987;&#19987;&#19994;&#25776;&#20889;&#65292;&#26356;&#27969;&#21033;&#65292;&#19981;&#20687;XCOPA-ID&#30340;&#32763;&#35793;&#23384;&#22312;&#23604;&#23596;&#30340;&#35789;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#26631;&#20934;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#21644;&#38597;&#21152;&#36798;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65288;&#19968;&#31181;&#22312;&#26085;&#24120;&#23545;&#35805;&#20013;&#24120;&#29992;&#30340;&#26041;&#35328;&#65289;&#21576;&#29616;COPAL-ID&#12290;COPAL-ID&#23545;&#20110;&#29616;&#26377;&#30340;&#24320;&#28304;&#21644;&#38381;&#28304;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#21364;&#26159;&#38750;&#24120;&#23481;&#26131;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#20063;&#24456;&#38590;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;65.47%&#65292;&#36828;&#20302;&#20110;&#27809;&#26377;&#25991;&#21270;&#32972;&#26223;&#30340;XCOPA-ID&#65288;79.40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present publicly available COPAL-ID, a novel Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPAL-ID in both standard Indonesian and in Jakartan Indonesian--a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closed state-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that even the current best open-source, multilingual model struggles to perform well, achieving 65.47% accuracy on COPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%). Despite GPT-4's impressiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.18122</link><description>&lt;p&gt;
OpinSummEval:&#20877;&#32771;&#33258;&#21160;&#21270;&#35780;&#20272;&#22312;&#24847;&#35265;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#19981;&#21516;&#65292;&#24847;&#35265;&#25688;&#35201;&#19987;&#27880;&#20110;&#35266;&#28857;&#21644;&#24773;&#24863;&#65292;&#22240;&#27492;&#19982;&#20247;&#19981;&#21516;&#12290;&#34429;&#28982;&#20687;ROUGE&#36825;&#26679;&#30340;&#26576;&#20123;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23545;&#35780;&#20272;&#24847;&#35265;&#25688;&#35201;&#30340;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23427;&#21253;&#25324;&#26469;&#33258;14&#20010;&#24847;&#35265;&#25688;&#35201;&#27169;&#22411;&#30340;&#20154;&#24037;&#21028;&#26029;&#21644;&#36755;&#20986;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;24&#20010;&#33258;&#21160;&#24230;&#37327;&#19982;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#65288;&#22914;BART&#21644;GPT-3/3.5&#65289;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#24847;&#35265;&#25688;&#35201;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#21487;&#29992;&#20110;https://github.com/A-Chicharito-S/OpinSummEval/tree/main&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
&lt;/p&gt;</description></item><item><title>StyleBART&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#26469;&#35013;&#39280;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#39118;&#26684;&#30340;&#26631;&#39064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;StyleBART&#23558;&#39118;&#26684;&#23398;&#20064;&#21644;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20998;&#31163;&#24320;&#26469;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#33258;&#30001;&#32452;&#21512;&#22522;&#30784;&#27169;&#22411;&#21644;&#39118;&#26684;&#36866;&#37197;&#22120;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;StyleBART&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17743</link><description>&lt;p&gt;
StyleBART: &#20351;&#29992;&#39118;&#26684;&#36866;&#37197;&#22120;&#35013;&#39280;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation. (arXiv:2310.17743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17743
&lt;/p&gt;
&lt;p&gt;
StyleBART&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#26469;&#35013;&#39280;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#39118;&#26684;&#30340;&#26631;&#39064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;StyleBART&#23558;&#39118;&#26684;&#23398;&#20064;&#21644;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20998;&#31163;&#24320;&#26469;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#33258;&#30001;&#32452;&#21512;&#22522;&#30784;&#27169;&#22411;&#21644;&#39118;&#26684;&#36866;&#37197;&#22120;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;StyleBART&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#26159;&#29983;&#25104;&#19968;&#20010;&#26082;&#24635;&#32467;&#25991;&#31456;&#20869;&#23481;&#21448;&#21453;&#26144;&#25152;&#38656;&#39118;&#26684;&#26469;&#21560;&#24341;&#29992;&#25143;&#30340;&#26631;&#39064;&#12290;&#30001;&#20110;&#39118;&#26684;&#29305;&#23450;&#30340;&#25991;&#31456;-&#26631;&#39064;&#23545;&#38750;&#24120;&#31232;&#32570;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#20351;&#29992;&#26631;&#20934;&#26631;&#39064;&#29983;&#25104;&#25968;&#25454;&#38598;&#21644;&#21333;&#19968;&#39118;&#26684;&#35821;&#26009;&#24211;&#36827;&#34892;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#19968;&#36335;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;StyleBART&#65292;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36866;&#37197;&#22120;&#23558;&#39044;&#35757;&#32451;&#30340;BART&#27169;&#22411;&#35013;&#39280;&#36215;&#26469;&#65292;&#36866;&#37197;&#22120;&#36127;&#36131;&#19981;&#21516;&#30340;&#39118;&#26684;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#20999;&#25442;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#39118;&#26684;&#30340;&#26631;&#39064;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;StyleBART&#23558;&#39118;&#26684;&#23398;&#20064;&#21644;&#26631;&#39064;&#29983;&#25104;&#30340;&#20219;&#21153;&#20998;&#31163;&#24320;&#26469;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#33258;&#30001;&#32452;&#21512;&#22522;&#30784;&#27169;&#22411;&#21644;&#39118;&#26684;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#25913;&#20889;&#20219;&#21153;&#20197;&#22686;&#24378;&#39118;&#26684;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;StyleBART&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stylistic headline generation is the task to generate a headline that not only summarizes the content of an article, but also reflects a desired style that attracts users. As style-specific article-headline pairs are scarce, previous researches focus on unsupervised approaches with a standard headline generation dataset and mono-style corpora. In this work, we follow this line and propose StyleBART, an unsupervised approach for stylistic headline generation. Our method decorates the pretrained BART model with adapters that are responsible for different styles and allows the generation of headlines with diverse styles by simply switching the adapters. Different from previous works, StyleBART separates the task of style learning and headline generation, making it possible to freely combine the base model and the style adapters during inference. We further propose an inverse paraphrasing task to enhance the style adapters. Extensive automatic and human evaluations show that StyleBART achi
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#30340;NLP&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#20381;&#36182;&#20110;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16607</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#19982;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Interplay between Fairness and Explainability. (arXiv:2310.16607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16607
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;NLP&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#20381;&#36182;&#20110;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20013;&#26082;&#20855;&#26377;&#20844;&#24179;&#24615;&#21448;&#21487;&#35299;&#37322;&#12290;&#36890;&#24120;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#21363;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20250;&#34987;&#29420;&#31435;&#22320;&#36827;&#34892;&#20248;&#21270;&#21644;/&#25110;&#30740;&#31350;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35748;&#20026;&#26410;&#26469;&#21487;&#20449;&#30340;NLP&#31995;&#32479;&#24212;&#35813;&#21516;&#26102;&#32771;&#34385;&#20004;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#65306;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#26159;&#21542;&#20381;&#36182;&#20110;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65311;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#33521;&#35821;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;BIOS&#21644;ECtHR&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20998;&#21035;&#25552;&#20379;&#20102;&#26377;&#20851;&#24615;&#21035;&#21644;&#22269;&#31821;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21253;&#25324;(i)&#20559;&#35265;&#32531;&#35299;&#65292;&#26088;&#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#65307;(ii)&#35299;&#37322;&#25552;&#21462;&#65292;&#26088;&#22312;&#20135;&#29983;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
In order to build reliable and trustworthy NLP applications, models need to be both fair across different demographics and explainable. Usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. Instead, we argue that forthcoming, trustworthy NLP systems should consider both. In this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. To this end, we conduct experiments on two English multi-class text classification datasets, BIOS and ECtHR, that provide information on gender and nationality, respectively, as well as human-annotated rationales. We fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. We find that bias mitigation algorithms do not always lead to fairer models. Moreover, we discover that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#24046;&#24322;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#31181;&#20449;&#24687;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36127;&#26679;&#26412;&#26377;&#21161;&#20110;&#27169;&#22411;&#25913;&#36827;&#20854;&#25512;&#29702;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12467</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#24046;&#24322;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#31181;&#20449;&#24687;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36127;&#26679;&#26412;&#26377;&#21161;&#20110;&#27169;&#22411;&#25913;&#36827;&#20854;&#25512;&#29702;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;,&#23588;&#20854;&#26159;&#37027;&#20123;&#26469;&#33258;&#24402;&#32435;&#36807;&#31243;&#30340;&#25512;&#29702;,&#26159;&#25105;&#20204;&#23545;&#35805;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#34917;&#20805;&#30001;&#35762;&#35805;&#32773;&#38544;&#21547;&#25110;&#26126;&#30830;&#20256;&#36798;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#28436;&#32462;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#20449;&#24687;&#24046;&#24322;&#26469;&#23450;&#20041;&#20219;&#21153;&#38590;&#24230;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#35813;&#24046;&#24322;&#21306;&#20998;&#20102;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#65288;Johnson-Laird, 1988, 1993&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#25152;&#38656;&#25512;&#29702;&#20043;&#38388;&#20449;&#24687;&#24046;&#24322;&#30340;&#24046;&#36317;&#23545;&#24402;&#32435;&#25512;&#29702;&#36807;&#31243;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20449;&#24687;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#36127;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36127;&#26679;&#26412;&#26377;&#21161;&#20110;&#27169;&#22411;&#29702;&#35299;&#38169;&#35823;&#24182;&#25913;&#36827;&#20854;&#25512;&#29702;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap -- which distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993). Our analysis reveals that the disparity in information between dialogue contexts and desired inferences poses a significant challenge to the inductive inference process. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38754;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#26102;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26597;&#35810;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#24182;&#19988;&#31532;&#19968;&#20010;&#35299;&#30721;&#30340;&#26631;&#35760;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25351;&#31034;&#31526;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#24182;&#20026;&#25913;&#36827;&#35299;&#30721;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11877</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#24615;&#26080;&#27861;&#22238;&#31572;&#24615;&#30340;&#22909;&#22855;&#26696;&#20363;&#65306;&#22312;&#36807;&#24230;&#33258;&#20449;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#23547;&#25214;&#30495;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38754;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#26102;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26597;&#35810;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#24182;&#19988;&#31532;&#19968;&#20010;&#35299;&#30721;&#30340;&#26631;&#35760;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25351;&#31034;&#31526;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#24182;&#20026;&#25913;&#36827;&#35299;&#30721;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#23545;&#20854;&#22238;&#31572;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#25285;&#24551;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;LLMs&#22914;&#20309;&#22788;&#29702;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#24187;&#35273;&#34892;&#20026;&#65292;&#21407;&#22240;&#26159;&#36807;&#24230;&#33258;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#38754;&#23545;&#26080;&#27861;&#22238;&#31572;&#30340;&#26597;&#35810;&#26102;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#38382;&#65306;&#24403;&#29983;&#25104;&#24187;&#35273;&#22238;&#31572;&#26102;&#65292;&#27169;&#22411;&#26159;&#21542;&#34920;&#31034;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#20107;&#23454;&#65311;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#28872;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#36755;&#20837;&#26597;&#35810;&#30340;&#21487;&#22238;&#31572;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#31532;&#19968;&#20010;&#35299;&#30721;&#30340;&#26631;&#35760;&#30340;&#34920;&#31034;&#24448;&#24448;&#26159;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25351;&#31034;&#31526;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#32452;&#32455;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20808;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20026;&#24320;&#21457;&#26356;&#22909;&#22320;&#36981;&#23432;&#20107;&#23454;&#29983;&#25104;&#30340;&#25913;&#36827;&#35299;&#30721;&#25216;&#26415;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particul
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11715</link><description>&lt;p&gt;
&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11715
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#32454;&#31890;&#24230;NER&#22330;&#26223;&#19979;&#24120;&#24120;&#38754;&#20020;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21487;&#20197;&#24212;&#29992;K-shot&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#24403;&#27880;&#37322;&#25968;&#37327;&#36229;&#36807;&#20960;&#21313;&#20010;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#24448;&#24448;&#36798;&#21040;&#39281;&#21644;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#30340;&#26631;&#27880;&#12290;&#19968;&#31181;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#39044;&#35757;&#32451;&#65292;&#23427;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#26080;&#27861;&#30452;&#25509;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23613;&#31649;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#24456;&#21487;&#33021;&#26159;&#31895;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#30340;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#65288;F2C&#65289;&#26144;&#23556;&#30697;&#38453;&#30340;&#32454;&#31890;&#24230;NER&#27169;&#22411;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#19982;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#30340;&#31895;&#31890;&#24230;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-gr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11689</link><description>&lt;p&gt;
&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#25913;&#36827;LLMs&#20013;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#20173;&#28982;&#38480;&#20110;&#20854;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#36873;&#25321;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#22312;LLMs&#19981;&#30830;&#23450;&#26102;&#20351;&#20854;&#36991;&#20813;&#39044;&#27979;&#32780;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20351;&#29992;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25913;&#36827;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#22312;CoQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;AUACC&#20174;91.23%&#25552;&#39640;&#21040;92.63%&#65292;&#24182;&#23558;AURO
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09017</link><description>&lt;p&gt;
&#19981;&#28155;&#21152;&#65292;&#19981;&#38169;&#36807;&#65306;&#20174;&#39044;&#36873;&#25991;&#26412;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#20869;&#23481;&#20445;&#30041;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#20219;&#21153;&#22312;&#20856;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#20013;&#23558;&#25991;&#26412;&#29983;&#25104;&#27493;&#39588;&#38548;&#31163;&#20986;&#26469;&#12290;&#23427;&#36890;&#36807;&#25361;&#25112;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#39044;&#36873;&#20869;&#23481;&#65288;"&#39640;&#20142;"&#65289;&#20013;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26694;&#26550;&#22312;&#31867;&#20284;&#25688;&#35201;&#30340;&#20219;&#21153;&#20013;&#22686;&#21152;&#20102;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#20801;&#35768;&#23558;&#21333;&#20010;CTR&#27169;&#22411;&#19982;&#21508;&#31181;&#20869;&#23481;&#36873;&#25321;&#35774;&#32622;&#21644;&#27169;&#22359;&#37197;&#23545;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#21487;&#38752;&#30340;CTR&#27169;&#22411;&#65292;&#32780;&#19988;&#29616;&#26377;&#20219;&#21153;&#22522;&#32447;&#30340;&#24615;&#33021;&#20013;&#31561;&#65292;&#26080;&#27861;&#23454;&#38469;&#20351;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;CTR&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#20808;&#21069;&#30340;&#20851;&#38190;&#38480;&#21046;&#65306;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#36890;&#36807;&#21463;&#25511;&#35299;&#30721;&#31574;&#30053;&#26469;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22823;&#24133;&#25913;&#36827;&#20102;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text ("highlights").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#29702;&#35299;&#21453;&#21521;&#20851;&#31995;&#26041;&#38754;&#30340;&#26080;&#25928;&#24615;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;ConvRe&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36870;&#21521;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;Re2Text&#21644;Text2Re&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#30830;&#23450;&#20851;&#31995;&#21644;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#21305;&#37197;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22312;&#27492;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.05163</link><description>&lt;p&gt;
LLMs&#22312;&#29702;&#35299;&#21453;&#21521;&#20851;&#31995;&#20013;&#30340;&#26080;&#25928;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of LLMs' Inefficacy in Understanding Converse Relations. (arXiv:2310.05163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#29702;&#35299;&#21453;&#21521;&#20851;&#31995;&#26041;&#38754;&#30340;&#26080;&#25928;&#24615;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;ConvRe&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36870;&#21521;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;Re2Text&#21644;Text2Re&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#30830;&#23450;&#20851;&#31995;&#21644;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#21305;&#37197;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22312;&#27492;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#24418;&#24335;&#35821;&#35328;&#23548;&#21521;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#65292;&#22914;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#22823;&#22810;&#36981;&#24490;LLMs&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30495;&#27491;&#29702;&#35299;&#24418;&#24335;&#35821;&#35328;&#30340;&#32467;&#26500;&#21270;&#35821;&#20041;&#21527;&#65311;&#26412;&#25991;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#21363;&#36870;&#21521;&#20108;&#36827;&#21046;&#20851;&#31995;&#19978;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;ConvRe&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36870;&#21521;&#20851;&#31995;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#25968;&#25454;&#38598;&#30340;17&#20010;&#20851;&#31995;&#21644;1240&#20010;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;ConvRE&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65292;Re2Text&#21644;Text2Re&#65292;&#36825;&#20123;&#20219;&#21153;&#34987;&#21046;&#23450;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30830;&#23450;&#20851;&#31995;&#21644;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#21305;&#37197;&#33021;&#21147;&#12290;&#22312;&#35780;&#20272;&#21327;&#35758;&#26041;&#38754;&#65292;&#38500;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27979;&#35797;&#25991;&#26412;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#25991;&#26412;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#39564;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#37329;&#34701;&#29615;&#22659;&#19979;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#22312;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04793</link><description>&lt;p&gt;
FinGPT: &#22312;&#37329;&#34701;&#25968;&#25454;&#38598;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25351;&#20196;&#35843;&#20248;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets. (arXiv:2310.04793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#37329;&#34701;&#29615;&#22659;&#19979;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#22312;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36805;&#36895;&#25193;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;GPT&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#28508;&#21147;&#26085;&#30410;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#37329;&#34701;&#25968;&#25454;&#38598;&#38598;&#25104;&#22312;&#19968;&#36215;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#23450;&#20854;&#29087;&#32451;&#31243;&#24230;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#35843;&#20248;&#33539;&#24335;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#37329;&#34701;&#29615;&#22659;&#19979;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#24320;&#28304;&#27169;&#22411;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#30830;&#20445;&#20102;&#26080;&#32541;&#36879;&#26126;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#25351;&#20196;&#35843;&#20248;&#33539;&#24335;&#65292;&#24182;&#24378;&#35843;&#20854;&#23545;&#20110;&#31435;&#21363;&#38598;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#65292;&#37319;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#36880;&#27493;&#25512;&#36827;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#26412;&#30340;&#33021;&#21147;&#21644;&#22522;&#30784;&#20219;&#21153;&#65292;&#27604;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#22686;&#24378;&#19987;&#19994;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#38889;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#26377;&#20559;&#35265;&#35328;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04313</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#26381;&#21153;&#20013;&#20998;&#31867;&#26377;&#20559;&#35265;&#35328;&#35770;&#30340;&#22823;&#35268;&#27169;&#38889;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services. (arXiv:2310.04313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#38889;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#26377;&#20559;&#35265;&#35328;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#26381;&#21153;&#30340;&#22686;&#38271;&#65292;&#23545;&#39640;&#32423;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65288;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#26377;&#20559;&#25991;&#26412;&#26816;&#27979;&#65289;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#22312;&#32447;&#26381;&#21153;&#30340;&#21311;&#21517;&#24615;&#24120;&#24120;&#23548;&#33268;&#26377;&#20559;&#35265;&#21644;&#26377;&#23475;&#35328;&#35821;&#30340;&#23384;&#22312;&#65292;&#23545;&#32500;&#25252;&#22312;&#32447;&#31038;&#21306;&#30340;&#20581;&#24247;&#24102;&#26469;&#25361;&#25112;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#38889;&#22269;&#23588;&#20854;&#30456;&#20851;&#65292;&#30446;&#21069;&#23578;&#26410;&#24191;&#27867;&#30740;&#31350;&#22823;&#35268;&#27169;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#30693;&#21517;&#30340;&#38889;&#22269;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21253;&#25324;(1)&#20559;&#22909;&#12289;(2)&#20302;&#20439;&#35821;&#35328;&#21644;(3)&#20061;&#31181;&#20559;&#35265;&#31867;&#22411;&#30340;&#25991;&#26412;&#26679;&#26412;&#30340;&#27880;&#37322;&#65292;&#20351;&#24471;&#33021;&#22815;&#21516;&#26102;&#23545;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20998;&#31867;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25351;&#26631;&#19979;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growth of online services, the need for advanced text classification algorithms, such as sentiment analysis and biased text detection, has become increasingly evident. The anonymous nature of online services often leads to the presence of biased and harmful language, posing challenges to maintaining the health of online communities. This phenomenon is especially relevant in South Korea, where large-scale hate speech detection algorithms have not yet been broadly explored. In this paper, we introduce a new comprehensive, large-scale dataset collected from a well-known South Korean SNS platform. Our proposed dataset provides annotations including (1) Preferences, (2) Profanities, and (3) Nine types of Bias for the text samples, enabling multi-task learning for simultaneous classification of user-generated texts. Leveraging state-of-the-art BERT-based language models, our approach surpasses human-level accuracy across diverse classification tasks, as measured by various metrics. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#25913;&#36827;&#25968;&#23398;&#38382;&#31572;&#65292;&#22312;&#21487;&#38752;&#24615;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;</title><link>http://arxiv.org/abs/2310.03184</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#25968;&#23398;&#38382;&#31572;&#65306;&#22312;&#21487;&#38752;&#24615;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03184
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#25913;&#36827;&#25968;&#23398;&#38382;&#31572;&#65292;&#22312;&#21487;&#38752;&#24615;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20013;&#23398;&#25968;&#23398;&#23398;&#29983;&#26469;&#35828;&#65292;&#19982;&#23548;&#24072;&#36827;&#34892;&#20114;&#21160;&#38382;&#31572;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26041;&#24335;&#12290;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#26032;&#20852;&#33021;&#21147;&#23548;&#33268;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#37096;&#20998;&#36741;&#23548;&#36807;&#31243;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#21253;&#25324;&#25903;&#25345;&#25968;&#23398;&#27010;&#24565;&#30340;&#27010;&#24565;&#35752;&#35770;&#30340;&#20114;&#21160;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#23398;&#38382;&#39064;&#30340;&#22238;&#31572;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#25110;&#32773;&#19982;&#25945;&#32946;&#32972;&#26223;&#19981;&#21305;&#37197;&#65292;&#20363;&#22914;&#19982;&#23398;&#26657;&#30340;&#35838;&#31243;&#19981;&#19968;&#33268;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#25552;&#31034;&#20013;&#21152;&#20837;&#32463;&#39564;&#35777;&#30340;&#22806;&#37096;&#30693;&#35782;&#36164;&#28304;&#26469;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#31034;&#26469;&#26816;&#32034;&#24182;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#25968;&#23398;&#25945;&#31185;&#20070;&#20013;&#30340;&#20869;&#23481;&#65292;&#20197;&#22238;&#31572;&#30495;&#23454;&#23398;&#29983;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#19968;&#39033;&#22810;&#26465;&#20214;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#20013;&#23398;&#20195;&#25968;&#21644;&#20960;&#20309;&#38382;&#31572;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.00312</link><description>&lt;p&gt;
&#29992;&#20110;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#30340;&#19981;&#21516;&#25253;&#21578;&#32467;&#26524;&#30340;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies. (arXiv:2309.00312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#39064;&#24314;&#27169;&#21644;&#25991;&#26412;&#25366;&#25496;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#38598;&#65292;&#36866;&#29992;&#20110;&#36827;&#34892;&#20803;&#20998;&#26512;&#21644;&#31995;&#32479;&#23457;&#26597;&#12290;&#23545;&#20110;&#35777;&#25454;&#32508;&#36848;&#65292;&#19978;&#36848;NLP&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#25991;&#29486;&#25628;&#32034;&#25110;&#20174;&#25253;&#21578;&#20013;&#25552;&#21462;&#20540;&#20197;&#33258;&#21160;&#21270;SR&#21644;MA&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21516;&#19968;&#24191;&#20041;&#30740;&#31350;&#38382;&#39064;&#19978;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#26681;&#25454;&#20854;&#27604;&#20363;&#21457;&#29983;&#21644;&#22312;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#20013;&#30340;&#19968;&#33268;&#24615;&#20998;&#24067;&#23545;&#20854;&#36827;&#34892;&#25490;&#21517;&#65292;&#25214;&#21040;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26174;&#33879;&#30456;&#20851;&#30340;&#35805;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;&#34917;&#20805;&#33829;&#20859;&#21270;&#21512;&#29289;&#26159;&#21542;&#26174;&#33879;&#26377;&#30410;&#20110;&#40644;&#26001;&#21464;&#24615;(MD)&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30830;&#23450;&#20102;&#20843;&#31181;&#21270;&#21512;&#29289;&#19982;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30340;&#29305;&#23450;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modeling and text mining are subsets of Natural Language Processing with relevance for conducting meta-analysis (MA) and systematic review (SR). For evidence synthesis, the above NLP methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of SR and MA. Instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. Specifically, the objective is to find topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence and consistency of distribution across reports of significant results. The proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (MD). Eight compounds were identified as having a particular association with reports of significant r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12166</link><description>&lt;p&gt;
&#27169;&#20223;&#28216;&#25103;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#26816;&#27979;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#38761;&#26032;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19981;&#21516;&#20307;&#35009;&#30340;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65306;&#35770;&#25991;&#12289;&#25925;&#20107;&#12289;&#35799;&#27468;&#21644;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#36825;&#20123;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#20998;&#31867;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25925;&#20107;&#20889;&#20316;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;&#22810;&#31867;&#21035;&#20219;&#21153;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;&#29305;&#23450;LLM&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-ge
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#32771;&#34385;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11254</link><description>&lt;p&gt;
&#23545;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#32771;&#34385;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22914;BERT&#21644;GPT&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#25935;&#24863;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#21307;&#30103;&#39046;&#22495;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#35775;&#38382;&#21644;&#30001;&#12298;&#20581;&#24247;&#20445;&#38505;&#20415;&#25658;&#24615;&#21644;&#36131;&#20219;&#27861;&#26696;&#12299;&#65288;HIPPA&#65289;&#21644;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#30340;&#38544;&#31169;&#32422;&#26463;&#65292;&#38754;&#20020;&#30528;&#35757;&#32451;LM&#30340;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26082;&#33021;&#22815;&#23454;&#29616;&#21327;&#21516;&#23398;&#20064;&#65292;&#21448;&#33021;&#22815;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#30340;&#20445;&#25252;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#21307;&#23398;&#20013;&#30340;FL&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#65292;&#20351;&#29992;&#20102;&#20843;&#20010;&#35821;&#26009;&#24211;&#21644;&#20845;&#20010;LM&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;FL&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#21333;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#35757;&#32451;&#30340;LM&#65292;&#24182;&#19988;&#26377;&#26102;&#33021;&#22815;&#19982;&#20351;&#29992;&#27719;&#24635;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21305;&#37197;&#65307;2&#65289;&#22312;&#24635;&#25968;&#25454;&#37327;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26356;&#22810;&#23458;&#25143;&#31471;&#36827;&#34892;FL&#35757;&#32451;&#30340;LM&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65307;3&#65289;LM&#20204;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) like BERT and GPT have revolutionized natural language processing (NLP). However, privacy-sensitive domains, particularly the medical field, face challenges to train LMs due to limited data access and privacy constraints imposed by regulations like the Health Insurance Portability and Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR). Federated learning (FL) offers a decentralized solution that enables collaborative learning while ensuring the preservation of data privacy. In this study, we systematically evaluate FL in medicine across $2$ biomedical NLP tasks using $6$ LMs encompassing $8$ corpora. Our results showed that: 1) FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data; 2) With the fixed number of total data, LMs trained using FL with more clients exhibit inferior performance, but pre-trained transformer-based models exhibited greater resilience. 3) LMs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#23613;&#31649;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#65292;&#31572;&#26696;&#20173;&#28982;&#34987;&#26356;&#39640;&#22320;&#35780;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;</title><link>http://arxiv.org/abs/2307.03025</link><description>&lt;p&gt;
&#39118;&#26684;&#32988;&#36807;&#23454;&#36136;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Style Over Substance: Evaluation Biases for Large Language Models. (arXiv:2307.03025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#23613;&#31649;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#65292;&#31572;&#26696;&#20173;&#28982;&#34987;&#26356;&#39640;&#22320;&#35780;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20934;&#30830;&#21644;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#23558;&#26368;&#20808;&#36827;&#30340;LLMs&#32435;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#22996;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21644;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#33021;&#21147;&#31243;&#24230;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20247;&#21253;&#20154;&#31867;&#35780;&#22996;&#21644;&#22522;&#20110;LLMs&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#25925;&#24847;&#26377;&#32570;&#38519;&#30340;&#26426;&#22120;&#29983;&#25104;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20107;&#23454;&#19978;&#30340;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#26356;&#22823;&#30340;&#21361;&#38505;&#65292;&#20294;&#24102;&#26377;&#20107;&#23454;&#38169;&#35823;&#30340;&#31572;&#26696;&#20173;&#28982;&#27604;&#38271;&#24230;&#36807;&#30701;&#25110;&#21253;&#21547;&#35821;&#27861;&#38169;&#35823;&#30340;&#31572;&#26696;&#35780;&#20998;&#26356;&#39640;&#12290;&#36825;&#31361;&#26174;&#20102;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Conventionally, human evaluations are considered the gold standard in natural language generation. Recent advancements incorporate state-of-the-art LLMs as proxies for human judges in evaluation processes. Nonetheless, the extent to which humans and LLMs are capable evaluators remains uncertain. This study aims to investigate the behavior of both crowd-sourced human and LLM-based judges when comparing outputs from different models. To accomplish this, we curate a dataset comprising intentionally flawed machine-generated answers. Our findings indicate that despite the potentially greater danger posed by factual errors, answers with factual errors were still rated more favorably compared to answers that were too short or contained grammatical errors. This highlights a concerning bias in the evaluation process. To address this issue, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11816</link><description>&lt;p&gt;
&#23398;&#20250;&#29983;&#25104;&#27604;&#20320;&#30340;LMM&#26356;&#22909;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;LLM&#65292;&#22914;ChatGPT&#21644;GPT - 4&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#21512;&#20102;RL&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#23398;&#20064;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#25506;&#32034;&#20102;&#36229;&#20986;&#36890;&#29992;RL&#31639;&#27861;&#22914;PPO&#20043;&#22806;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;RL&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21160;&#24577;&#40657;&#21283;&#23376;&#30340;&#25351;&#23548;LLM&#22914;GPT-3&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#24341;&#23548;&#21453;&#39304;&#30340;RL(RLGF)&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;LLM&#24494;&#35843;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;GRUE&#22522;&#20934;&#27979;&#35797;&#30340;IMDB&#27491;&#21521;&#35780;&#35770;&#21644;CommonGen&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;RL&#31639;&#27861;&#27604;&#30417;&#30563;&#23398;&#20064;(SL)&#21644;&#40664;&#35748;PPO&#22522;&#32447;&#34920;&#29616;&#26356;&#39640;&#65292;&#35777;&#26126;&#20102;&#19982;&#25351;&#23548;LLM&#20114;&#21160;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19915</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20419;&#36827;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#65288;&#20363;&#22914;&#20581;&#22766;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;DA&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20294;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#21644;&#23457;&#26597;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21547;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20840;&#38754;&#32780;&#32508;&#21512;&#30340;&#35843;&#26597;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#21644;&#27010;&#36848;&#29616;&#26377;&#25991;&#29486;&#65292;&#20197;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#33879;&#21517;&#30340;&#12289;&#26041;&#27861;&#19978;&#20855;&#26377;&#35828;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;DA&#36136;&#37327;&#30340;&#19968;&#33324;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#21576;&#29616;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DA&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24102;&#35299;&#37322;&#30340;&#22522;&#20110;&#30446;&#26631;&#30340;&#32858;&#31867;&#8221;&#65288;GoalEx&#65289;&#30340;&#26032;&#20219;&#21153;&#24418;&#24335;&#65292;&#23427;&#23558;&#30446;&#26631;&#21644;&#35299;&#37322;&#37117;&#34920;&#31034;&#20026;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#36890;&#36807;&#23558;&#25688;&#35201;&#31995;&#32479;&#30340;&#27880;&#37322;&#36827;&#34892;&#20998;&#31867;&#26469;&#35828;&#26126;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13749</link><description>&lt;p&gt;
&#22522;&#20110;&#30446;&#26631;&#30340;&#21487;&#35299;&#37322;&#32858;&#31867;&#22312;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Goal-Driven Explainable Clustering via Language Descriptions. (arXiv:2305.13749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24102;&#35299;&#37322;&#30340;&#22522;&#20110;&#30446;&#26631;&#30340;&#32858;&#31867;&#8221;&#65288;GoalEx&#65289;&#30340;&#26032;&#20219;&#21153;&#24418;&#24335;&#65292;&#23427;&#23558;&#30446;&#26631;&#21644;&#35299;&#37322;&#37117;&#34920;&#31034;&#20026;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#36890;&#36807;&#23558;&#25688;&#35201;&#31995;&#32479;&#30340;&#27880;&#37322;&#36827;&#34892;&#20998;&#31867;&#26469;&#35828;&#26126;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#32858;&#31867;&#24191;&#27867;&#29992;&#20110;&#25506;&#32034;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#20294;&#29616;&#26377;&#34920;&#36848;&#26082;&#19981;&#32771;&#34385;&#29992;&#25143;&#30340;&#30446;&#26631;&#65292;&#20063;&#19981;&#35299;&#37322;&#32858;&#31867;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#8212;&#8212;&#24102;&#35299;&#37322;&#30340;&#22522;&#20110;&#30446;&#26631;&#30340;&#32858;&#31867;&#65288;GoalEx&#65289;&#65292;&#23427;&#23558;&#30446;&#26631;&#21644;&#35299;&#37322;&#37117;&#34920;&#31034;&#20026;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#23545;&#20110;&#19968;&#20010;&#24635;&#32467;&#31995;&#32479;&#25152;&#29359;&#30340;&#38169;&#35823;&#36827;&#34892;&#20998;&#31867;&#65292;GoalEx&#30340;&#36755;&#20837;&#26159;&#19968;&#20010;&#27880;&#37322;&#32773;&#20026;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#25776;&#20889;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#30446;&#26631;&#25551;&#36848;&#8220;&#26681;&#25454;&#27880;&#37322;&#32773;&#35748;&#20026;&#25688;&#35201;&#19981;&#23436;&#32654;&#30340;&#21407;&#22240;&#23545;&#27880;&#37322;&#36827;&#34892;&#20998;&#31867;&#8221;;&#36755;&#20986;&#26159;&#27599;&#20010;&#20855;&#26377;&#35299;&#37322;&#30340;&#25991;&#26412;&#32858;&#31867;(&#8220;&#27492;&#32858;&#31867;&#25552;&#21040;&#25688;&#35201;&#32570;&#23569;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#8220;)&#65292;&#36825;&#20123;&#32858;&#31867;&#19982;&#30446;&#26631;&#30456;&#20851;&#65292;&#24182;&#20934;&#30830;&#35299;&#37322;&#21738;&#20123;&#27880;&#37322;&#24212;&#35813;(&#19981;&#24212;&#35813;)&#23646;&#20110;&#19968;&#20010;&#32858;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;GoalEx&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#8220; [&#25968;&#25454;&#38598;&#23376;&#38598;]+[&#30446;&#26631;]+&#22836;&#33041;&#39118;&#26292;&#19968;&#20010;&#20195;&#34920;&#32858;&#31867;&#30340;&#35299;&#37322;&#21015;&#34920;&#8221;&#65292;&#28982;&#21518;&#20998;&#31867;&#21738;&#20123;&#35299;&#37322;&#23646;&#20110;&#27599;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#21253;&#25324;&#27719;&#24635;&#21453;&#39304;&#12289;&#26032;&#38395;&#25991;&#31456;&#12289;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#12289;&#31185;&#23398;&#25991;&#31456;&#21644;&#25209;&#35780;&#35780;&#35770;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users' goals nor explain clusters' meanings. We propose a new task formulation, "Goal-Driven Clustering with Explanations" (GoalEx), which represents both the goal and the explanations as free-form language descriptions. For example, to categorize the errors made by a summarization system, the input to GoalEx is a corpus of annotator-written comments for system-generated summaries and a goal description "cluster the comments based on why the annotators think the summary is imperfect.''; the outputs are text clusters each with an explanation ("this cluster mentions that the summary misses important context information."), which relates to the goal and precisely explain which comments should (not) belong to a cluster. To tackle GoalEx, we prompt a language model with "[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster."; then we classify wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13583</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19981;&#36275;&#65306;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#19982;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition. (arXiv:2305.13583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26426;&#29702;&#23578;&#19981;&#28165;&#26970;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#23427;&#36890;&#24120;&#20250;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#20013;&#19968;&#20010;&#27169;&#24577;&#20013;&#31361;&#20986;&#30340;&#24773;&#24863;&#20449;&#24687;&#22914;&#20309;&#21463;&#21040;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#36328;&#27169;&#24577;&#30340;&#20851;&#27880;&#65292;&#27169;&#24577;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;(HCT-MG)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#20132;&#21449;&#27169;&#24577;Transformer&#19982;&#27169;&#24577;&#38376;&#25511;&#21046;&#26469;&#30830;&#23450;&#20027;&#35201;&#30340;&#27169;&#24577;&#65292;&#24182;&#20998;&#23618;&#22320;&#23558;&#36741;&#21161;&#27169;&#24577;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#24182;&#20943;&#23569;&#20449;&#24687;&#20887;&#20313;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;CMU-MOSI&#12289;CMU-MOSEI&#21644;IEMOCAP&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#65306;1&#65289;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65307;2&#65289;&#23427;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#36229;&#21442;&#25968;&#21644;&#21442;&#25968;&#65307;3&#65289;&#23427;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing multiple modalities for affective computing tasks has proven effective for performance improvement. However, how multimodal fusion works is not well understood, and its use in the real world usually results in large model sizes. In this work, on sentiment and emotion analysis, we first analyze how the salient affective information in one modality can be affected by the other in crossmodal attention. We find that inter-modal incongruity exists at the latent level due to crossmodal attention. Based on this finding, we propose a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy. The experimental evaluation on three benchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP verifies the efficacy of our approach, showing that it: 1) outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#33521;&#25991;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#39640;&#25928;&#26631;&#35760;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#27604;&#36328;&#35821;&#35328;&#36716;&#31227;&#22522;&#20934;&#26174;&#30528;&#25552;&#39640;&#65288;&#26368;&#22810;&#25552;&#39640;22%&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.13528</link><description>&lt;p&gt;
&#26080;&#38656;&#36716;&#31227;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#30701;&#35821;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transfer-Free Data-Efficient Multilingual Slot Labeling. (arXiv:2305.13528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#33521;&#25991;&#25968;&#25454;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#39640;&#25928;&#26631;&#35760;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#27604;&#36328;&#35821;&#35328;&#36716;&#31227;&#22522;&#20934;&#26174;&#30528;&#25552;&#39640;&#65288;&#26368;&#22810;&#25552;&#39640;22%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#26631;&#35760;&#65288;SL&#65289;&#26159;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#65288;ToD&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#32780;&#20854;&#20013;&#30340;&#30701;&#35821;&#21644;&#30456;&#24212;&#30340;&#20540;&#36890;&#24120;&#26159;&#29305;&#23450;&#20110;&#35821;&#35328;&#12289;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#31995;&#32479;&#25193;&#23637;&#21040;&#20219;&#20309;&#26032;&#30340;&#35821;&#35328;-&#39046;&#22495;-&#20219;&#21153;&#37197;&#32622;&#38656;&#35201;&#37325;&#26032;&#36816;&#34892;&#26114;&#36149;&#32780;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#25968;&#25454;&#26631;&#27880;&#27969;&#31243;&#12290;&#20026;&#20102;&#20943;&#36731;&#22266;&#26377;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24403;&#21069;&#22810;&#35821;&#35328;ToD&#30740;&#31350;&#20551;&#35774;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#36275;&#22815;&#33521;&#35821;&#27880;&#37322;&#25968;&#25454;&#22987;&#32456;&#21487;&#29992;&#65292;&#22240;&#27492;&#22312;&#26631;&#20934;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#36825;&#31181;&#24120;&#24120;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30740;&#31350;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#21363;&#26080;&#27861;&#20445;&#35777;&#20855;&#26377;&#20256;&#36755;&#21151;&#33021;&#30340;&#33521;&#25991;&#27880;&#37322;&#25968;&#25454;&#65292;&#24182;&#19987;&#27880;&#20110;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#30452;&#25509;&#36827;&#34892;&#22810;&#35821;&#35328;&#25968;&#25454;&#39640;&#25928;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30701;&#35821;&#26631;&#35760;&#26041;&#27861;&#65288;&#31216;&#20026;TWOSL&#65289;&#65292;&#23558;&#26631;&#20934;&#30340;&#22810;&#35821;&#35328;SL&#36716;&#21270;&#20026;&#26080;&#38656;&#36716;&#31227;&#25968;&#25454;&#30340;&#39640;&#25928;&#35774;&#32622;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#23545;&#40784;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#30701;&#35821;&#38598;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#23545;&#40784;&#26469;&#36890;&#36807;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#30701;&#35821;&#24863;&#24212;&#26694;&#26550;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20256;&#36882;&#27880;&#37322;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#36890;&#36807;&#21253;&#21547;&#26469;&#33258;&#30446;&#26631;&#35821;&#35328;&#30340;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#26469;&#36845;&#20195;&#26356;&#26032;&#21644;&#25913;&#36827;&#22810;&#35821;&#35328;&#30701;&#35821;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#39046;&#22495;&#20013;&#65292;TWOSL&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#22522;&#32447;&#26174;&#30528;&#25552;&#39640;&#20102;&#30701;&#35821;F1&#20998;&#25968;&#65288;&#26368;&#22810;&#25552;&#39640;22%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slot labeling (SL) is a core component of task-oriented dialogue (ToD) systems, where slots and corresponding values are usually language-, task- and domain-specific. Therefore, extending the system to any new language-domain-task configuration requires (re)running an expensive and resource-intensive data annotation process. To mitigate the inherent data scarcity issue, current research on multilingual ToD assumes that sufficient English-language annotated data are always available for particular tasks and domains, and thus operates in a standard cross-lingual transfer setup. In this work, we depart from this often unrealistic assumption. We examine challenging scenarios where such transfer-enabling English annotated data cannot be guaranteed, and focus on bootstrapping multilingual data-efficient slot labelers in transfer-free scenarios directly in the target languages without any English-ready data. We propose a two-stage slot labeling approach (termed TWOSL) which transforms standar
&lt;/p&gt;</description></item><item><title>MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09402</link><description>&lt;p&gt;
MixPro&#65306;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09402
&lt;/p&gt;
&lt;p&gt;
MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36890;&#36807;&#23558;&#36755;&#20837;&#19982;&#27169;&#26495;&#32452;&#21512;&#36215;&#26469;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#20026;&#22635;&#31354;&#38382;&#39064;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#28982;&#32780;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#27169;&#26495;&#21644;&#25991;&#26412;&#20173;&#28982;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#30340;&#26041;&#27861;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixPro&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26631;&#35760;&#32423;&#12289;&#21477;&#23376;&#32423;&#21644;&#26102;&#20195;&#32423;&#30340;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#21407;&#22987;&#36755;&#20837;&#25991;&#26412;&#21644;&#27169;&#26495;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;MixPro&#20248;&#20110;&#20854;&#20182;&#22686;&#24378;&#22522;&#32447;&#65292;&#30456;&#27604;&#22686;&#24378;&#21069;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21542;&#23450;&#21644;&#35282;&#33394;&#21453;&#36716;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36807;&#21435;&#30340;&#32467;&#35770;&#21487;&#33021;&#34987;&#23567;&#22411;&#27979;&#35797;&#38598;&#35823;&#23548;&#12290;&#21516;&#26102;&#65292;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16445</link><description>&lt;p&gt;
&#26356;&#22823;&#30340;&#25506;&#38024;&#35762;&#36848;&#19981;&#21516;&#30340;&#25925;&#20107;: &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#24515;&#29702;&#35821;&#35328;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21542;&#23450;&#21644;&#35282;&#33394;&#21453;&#36716;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36807;&#21435;&#30340;&#32467;&#35770;&#21487;&#33021;&#34987;&#23567;&#22411;&#27979;&#35797;&#38598;&#35823;&#23548;&#12290;&#21516;&#26102;&#65292;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#36890;&#24120;&#29992;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#25506;&#27979;&#22522;&#20934;&#23567;&#19988;&#32570;&#20047;&#32479;&#35745;&#21151;&#25928;&#26102;&#65292;&#36825;&#31867;&#30740;&#31350;&#30340;&#32467;&#35770;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21463;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#21551;&#21457;&#30340;&#21542;&#23450;&#65288;NEG-1500-SIMP&#65289;&#21644;&#35282;&#33394;&#21453;&#36716;&#65288;ROLE-1500&#65289;&#30340;&#26032;&#30340;&#12289;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;GPT3&#23558;&#29616;&#26377;&#30340;NEG-136&#21644;ROLE-88&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#24133;&#25193;&#23637;&#65292;&#23558;&#23427;&#20204;&#30340;&#35268;&#27169;&#20174;18&#21644;44&#20010;&#21477;&#23545;&#20998;&#21035;&#22686;&#21152;&#21040;&#20102;750&#20010;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#21019;&#24314;&#30340;&#25193;&#23637;&#21542;&#23450;&#25968;&#25454;&#38598;(NEG-1500-SIMP-TEMP)&#65292;&#23427;&#30001;770&#20010;&#21477;&#23545;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;22&#20010;&#27169;&#22411;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#19982;&#21407;&#22987;&#36739;&#23567;&#22522;&#20934;&#30456;&#27604;&#19979;&#38477;&#20102;20-57%&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24615;&#65292;&#36825;&#34920;&#26126;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#33021;&#30001;&#20110;&#36739;&#23567;&#30340;&#27979;&#35797;&#38598;&#32780;&#23384;&#22312;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;GPT3&#29983;&#25104;&#20102;&#25152;&#26377;&#30340;&#23454;&#20363;&#65292;&#20294;&#21477;&#23376;&#30340;&#35821;&#27861;&#36136;&#37327;&#21463;&#21040;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model probing is often used to test specific capabilities of these models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23384;&#22312;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#20013;&#30340;&#21452;&#27169;&#25240;&#21472;&#19981;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#20013;&#21457;&#29616;&#20102;&#23545;&#36825;&#31181;&#38590;&#39064;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;LEE&#23646;&#24615;&#30340;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26041;&#31243;&#35777;&#26126;&#31995;&#32479;&#26159;&#23436;&#22791;&#30340;&#65292;&#24182;&#19988;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#20197;&#35299;&#20915;&#35299;&#37322;&#21644;&#36807;&#31243;&#22270;&#21452;&#27169;&#30456;&#20284;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08553</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#30340;&#21452;&#27169;&#25240;&#21472;&#19981;&#23553;&#38381;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Image of the Process Interpretation of Regular Expressions is Not Closed under Bisimulation Collapse. (arXiv:2303.08553v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08553
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23384;&#22312;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#20013;&#30340;&#21452;&#27169;&#25240;&#21472;&#19981;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#20013;&#21457;&#29616;&#20102;&#23545;&#36825;&#31181;&#38590;&#39064;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;LEE&#23646;&#24615;&#30340;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26041;&#31243;&#35777;&#26126;&#31995;&#32479;&#26159;&#23436;&#22791;&#30340;&#65292;&#24182;&#19988;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#20197;&#35299;&#20915;&#35299;&#37322;&#21644;&#36807;&#31243;&#22270;&#21452;&#27169;&#30456;&#20284;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Milner&#25552;&#20986;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#30340;&#20844;&#29702;&#21270;&#21644;&#34920;&#36798;&#38382;&#39064;&#22312;&#32771;&#34385;&#27515;&#38145;0&#21644;&#31354;&#27493;&#38271;1&#30340;&#23436;&#25972;&#34920;&#36798;&#24335;&#31867;&#20013;&#21464;&#24471;&#22256;&#38590;&#36215;&#26469;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24403;0&#23384;&#22312;&#26102;&#28155;&#21152;1&#20250;&#20135;&#29983;&#30340;&#29616;&#35937;&#65292;&#36825;&#23558;&#36825;&#20010;&#22256;&#38590;&#30340;&#20851;&#38190;&#21407;&#22240;&#24102;&#21040;&#20102;&#32858;&#28966;&#28857;&#12290;&#21363;&#65292;&#34429;&#28982;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#22312;&#21452;&#27169;&#25240;&#21472;&#19979;&#26159;&#23553;&#38381;&#30340;&#65292;&#20294;&#20219;&#24847;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#36807;&#31243;&#22270;&#35299;&#37322;&#28385;&#36275;&#24490;&#29615;&#23384;&#22312;&#21644;&#28040;&#38500;&#23646;&#24615;LEE&#65292;&#35813;&#23646;&#24615;&#22312;&#21452;&#27169;&#25240;&#21472;&#19979;&#24471;&#20197;&#20445;&#30041;&#12290;LEE&#30340;&#36825;&#20123;&#29305;&#24449;&#34987;&#29992;&#20110;&#35777;&#26126;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26041;&#31243;&#35777;&#26126;&#31995;&#32479;&#26159;&#23436;&#22791;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#21028;&#26029;&#19968;&#20010;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#26159;&#21542;&#19982;&#19968;&#20010;&#36807;&#31243;&#22270;&#21452;&#27169;&#30456;&#20284;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Axiomatization and expressibility problems for Milner's process semantics (1984) of regular expressions modulo bisimilarity have turned out to be difficult for the full class of expressions with deadlock 0 and empty step~1. We report on a phenomenon that arises from the added presence of 1 when 0 is available, and that brings a crucial reason for this difficulty into focus. To wit, while interpretations of 1-free regular expressions are closed under bisimulation collapse, this is not the case for the interpretations of arbitrary regular expressions.  Process graph interpretations of 1-free regular expressions satisfy the loop existence and elimination property LEE, which is preserved under bisimulation collapse. These features of LEE were applied for showing that an equational proof system for 1-free regular expressions modulo bisimilarity is complete, and that it is decidable in polynomial time whether a process graph is bisimilar to the interpretation of a 1-free regular expression. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#20132;&#20114;&#24335;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#30495;&#23454;&#29992;&#25143;&#21442;&#19982;&#30340;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#27169;&#22411;&#26397;&#30528;&#32473;&#23450;&#30446;&#26631;&#21069;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.00908</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Interactive Text Generation. (arXiv:2303.00908v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#20132;&#20114;&#24335;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#30495;&#23454;&#29992;&#25143;&#21442;&#19982;&#30340;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#27169;&#22411;&#26397;&#30528;&#32473;&#23450;&#30446;&#26631;&#21069;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#27599;&#22825;&#37117;&#35201;&#19982;&#25991;&#26412;&#12289;&#22270;&#29255;&#12289;&#20195;&#30721;&#25110;&#20854;&#20182;&#32534;&#36753;&#22120;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#23569;&#22312;&#21453;&#26144;&#29992;&#25143;&#19982;&#32534;&#36753;&#22120;&#20043;&#38388;&#20114;&#21160;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#26159;&#21487;&#20197;&#29702;&#35299;&#30340;&#65292;&#22240;&#20026;&#20351;&#29992;&#30495;&#23454;&#29992;&#25143;&#36827;&#34892;AI&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#20165;&#36895;&#24230;&#24930;&#19988;&#25104;&#26412;&#39640;&#65292;&#32780;&#19988;&#36825;&#20123;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;&#21487;&#33021;&#29305;&#23450;&#20110;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#36873;&#25321;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#24847;&#21619;&#30528;&#22823;&#22810;&#25968;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#38750;&#20132;&#20114;&#35774;&#32622;&#19978;&#65292;&#21363;&#27169;&#22411;&#34987;&#26399;&#26395;&#22312;&#27809;&#26377;&#32771;&#34385;&#20219;&#20309;&#26469;&#33258;&#24895;&#24847;&#24110;&#21161;&#30340;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#25152;&#26377;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#20132;&#20114;&#24335;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#26080;&#38656;&#28041;&#21450;&#30495;&#23454;&#29992;&#25143;&#30340;&#25104;&#26412;&#65292;&#27169;&#25311;&#22120;&#36890;&#36807;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#27169;&#22411;&#26397;&#30528;&#32473;&#23450;&#30340;&#30446;&#26631;&#25991;&#26412;&#21069;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#65292;&#24182;&#19982;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38750;&#20132;&#20114;&#24335;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but what these models learn may be specific to user interface design choices. Unfortunately, this means most of the research on text, code, and image generation has focused on non-interactive settings, whereby the model is expected to get everything right without accounting for any input from a user who may be willing to help.  We introduce a new Interactive Text Generation task that allows training generation models interactively without the costs of involving real users, by using user simulators that provide edits that guide the model towards a given target text. We train our interactive models using Imitation Learning, and our experiments against competitive non-interactive generation models s
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#8220;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;40&#20010;ToM&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-3&#21644;GPT-4&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;&#20219;&#21153;&#65292;&#35828;&#26126;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#21487;&#33021;&#26159;&#35821;&#35328;&#27169;&#22411;&#33258;&#21457;&#20986;&#29616;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;</title><link>http://arxiv.org/abs/2302.02083</link><description>&lt;p&gt;
&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#33258;&#21457;&#20986;&#29616;&#24515;&#26234;&#29702;&#35770;&#8221;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02083
&lt;/p&gt;
&lt;p&gt;
&#8220;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;40&#20010;ToM&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-3&#21644;GPT-4&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;&#20219;&#21153;&#65292;&#35828;&#26126;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#21487;&#33021;&#26159;&#35821;&#35328;&#27169;&#22411;&#33258;&#21457;&#20986;&#29616;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#25351;&#33021;&#22815;&#25512;&#29702;&#20182;&#20154;&#20869;&#24515;&#30340;&#19981;&#21487;&#35266;&#23519;&#29366;&#24577;&#65292;&#23545;&#20110;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#12289;&#20132;&#27969;&#12289;&#31227;&#24773;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#36947;&#24503;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;40&#20010;&#24191;&#27867;&#29992;&#20110;&#27979;&#35797;&#20154;&#31867;ToM&#30340;&#32463;&#20856;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#26469;&#27979;&#35797;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#12290;2020&#24180;&#20043;&#21069;&#21457;&#24067;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;ToM&#20219;&#21153;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;2020&#24180;5&#26376;&#21457;&#24067;&#30340;&#31532;&#19968;&#20010;GPT-3&#29256;&#26412;&#65288;&#8220;davinci-001&#8221;&#65289;&#35299;&#20915;&#20102;&#32422;40&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#19982;3.5&#23681;&#30340;&#20799;&#31461;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#23427;&#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65288;&#8220;davinci-002&#8221;&#65292;2022&#24180;1&#26376;&#65289;&#35299;&#20915;&#20102;70&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#19982;6&#23681;&#20799;&#31461;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#26368;&#26032;&#29256;&#26412;&#30340;GPT-3.5&#65288;&#8220;davinci-003&#8221;&#65292;2022&#24180;11&#26376;&#65289;&#35299;&#20915;&#20102;90&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#36798;&#21040;&#20102;7&#23681;&#20799;&#31461;&#27700;&#24179;&#12290;&#20110;2023&#24180;3&#26376;&#21457;&#24067;&#30340;GPT-4&#35299;&#20915;&#20102;&#20960;&#20046;&#25152;&#26377;&#30340;&#20219;&#21153;&#65288;95&#65285;&#65289;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#65288;&#36804;&#20170;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#29420;&#26377;&#30340;&#65289;&#21487;&#33021;&#26159;&#35821;&#35328;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM), or the ability to impute unobservable mental states to others, is central to human social interactions, communication, empathy, self-consciousness, and morality. We tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. The models published before 2020 showed virtually no ability to solve ToM tasks. Yet, the first version of GPT-3 ("davinci-001"), published in May 2020, solved about 40% of false-belief tasks-performance comparable with 3.5-year-old children. Its second version ("davinci-002"; January 2022) solved 70% of false-belief tasks, performance comparable with six-year-olds. Its most recent version, GPT-3.5 ("davinci-003"; November 2022), solved 90% of false-belief tasks, at the level of seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks (95%). These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.03691</link><description>&lt;p&gt;
HyperMixer&#65306;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#20302;&#25104;&#26412;Transformer&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03691
&lt;/p&gt;
&lt;p&gt;
HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36755;&#20837;&#38271;&#24230;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#33021;&#38590;&#20197;&#35843;&#25972;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;MLPMixer&#65289;&#36890;&#36807;&#38745;&#24577;&#30340;MLP&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#29305;&#24449;&#65292;&#32780;&#36807;&#20110;&#33073;&#31163;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21363;HyperMixer&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#21160;&#24577;&#22320;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26367;&#20195;&#30340;&#22522;&#20110;MLP&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Transformer&#23218;&#32654;&#12290;&#19982;Transformer&#19981;&#21516;&#65292;HyperMixer&#22312;&#22788;&#29702;&#26102;&#38388;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#22823;&#22823;&#38477;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
&lt;/p&gt;</description></item></channel></rss>