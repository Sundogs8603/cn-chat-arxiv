<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#21512;&#20316;&#39550;&#39542;&#27719;&#20837;&#33258;&#21160;&#39550;&#39542;&#23558;&#20250;&#23545;&#20844;&#36335;&#25972;&#20307;&#27969;&#37327;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#21322;&#21512;&#20316;&#30340;&#22909;&#22788;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21033;&#24049;&#21644;&#39640;&#36895;&#39550;&#39542;&#21592;&#12290;</title><link>http://arxiv.org/abs/2304.11693</link><description>&lt;p&gt;
&#30740;&#31350;&#21322;&#21512;&#20316;&#39550;&#39542;&#23545;&#20844;&#36335;&#25972;&#20307;&#27969;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Semi-Cooperative Drivers on Overall Highway Flow. (arXiv:2304.11693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#21512;&#20316;&#39550;&#39542;&#27719;&#20837;&#33258;&#21160;&#39550;&#39542;&#23558;&#20250;&#23545;&#20844;&#36335;&#25972;&#20307;&#27969;&#37327;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#21322;&#21512;&#20316;&#30340;&#22909;&#22788;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21033;&#24049;&#21644;&#39640;&#36895;&#39550;&#39542;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#21512;&#20316;&#34892;&#20026;&#26159;&#20154;&#31867;&#39550;&#39542;&#21592;&#22266;&#26377;&#30340;&#29305;&#24615;&#65292;&#24212;&#35813;&#32771;&#34385;&#21040;&#33258;&#21160;&#39550;&#39542;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;&#33258;&#20027;&#35268;&#21010;&#22120;&#21487;&#20197;&#32771;&#34385;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#31038;&#20250;&#20215;&#20540;&#21462;&#21521;&#65288;SVO&#65289;&#20197;&#29983;&#25104;&#31526;&#21512;&#31038;&#20250;&#20934;&#21017;&#30340;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26032;&#22411;&#35268;&#21010;&#22120;&#23545;&#20132;&#36890;&#27969;&#37327;&#30340;&#25972;&#20307;&#24433;&#21709;&#20173;&#38656;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38544;&#24335;&#21322;&#21512;&#20316;&#39550;&#39542;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#37096;&#32626;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#29256;&#26412;&#30340;&#36845;&#20195;&#26368;&#20339;&#21709;&#24212;&#65292;&#20551;&#23450;&#30693;&#36947;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;SVO&#12290;&#25105;&#20204;&#27169;&#25311;&#21517;&#20041;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#36947;&#36335;&#19978;&#30340;&#21033;&#20182;&#20195;&#29702;&#20154;&#27604;&#20363;&#26159;&#21542;&#24433;&#21709;&#20010;&#20307;&#25110;&#31995;&#32479;&#32423;&#39550;&#39542;&#34920;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#20182;&#20195;&#29702;&#20154;&#30340;&#27604;&#20363;&#23545;&#25972;&#20307;&#20132;&#36890;&#27969;&#37327;&#24433;&#21709;&#36739;&#23567;&#65292;&#32780;&#21322;&#21512;&#20316;&#30340;&#22909;&#22788;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21033;&#24049;&#21644;&#39640;&#36895;&#39550;&#39542;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-cooperative behaviors are intrinsic properties of human drivers and should be considered for autonomous driving. In addition, new autonomous planners can consider the social value orientation (SVO) of human drivers to generate socially-compliant trajectories. Yet the overall impact on traffic flow for this new class of planners remain to be understood. In this work, we present study of implicit semi-cooperative driving where agents deploy a game-theoretic version of iterative best response assuming knowledge of the SVOs of other agents. We simulate nominal traffic flow and investigate whether the proportion of prosocial agents on the road impact individual or system-wide driving performance. Experiments show that the proportion of prosocial agents has a minor impact on overall traffic flow and that benefits of semi-cooperation disproportionally affect egoistic and high-speed drivers.
&lt;/p&gt;</description></item><item><title>DomMa &#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#23427;&#20998;&#20026;&#20013;&#33521;&#25991;10&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;112&#20010;&#19968;&#32423;&#23398;&#31185;&#20998;&#31867;&#19981;&#26029;&#26356;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11679</link><description>&lt;p&gt;
&#39046;&#22495;&#25484;&#25569;&#27700;&#24179;&#22522;&#20934;&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#39046;&#22495;&#30693;&#35782;&#30340;&#19981;&#26029;&#26356;&#26032;&#30340;&#22522;&#20934;&#8212;&#8212;&#21021;&#27493;&#21457;&#24067;&#12290;&#65288;arXiv:2304.11679v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release. (arXiv:2304.11679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11679
&lt;/p&gt;
&lt;p&gt;
DomMa &#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#23427;&#20998;&#20026;&#20013;&#33521;&#25991;10&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;112&#20010;&#19968;&#32423;&#23398;&#31185;&#20998;&#31867;&#19981;&#26029;&#26356;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#30693;&#35782;&#25351;&#23545;&#29305;&#23450;&#20027;&#39064;&#12289;&#34892;&#19994;&#12289;&#39046;&#22495;&#25110;&#29305;&#21035;&#20852;&#36259;&#39046;&#22495;&#30340;&#28145;&#20837;&#29702;&#35299;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#29087;&#24713;&#31243;&#24230;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#37117;&#32570;&#20047;&#23545;&#39046;&#22495;&#30693;&#35782;&#35780;&#20272;&#30340;&#25972;&#20307;&#35774;&#35745;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#39046;&#22495;&#35821;&#35328;&#29702;&#35299;&#30340;&#30495;&#27491;&#33021;&#21147;&#21482;&#33021;&#36890;&#36807;&#20840;&#38754;&#28145;&#20837;&#30340;&#22522;&#20934;&#26469;&#20844;&#24179;&#22320;&#35780;&#20272;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;Domma&#39046;&#22495;&#25484;&#25569;&#27700;&#24179;&#22522;&#20934;&#12290;DomMa&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#39046;&#22495;&#35206;&#30422;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#22522;&#20110;&#20013;&#22269;112&#20010;&#19968;&#32423;&#23398;&#31185;&#20998;&#31867;&#19981;&#26029;&#26356;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;DomMa&#21253;&#21547;10&#19975;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20013;&#25991;&#21644;&#33521;&#25991;&#65292;&#26469;&#28304;&#20110;&#20013;&#22269;&#22823;&#23398;&#30340;&#30740;&#31350;&#29983;&#20837;&#23398;&#32771;&#35797;&#21644;&#26412;&#31185;&#32771;&#35797;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#36866;&#21512;LLMs&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#27969;&#31243;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain knowledge refers to the in-depth understanding, expertise, and familiarity with a specific subject, industry, field, or area of special interest. The existing benchmarks are all lack of an overall design for domain knowledge evaluation. Holding the belief that the real ability of domain language understanding can only be fairly evaluated by an comprehensive and in-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa targets at testing Large Language Models (LLMs) on their domain knowledge understanding, it features extensive domain coverage, large data volume, and a continually updated data set based on Chinese 112 first-level subject classifications. DomMa consist of 100,000 questions in both Chinese and English sourced from graduate entrance examinations and undergraduate exams in Chinese college. We have also propose designs to make benchmark and evaluation process more suitable to LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#38889;&#22269;&#21069;40&#23478;&#26032;&#38395;&#25552;&#20379;&#21830;&#30340;10.9K&#31687;&#25991;&#31456;&#65292;&#37319;&#29992;&#35789;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#26792;&#27888;&#38498;&#19975;&#22307;&#33410;&#36393;&#36367;&#20107;&#20214;&#30340;&#23186;&#20307;&#26694;&#26550;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#20445;&#23432;&#27966;&#23186;&#20307;&#27880;&#37325;&#25919;&#27835;&#21453;&#24212;&#21644;&#23244;&#30097;&#20154;&#36523;&#20221;&#65292;&#32780;&#33258;&#30001;&#27966;&#23186;&#20307;&#21017;&#20851;&#27880;&#25919;&#24220;&#30340;&#36131;&#20219;&#21644;&#23545;&#20302;&#25910;&#20837;&#20135;&#19994;&#24037;&#20154;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20844;&#24179;&#28322;&#20986;&#25928;&#24212;&#12290;&#23186;&#20307;&#22312;&#25253;&#36947;&#36807;&#31243;&#20013;&#26126;&#26174;&#23637;&#29616;&#20102;&#25919;&#27835;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.11666</link><description>&lt;p&gt;
&#8220;&#25235;&#20303;&#23244;&#30097;&#20154;&#65281;&#8221;&#65306;&#20998;&#26512;&#26792;&#27888;&#38498;&#19975;&#22307;&#33410;&#36393;&#36367;&#20107;&#20214;&#30340;&#23186;&#20307;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hold the Suspect! : An Analysis on Media Framing of Itaewon Halloween Crowd Crush. (arXiv:2304.11666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#38889;&#22269;&#21069;40&#23478;&#26032;&#38395;&#25552;&#20379;&#21830;&#30340;10.9K&#31687;&#25991;&#31456;&#65292;&#37319;&#29992;&#35789;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#26792;&#27888;&#38498;&#19975;&#22307;&#33410;&#36393;&#36367;&#20107;&#20214;&#30340;&#23186;&#20307;&#26694;&#26550;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#20445;&#23432;&#27966;&#23186;&#20307;&#27880;&#37325;&#25919;&#27835;&#21453;&#24212;&#21644;&#23244;&#30097;&#20154;&#36523;&#20221;&#65292;&#32780;&#33258;&#30001;&#27966;&#23186;&#20307;&#21017;&#20851;&#27880;&#25919;&#24220;&#30340;&#36131;&#20219;&#21644;&#23545;&#20302;&#25910;&#20837;&#20135;&#19994;&#24037;&#20154;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20844;&#24179;&#28322;&#20986;&#25928;&#24212;&#12290;&#23186;&#20307;&#22312;&#25253;&#36947;&#36807;&#31243;&#20013;&#26126;&#26174;&#23637;&#29616;&#20102;&#25919;&#27835;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#38889;&#22269;&#21069;40&#23478;&#26032;&#38395;&#25552;&#20379;&#21830;&#30340;10.9K&#31687;&#25991;&#31456;&#65292;&#20998;&#26512;&#20102;&#20107;&#20214;&#21457;&#29983;&#21518;&#21069;72&#23567;&#26102;&#20869;&#23186;&#20307;&#23545;&#26792;&#27888;&#38498;&#19975;&#22307;&#33410;&#36393;&#36367;&#20107;&#20214;&#30340;&#25253;&#36947;&#12290;&#36890;&#36807;&#37319;&#29992;&#35789;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#21457;&#29616;&#20445;&#23432;&#27966;&#23186;&#20307;&#27880;&#37325;&#25919;&#20826;&#30340;&#21453;&#24212;&#21644;&#23244;&#30097;&#20154;&#30340;&#36523;&#20221;&#65292;&#32780;&#33258;&#30001;&#27966;&#23186;&#20307;&#21017;&#20851;&#27880;&#25919;&#24220;&#30340;&#36131;&#20219;&#21644;&#23545;&#20302;&#25910;&#20837;&#20135;&#19994;&#24037;&#20154;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20844;&#24179;&#28322;&#20986;&#25928;&#24212;&#12290;&#23613;&#31649;&#35813;&#31038;&#20250;&#24754;&#21095;&#19982;&#21046;&#24230;&#25919;&#27835;&#27809;&#26377;&#30452;&#25509;&#32852;&#31995;&#65292;&#20294;&#23186;&#20307;&#22312;&#25253;&#36947;&#36807;&#31243;&#20013;&#26126;&#26174;&#23637;&#29616;&#20102;&#25919;&#27835;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the 10.9K articles from top 40 news providers of South Korea, this paper analyzed the media framing of Itaewon Halloween Crowd Crush during the first 72 hours after the incident. By adopting word-vector embedding and clustering, we figured out that conservative media focused on political parties' responses and the suspect's identity while the liberal media covered the responsibility of the government and possible unequal spillover effect on the low-income industry workers. Although the social tragedy was not directly connected to institutional politics, the media clearly exhibited political bias in the coverage process.
&lt;/p&gt;</description></item><item><title>IslamicPCQA&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;9&#37096;&#20234;&#26031;&#20848;&#30334;&#31185;&#20840;&#20070;&#20013;&#25552;&#21462;&#30340;12,282&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#26088;&#22312;&#26041;&#20415;&#22238;&#31572;&#28041;&#21450;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#22797;&#26434;&#27874;&#26031;&#35821;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11664</link><description>&lt;p&gt;
IslamicPCQA&#65306;&#22522;&#20110;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#27874;&#26031;&#35821;&#22810;&#36339;&#22797;&#26434;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering in Islamic Text Resources. (arXiv:2304.11664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11664
&lt;/p&gt;
&lt;p&gt;
IslamicPCQA&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;9&#37096;&#20234;&#26031;&#20848;&#30334;&#31185;&#20840;&#20070;&#20013;&#25552;&#21462;&#30340;12,282&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#26088;&#22312;&#26041;&#20415;&#22238;&#31572;&#28041;&#21450;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#22797;&#26434;&#27874;&#26031;&#35821;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#38382;&#31572;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20351;&#29992;&#21508;&#31181;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#22810;&#36339;&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25165;&#33021;&#22238;&#31572;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IslamicPCQA&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#28304;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;9&#37096;&#20234;&#26031;&#20848;&#30334;&#31185;&#20840;&#20070;&#20013;&#25552;&#21462;&#30340;12,282&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#35813;&#25968;&#25454;&#38598;&#21463;HotpotQA&#33521;&#35821;&#25968;&#25454;&#38598;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#32463;&#36807;&#23450;&#21046;&#20197;&#36866;&#24212;&#27874;&#26031;&#35821;&#30340;&#22797;&#26434;&#24615;&#12290;&#22238;&#31572;&#35813;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#38656;&#35201;&#22810;&#20010;&#27573;&#33853;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#38382;&#39064;&#19981;&#38480;&#20110;&#20219;&#20309;&#20808;&#21069;&#30340;&#30693;&#35782;&#24211;&#25110;&#26412;&#20307;&#35770;&#65292;&#24182;&#19988;&#20026;&#20102;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#25903;&#25345;&#20107;&#23454;&#21644;&#20851;&#38190;&#21477;&#23376;&#12290;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20234;&#26031;&#20848;&#20027;&#39064;&#65292;&#26088;&#22312;&#26041;&#20415;&#22238;&#31572;&#28041;&#21450;&#20234;&#26031;&#20848;&#25991;&#26412;&#36164;&#28304;&#30340;&#22797;&#26434;&#27874;&#26031;&#35821;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, one of the main challenges for Question Answering Systems is to answer complex questions using various sources of information. Multi-hop questions are a type of complex questions that require multi-step reasoning to answer. In this article, the IslamicPCQA dataset is introduced. This is the first Persian dataset for answering complex questions based on non-structured information sources and consists of 12,282 question-answer pairs extracted from 9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA English dataset approach, which was customized to suit the complexities of the Persian language. Answering questions in this dataset requires more than one paragraph and reasoning. The questions are not limited to any prior knowledge base or ontology, and to provide robust reasoning ability, the dataset also includes supporting facts and key sentences. The prepared dataset covers a wide range of Islamic topics and aims to facilitate answering complex Persi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.11657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#24378;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#24605;&#32500;&#38142; (CoT) &#20316;&#20026;&#31034;&#33539;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs &#29983;&#25104;&#30340;&#28436;&#31034;&#25512;&#29702;&#38142;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#24688;&#24403;&#30340;&#31034;&#20363; (&#36807;&#20110;&#31616;&#21333;&#25110;&#22797;&#26434;) &#21487;&#20197;&#24433;&#21709;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Iter-CoT (&#36845;&#20195;&#24341;&#23548;&#24605;&#32500;&#38142;&#25552;&#31034;) &#30340;&#36845;&#20195;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23454;&#20363;&#24182;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs &#33258;&#20027;&#26356;&#27491;&#38169;&#35823;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#25512;&#29702;&#38142;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;7&#20010;&#32454;&#31890;&#24230;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#35780;&#20272;ChatGPT&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#26631;&#20934;IE&#19979;&#34920;&#29616;&#36739;&#24046;&#20294;&#22312;OpenIE&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#39640;&#21697;&#36136;&#21644;&#21487;&#20449;&#30340;&#35299;&#37322;&#65292;&#20294;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#20302;&#26631;&#23450;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11633</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#65306;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#26631;&#23450;&#24230;&#21644;&#24544;&#23454;&#24230;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. (arXiv:2304.11633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;7&#20010;&#32454;&#31890;&#24230;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#35780;&#20272;ChatGPT&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#26631;&#20934;IE&#19979;&#34920;&#29616;&#36739;&#24046;&#20294;&#22312;OpenIE&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#39640;&#21697;&#36136;&#21644;&#21487;&#20449;&#30340;&#35299;&#37322;&#65292;&#20294;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#20302;&#26631;&#23450;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#24182;&#25552;&#20379;&#21512;&#29702;&#30340;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#20351;&#29992;7&#20010;&#32454;&#31890;&#24230;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#35780;&#20272;ChatGPT&#30340;&#24635;&#20307;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;ChatGPT&#30340;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#26631;&#23450;&#24230;&#21644;&#24544;&#23454;&#24230;&#26469;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#32467;&#26524;&#24471;&#20986;&#20102;15&#20010;&#26469;&#33258;ChatGPT&#25110;&#39046;&#22495;&#19987;&#23478;&#30340;&#20851;&#38190;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#22312;&#26631;&#20934;IE&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#22312;OpenIE&#35774;&#32622;&#19979;&#21364;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#25552;&#20379;&#20102;&#39640;&#21697;&#36136;&#21644;&#21487;&#20449;&#30340;&#20915;&#31574;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#39044;&#27979;&#26102;&#26377;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#20102;&#20302;&#26631;&#23450;&#24230;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#20027;&#35201;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#24230;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the major
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;&#26041;&#27861;(MANS)&#65292;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#32467;&#26500;&#24615;&#21644;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#65292;MANS&#33021;&#22815;&#23398;&#20064;&#26356;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20197;&#22312;&#22810;&#27169;&#24577;KGE&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11618</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding. (arXiv:2304.11618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;&#26041;&#27861;(MANS)&#65292;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#32467;&#26500;&#24615;&#21644;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#65292;MANS&#33021;&#22815;&#23398;&#20064;&#26356;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20197;&#22312;&#22810;&#27169;&#24577;KGE&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#65292;&#36127;&#37319;&#26679;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#36127;&#19977;&#20803;&#32452;&#20197;&#36827;&#34892;&#27491;&#36127;&#21306;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#32771;&#34385;&#26102;&#19981;&#36866;&#29992;&#20110;KGE&#27169;&#22411;&#65292;&#32780;&#19988;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#35774;&#35745;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;MMKGE&#65289;&#30340;&#27169;&#24577;&#24863;&#30693;&#36127;&#37319;&#26679;&#65288;MANS&#65289;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;MANS&#33021;&#22815;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#36827;&#34892;&#32467;&#26500;&#24615;&#21644;&#35270;&#35273;&#23884;&#20837;&#30340;&#23545;&#40784;&#65292;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20197;&#22312;&#22810;&#27169;&#24577;KGE&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#29575;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MANS&#20248;&#20110;&#29616;&#26377;&#30340;NS&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;MANS&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#20197;&#35777;&#23454;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21306;&#20998;&#30001;ChatGPT&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#26469;&#26377;&#25928;&#26816;&#27979;&#21307;&#23398;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#23548;&#33268;&#30340;&#20551;&#28040;&#24687;&#21644;&#23545;&#20844;&#20247;&#36896;&#25104;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2304.11567</link><description>&lt;p&gt;
&#21306;&#20998;ChatGPT&#29983;&#25104;&#21644;&#20154;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Differentiate ChatGPT-generated and Human-written Medical Texts. (arXiv:2304.11567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21306;&#20998;&#30001;ChatGPT&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#26469;&#26377;&#25928;&#26816;&#27979;&#21307;&#23398;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#23548;&#33268;&#30340;&#20551;&#28040;&#24687;&#21644;&#23545;&#20844;&#20247;&#36896;&#25104;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#35821;&#27861;&#23436;&#32654;&#12289;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#20869;&#23481;&#30340;&#22823;&#37327;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#30149;&#21382;&#21644;&#35786;&#26029;&#31561;&#21307;&#23398;&#25991;&#26412;&#38656;&#35201;&#20005;&#26684;&#30340;&#39564;&#35777;&#65292;&#30001;ChatGPT&#29983;&#25104;&#30340;&#38169;&#35823;&#21307;&#23398;&#20869;&#23481;&#21487;&#33021;&#23548;&#33268;&#20551;&#28040;&#24687;&#65292;&#20174;&#32780;&#23545;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20247;&#36896;&#25104;&#37325;&#22823;&#21361;&#23475;&#12290;&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#21307;&#30103;&#39046;&#22495;&#36127;&#36131;&#21644;&#36947;&#24503;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#26512;&#30001;&#20154;&#31867;&#19987;&#23478;&#25776;&#20889;&#30340;&#21307;&#23398;&#25991;&#26412;&#21644;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#21306;&#20998;&#30001;ChatGPT&#29983;&#25104;&#30340;&#21307;&#23398;&#25991;&#26412;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#22871;&#21253;&#21547;&#20154;&#31867;&#19987;&#23478;&#25776;&#20889;&#21644;&#30001;ChatGPT&#29983;&#25104;&#30340;&#21307;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Large language models such as ChatGPT are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the Internet. However, medical texts such as clinical notes and diagnoses require rigorous validation, and erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.  Objective: This research is among the first studies on responsible and ethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus on analyzing the differences between medical texts written by human experts and generated by ChatGPT, and designing machine learning workflows to effectively detect and differentiate medical texts generated by ChatGPT.  Methods: We first construct a suite of datasets containing medical texts written by human experts and generated by ChatGPT. In the next step, we analyze the linguistic features o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#36880;&#20010;&#35299;&#20915;&#23376;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Text-to-SQL&#25552;&#31034;&#26041;&#27861;&#30340;&#33539;&#20363;&#65292;&#36816;&#29992;&#20110;LLM&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#20854;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11556</link><description>&lt;p&gt;
&#20998;&#32780;&#27835;&#20043;&#65292;&#24605;&#32500;&#38142;&#25351;&#23548;&#19979;&#30340;Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
Divide and Prompt: Chain of Thought Prompting for Text-to-SQL. (arXiv:2304.11556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#36880;&#20010;&#35299;&#20915;&#23376;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Text-to-SQL&#25552;&#31034;&#26041;&#27861;&#30340;&#33539;&#20363;&#65292;&#36816;&#29992;&#20110;LLM&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#20854;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#24050;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;Text-to-SQL&#26159;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SQL&#35821;&#21477;&#30340;&#20851;&#38190;&#35821;&#20041;&#20998;&#26512;&#20219;&#21153;&#65292;&#28041;&#21450;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20351;&#29992;&#24605;&#32500;&#38142;&#25351;&#23548;&#26469;&#28608;&#27963;LLM&#22312;Text-to-SQL&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Text-to-SQL&#25552;&#31034;&#26041;&#27861;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#65292;&#36890;&#36807;&#20808;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#24605;&#32500;&#38142;&#36880;&#20010;&#35299;&#20915;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;3&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;LLM&#30340;Text-to-SQL&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLM&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#25191;&#34892;&#20934;&#30830;&#24615;&#30340;Text-to-SQL&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#12290;&#26412;&#32508;&#36848;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11534</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#12290;&#26412;&#32508;&#36848;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#22522;&#26412;&#21644;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#37319;&#29992;&#20102;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#25429;&#33719;&#20102;&#21333;&#35789;&#12289;&#25991;&#26723;&#21644;&#35821;&#26009;&#24211;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#26412;&#32508;&#36848;&#23558;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#38500;&#20102;&#25216;&#26415;&#32508;&#36848;&#65292;&#25105;&#20204;&#36824;&#20851;&#27880;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#21576;&#29616;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#24635;&#32467;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2304.11520</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#27969;&#34892;&#20808;&#36827;&#25216;&#26415;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#24378;&#65292;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#21644;&#33021;&#37327;&#65292;&#32463;&#24120;&#38459;&#30861;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#26102;&#12289;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37096;&#32626;&#12290;&#29616;&#26377;&#30340;BERT&#36731;&#37327;&#32423;&#29256;&#26412;&#65288;&#20363;&#22914;DistilBERT&#21644;TinyBERT&#65289;&#36890;&#24120;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#26080;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20174;&#35774;&#35745;&#24072;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35201;&#20026;&#29305;&#23450;&#30340;NLP&#20219;&#21153;&#20351;&#29992;&#20309;&#31181;&#8220;&#27491;&#30830;&#30340;&#8221;&#22522;&#20110;BERT&#30340;&#26550;&#26500;&#65292;&#20197;&#22312;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#30340;&#26368;&#23567;&#31934;&#24230;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;&#31995;&#32479;&#24037;&#31243;&#24072;&#24517;&#39035;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#35797;&#38169;&#23454;&#39564;&#65292;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#22312;&#19981;&#21516;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#31934;&#24230;&#39044;&#31639;&#19979;&#23545;BERT-based&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#30740;&#31350;&#65292;&#20197;&#24471;&#20986;&#26377;&#20851;&#27492;&#36164;&#28304;/&#31934;&#24230;&#26435;&#34913;&#30340;&#32463;&#39564;&#24615;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#26356;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#30456;&#23545;BERT-base&#21344;&#29992;&#30340;&#20869;&#23384;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#20013;&#31934;&#24230;&#30340;&#19979;&#38477;&#26159;&#26126;&#26174;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;ResNet&#30340;BERT&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#37096;&#32626;&#30340;&#33391;&#22909;&#20505;&#36873;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer's perspective, it is unclear what is the "right" BERT-based architecture to use for a given NLP task that can strike the optimal trade-off between the resources available and the minimum accuracy desired by the end user. System engineers have to spend a lot of time conducting trial-and-error experiments to find a suitable answer to this question. This paper presents an exploratory study of BERT-based models under different resource constraints and accuracy budgets to derive empirical observations about this resource/accuracy trade-offs. Our findin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#20316;&#20026;&#36328;&#35821;&#35328;&#26041;&#26696;&#21487;&#20197;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.11501</link><description>&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Translationese Reduction using Abstract Meaning Representation. (arXiv:2304.11501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11501
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#20316;&#20026;&#36328;&#35821;&#35328;&#26041;&#26696;&#21487;&#20197;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#25991;&#26412;&#25110;&#35805;&#35821;&#20855;&#26377;&#19982;&#26412;&#22320;&#35821;&#35328;&#19981;&#21516;&#30340;&#20960;&#20010;&#26126;&#26174;&#29305;&#24449;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#32763;&#35793;&#35821;&#35328;&#23398;&#65292;&#24050;&#32463;&#26377;&#20102;&#24456;&#22909;&#30340;&#25991;&#29486;&#35760;&#24405;&#65292;&#24403;&#20986;&#29616;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#38598;&#20013;&#26102;&#65292;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#25991;&#26412;&#20013;&#32763;&#35793;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#20551;&#35774;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#20316;&#20026;&#19968;&#31181;&#20174;&#34920;&#38754;&#24418;&#24335;&#25277;&#35937;&#20986;&#26469;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20316;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#25968;&#37327;&#30340;&#19968;&#31181;&#36328;&#35821;&#35328;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558;&#33521;&#25991;&#32763;&#35793;&#35299;&#26512;&#25104;AMR&#22270;&#65292;&#28982;&#21518;&#20174;&#35813;AMR&#29983;&#25104;&#25991;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#25509;&#36817;&#38750;&#32763;&#35793;&#35821;&#35328;&#25991;&#26412;&#30340;&#25991;&#26412;&#65292;&#36890;&#36807;&#23439;&#35266;&#32423;&#21035;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;AMR&#20316;&#20026;&#36328;&#35821;&#35328;&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#22522;&#20110;&#24448;&#36820;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#21477;&#27861;&#25511;&#21046;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translated texts or utterances bear several hallmarks distinct from texts originating in the language. This phenomenon, known as translationese, is well-documented, and when found in training or test sets can affect model performance. Still, work to mitigate the effect of translationese in human translated text is understudied. We hypothesize that Abstract Meaning Representation (AMR), a semantic representation which abstracts away from the surface form, can be used as an interlingua to reduce the amount of translationese in translated texts. By parsing English translations into an AMR graph and then generating text from that AMR, we obtain texts that more closely resemble non-translationese by macro-level measures. We show that across four metrics, and qualitatively, using AMR as an interlingua enables the reduction of translationese and we compare our results to two additional approaches: one based on round-trip machine translation and one based on syntactically controlled generation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11490</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#24565;&#12289;&#30446;&#26631;&#21644;&#24515;&#29702;&#29366;&#24577;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#30340;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#39640;LLM&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#65289;&#30340;ToM&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#23427;&#20204;&#30340;ToM&#29702;&#35299;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20004;&#27493;&#24605;&#32500;&#25512;&#29702;&#21644;&#36880;&#27493;&#24605;&#32771;&#35828;&#26126;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;LLMs&#65288;&#38500;Davinci-2&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;ToM&#20934;&#30830;&#24615;&#12290;GPT-4&#22312;&#38646;&#36718;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#36817;80%&#30340;ToM&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#19981;&#36275;&#27979;&#35797;&#38598;&#19978;87%&#30340;&#20154;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#30340;ToM&#20934;&#30830;&#24615;&#26174;&#33879;&#39640;&#20110;&#26080;&#25552;&#31034;&#26102;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;GPT-3.5-Turbo&#65289;&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#22312;&#22797;&#26434;&#25512;&#29702;&#23588;&#20854;&#26159;ToM&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26377;&#25928;&#35782;&#21035;&#20102;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#36164;&#28304;&#24110;&#21161;&#30340;&#20154;&#32676;&#65292;&#25299;&#23637;&#20102;&#31038;&#21306;&#25104;&#21592;&#29031;&#39038;&#30340;&#33539;&#30068;&#12290;</title><link>http://arxiv.org/abs/2304.11485</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#24110;&#21161;&#31038;&#21306;&#25104;&#21592;&#30340;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#30340;&#35789;&#27719;&#20559;&#24046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Lexical Biases when Identifying Gang-related Social Media Communications. (arXiv:2304.11485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26377;&#25928;&#35782;&#21035;&#20102;&#24110;&#27966;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#36164;&#28304;&#24110;&#21161;&#30340;&#20154;&#32676;&#65292;&#25299;&#23637;&#20102;&#31038;&#21306;&#25104;&#21592;&#29031;&#39038;&#30340;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#24110;&#27966;&#27963;&#21160;&#30340;&#20154;&#20351;&#29992;&#21253;&#25324;Facebook&#21644;Twitter&#22312;&#20869;&#30340;&#20027;&#27969;&#31038;&#20132;&#23186;&#20307;&#26469;&#34920;&#36798;&#22066;&#35773;&#21644;&#23041;&#32961;&#20197;&#21450;&#21696;&#24764;&#21644;&#32426;&#24565;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#24110;&#27966;&#30456;&#20851;&#27963;&#21160;&#30340;&#24433;&#21709;&#20197;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20026;&#31038;&#21306;&#25104;&#21592;&#25552;&#20379;&#24110;&#21161;&#26159;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#30340;&#65292;&#36825;&#21253;&#25324;&#36947;&#24503;&#19978;&#35782;&#21035;&#21463;&#24110;&#27966;&#27963;&#21160;&#24433;&#21709;&#30340;&#20010;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#38656;&#35201;&#32771;&#34385;&#36825;&#20123;&#20010;&#20307;&#22312;&#25512;&#25991;&#20013;&#24120;&#29992;&#30340;&#38750;&#26631;&#20934;&#35821;&#35328;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#31038;&#21306;&#29031;&#39038;&#36164;&#28304;&#65292;&#22914;&#39038;&#38382;&#12289;&#20914;&#31361;&#35843;&#35299;&#32773;&#25110;&#23398;&#26415;/&#19987;&#19994;&#22521;&#35757;&#35745;&#21010;&#30340;&#20010;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20108;&#20803;&#36923;&#36753;&#20998;&#31867;&#22120;&#22312;&#35782;&#21035;&#21463;&#24110;&#27966;&#30456;&#20851;&#26292;&#21147;&#24433;&#21709;&#30340;&#20010;&#20307;&#26102;&#65292;&#22312;&#20351;&#29992;&#19982;2015&#24180;&#24052;&#23572;&#30340;&#25705;&#26292;&#21160;&#30456;&#20851;&#30340;&#24110;&#27966;&#30456;&#20851;&#25512;&#25991;&#26679;&#26412;&#26102;&#65292;&#20248;&#20110;&#22522;&#32447;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals involved in gang-related activity use mainstream social media including Facebook and Twitter to express taunts and threats as well as grief and memorializing. However, identifying the impact of gang-related activity in order to serve community member needs through social media sources has a unique set of challenges. This includes the difficulty of ethically identifying training data of individuals impacted by gang activity and the need to account for a non-standard language style commonly used in the tweets from these individuals. Our study provides evidence of methods where natural language processing tools can be helpful in efficiently identifying individuals who may be in need of community care resources such as counselors, conflict mediators, or academic/professional training programs. We demonstrate that our binary logistic classifier outperforms baseline standards in identifying individuals impacted by gang-related violence using a sample of gang-related tweets associ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.11473</link><description>&lt;p&gt;
(&#21521;&#37327;)&#31354;&#38388;&#19981;&#26159;&#26368;&#21518;&#30340;&#30086;&#22495;&#65306;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24040;&#39069;&#25237;&#36164;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#34429;&#28982;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20027;&#23472;&#20102;&#20135;&#21697;&#25628;&#32034;&#20013;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#21521;&#37327;&#21270;&#26412;&#36523;&#20063;&#21457;&#29983;&#20102;&#24040;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20197;&#30456;&#21453;&#30340;&#26041;&#24335;&#20027;&#24352;&#65292;&#21363;&#31243;&#24207;&#21512;&#25104;&#23545;&#35768;&#22810;&#26597;&#35810;&#21644;&#24066;&#22330;&#20013;&#30340;&#22823;&#37327;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#34892;&#19994;&#37325;&#35201;&#24615;&#65292;&#27010;&#36848;&#20102;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#22522;&#20110;&#25105;&#20204;&#22312;Tooso&#26500;&#24314;&#31867;&#20284;&#31995;&#32479;&#30340;&#32463;&#39564;&#65292;&#22238;&#31572;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#21453;&#23545;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#25945;&#31243;&#21644;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#21452;&#21521;RNN&#12289;&#21452;&#21521;LSTM&#21644;ELMo&#32593;&#32476;&#31561;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11461</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65306;&#25945;&#31243;&#21644;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial and Survey. (arXiv:2304.11461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#25945;&#31243;&#21644;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#21452;&#21521;RNN&#12289;&#21452;&#21521;LSTM&#21644;ELMo&#32593;&#32476;&#31561;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#25945;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#21160;&#24577;&#31995;&#32479;&#21644;RNN&#30340;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#24320;&#22987;&#35762;&#36848;&#65292;&#28982;&#21518;&#35752;&#35770;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25509;&#36817;&#21333;&#20301;&#26435;&#37325;&#30697;&#38453;&#12289;&#38271;&#24310;&#36831;&#12289;&#27844;&#28431;&#21333;&#20803;&#21644;&#22238;&#38899;&#29366;&#24577;&#32593;&#32476;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LSTM&#38376;&#21644;&#21333;&#20803;&#12289;LSTM&#30340;&#21382;&#21490;&#21644;&#21464;&#20307;&#20197;&#21450;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#21452;&#21521;RNN&#12289;&#21452;&#21521;LSTM&#21644;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;&#65288;ELMo&#65289;&#32593;&#32476;&#65292;&#20197;&#22312;&#20004;&#20010;&#26041;&#21521;&#19978;&#22788;&#29702;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a tutorial paper on Recurrent Neural Network (RNN), Long Short-Term Memory Network (LSTM), and their variants. We start with a dynamical system and backpropagation through time for RNN. Then, we discuss the problems of gradient vanishing and explosion in long-term dependencies. We explain close-to-identity weight matrix, long delays, leaky units, and echo state networks for solving this problem. Then, we introduce LSTM gates and cells, history and variants of LSTM, and Gated Recurrent Units (GRU). Finally, we introduce bidirectional RNN, bidirectional LSTM, and the Embeddings from Language Model (ELMo) network, for processing a sequence in both directions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#35821;&#26009;&#24211;&#23558;BERT&#27169;&#22411;&#36716;&#25442;&#25104;SBERT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#27431;&#35821;&#35328;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11434</link><description>&lt;p&gt;
L3Cube-IndicSBERT: &#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23398;&#20064;&#36328;&#35821;&#35328;&#21477;&#23376;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#35821;&#26009;&#24211;&#23558;BERT&#27169;&#22411;&#36716;&#25442;&#25104;SBERT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#27431;&#35821;&#35328;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#21477;&#23376;BERT (SBERT) &#27169;&#22411;&#23558;&#19981;&#21516;&#35821;&#35328;&#26144;&#23556;&#21040;&#20849;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#23545;&#20110;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#35821;&#26009;&#24211;&#23558;&#26222;&#36890;&#30340;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36716;&#25442;&#25104;&#22810;&#35821;&#35328;&#21477;&#23376;BERT&#27169;&#22411;&#12290;&#25105;&#20204;&#31616;&#21333;&#22320;&#32858;&#21512;&#20302;&#36164;&#28304;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793; NLI &#25110; STS &#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26222;&#36890;&#30340;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36827;&#34892;&#31867;&#20284;SBERT&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#20855;&#26377;&#20869;&#22312;&#30340;&#36328;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#24494;&#35843;&#26041;&#27861;&#27809;&#26377;&#26174;&#24335;&#30340;&#36328;&#35821;&#35328;&#35757;&#32451;&#65292;&#21364;&#20135;&#29983;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#38750;&#21360;&#27431;&#35821;&#35328;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;L3Cube-IndicSBERT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21360;&#24230;&#35821;&#35328;&#21360;&#22320;&#35821;&#21644;&#39532;&#26469;&#35821;&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#22312;&#21326;&#35821;&#20013;&#36861;&#27714;&#20381;&#36182;&#38271;&#24230;&#26497;&#23567;&#21270;&#30340;&#20570;&#27861;&#65292;&#21457;&#29616;&#20165;&#23558;&#26368;&#30701;&#30340;&#21069;&#32622;&#25104;&#20998;&#25918;&#32622;&#22312;&#20027;&#35859;&#21160;&#35789;&#26049;&#36793;&#21487;&#20197;&#35299;&#37322;&#35789;&#24207;&#20559;&#22909;&#65292;&#26222;&#21450;&#35813;&#26041;&#27861;&#21487;&#35270;&#20026;&#19968;&#31181;&#26368;&#23567;&#21270;&#21162;&#21147;&#31574;&#30053;&#65292;&#31526;&#21512;&#26377;&#38480;&#29702;&#24615;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11410</link><description>&lt;p&gt;
&#21326;&#35821;&#20013;&#20381;&#36182;&#38271;&#24230;&#26497;&#23567;&#21270;&#30340;&#26377;&#38480;&#29702;&#24615;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A bounded rationality account of dependency length minimization in Hindi. (arXiv:2304.11410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#22312;&#21326;&#35821;&#20013;&#36861;&#27714;&#20381;&#36182;&#38271;&#24230;&#26497;&#23567;&#21270;&#30340;&#20570;&#27861;&#65292;&#21457;&#29616;&#20165;&#23558;&#26368;&#30701;&#30340;&#21069;&#32622;&#25104;&#20998;&#25918;&#32622;&#22312;&#20027;&#35859;&#21160;&#35789;&#26049;&#36793;&#21487;&#20197;&#35299;&#37322;&#35789;&#24207;&#20559;&#22909;&#65292;&#26222;&#21450;&#35813;&#26041;&#27861;&#21487;&#35270;&#20026;&#19968;&#31181;&#26368;&#23567;&#21270;&#21162;&#21147;&#31574;&#30053;&#65292;&#31526;&#21512;&#26377;&#38480;&#29702;&#24615;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#38271;&#24230;&#26497;&#23567;&#21270;&#21407;&#21017;&#26088;&#22312;&#25226;&#21477;&#23376;&#20013;&#35821;&#27861;&#20851;&#32852;&#30340;&#21333;&#35789;&#25918;&#22312;&#25509;&#36817;&#30340;&#20301;&#32622;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#26377;&#25928;&#30340;&#27807;&#36890;&#26469;&#35828;&#65292;&#36825;&#31181;&#21407;&#21017;&#22312;&#22609;&#36896;&#20154;&#31867;&#35821;&#35328;&#32467;&#26500;&#26102;&#26222;&#36941;&#36215;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35821;&#35328;&#31995;&#32479;&#20013;&#24212;&#29992;&#20381;&#36182;&#38271;&#24230;&#26497;&#23567;&#21270;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#21477;&#23376;&#20013;&#65292;&#38271;&#26102;&#38480;&#35789;&#20803;&#25918;&#22312;&#30701;&#26102;&#38480;&#35789;&#20803;&#20043;&#21069;&#65292;&#30701;&#26102;&#38480;&#35789;&#20803;&#25918;&#22312;&#38271;&#26102;&#38480;&#35789;&#20803;&#20043;&#21069;&#24050;&#30693;&#21487;&#20197;&#26368;&#23567;&#21270;&#21477;&#23376;&#30340;&#25972;&#20307;&#20381;&#36182;&#38271;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31181;&#20551;&#35774;&#65306;&#22312;&#21326;&#35821;&#65288;&#19968;&#31181;SOV&#35821;&#35328;&#65289;&#20013;&#65292;&#23558;&#21482;&#26377;&#26368;&#30701;&#30340;&#21069;&#32622;&#25104;&#20998;&#25918;&#32622;&#22312;&#20027;&#35859;&#21160;&#35789;&#26049;&#36793;&#21487;&#20197;&#35299;&#37322;&#35789;&#24207;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#26368;&#23567;&#21270;&#20381;&#36182;&#38271;&#24230;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#25551;&#36848;&#20026;&#26368;&#23567;&#21270;&#21162;&#21147;&#31574;&#30053;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#21160;&#35789;&#21644;&#20854;&#21069;&#32622;&#20381;&#36182;&#20043;&#38388;&#30340;&#25152;&#26377;&#20381;&#36182;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#31526;&#21512;&#26377;&#38480;&#29702;&#24615;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of DEPENDENCY LENGTH MINIMIZATION, which seeks to keep syntactically related words close in a sentence, is thought to universally shape the structure of human languages for effective communication. However, the extent to which dependency length minimization is applied in human language systems is not yet fully understood. Preverbally, the placement of long-before-short constituents and postverbally, short-before-long constituents are known to minimize overall dependency length of a sentence. In this study, we test the hypothesis that placing only the shortest preverbal constituent next to the main-verb explains word order preferences in Hindi (a SOV language) as opposed to the global minimization of dependency length. We characterize this approach as a least-effort strategy because it is a cost-effective way to shorten all dependencies between the verb and its preverbal dependencies. As such, this approach is consistent with the bounded-rationality perspective according t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LaMP&#65288;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#30340;&#20010;&#24615;&#21270;&#22522;&#20934;&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19971;&#39033;&#20010;&#24615;&#21270;&#20219;&#21153;&#20197;&#21450;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21033;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#29983;&#25104;&#32467;&#26524;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11406</link><description>&lt;p&gt;
LaMP&#65306;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LaMP&#65288;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#30340;&#20010;&#24615;&#21270;&#22522;&#20934;&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19971;&#39033;&#20010;&#24615;&#21270;&#20219;&#21153;&#20197;&#21450;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21033;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#29983;&#25104;&#32467;&#26524;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#39046;&#22495;&#30340;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LaMP&#22522;&#20934;&#8212;&#8212;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#29983;&#25104;&#20010;&#24615;&#21270;&#36755;&#20986;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20856;&#33539;&#12290;LaMP&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#20219;&#21153;&#21644;&#27599;&#20010;&#29992;&#25143;&#30340;&#22810;&#20010;&#26465;&#30446;&#65292;&#21253;&#25324;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#21644;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#19971;&#20010;&#20010;&#24615;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20174;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20013;&#26816;&#32034;&#20010;&#24615;&#21270;&#39033;&#30446;&#65292;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#38646;-shot&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#20010;&#20154;&#36164;&#26009;&#25193;&#23637;&#30340;LM&#20248;&#20110;&#19981;&#32771;&#34385;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35266;&#23519;&#32422;20&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21518;&#33021;&#22815;&#25552;&#20379;&#26368;&#20339;&#25311;&#21512;&#65292;&#20854;surprisal&#20272;&#35745;&#33021;&#21147;&#33021;&#22815;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#21464;&#20307;&#20250;&#20986;&#29616;&#8220;&#20020;&#30028;&#28857;&#8221;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#22256;&#24785;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23548;&#33268;&#36739;&#24046;&#30340;&#20154;&#31867;&#25311;&#21512;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11389</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;Surprisal&#26368;&#20339;&#30340;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#30340;&#35757;&#32451;&#20196;&#29260;&#25968;&#32422;&#20026;20&#20159;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35266;&#23519;&#32422;20&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21518;&#33021;&#22815;&#25552;&#20379;&#26368;&#20339;&#25311;&#21512;&#65292;&#20854;surprisal&#20272;&#35745;&#33021;&#21147;&#33021;&#22815;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#21464;&#20307;&#20250;&#20986;&#29616;&#8220;&#20020;&#30028;&#28857;&#8221;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#22256;&#24785;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23548;&#33268;&#36739;&#24046;&#30340;&#20154;&#31867;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#19982;&#20854;surprisal&#20272;&#35745;&#33021;&#21147;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#30340;&#20851;&#31995;&#24471;&#20986;&#20102;&#30456;&#20114;&#30683;&#30462;&#30340;&#32467;&#35770;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#30740;&#31350;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#24040;&#22823;&#24046;&#36317;&#25152;&#33268;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#20307;&#30340;Surprisal&#20272;&#35745;&#33021;&#21147;&#65292;&#36825;&#20123;&#21464;&#20307;&#22312;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23481;&#37327;&#26041;&#38754;&#26377;&#31995;&#32479;&#21464;&#21270;&#65292;&#20197;&#25972;&#21512;&#36825;&#20123;&#21457;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20855;&#26377;&#29616;&#20195;&#27169;&#22411;&#23481;&#37327;&#30340;&#21464;&#20307;&#30340;Surprisal&#20272;&#35745;&#22312;&#35266;&#23519;&#32422;20&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21518;&#25552;&#20379;&#26368;&#20339;&#25311;&#21512;&#65292;&#27492;&#21518;&#23427;&#20204;&#24320;&#22987;&#20559;&#31163;&#19982;&#20154;&#31867;&#26399;&#26395;&#30456;&#31526;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#26032;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#21464;&#20307;&#22312;&#25910;&#25947;&#26102;&#26174;&#31034;&#20986;&#8220;&#20020;&#30028;&#28857;&#8221;&#65292;&#22312;&#27492;&#20043;&#21518;&#65292;&#35821;&#35328;&#27169;&#22411;&#22256;&#24785;&#24230;&#30340;&#19979;&#38477;&#24320;&#22987;&#23548;&#33268;&#36739;&#24046;&#30340;&#20154;&#31867;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;SAILER&#65292;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#38271;&#25991;&#26412;&#24207;&#21015;&#21644;&#20851;&#38190;&#27861;&#24459;&#35201;&#32032;&#25935;&#24863;&#38382;&#39064;&#65292;&#37319;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#21644;&#32467;&#26500;&#24863;&#30693;&#36830;&#36143;&#24615;&#39044;&#27979;&#20219;&#21153;&#30456;&#32467;&#21512;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11370</link><description>&lt;p&gt;
SAILER: &#38754;&#21521;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval. (arXiv:2304.11370v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;SAILER&#65292;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#38271;&#25991;&#26412;&#24207;&#21015;&#21644;&#20851;&#38190;&#27861;&#24459;&#35201;&#32032;&#25935;&#24863;&#38382;&#39064;&#65292;&#37319;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#21644;&#32467;&#26500;&#24863;&#30693;&#36830;&#36143;&#24615;&#39044;&#27979;&#20219;&#21153;&#30456;&#32467;&#21512;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#24037;&#20316;&#8212;&#8212;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;SAILER&#12290;&#19982;&#36890;&#29992;&#25991;&#26723;&#30456;&#27604;&#65292;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#36890;&#24120;&#20855;&#26377;&#22266;&#26377;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#24182;&#21253;&#21547;&#20851;&#38190;&#30340;&#27861;&#24459;&#35201;&#32032;&#12290;SAILER&#37319;&#29992;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#21644;&#36866;&#29992;&#20110;&#27861;&#24459;&#25991;&#26723;&#30340;&#32467;&#26500;&#24863;&#30693;&#36830;&#36143;&#24615;&#39044;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAILER&#22312;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case retrieval, which aims to find relevant cases for a query case, plays a core role in the intelligent legal system. Despite the success that pre-training has achieved in ad-hoc retrieval tasks, effective pre-training strategies for legal case retrieval remain to be explored. Compared with general documents, legal case documents are typically long text sequences with intrinsic logical structures. However, most existing language models have difficulty understanding the long-distance dependencies between different structures. Moreover, in contrast to the general retrieval, the relevance in the legal domain is sensitive to key legal elements. Even subtle differences in key legal elements can significantly affect the judgement of relevance. However, existing pre-trained language models designed for general purposes have not been equipped to handle legal elements.  To address these issues, in this paper, we propose SAILER, a new Structure-Aware pre-traIned language model for LEgal c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#35821;&#35328;&#23545;&#25239;&#35757;&#32451;&#21644;&#27178;&#21521;&#25233;&#21046;&#25216;&#26415;&#23545;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#36827;&#34892;&#33258;&#21160;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#24050;&#26377;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#22810;&#35789;&#34920;&#36798;&#19978;&#30340;F1&#20998;&#25968;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.11350</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#35821;&#35328;&#23545;&#25239;&#35757;&#32451;&#21644;&#27178;&#21521;&#25233;&#21046;&#25216;&#26415;&#30340;&#26041;&#27861;&#20248;&#21270;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Romanian Multiword Expression Detection Using Multilingual Adversarial Training and Lateral Inhibition. (arXiv:2304.11350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#35821;&#35328;&#23545;&#25239;&#35757;&#32451;&#21644;&#27178;&#21521;&#25233;&#21046;&#25216;&#26415;&#23545;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#36827;&#34892;&#33258;&#21160;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#24050;&#26377;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#22810;&#35789;&#34920;&#36798;&#19978;&#30340;F1&#20998;&#25968;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35789;&#34920;&#36798;&#26159;&#24320;&#21457;&#22823;&#35268;&#27169;&#12289;&#35821;&#35328;&#23398;&#19978;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;PARSEM v1.2&#20849;&#20139;&#20219;&#21153;&#35821;&#26009;&#24211;&#19978;&#33258;&#21160;&#35782;&#21035;&#32599;&#39532;&#23612;&#20122;&#35821;&#22810;&#35789;&#34920;&#36798;&#26041;&#38754;&#25152;&#20570;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#27178;&#21521;&#25233;&#21046;&#23618;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35270;&#35282;&#65292;&#20197;&#25552;&#39640;&#25152;&#37319;&#29992;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;XLM-RoBERTa&#22312;&#26410;&#35265;&#36807;&#30340;&#22810;&#35789;&#34920;&#36798;&#26041;&#38754;&#30340;F1&#20998;&#25968;&#65292;&#20063;&#23601;&#26159;PARSEME 1.2&#29256;&#26412;&#30340;&#20027;&#35201;&#20219;&#21153;&#65292;&#32422;&#20026;2.7%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#20204;&#36229;&#36807;&#20102;&#26412;&#27425;&#27604;&#36187;&#20013;&#21442;&#36187;&#32773;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#26041;&#38754;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiword expressions are a key ingredient for developing large-scale and linguistically sound natural language processing technology. This paper describes our improvements in automatically identifying Romanian multiword expressions on the corpus released for the PARSEME v1.2 shared task. Our approach assumes a multilingual perspective based on the recently introduced lateral inhibition layer and adversarial training to boost the performance of the employed multilingual language models. With the help of these two methods, we improve the F1-score of XLM-RoBERTa by approximately 2.7% on unseen multiword expressions, the main task of the PARSEME 1.2 edition. In addition, our results can be considered SOTA performance, as they outperform the previous results on Romanian obtained by the participants in this competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#30693;&#35782;&#30340;&#35789;&#20041;&#28040;&#27495;&#35821;&#20041;&#19987;&#19994;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#19978;&#19979;&#25991;&#23884;&#20837;&#21521;&#37327;&#65292;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#8220;sense&#8221;&#21644;&#19978;&#19979;&#25991;&#24444;&#27492;&#21152;&#36817;&#65292;&#23558;&#19981;&#30456;&#20851;&#30340;&#8220;sense&#8221;&#24444;&#27492;&#36828;&#31163;&#12290;</title><link>http://arxiv.org/abs/2304.11340</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#19987;&#19994;&#21270;&#30340;&#30693;&#35782;&#39537;&#21160;&#35789;&#20041;&#28040;&#27495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Specialization for Knowledge-based Word Sense Disambiguation. (arXiv:2304.11340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#30693;&#35782;&#30340;&#35789;&#20041;&#28040;&#27495;&#35821;&#20041;&#19987;&#19994;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#19978;&#19979;&#25991;&#23884;&#20837;&#21521;&#37327;&#65292;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#8220;sense&#8221;&#21644;&#19978;&#19979;&#25991;&#24444;&#27492;&#21152;&#36817;&#65292;&#23558;&#19981;&#30456;&#20851;&#30340;&#8220;sense&#8221;&#24444;&#27492;&#36828;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#8220;sense&#8221;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#21521;&#37327;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#26159;&#30446;&#21069;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#30693;&#35782;&#30340;&#35789;&#20041;&#28040;&#27495;&#35821;&#20041;&#19987;&#19994;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#19978;&#19979;&#25991;&#23884;&#20837;&#21521;&#37327;&#65292;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#8220;sense&#8221;&#21644;&#19978;&#19979;&#25991;&#24444;&#27492;&#21152;&#36817;&#65292;&#23558;&#19981;&#30456;&#20851;&#30340;&#8220;sense&#8221;&#24444;&#27492;&#36828;&#31163;&#12290;&#37319;&#29992;Attract-Repel&#20248;&#21270;&#26041;&#24335;&#20248;&#21270;sense&#23545;&#65292;&#37319;&#29992;&#33258;&#35757;&#32451;&#30340;&#26041;&#24335;&#20248;&#21270;context-sense&#23545;&#65292;&#24182;&#25511;&#21046;&#23884;&#20837;&#21521;&#37327;&#19982;&#21407;&#26469;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#20013;&#25991;&#19977;&#31181;&#35821;&#35328;&#30340;&#30693;&#35782;&#39537;&#21160;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the \textit{sense} and \textit{context} embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.11292</link><description>&lt;p&gt;
&#20851;&#20110;&#24212;&#29992;&#35780;&#35770;&#20013;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On the Identification of the Energy related Issues from the App Reviews. (arXiv:2304.11292v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#30340;&#19981;&#21516;&#25216;&#26415;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#31243;&#24207;&#30340;&#33021;&#28304;&#25928;&#29575;&#38382;&#39064;&#21487;&#33021;&#20250;&#23545;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#36896;&#25104;&#37325;&#22823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24212;&#29992;&#21830;&#24215;&#24191;&#27867;&#35752;&#35770;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30740;&#31350;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#20197;&#30830;&#23450;&#33021;&#28304;&#30456;&#20851;&#29992;&#25143;&#21453;&#39304;&#30340;&#20027;&#35201;&#21407;&#22240;&#25110;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#26377;&#25928;&#22320;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#35780;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#25552;&#21462;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#21644;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19982;&#30456;&#20851;&#29305;&#24449;&#32452;&#21512;&#21644;&#30456;&#23545;&#36739;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#24635;&#20849;&#27604;&#36739;&#20102;60&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20845;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#19977;&#31181;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#26500;&#24314;&#30340;30&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#35813;&#24037;&#20855;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36941;&#21382;&#36825;&#20010;&#22823;&#35268;&#27169;&#30340;&#32467;&#26524;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The energy inefficiency of the apps can be a major issue for the app users which is discussed on App Stores extensively. Previous research has shown the importance of investigating the energy related app reviews to identify the major causes or categories of energy related user feedback. However, there is no study that efficiently extracts the energy related app reviews automatically. In this paper, we empirically study different techniques for automatic extraction of the energy related user feedback. We compare the accuracy, F1-score and run time of numerous machine-learning models with relevant feature combinations and relatively modern Neural Network-based models. In total, 60 machine learning models are compared to 30 models that we build using six neural network architectures and three word embedding models. We develop a visualization tool for this study through which a developer can traverse through this large-scale result set. The results show that neural networks outperform the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#21019;&#24847;&#20889;&#20316;&#26041;&#38754;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#24110;&#21161;&#23398;&#29983;&#20316;&#23478;&#26356;&#26377;&#21019;&#36896;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#19981;&#21516;&#30340;&#20316;&#29992;&#65292;&#20363;&#22914;&#20316;&#20026;&#21512;&#20316;&#32773;&#12289;&#25361;&#34885;&#32773;&#31561;&#12290;</title><link>http://arxiv.org/abs/2304.11276</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#39321;&#28207;&#20013;&#23398;&#29983;&#21019;&#24847;&#20889;&#20316;&#20013;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students. (arXiv:2304.11276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#39321;&#28207;&#20013;&#23398;&#29983;&#22312;&#21019;&#24847;&#20889;&#20316;&#26041;&#38754;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#24110;&#21161;&#23398;&#29983;&#20316;&#23478;&#26356;&#26377;&#21019;&#36896;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#19981;&#21516;&#30340;&#20316;&#29992;&#65292;&#20363;&#22914;&#20316;&#20026;&#21512;&#20316;&#32773;&#12289;&#25361;&#34885;&#32773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#30340;&#24320;&#21457;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#30340;&#35821;&#35328;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#21019;&#24847;&#20889;&#20316;&#30340;&#26500;&#24605;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#24110;&#21161;&#23398;&#29983;&#20316;&#23478;&#26356;&#26377;&#21019;&#36896;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#19981;&#21516;&#30340;&#20316;&#29992;&#65292;&#20363;&#22914;&#20316;&#20026;&#21512;&#20316;&#32773;&#12289;&#25361;&#34885;&#32773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancement in Natural Language Processing (NLP) capability has led to the development of language models (e.g., ChatGPT) that is capable of generating human-like language. In this study, we explore how language models can be utilized to help the ideation aspect of creative writing. Our empirical findings show that language models play different roles in helping student writers to be more creative, such as the role of a collaborator, a provocateur, etc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#38750;&#27954;14&#20010;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.11256</link><description>&lt;p&gt;
UBC-DLNLP&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#36129;&#29486;&#65306;&#36716;&#31227;&#23398;&#20064;&#23545;&#38750;&#27954;&#24773;&#24863;&#20998;&#26512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis. (arXiv:2304.11256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#38750;&#27954;14&#20010;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023 AfriSenti-SemEval&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#36129;&#29486;&#65292;&#20854;&#20013;&#25105;&#20204;&#35299;&#20915;&#20102;14&#31181;&#19981;&#21516;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#24320;&#21457;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65288;&#23376;&#20219;&#21153;A&#21644;B&#65289;&#12290;&#25105;&#20204;&#36824;&#20026;&#38646;-shot&#35774;&#32622;&#65288;&#23376;&#20219;&#21153;C&#65289;&#24320;&#21457;&#20102;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20845;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#20123;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#35843;&#25972;&#20197;&#21450;&#26368;&#21518;&#30340;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#25928;&#26524;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;70.36&#30340;F1&#20998;&#25968;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;66.13&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#36716;&#31227;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#20854;&#20182;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared task, where we tackle the task of sentiment analysis in 14 different African languages. We develop both monolingual and multilingual models under a full supervised setting (subtasks A and B). We also develop models for the zero-shot setting (subtask C). Our approach involves experimenting with transfer learning using six language models, including further pertaining of some of these models as well as a final finetuning stage. Our best performing models achieve an F1-score of 70.36 on development data and an F1-score of 66.13 on test data. Unsurprisingly, our results demonstrate the effectiveness of transfer learning and fine-tuning techniques for sentiment analysis across multiple languages. Our approach can be applied to other sentiment analysis tasks in different languages and domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#32676;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#23450;&#32676;&#20307;&#30340;&#21382;&#21490;&#21644;&#35821;&#35328;&#30693;&#35782;&#32435;&#20837;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#20013;&#65292;&#20998;&#26512;&#26377;&#20851;&#27495;&#35270;&#21382;&#21490;&#30340;&#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#23545;&#35813;&#32676;&#20307;&#30340;&#20167;&#24680;&#35328;&#35770;&#28608;&#22686;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#24615;&#21644;&#20262;&#29702;&#26631;&#20934;&#23545;&#27169;&#22411;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.11223</link><description>&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#32676;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Group-Specific Approach to NLP for Hate Speech Detection. (arXiv:2304.11223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#32676;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#23450;&#32676;&#20307;&#30340;&#21382;&#21490;&#21644;&#35821;&#35328;&#30693;&#35782;&#32435;&#20837;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#20013;&#65292;&#20998;&#26512;&#26377;&#20851;&#27495;&#35270;&#21382;&#21490;&#30340;&#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#23545;&#35813;&#32676;&#20307;&#30340;&#20167;&#24680;&#35328;&#35770;&#28608;&#22686;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#24615;&#21644;&#20262;&#29702;&#26631;&#20934;&#23545;&#27169;&#22411;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20102;&#35299;&#24120;&#35782;&#12289;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#27495;&#35270;&#21382;&#21490;&#65292;&#36825;&#20123;&#20869;&#23481;&#21487;&#33021;&#20250;&#19981;&#26029;&#28436;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#32676;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23558;&#26377;&#20851;&#29305;&#23450;&#20445;&#25252;&#32676;&#20307;&#30340;&#21382;&#21490;&#21644;&#35821;&#35328;&#30693;&#35782;&#32435;&#20837;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#20013;&#65292;&#20998;&#26512;&#26377;&#20851;&#21463;&#20445;&#25252;&#32676;&#20307;&#27495;&#35270;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#23545;&#35813;&#32676;&#20307;&#30340;&#20167;&#24680;&#35328;&#35770;&#28608;&#22686;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#24615;&#21644;&#20262;&#29702;&#26631;&#20934;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#38024;&#23545;&#21453;&#29369;&#20027;&#20041;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#28436;&#31034;&#36825;&#31181;&#26041;&#27861;&#12290;&#35813;&#26696;&#20363;&#30740;&#31350;&#32508;&#21512;&#20102;&#30446;&#21069;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26816;&#27979;&#21453;&#29369;&#20027;&#20041;&#35328;&#35770;&#30340;&#33521;&#35821;&#25991;&#29486;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#28085;&#30422; 20 &#19990;&#32426;&#33267;&#20170;&#21453;&#29369;&#20027;&#20041;&#21382;&#21490;&#21644;&#35821;&#35328;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic hate speech detection is an important yet complex task, requiring knowledge of common sense, stereotypes of protected groups, and histories of discrimination, each of which may constantly evolve. In this paper, we propose a group-specific approach to NLP for online hate speech detection. The approach consists of creating and infusing historical and linguistic knowledge about a particular protected group into hate speech detection models, analyzing historical data about discrimination against a protected group to better predict spikes in hate speech against that group, and critically evaluating hate speech detection models through lenses of intersectionality and ethics. We demonstrate this approach through a case study on NLP for detection of antisemitic hate speech. The case study synthesizes the current English-language literature on NLP for antisemitism detection, introduces a novel knowledge graph of antisemitic history and language from the 20th century to the present, in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;LOT&#8221;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#35757;&#32451;&#20449;&#21495;&#20013;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#31163;&#25955;&#24230;&#23558;&#29983;&#25104;&#21521;&#37327;&#20174;&#19981;&#23433;&#20840;&#23376;&#31354;&#38388;&#25351;&#21521;&#23433;&#20840;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2304.11220</link><description>&lt;p&gt;
&#23398;&#20064;&#8220;&#19981;&#23398;&#20064;&#8221;: &#26397;&#21521;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#29983;&#25104;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;LOT&#8221;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#35757;&#32451;&#20449;&#21495;&#20013;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#31163;&#25955;&#24230;&#23558;&#29983;&#25104;&#21521;&#37327;&#20174;&#19981;&#23433;&#20840;&#23376;&#31354;&#38388;&#25351;&#21521;&#23433;&#20840;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#27169;&#22411;&#23588;&#20854;&#23481;&#26131;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#22522;&#20110;Web&#30340;&#31038;&#20132;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#12290;&#20808;&#21069;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#23384;&#22312;&#32570;&#28857;&#65292;&#22914;&#25171;&#26029;&#23545;&#35805;&#27969;&#31243;&#12289;&#23545;&#26410;&#35265;&#36807;&#30340;&#26377;&#27602;&#36755;&#20837;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#20026;&#20102;&#23433;&#20840;&#32780;&#29306;&#29298;&#23545;&#35805;&#36136;&#37327;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;LOT&#8221;&#65288;Learn NOT to&#65289;&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#21516;&#26102;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#35757;&#32451;&#20449;&#21495;&#20013;&#23398;&#20064;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#30456;&#36739;&#20110;&#26631;&#20934;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#35821;&#35328;&#20998;&#24067;&#20013;&#33258;&#21160;&#33719;&#24471;&#27491;&#12289;&#36127;&#20449;&#21495;&#12290;LOT&#26694;&#26550;&#21033;&#29992;&#31163;&#25955;&#24230;&#23558;&#29983;&#25104;&#21521;&#37327;&#20174;&#19981;&#23433;&#20840;&#23376;&#31354;&#38388;&#25351;&#21521;&#23433;&#20840;&#23376;&#31354;&#38388;&#65292;&#21516;&#26102;&#32500;&#25345;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#39640;&#65292;&#22312;SafeDialog&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational models that are generative and open-domain are particularly susceptible to generating unsafe content since they are trained on web-based social data. Prior approaches to mitigating this issue have drawbacks, such as disrupting the flow of conversation, limited generalization to unseen toxic input contexts, and sacrificing the quality of the dialogue for the sake of safety. In this paper, we present a novel framework, named "LOT" (Learn NOT to), that employs a contrastive loss to enhance generalization by learning from both positive and negative training signals. Our approach differs from the standard contrastive learning framework in that it automatically obtains positive and negative signals from the safe and unsafe language distributions that have been learned beforehand. The LOT framework utilizes divergence to steer the generations away from the unsafe subspace and towards the safe subspace while sustaining the flow of conversation. Our approach is memory and time-ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36777;&#35777;&#35780;&#20272;&#26041;&#24335;&#65292;&#26088;&#22312;&#25551;&#32472;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#22833;&#36133;&#30340;&#36793;&#30028;&#24182;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23545;LLMs&#36890;&#35782;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.11164</link><description>&lt;p&gt;
&#36777;&#35777;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65306;&#23545;LLMs&#36890;&#35782;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs. (arXiv:2304.11164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36777;&#35777;&#35780;&#20272;&#26041;&#24335;&#65292;&#26088;&#22312;&#25551;&#32472;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#22833;&#36133;&#30340;&#36793;&#30028;&#24182;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23545;LLMs&#36890;&#35782;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#24182;&#19988;&#20854;&#33021;&#21147;&#24471;&#21040;&#20102;&#35768;&#22810;&#36190;&#35465;&#65292;&#21253;&#25324;&#36890;&#35782;&#25512;&#29702;&#12290;&#37492;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#20197;&#24448;&#36890;&#35782;&#25512;&#29702;&#38745;&#24577;&#22522;&#20934;&#19978;&#21462;&#24471;&#36234;&#26469;&#36234;&#22909;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21478;&#31867;&#30340;&#36777;&#35777;&#35780;&#20272;&#26041;&#24335;&#12290;&#36825;&#31181;&#35780;&#20272;&#30340;&#30446;&#26631;&#19981;&#26159;&#33719;&#24471;&#19968;&#20010;&#24635;&#20307;&#24615;&#33021;&#20540;&#65292;&#32780;&#26159;&#25214;&#21040;&#22833;&#36133;&#30340;&#22320;&#26041;&#24182;&#25551;&#32472;&#31995;&#32479;&#30340;&#36793;&#30028;&#12290;&#19982;&#31995;&#32479;&#23545;&#35805;&#21487;&#20197;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#65292;&#24182;&#33719;&#24471;&#36234;&#36807;&#21333;&#19968;&#35777;&#25454;&#30340;&#36793;&#30028;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23601;&#31354;&#38388;&#25512;&#29702;&#65288;&#36890;&#35782;&#25512;&#29702;&#30340;&#22522;&#26412;&#26041;&#38754;&#65289;&#30340;&#29305;&#23450;&#24773;&#20917;&#36827;&#34892;&#20102;&#19968;&#20123;&#23450;&#24615;&#30740;&#31350;&#12290;&#32467;&#35770;&#21253;&#25324;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#19968;&#20123;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#23558;&#36825;&#31181;&#36777;&#35777;&#35780;&#20272;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
&lt;/p&gt;</description></item><item><title>&#24517;&#39035;&#25237;&#36164;&#20110;&#21327;&#20316;&#30340;AI&#23433;&#20840;&#21644;&#20262;&#29702;&#30740;&#31350;&#65292;&#24320;&#21457;&#21487;&#25345;&#32493;&#21644;&#20844;&#27491;&#30340;&#26631;&#20934;&#20197;&#21019;&#24314;&#26377;&#30410;&#20154;&#24037;&#26234;&#33021;&#65292;&#21542;&#21017;&#25105;&#20204;&#23558;&#30475;&#21040;&#25216;&#26415;&#36827;&#27493;&#36229;&#36234;&#25105;&#20204;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#33021;&#21147;&#30340;&#26410;&#26469;&#12290;</title><link>http://arxiv.org/abs/2304.11163</link><description>&lt;p&gt;
ChatGPT&#12289;&#22823;&#35821;&#35328;&#25216;&#26415;&#20197;&#21450;&#36896;&#31119;&#20154;&#31867;&#30340;&#26354;&#25240;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity. (arXiv:2304.11163v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11163
&lt;/p&gt;
&lt;p&gt;
&#24517;&#39035;&#25237;&#36164;&#20110;&#21327;&#20316;&#30340;AI&#23433;&#20840;&#21644;&#20262;&#29702;&#30740;&#31350;&#65292;&#24320;&#21457;&#21487;&#25345;&#32493;&#21644;&#20844;&#27491;&#30340;&#26631;&#20934;&#20197;&#21019;&#24314;&#26377;&#30410;&#20154;&#24037;&#26234;&#33021;&#65292;&#21542;&#21017;&#25105;&#20204;&#23558;&#30475;&#21040;&#25216;&#26415;&#36827;&#27493;&#36229;&#36234;&#25105;&#20204;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#33021;&#21147;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26080;&#30097;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#24403;&#25105;&#20204;&#32570;&#20047;&#23545;&#20110;&#26085;&#30410;&#25193;&#22823;&#30340;&#20840;&#29699;&#19981;&#24179;&#31561;&#21644;&#32039;&#36843;&#30340;&#23384;&#22312;&#23041;&#32961;&#19979;&#20154;&#24615;&#31350;&#31455;&#24212;&#35813;&#26159;&#20160;&#20040;&#30340;&#32454;&#33268;&#29702;&#35299;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23558;&#35753;&#25152;&#26377;&#20154;&#21463;&#30410;&#30340;&#25215;&#35834;&#26159;&#31354;&#27934;&#30340;&#12290;&#20026;&#20102;&#39034;&#21033;&#21069;&#36827;&#65292;&#25105;&#20204;&#38656;&#35201;&#25237;&#36164;&#20110;&#20005;&#35880;&#32780;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#20262;&#29702;&#30740;&#31350;&#65292;&#38656;&#35201;&#24320;&#21457;&#21487;&#25345;&#32493;&#19988;&#20844;&#27491;&#30340;&#26631;&#20934;&#20197;&#21306;&#20998;&#20165;&#20165;&#26159;&#25512;&#27979;&#24615;&#30340;&#21644;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#21482;&#26377;&#21518;&#32773;&#25165;&#33021;&#20351;&#25105;&#20204;&#20849;&#21516;&#26500;&#24314;&#21644;&#25512;&#24191;&#21019;&#36896;&#26377;&#30410;&#20154;&#24037;&#26234;&#33021;&#25152;&#24517;&#38656;&#30340;&#20215;&#20540;&#35266;&#12290;&#19981;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#23548;&#33268;&#26410;&#26469;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#27493;&#36229;&#36807;&#25105;&#20204;&#23548;&#33322;&#23427;&#20204;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#19981;&#24819;&#36208;&#19978;&#36825;&#26465;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The allure of emerging AI technologies is undoubtedly thrilling. However, the promise that AI technologies will benefit all of humanity is empty so long as we lack a nuanced understanding of what humanity is supposed to be in the face of widening global inequality and pressing existential threats. Going forward, it is crucial to invest in rigorous and collaborative AI safety and ethics research. We also need to develop standards in a sustainable and equitable way that differentiate between merely speculative and well-researched questions. Only the latter enable us to co-construct and deploy the values that are necessary for creating beneficial AI. Failure to do so could result in a future in which our AI technological advancements outstrip our ability to navigate their ethical and social implications. This path we do not want to go down.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25991;&#31456;&#26102;&#38388;&#27573;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#30340;&#32467;&#26524;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#23581;&#35797;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10859</link><description>&lt;p&gt;
Text2Time: &#22522;&#20110;Transformer&#30340;&#25991;&#31456;&#26102;&#38388;&#27573;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Text2Time: Transformer-based article time period predictor. (arXiv:2304.10859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25991;&#31456;&#26102;&#38388;&#27573;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#30340;&#32467;&#26524;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#23581;&#35797;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#21033;&#29992;&#25991;&#26412;&#20869;&#23481;&#39044;&#27979;&#25991;&#31456;&#21457;&#34920;&#26102;&#38388;&#27573;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;35&#19975;&#31687;&#12298;&#32445;&#32422;&#26102;&#25253;&#12299;&#21382;&#26102;&#20845;&#21313;&#24180;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#22522;&#20934;&#27169;&#22411;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20154;&#24847;&#26009;&#20043;&#22806;&#30340;&#19981;&#38169;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#24494;&#35843;&#20197;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25105;&#20204;&#30340;&#39044;&#26399;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#38750;&#24120;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20934;&#30830;&#22320;&#23558;&#26032;&#38395;&#25991;&#31456;&#20998;&#31867;&#33267;&#20854;&#20986;&#29256;&#30340;&#24180;&#20195;&#12290;&#32467;&#26524;&#36229;&#36807;&#20102;&#20808;&#21069;&#23581;&#35797;&#30340;&#36825;&#31181;&#30456;&#23545;&#19981;&#21463;&#20851;&#27880;&#30340;&#25991;&#26412;&#39044;&#27979;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the problem of predicting the publication period of text document, such as a news article, using the text from that document. In order to do so, we created our own extensive labeled dataset of over 350,000 news articles published by The New York Times over six decades. We then provide an implementation of a simple Naive Bayes baseline model, which surprisingly achieves decent performance in terms of accuracy.Finally, for our approach, we use a pretrained BERT model fine-tuned for the task of text classification. This model exceeds our expectations and provides some very impressive results in terms of accurately classifying news articles into their respective publication decades. The results beat the performance of the few previously tried models for this relatively unexplored task of time prediction from text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#12289;&#38142;&#25509;&#21644;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10637</link><description>&lt;p&gt;
IXA/Cogcomp&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#12289;&#38142;&#25509;&#21644;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;&#19968;&#39033;&#26680;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687;CoNLL 2003&#31561;&#26631;&#20934;&#22522;&#20934;&#24182;&#27809;&#26377;&#35299;&#20915;&#37096;&#32626;NER&#31995;&#32479;&#38656;&#35201;&#38754;&#23545;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#23545;&#26032;&#20852;&#25110;&#22797;&#26434;&#23454;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NER&#32423;&#32852;&#26041;&#27861;&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#20505;&#36873;&#39033;&#65307;&#20854;&#27425;&#65292;&#23558;&#27599;&#20010;&#20505;&#36873;&#39033;&#38142;&#25509;&#21040;&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#65307;&#31532;&#19977;&#65292;&#39044;&#27979;&#27599;&#20010;&#23454;&#20307;&#20505;&#36873;&#39033;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22806;&#37096;&#30693;&#35782;&#24211;&#22312;&#20934;&#30830;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;MultiCoNER2&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#20063;&#33021;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a core natural language processing task in which pre-trained language models have shown remarkable performance. However, standard benchmarks like CoNLL 2003 \cite{conll03} do not address many of the challenges that deployed NER systems face, such as having to classify emerging or complex entities in a fine-grained way. In this paper we present a novel NER cascade approach comprising three steps: first, identifying candidate entities in the input sentence; second, linking the each candidate to an existing knowledge base; third, predicting the fine-grained category for each entity candidate. We empirically demonstrate the significance of external knowledge bases in accurately classifying fine-grained and emerging entities. Our system exhibits robust performance in the MultiCoNER2 \cite{multiconer2-data} shared task, even in the low-resource language setting where we leverage knowledge bases of high-resource languages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10464</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#36825;&#24341;&#36215;&#20102;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24076;&#26395;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32534;&#31243;&#65292;&#28982;&#21518;&#25353;&#29031;&#31243;&#24207;&#29983;&#25104;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#26469;&#25551;&#36848;&#20219;&#21153;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#26131;&#20110;&#20154;&#31867;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#12290;&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#20294;&#36825;&#20123;&#31243;&#24207;&#21487;&#33021;&#20173;&#28982;&#23384;&#22312;&#38169;&#35823;&#25110;&#19981;&#23436;&#25972;&#30340;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23398;&#20064;&#32534;&#31243;&#65288;LP&#65289;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#22823;&#35821;&#35328;&#27169;&#22411;&#20174;&#22797;&#26434;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#31243;&#24207;&#26469;&#25351;&#23548;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;AMPS&#65288;&#39640;&#20013;&#25968;&#23398;&#65289;&#21644;Math&#65288;&#31454;&#36187;&#25968;&#23398;&#38382;&#39064;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#27979;&#35797;ChatGP&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#26102;&#65292;LP&#33021;&#22815;&#23454;&#29616;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks, which raises hopes for achieving Artificial General Intelligence. To better complete complex tasks, we need LLMs to program for the task and then follow the program to generate a specific solution for the test sample. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs. LLMs are capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program (LP) method to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference. Our experiments on the AMPS (high school math) and Math (competition mathematics problems) datasets demonstrate the effectiveness of our approach. When testing ChatGP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10145</link><description>&lt;p&gt;
ChatGPT&#33021;&#21542;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#65311;&#23545;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#21457;&#24067;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#20195;&#20154;&#31867;&#26234;&#24935;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;ChatGPT&#26159;&#21542;&#26377;&#28508;&#21147;&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#12290;&#36825;&#26679;&#30340;&#25104;&#23601;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31038;&#20132;&#35745;&#31639;&#30740;&#31350;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#37325;&#26032;&#26631;&#35760;&#20102;&#20116;&#20010;&#20855;&#26377;&#37324;&#31243;&#30865;&#24847;&#20041;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#31435;&#22330;&#26816;&#27979;&#65288;2&#20010;&#65289;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26426;&#22120;&#20154;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;ChatGPT&#33719;&#24471;&#20102;&#24179;&#22343;&#31934;&#24230;0.609&#12290; ChatGPT&#23545;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#26368;&#20339;&#65292;&#27491;&#30830;&#27880;&#37322;&#20102;64.9&#65285;&#30340;&#25512;&#25991;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26174;&#31034;&#24615;&#33021;&#22312;&#19981;&#21516;&#26631;&#31614;&#20043;&#38388;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#39033;&#24037;&#20316;&#21487;&#20197;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#32447;&#36335;&#65292;&#24182;&#20316;&#20026;&#26410;&#26469;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to re-label five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average precision 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09667</link><description>&lt;p&gt;
GeneGPT: &#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API
&lt;/p&gt;
&lt;p&gt;
GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09667
&lt;/p&gt;
&lt;p&gt;
GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeneGPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22269;&#23478;&#29983;&#29289;&#25216;&#26415;&#20449;&#24687;&#20013;&#24515;&#65288;NCBI&#65289;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#24182;&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#30340;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#21551;&#21457;Codex&#65288;code-davinci-002&#65289;&#35299;&#20915;GeneTuring&#27979;&#35797;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19968;&#26086;&#26816;&#27979;&#21040;&#35843;&#29992;&#35831;&#27714;&#65292;&#25105;&#20204;&#23601;&#20572;&#27490;&#35299;&#30721;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#36827;&#34892;API&#35843;&#29992;&#12290;&#25105;&#20204;&#28982;&#21518;&#23558;NCBI API&#36820;&#22238;&#30340;&#21407;&#22987;&#25191;&#34892;&#32467;&#26524;&#38468;&#21152;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#24182;&#32487;&#32493;&#29983;&#25104;&#30452;&#21040;&#25214;&#21040;&#31572;&#26696;&#25110;&#26816;&#27979;&#21040;&#21478;&#19968;&#20010;API&#35843;&#29992;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GeneGPT&#22312;GeneTuring&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;One-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20116;&#20010;Zero-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GeneGPT&#30340;&#23439;&#24179;&#22343;&#20998;&#25968;&#20026;0.76&#65292;&#36828;&#39640;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#65292;&#22914;New Bin&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;ChatGPT&#30340;&#39044;&#27979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21270;&#65292;&#34920;&#26126;&#22797;&#26434;&#27169;&#22411;&#21487;&#39044;&#27979;&#33021;&#21147;&#30340;&#23835;&#36215;&#12290;&#36825;&#34920;&#26126;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#24341;&#20837;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07619</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#65311;&#22238;&#25253;&#21487;&#39044;&#27979;&#24615;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;ChatGPT&#30340;&#39044;&#27979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21270;&#65292;&#34920;&#26126;&#22797;&#26434;&#27169;&#22411;&#21487;&#39044;&#27979;&#33021;&#21147;&#30340;&#23835;&#36215;&#12290;&#36825;&#34920;&#26126;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#24341;&#20837;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;ChatGPT&#20197;&#21450;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21028;&#26029;&#26032;&#38395;&#26631;&#39064;&#23545;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#26159;&#22909;&#28040;&#24687;&#12289;&#22351;&#28040;&#24687;&#25110;&#26080;&#20851;&#28040;&#24687;&#12290;&#36890;&#36807;&#35745;&#31639;&#25968;&#23383;&#20998;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;"ChatGPT&#20998;&#25968;"&#21644;&#38543;&#21518;&#30340;&#26085;&#24120;&#32929;&#31080;&#24066;&#22330;&#22238;&#25253;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#24615;&#12290;&#32780;&#19988;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-1&#12289;GPT-2&#21644;BERT&#31561;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#22238;&#25253;&#65292;&#36825;&#34920;&#26126;&#22238;&#25253;&#21487;&#39044;&#27979;&#24615;&#26159;&#22797;&#26434;&#27169;&#22411;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#32435;&#20837;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#25552;&#39640;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the potential of ChatGPT, and other large language models, in predicting stock market returns using sentiment analysis of news headlines. We use ChatGPT to indicate whether a given headline is good, bad, or irrelevant news for firms' stock prices. We then compute a numerical score and document a positive correlation between these ``ChatGPT scores'' and subsequent daily stock market returns. Further, ChatGPT outperforms traditional sentiment analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT cannot accurately forecast returns, indicating return predictability is an emerging capacity of complex models. Our results suggest that incorporating advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06447</link><description>&lt;p&gt;
PDF-VQA: &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;PDF&#25991;&#20214;&#30495;&#23454;&#19990;&#30028;VQA&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30740;&#31350;&#25991;&#26723;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20174;&#25991;&#26723;&#20803;&#32032;&#35782;&#21035;&#12289;&#25991;&#26723;&#24067;&#23616;&#32467;&#26500;&#29702;&#35299;&#20197;&#21450;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#20010;&#26041;&#38754;&#20840;&#38754;&#25506;&#35752;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;PDF-VQA&#25968;&#25454;&#38598;&#23558;&#25991;&#26723;&#29702;&#35299;&#30340;&#35268;&#27169;&#20174;&#21333;&#20010;&#25991;&#26723;&#39029;&#38754;&#25193;&#23637;&#21040;&#35810;&#38382;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;VQA&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#38598;&#25104;&#20102;&#19981;&#21516;&#25991;&#26723;&#20803;&#32032;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#23618;&#27425;&#32467;&#26500;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25991;&#26723;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#24615;&#33021;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.03518</link><description>&lt;p&gt;
SSS&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#35770;&#25991;&#65306;&#20351;&#29992;&#25237;&#31080;&#32454;&#35843;&#21464;&#21387;&#22120;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#12290; (arXiv&#65306;2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023&#20219;&#21153;10&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;-&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#65292;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#23548;&#33268;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#38754;&#20020;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#12290;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#21464;&#24471;&#27604;&#20197;&#24448;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#23545;&#22899;&#24615;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23454;&#39564;&#21644;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#38598;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#21333;&#20010;&#22522;&#32447;&#27169;&#22411;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;A&#20013;&#23454;&#29616;&#20102;&#23439;F1&#20998;&#25968;0.8392&#65292;&#22312;&#20219;&#21153;B&#20013;&#20026;0.6092&#65292;&#22312;&#20219;&#21153;C&#20013;&#20026;0.4319&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23588;&#40065;&#24052;&#25991;&#21270;&#38382;&#20505;&#35821;&#65288;$\mathcal{E}$ k\'u [MASK]&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;IkiniYor\`ub\'a&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#65292;&#32780;&#24494;&#35843;&#29616;&#26377;&#30340;NMT&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17972</link><description>&lt;p&gt;
$\mathcal{E}$ K\'U [MASK]: &#23558;&#23588;&#40065;&#24052;&#25991;&#21270;&#38382;&#20505;&#35821;&#25972;&#21512;&#21040;&#26426;&#22120;&#32763;&#35793;&#20013;
&lt;/p&gt;
&lt;p&gt;
$\mathcal{E}$ K\'U [MASK]: Integrating Yor\`ub\'a cultural greetings into machine translation. (arXiv:2303.17972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23588;&#40065;&#24052;&#25991;&#21270;&#38382;&#20505;&#35821;&#65288;$\mathcal{E}$ k\'u [MASK]&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;IkiniYor\`ub\'a&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#65292;&#32780;&#24494;&#35843;&#29616;&#26377;&#30340;NMT&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#22312;&#23558;&#23588;&#40065;&#24052;&#35821;&#38382;&#20505;&#35821;&#65288;$\mathcal{E}$ k\'u [MASK]&#65289;&#32763;&#35793;&#25104;&#33521;&#25991;&#26102;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23588;&#40065;&#24052;&#35821;-&#33521;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;IkiniYor\`ub\'a&#65292;&#20854;&#20013;&#21253;&#21547;&#23588;&#40065;&#24052;&#35821;&#38382;&#20505;&#35821;&#21644;&#26679;&#20363;&#29992;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;Google&#21644;NLLB&#22312;&#20869;&#30340;&#19981;&#21516;&#22810;&#35821;&#35328;NMT&#31995;&#32479;&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#32763;&#35793;&#23588;&#40065;&#24052;&#35821;&#38382;&#20505;&#35821;&#21040;&#33521;&#35821;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;IkiniYor\`ub\'a&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#29616;&#26377;&#30340;NMT&#27169;&#22411;&#26469;&#35757;&#32451;&#19968;&#20010;&#23588;&#40065;&#24052;&#35821;-&#33521;&#35821;&#27169;&#22411;&#65292;&#19982;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;NMT&#27169;&#22411;&#30456;&#27604;&#65292;&#20854;&#34920;&#29616;&#26356;&#22909;&#65292;&#23613;&#31649;&#23427;&#20204;&#32463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yor\`ub\'a greetings ($\mathcal{E}$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings, and sample use cases. We analysed the performance of different multilingual NMT systems including Google and NLLB and show that these models struggle to accurately translate Yor\`ub\'a greetings into English. In addition, we trained a Yor\`ub\'a-English model by finetuning an existing NMT model on the training split of IkiniYor\`ub\'a and this achieved better performance when compared to the pre-trained multilingual NMT models, although they were trained on a large volume of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2303.14334</link><description>&lt;p&gt;
&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#38405;&#35835;&#30028;&#38754;&#22686;&#24378;&#23398;&#26415;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#26159;&#23398;&#32773;&#21521;&#20182;&#20154;&#20256;&#36882;&#30693;&#35782;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#35770;&#25991;&#20449;&#24687;&#23494;&#38598;&#65292;&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#37327;&#30340;&#22686;&#38271;&#65292;&#38656;&#35201;&#26032;&#25216;&#26415;&#25903;&#25345;&#38405;&#35835;&#36807;&#31243;&#12290;&#19982;&#36890;&#36807;&#20114;&#32852;&#32593;&#25216;&#26415;&#36716;&#21464;&#30340;&#26597;&#25214;&#35770;&#25991;&#36807;&#31243;&#19981;&#21516;&#65292;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#30340;&#20307;&#39564;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;&#34429;&#28982;PDF&#26684;&#24335;&#22240;&#20854;&#20415;&#25658;&#24615;&#32780;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#26377;&#37325;&#22823;&#32570;&#28857;&#65292;&#21253;&#25324;&#65306;&#38745;&#24577;&#20869;&#23481;&#65292;&#20302;&#35270;&#35273;&#35835;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#24046;&#65292;&#20197;&#21450;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#38405;&#35835;&#22256;&#38590;&#12290;&#26412;&#25991;&#25506;&#35752;&#8220;&#26368;&#36817;&#30340;AI&#21644;HCI&#36827;&#23637;&#33021;&#21542;&#20026;&#36951;&#30041;&#30340;PDF&#25552;&#20379;&#26234;&#33021;&#65292;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#65311;&#8221;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65292;&#36825;&#26159;&#22810;&#20010;&#26426;&#26500;&#30340;&#21327;&#20316;&#21162;&#21147;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;&#30740;&#31350;&#35770;&#25991;&#33258;&#21160;&#21019;&#24314;&#21160;&#24577;&#38405;&#35835;&#30028;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.13099</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#24847;&#22270;&#24402;&#32435;&#65306;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#30340;&#24847;&#22270;&#26159;&#23558;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#29992;&#20110;&#19968;&#33324;&#23884;&#20837;&#30340;SBERT&#65288;2&#65289;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#65288;MDB&#65289;&#29992;&#20110;&#23545;&#35805;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;3&#65289;&#29992;&#20110;&#38598;&#32676;&#19987;&#19994;&#35821;&#20041;&#30340;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#65288;PGT&#65289;&#12290; MDB&#19968;&#27425;&#21521;&#27169;&#22411;&#25552;&#20379;&#22810;&#31181;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#20915;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PGT&#65292;&#23427;&#37319;&#29992;Siamese&#32593;&#32476;&#30452;&#25509;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;PGT&#32858;&#31867;&#23545;&#35805;&#35821;&#21477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#27169;&#22411;&#19982;MDB&#21644;PGT&#26174;&#30528;&#25552;&#39640;&#20102;Open Intent Induction&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38544;&#24335;&#21361;&#23475;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;TOT&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.09314</link><description>&lt;p&gt;
TOT&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#20167;&#24680;&#26816;&#27979;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38544;&#24335;&#21361;&#23475;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;TOT&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20167;&#24680;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22312;&#32447;&#26377;&#23475;&#20869;&#23481;&#65288;&#22914;&#34920;&#24773;&#21253;&#31561;&#65289;&#65292;&#26159;&#26500;&#24314;&#20581;&#24247;&#30340;&#20114;&#32852;&#32593;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#26174;&#24335;&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#21361;&#23475;&#30340;&#20998;&#26512;&#65292;&#36825;&#22312;&#23384;&#22312;&#30528;&#25197;&#26354;&#25110;&#32570;&#20047;&#26126;&#26174;&#25991;&#26412;&#26631;&#35760;&#21644;&#20154;&#21475;&#32479;&#35745;&#35270;&#35273;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#29305;&#21035;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TOT&#65306;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#23558;&#36328;&#27169;&#24577;&#23545;&#40784;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#30340;&#27714;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;&#26680;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#36716;&#25442;&#33021;&#21147;&#65292;&#20197;&#37325;&#29616;&#36755;&#20837;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal hate detection, which aims to identify harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these semantic gaps issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce 
&lt;/p&gt;</description></item><item><title>DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.08991</link><description>&lt;p&gt;
DeltaScore: &#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#35780;&#20215;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08991
&lt;/p&gt;
&lt;p&gt;
DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#21508;&#31181;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#25925;&#20107;&#29983;&#25104;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19981;&#24378;&#65292;&#20063;&#19981;&#33021;&#27979;&#37327;&#32454;&#31890;&#24230;&#30340;&#25925;&#20107;&#26041;&#38754;&#65292;&#20363;&#22914;&#27969;&#30021;&#24230;&#19982;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#35780;&#20272;&#25972;&#20307;&#29983;&#25104;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;DeltaScore&#65292;&#19968;&#31181;&#21033;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#34920;&#29616;&#24471;&#36234;&#22909;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;&#23427;&#23601;&#20250;&#21463;&#21040;&#29305;&#23450;&#25200;&#21160;&#65288;&#20363;&#22914;&#24341;&#20837;&#38169;&#21035;&#23383;&#65289;&#30340;&#24433;&#21709;&#36234;&#22823;&#12290;&#20026;&#20102;&#34913;&#37327;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25200;&#21160;&#21069;&#21518;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#20351;&#29992;DeltaScore&#35780;&#20272;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#20256;&#32479;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20154;&#31867;&#22312;&#20116;&#20010;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.02468</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#30340;Lon-ea&#65306;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#23398;&#20064;&#19981;&#21516;&#24847;&#20219;&#21153;&#30340;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#23618;&#20013;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#36719;&#26631;&#31614;&#26469;&#37327;&#21270;&#19981;&#21516;&#24847;&#37327;&#12290;&#20026;&#20102;&#39044;&#27979;&#36719;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#24182;&#25913;&#21464;&#36755;&#20986;&#23618;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21442;&#25968;&#19981;&#21464;&#12290;&#28982;&#21518;&#23558;&#36719;&#26631;&#31614;&#29992;&#20110;&#30828;&#26631;&#31614;&#39044;&#27979;&#12290;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#21253;&#25324;sigmoid&#20989;&#25968;&#20197;&#21450;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#30340;&#38454;&#36291;&#20989;&#25968;&#21644;&#26412;&#25991;&#20013;&#39318;&#27425;&#20171;&#32461;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;UZH_CLyp&#22312;SemEval 2023&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21253;&#21547;&#20102;&#20808;&#20351;&#29992;Head-First Fine-Tuning&#65288;HeFiT&#65289;&#26041;&#27861;&#26356;&#26032;&#22238;&#24402;&#22836;&#21442;&#25968;&#65292;&#20877;&#38477;&#20302;&#23398;&#20064;&#29575;&#26356;&#26032;&#39044;&#35757;&#32451;transformer&#30340;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#33258;&#21160;&#23567;&#22411;&#26679;&#20363;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;HeFiT&#31283;&#23450;&#35757;&#32451;&#24182;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26102;&#20063;&#33021;&#25552;&#39640;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01194</link><description>&lt;p&gt;
UZH_CLyp&#22312;SemEval-2023&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;Head-First Fine-Tuning&#21644;ChatGPT&#25968;&#25454;&#29983;&#25104;&#30340;&#36328;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25512;&#25991;&#20146;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UZH_CLyp&#22312;SemEval 2023&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21253;&#21547;&#20102;&#20808;&#20351;&#29992;Head-First Fine-Tuning&#65288;HeFiT&#65289;&#26041;&#27861;&#26356;&#26032;&#22238;&#24402;&#22836;&#21442;&#25968;&#65292;&#20877;&#38477;&#20302;&#23398;&#20064;&#29575;&#26356;&#26032;&#39044;&#35757;&#32451;transformer&#30340;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#33258;&#21160;&#23567;&#22411;&#26679;&#20363;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;HeFiT&#31283;&#23450;&#35757;&#32451;&#24182;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26102;&#20063;&#33021;&#25552;&#39640;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UZH_CLyp&#22312;SemEval 2023&#20219;&#21153;9&#8220;&#22810;&#35821;&#35328;&#25512;&#25991;&#20146;&#23494;&#24230;&#20998;&#26512;&#8221;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;10&#31181;&#35821;&#35328;&#20013;&#22343;&#21462;&#24471;&#20102;&#31532;&#20108;&#22909;&#30340;&#32467;&#26524;&#65292;&#26681;&#25454;&#23448;&#26041;&#30340;Pearson&#30456;&#20851;&#31995;&#25968;&#22238;&#24402;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#20351;&#29992;Head-First Fine-Tuning&#26041;&#27861;&#65288;HeFiT&#65289;&#30340;&#30410;&#22788;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#20165;&#26356;&#26032;&#22238;&#24402;&#22836;&#21442;&#25968;&#65292;&#28982;&#21518;&#20877;&#20197;&#38477;&#20302;&#30340;&#23398;&#20064;&#29575;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;transformer&#32534;&#30721;&#22120;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#20351;&#29992;&#19968;&#23567;&#32452;&#33258;&#21160;&#29983;&#25104;&#30340;&#31034;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;ChatGPT&#65289;&#23545;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;HeFiT&#31283;&#23450;&#20102;&#22521;&#35757;&#24182;&#19988;&#23545;&#20110;&#32570;&#20047;&#25512;&#25991;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19968;&#33268;&#22320;&#25552;&#39640;&#20102;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26102;&#65292;&#36328;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#65292;&#35777;&#23454;&#20102;&#24403;&#21069;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#24773;&#20917;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9 "Multilingual Tweet Intimacy Analysis". We achieved second-best results in all 10 languages according to the official Pearson's correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LeMDA&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#33258;&#21160;&#23398;&#20064;&#32852;&#21512;&#22686;&#24378;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.14453</link><description>&lt;p&gt;
&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LeMDA&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#33258;&#21160;&#23398;&#20064;&#32852;&#21512;&#22686;&#24378;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#65288;&#22914;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#65289;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#23613;&#31649;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#25968;&#25454;&#22686;&#24378;&#30340;&#24040;&#22823;&#25104;&#21151;&#20173;&#28982;&#23616;&#38480;&#20110;&#21333;&#27169;&#24577;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#30830;&#23454;&#65292;&#22312;&#20445;&#30041;&#25968;&#25454;&#30340;&#25972;&#20307;&#35821;&#20041;&#32467;&#26500;&#30340;&#21516;&#26102;&#22686;&#24378;&#27599;&#20010;&#27169;&#24577;&#26159;&#29305;&#21035;&#22256;&#38590;&#30340;&#65307;&#20363;&#22914;&#65292;&#22312;&#24212;&#29992;&#20102;&#26631;&#20934;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#32763;&#35793;&#65289;&#20043;&#21518;&#65292;&#26631;&#39064;&#21487;&#33021;&#19981;&#20877;&#26159;&#22270;&#20687;&#30340;&#33391;&#22909;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#20173;&#28982;&#24456;&#38590;&#25351;&#23450;&#19981;&#38024;&#23545;&#29305;&#23450;&#27169;&#24577;&#30340;&#21512;&#29702;&#21464;&#25442;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;LeMDA&#65288;Learning Multimodal Data Augmentation&#65289;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#33258;&#21160;&#23398;&#20064;&#32852;&#21512;&#22686;&#24378;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#38480;&#21046;&#27169;&#24577;&#30340;&#36523;&#20221;&#25110;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;LeMDA&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#38899;&#35782;&#21035;&#65289;&#20013;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2212.12061</link><description>&lt;p&gt;
MN-DS&#65306;&#26032;&#38395;&#25991;&#31456;&#23618;&#27425;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#20174;2019&#24180;1&#26376;1&#26085;&#21040;2019&#24180;12&#26376;31&#26085;&#30340;&#23618;&#27425;&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#26681;&#25454;17&#20010;&#19968;&#32423;&#31867;&#21035;&#21644;109&#20010;&#20108;&#32423;&#31867;&#21035;&#30340;&#23618;&#27425;&#20998;&#31867;&#25163;&#21160;&#26631;&#35760;&#20102;&#36825;&#20123;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#25353;&#20027;&#39064;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20174;&#20107;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#26681;&#25454;&#21457;&#24067;&#30340;&#26032;&#38395;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDMem&#30340;&#20855;&#26377;&#23454;&#20307;&#35760;&#24518;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23454;&#20307;&#30693;&#35782;&#34701;&#20837;&#21040;&#20449;&#24687;&#24615;&#25991;&#26412;&#29983;&#25104;&#20013;&#12290;&#23454;&#20307;&#30693;&#35782;&#20197;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#23384;&#20648;&#22312;&#35760;&#24518;&#20013;&#65292;&#24182;&#21033;&#29992;&#35760;&#24518;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;&#38480;&#21046;&#23454;&#20307;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EDMem&#20248;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#38750;&#35760;&#24518;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.03273</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#23454;&#20307;&#35760;&#24518;&#30340;&#32479;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDMem&#30340;&#20855;&#26377;&#23454;&#20307;&#35760;&#24518;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23454;&#20307;&#30693;&#35782;&#34701;&#20837;&#21040;&#20449;&#24687;&#24615;&#25991;&#26412;&#29983;&#25104;&#20013;&#12290;&#23454;&#20307;&#30693;&#35782;&#20197;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#23384;&#20648;&#22312;&#35760;&#24518;&#20013;&#65292;&#24182;&#21033;&#29992;&#35760;&#24518;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;&#38480;&#21046;&#23454;&#20307;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EDMem&#20248;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#38750;&#35760;&#24518;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#30340;&#37325;&#35201;&#36733;&#20307;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#23558;&#23454;&#20307;&#30693;&#35782;&#34701;&#20837;&#21040;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#65292;&#20197;&#36827;&#34892;&#20449;&#24687;&#24615;&#25991;&#26412;&#29983;&#25104;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#32034;&#24341;&#12289;&#26816;&#32034;&#21644;&#38405;&#35835;&#22806;&#37096;&#25991;&#26723;&#20316;&#20026;&#35777;&#25454;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EDMem&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#20854;&#20013;&#23454;&#20307;&#30693;&#35782;&#20197;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#23384;&#20648;&#22312;&#35760;&#24518;&#20013;&#65292;&#24182;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#39044;&#20808;&#35757;&#32451;&#35760;&#24518;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21442;&#25968;&#12290;&#20026;&#20102;&#31934;&#30830;&#29983;&#25104;&#23454;&#20307;&#21517;&#31216;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#35299;&#30721;&#26041;&#27861;&#65292;&#20197;&#36890;&#36807;&#35760;&#24518;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;&#38480;&#21046;&#23454;&#20307;&#29983;&#25104;&#12290;EDMem&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#23454;&#20307;&#23494;&#38598;&#22411;&#38382;&#31572;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EDMem&#20248;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#38750;&#35760;&#24518;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#23398;&#20064;&#32452;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#31070;&#32463;&#27169;&#22359;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21487;&#21306;&#20998;&#30340;&#27169;&#22359;&#35774;&#35745;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#27169;&#22359;&#32452;&#21512;&#21644;&#22810;&#30446;&#26631;&#23398;&#20064;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;MSCOCO&#21644;Flickr30k&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01338</link><description>&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#35270;&#35273;&#35821;&#35328;&#31070;&#32463;&#27169;&#22359;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning. (arXiv:2210.01338v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#23398;&#20064;&#32452;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#31070;&#32463;&#27169;&#22359;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21487;&#21306;&#20998;&#30340;&#27169;&#22359;&#35774;&#35745;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#27169;&#22359;&#32452;&#21512;&#21644;&#22810;&#30446;&#26631;&#23398;&#20064;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;MSCOCO&#21644;Flickr30k&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20542;&#21521;&#20110;&#23558;&#21477;&#23376;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#22914;&#8220;sth&#22312;someplace&#20570;sth&#8221;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#37096;&#20998;&#22635;&#20837;&#26576;&#20123;&#20869;&#23481;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#36981;&#24490;&#8220;&#27169;&#22359;&#21270;&#35774;&#35745;&#21407;&#21017;&#8221;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#65306;&#23398;&#20064;&#32452;&#21512;&#35270;&#35273;&#35821;&#35328;&#31070;&#32463;&#27169;&#22359;&#65288;CVLNM&#65289;&#12290;&#19982;VQA&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;&#19981;&#21516;&#65292;&#20854;&#20013;&#35821;&#35328;&#65288;&#21363;&#38382;&#39064;&#65289;&#26159;&#23436;&#20840;&#21487;&#35265;&#30340;&#65292;&#32452;&#21512;&#35270;&#35273;&#35821;&#35328;&#27169;&#22359;&#30340;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35821;&#35328;&#20165;&#37096;&#20998;&#21487;&#35265;&#65292;&#38656;&#35201;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#32452;&#21512;&#27169;&#22359;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#65306;1&#65289;&#21487;&#21306;&#20998;&#30340;&#27169;&#22359;&#35774;&#35745;&#8212;&#8212;&#32534;&#30721;&#22120;&#20013;&#21253;&#25324;&#19968;&#20010;&#20989;&#25968;&#35789;&#35821;&#35328;&#27169;&#22359;&#21644;&#19977;&#20010;&#19981;&#21516;&#20869;&#23481;&#35789;&#30340;&#35270;&#35273;&#27169;&#22359;&#65288;&#21363;&#21517;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21160;&#35789;&#65289;&#65292;&#35299;&#30721;&#22120;&#20013;&#21478;&#22806;&#19968;&#20010;&#35821;&#35328;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#35821;&#35328;&#65307;2&#65289;&#26377;&#25928;&#30340;&#27169;&#22359;&#32452;&#21512;&#8212;&#8212;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#21477;&#23376;&#27573;&#21160;&#24577;&#32452;&#21512;&#36866;&#24403;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22359;&#65307;3&#65289;&#22810;&#30446;&#26631;&#23398;&#20064;&#8212;&#8212;&#32852;&#21512;&#35757;&#32451;&#30446;&#26631;&#65292;&#24179;&#34913;&#23383;&#24149;&#29983;&#25104;&#36136;&#37327;&#21644;&#27169;&#22359;&#24046;&#24322;&#24615;&#12290;&#22312;MSCOCO&#21644;Flickr30k&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans tend to decompose a sentence into different parts like \textsc{sth do sth at someplace} and then fill each part with certain content. Inspired by this, we follow the \textit{principle of modular design} to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the \re{widely used} neural module networks in VQA, where the language (\ie, question) is fully observable, \re{the task of collocating visual-linguistic modules is more challenging.} This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: 1) \textit{distinguishable module design} -- \re{four modules in the encoder} including one linguistic module for function words and three visual modules for different content words (\ie, noun, adjective, and verb) and another linguistic one in the decoder 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;Tetris&#65292;&#24182;&#25552;&#20986;&#20102;&#27010;&#24565;&#25552;&#31034;&#21644;&#38754;&#21521;&#33050;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#36755;&#20837;&#21253;&#25324;&#29992;&#25143;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.00068</link><description>&lt;p&gt;
&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#27010;&#24565;&#30693;&#35782;&#32435;&#20837;&#33050;&#26412;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Task-specific Concept Knowledge into Script Learning. (arXiv:2209.00068v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;Tetris&#65292;&#24182;&#25552;&#20986;&#20102;&#27010;&#24565;&#25552;&#31034;&#21644;&#38754;&#21521;&#33050;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#36755;&#20837;&#21253;&#25324;&#29992;&#25143;&#19978;&#19979;&#25991;&#26102;&#30340;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Tetris&#30340;&#26032;&#20219;&#21153;&#65292;&#21517;&#20026;&#30446;&#26631;&#23548;&#21521;&#33050;&#26412;&#23436;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#26222;&#36941;&#30340;&#22330;&#26223;&#65292;&#36755;&#20837;&#19981;&#20165;&#21253;&#25324;&#30446;&#26631;&#65292;&#36824;&#21253;&#25324;&#29992;&#25143;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#21916;&#22909;&#21644;&#21382;&#21490;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#65306;&#27010;&#24565;&#25552;&#31034;&#21644;&#38754;&#21521;&#33050;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#27493;&#39588;&#37325;&#22797;&#21644;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;WikiHow&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#12289;&#20179;&#24211;&#21644;&#27169;&#22411;&#23558;&#20844;&#24320;&#25552;&#20379;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20010;&#26032;&#20219;&#21153;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Tetris, a new task of Goal-Oriented Script Completion. Unlike previous work, it considers a more realistic and general setting, where the input includes not only the goal but also additional user context, including preferences and history. To address this problem, we propose a novel approach, which uses two techniques to improve performance: (1) concept prompting, and (2) script-oriented contrastive learning that addresses step repetition and hallucination problems. On our WikiHow-based dataset, we find that both methods improve performance. The dataset, repository, and models will be publicly available to facilitate further research on this new task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.12261</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20849;&#24773;&#24515;&#29702;&#30340;&#26381;&#21153;&#12290;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#32531;&#35299;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20851;&#31995;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#21462;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#36328;&#27169;&#24577;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;MMGCN&#65289;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20887;&#20313;&#20449;&#24687;&#65292;&#19988;&#21487;&#33021;&#20002;&#22833;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#65288;GraphCFC&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#21644;&#20114;&#21160;&#20449;&#24687;&#12290;GraphCFC&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23376;&#31354;&#38388;&#25552;&#21462;&#22120;&#21644;&#25104;&#23545;&#36328;&#27169;&#24577;&#34917;&#20805;&#65288;PairCC&#65289;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03578</link><description>&lt;p&gt;
&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#32423;&#21035;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;IR&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21644;IR&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#36991;&#20813;&#20197;&#24448;&#26041;&#27861;&#23384;&#22312;&#30340;&#24120;&#35265;&#38169;&#35823;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;EFL&#23398;&#29983;&#22914;&#20309;&#20351;&#29992;NLG&#24037;&#20855;&#36827;&#34892;&#21019;&#24847;&#20889;&#20316;&#12290;&#30740;&#31350;&#21457;&#29616;&#23398;&#29983;&#22312;&#20351;&#29992;NLG&#24037;&#20855;&#25628;&#32034;&#21644;&#35780;&#20272;&#24605;&#36335;&#26102;&#21487;&#33021;&#24050;&#32463;&#26377;&#29616;&#26377;&#30340;&#24819;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.01484</link><description>&lt;p&gt;
&#20351;&#29992;NLG&#24037;&#20855;&#20102;&#35299;EFL&#23398;&#29983;&#21019;&#24847;&#20889;&#20316;&#30340;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;
Understanding EFL Student Idea Generation Strategies for Creative Writing with NLG Tools. (arXiv:2207.01484v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;EFL&#23398;&#29983;&#22914;&#20309;&#20351;&#29992;NLG&#24037;&#20855;&#36827;&#34892;&#21019;&#24847;&#20889;&#20316;&#12290;&#30740;&#31350;&#21457;&#29616;&#23398;&#29983;&#22312;&#20351;&#29992;NLG&#24037;&#20855;&#25628;&#32034;&#21644;&#35780;&#20272;&#24605;&#36335;&#26102;&#21487;&#33021;&#24050;&#32463;&#26377;&#29616;&#26377;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#36807;&#31243;&#65292;&#20854;&#20013;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#20449;&#24687;&#20013;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35821;&#35328;&#25991;&#26412;&#12290; EFL&#23398;&#29983;&#20351;&#29992;NLG&#24037;&#20855;&#21487;&#33021;&#26377;&#21161;&#20110;&#20182;&#20204;&#30340;&#21019;&#24847;&#29983;&#25104;&#65292;&#36825;&#23545;&#20110;&#21019;&#24847;&#20889;&#20316;&#26159;&#22522;&#30784;&#12290; &#28982;&#32780;&#65292;&#25105;&#20204;&#23545;EFL&#23398;&#29983;&#22914;&#20309;&#19982;NLG&#24037;&#20855;&#20132;&#20114;&#29983;&#25104;&#24605;&#36335;&#30693;&#20043;&#29978;&#23569;&#12290; &#26412;&#30740;&#31350;&#25506;&#35752;&#20102;EFL&#23398;&#29983;&#20351;&#29992;NLG&#24037;&#20855;&#25628;&#32034;&#24605;&#36335;&#12289;&#35780;&#20272;NLG&#24037;&#20855;&#29983;&#25104;&#30340;&#24605;&#36335;&#20197;&#21450;&#36873;&#25321;NLG&#24037;&#20855;&#36827;&#34892;&#24605;&#36335;&#29983;&#25104;&#26102;&#37319;&#29992;&#30340;&#31574;&#30053;&#12290; &#22235;&#21517;&#39321;&#28207;&#20013;&#23398;&#29983;&#21442;&#21152;&#20102;&#30740;&#35752;&#20250;&#65292;&#20182;&#20204;&#23398;&#20064;&#32534;&#20889;&#21253;&#25324;&#33258;&#24049;&#30340;&#35805;&#35821;&#21644;NLG&#24037;&#20855;&#29983;&#25104;&#30340;&#21333;&#35789;&#30340;&#25925;&#20107;&#12290; &#30740;&#35752;&#20250;&#32467;&#26463;&#21518;&#65292;&#20182;&#20204;&#22238;&#31572;&#38382;&#39064;&#20197;&#21453;&#24605;&#20351;&#29992;NLG&#24037;&#20855;&#30340;&#20889;&#20316;&#32463;&#39564;&#12290; &#22312;&#20070;&#38754;&#21453;&#24605;&#30340;&#20027;&#39064;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#22312;&#20351;&#29992;NLG&#24037;&#20855;&#25628;&#32034;&#21644;&#35780;&#20272;&#24605;&#36335;&#26102;&#21487;&#33021;&#24050;&#32463;&#26377;&#29616;&#26377;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation (NLG) is a process within artificial intelligence where computer systems produce human-comprehensible language texts from information. English as a foreign language (EFL) students' use of NLG tools might facilitate their idea generation, which is fundamental to creative writing. However, little is known about how EFL students interact with NLG tools to generate ideas. This study explores strategies adopted by EFL students when searching for ideas using NLG tools, evaluating ideas generated by NLG tools and selecting NLG tools for ideas generation. Four Hong Kong secondary school students attended workshops where they learned to write stories comprising their own words and words generated by NLG tools. After the workshops, they answered questions to reflect on their writing experience with NLG tools. In a thematic analysis of the written reflections, we found students may have existing ideas when searching for ideas and evaluating ideas with NLG tools. Studen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31532;&#19968;&#27425;&#20840;&#38754;&#25506;&#35752;&#20102; DR &#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#38646;-shot DR &#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2204.12755</link><description>&lt;p&gt;
&#38646;-shot &#23494;&#38598;&#26816;&#32034;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
A Thorough Examination on Zero-shot Dense Retrieval. (arXiv:2204.12755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31532;&#19968;&#27425;&#20840;&#38754;&#25506;&#35752;&#20102; DR &#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#38646;-shot DR &#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;DR &#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38646;-shot&#26816;&#32034;&#35774;&#32622;&#19979;&#65292;&#23427;&#20204;&#26174;&#31034;&#20986;&#30340;&#31454;&#20105;&#21147;&#19981;&#22914;&#20256;&#32479;&#30340;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#65288;&#20363;&#22914;BM25&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#20851;&#25991;&#29486;&#20013;&#65292;&#20173;&#32570;&#20047;&#23545;&#38646;-shot&#26816;&#32034;&#30340;&#35814;&#32454;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; DR &#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#24182;&#20998;&#26512;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#38646;-shot&#26816;&#32034;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#28304;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20998;&#26512;&#20102;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#20559;&#24046;&#65292;&#24182;&#22238;&#39038;&#21644;&#27604;&#36739;&#29616;&#26377;&#30340;&#38646;-shot DR &#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#38646;-shot DR &#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting. However, in the related literature, there still lacks a detailed and comprehensive study on zero-shot retrieval. In this paper, we present the first thorough examination of the zero-shot capability of DR models. We aim to identify the key factors and analyze how they affect zero-shot retrieval performance. In particular, we discuss the effect of several key factors related to source training set, analyze the potential bias from the target dataset, and review and compare existing zero-shot DR models. Our findings provide important evidence to better understand and develop zero-shot DR models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26816;&#32034;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#24182;&#24212;&#29992;&#20110;&#38544;&#31169;&#25919;&#31574;&#38382;&#31572;&#20013;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#25429;&#33719;&#25919;&#31574;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#24182;&#21033;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22686;&#24378;&#25968;&#25454;&#36827;&#34892;&#32423;&#32852;&#21644;&#22122;&#22768;&#20943;&#23569;&#28388;&#27874;&#26469;&#25552;&#39640;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#33719;&#24471;&#20102;&#35813;&#39046;&#22495;&#30340;&#26032;&#26368;&#20248;F1&#20998;&#25968;50\%&#12290;</title><link>http://arxiv.org/abs/2204.08952</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#25919;&#31574;&#30340;&#38382;&#31572;&#26816;&#32034;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26816;&#32034;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#24182;&#24212;&#29992;&#20110;&#38544;&#31169;&#25919;&#31574;&#38382;&#31572;&#20013;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#25429;&#33719;&#25919;&#31574;&#25991;&#26723;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#24182;&#21033;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22686;&#24378;&#25968;&#25454;&#36827;&#34892;&#32423;&#32852;&#21644;&#22122;&#22768;&#20943;&#23569;&#28388;&#27874;&#26469;&#25552;&#39640;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#33719;&#24471;&#20102;&#35813;&#39046;&#22495;&#30340;&#26032;&#26368;&#20248;F1&#20998;&#25968;50\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#25919;&#31574;&#39046;&#22495;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#38382;&#31572;&#20219;&#21153;&#26694;&#26550;&#21270;&#20026;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#20174;&#25919;&#31574;&#25991;&#20214;&#20013;&#35782;&#21035;&#26368;&#30456;&#20851;&#30340;&#25991;&#26412;&#27573;&#33853;&#25110;&#21477;&#23376;&#21015;&#34920;&#12290;&#29616;&#26377;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#65288;&#21482;&#26377;&#23569;&#25968;&#30456;&#20851;&#27573;&#33853;&#65289;&#65292;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#20013;QA&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#26816;&#32034;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25919;&#31574;&#25991;&#20214;&#20013;&#25429;&#33719;&#30456;&#20851;&#30340;&#25991;&#26412;&#27573;&#24182;&#25193;&#23637;&#35757;&#32451;&#38598;&#20013;&#30340;&#27491;&#20363;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#22686;&#24378;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24182;&#23558;&#23427;&#20204;&#32423;&#32852;&#22122;&#22768;&#20943;&#23569;&#28388;&#27874;&#27169;&#22411;&#12290;&#22312;PrivacyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;&#25105;&#20204;&#30340;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#22522;&#32447;&#25552;&#39640;&#20102;&#24456;&#22823;&#31243;&#24230;&#65288;10\% F1&#65289;&#65292;&#24182;&#21462;&#24471;&#20102;50\%&#30340;&#26032;&#30340;&#26368;&#20248;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior studies in privacy policies frame the question answering (QA) task as identifying the most relevant text segment or a list of sentences from a policy document given a user query. Existing labeled datasets are heavily imbalanced (only a few relevant segments), limiting the QA performance in this domain. In this paper, we develop a data augmentation framework based on ensembling retriever models that captures the relevant text segments from unlabeled policy documents and expand the positive examples in the training set. In addition, to improve the diversity and quality of the augmented data, we leverage multiple pre-trained language models (LMs) and cascade them with noise reduction filter models. Using our augmented data on the PrivacyQA benchmark, we elevate the existing baseline by a large margin (10\% F1) and achieve a new state-of-the-art F1 score of 50\%. Our ablation studies provide further insights into the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#24335;&#36719;&#25552;&#31034;&#23454;&#29616;&#20102;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#23646;&#24615;-&#23545;&#35937;&#32452;&#21512;&#65292;&#36229;&#36807;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#23450;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#22312;&#26354;&#32447;&#19979;&#38754;&#31215;&#19978;&#24179;&#22343;&#39640;10.9&#65285;&#12290;</title><link>http://arxiv.org/abs/2204.03574</link><description>&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#36719;&#25552;&#31034;&#20197;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03574
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#32452;&#21512;&#24335;&#36719;&#25552;&#31034;&#23454;&#29616;&#20102;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#23646;&#24615;-&#23545;&#35937;&#32452;&#21512;&#65292;&#36229;&#36807;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#23450;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#22312;&#26354;&#32447;&#19979;&#38754;&#31215;&#19978;&#24179;&#22343;&#39640;10.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#36719;&#25552;&#31034;&#65288;CSP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#23558;CSP&#24320;&#21457;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20063;&#23601;&#26159;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;&#23646;&#24615;-&#23545;&#35937;&#32452;&#21512;&#65288;&#20363;&#22914;&#32769;&#29483;&#21644;&#23567;&#32769;&#34382;&#65289;&#12290;VLM&#20855;&#26377;&#28789;&#27963;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#34920;&#31034;&#20219;&#24847;&#31867;&#21035;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#32452;&#21512;&#38646;&#26679;&#26412;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#22914;&#29305;&#23450;&#20219;&#21153;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;CSP&#23558;&#23450;&#20041;&#31867;&#21035;&#30340;&#23646;&#24615;&#21644;&#23545;&#35937;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#35789;&#27719;&#26631;&#35760;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#35789;&#27719;&#34920;&#34987;&#35843;&#25972;&#20026;&#35782;&#21035;&#20197;&#22810;&#31181;&#26041;&#24335;&#32452;&#25104;&#20196;&#29260;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#32769;&#29483;&#21644;&#30333;&#29483;&#65289;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#23558;&#25152;&#23398;&#30340;&#23646;&#24615;-&#23545;&#35937;&#35789;&#27719;&#34920;&#20197;&#26032;&#30340;&#32452;&#21512;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#35782;&#21035;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CSP&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;CLIP&#24179;&#22343;&#39640;10.9&#20010;&#30334;&#20998;&#28857;&#30340;AUC&#65288;&#26354;&#32447;&#19979;&#38754;&#31215;&#25351;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29992;&#20110;&#31572;&#26696;&#36873;&#25321;&#65292;&#36890;&#36807;&#26500;&#24314;&#30456;&#20851;&#35757;&#32451;&#22270;&#24182;&#38598;&#25104;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26816;&#32034;&#22411;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;AS2&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2203.03549</link><description>&lt;p&gt;
&#38382;&#39064;-&#22238;&#31572;&#21477;&#23376;&#22270;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#31572;&#26696;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Question-Answer Sentence Graph for Joint Modeling Answer Selection. (arXiv:2203.03549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29992;&#20110;&#31572;&#26696;&#36873;&#25321;&#65292;&#36890;&#36807;&#26500;&#24314;&#30456;&#20851;&#35757;&#32451;&#22270;&#24182;&#38598;&#25104;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26816;&#32034;&#22411;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;AS2&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29992;&#20110;&#31572;&#26696;&#36873;&#25321;&#65288;AS2&#65289;&#65292;&#36825;&#26159;&#26816;&#32034;&#22411;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#30456;&#20851;&#35757;&#32451;&#22270;&#65292;&#24182;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#12290;&#22270;&#33410;&#28857;&#26159;&#38382;&#39064;&#21477;&#23376;&#21040;&#31572;&#26696;&#21477;&#23376;&#30340;&#23545;&#12290;&#25105;&#20204;&#35757;&#32451;&#24182;&#38598;&#25104;&#20102;&#29992;&#20110;&#35745;&#31639;&#38382;&#39064;-&#38382;&#39064;&#12289;&#38382;&#39064;-&#31572;&#26696;&#21644;&#31572;&#26696;-&#31572;&#26696;&#23545;&#20043;&#38388;&#24471;&#20998;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#30456;&#20851;&#24471;&#20998;&#38408;&#20540;&#26469;&#21019;&#24314;&#22270;&#36793;&#32536;&#12290;&#28982;&#21518;&#36827;&#34892;&#22312;&#32447;&#25512;&#29702;&#20197;&#35299;&#20915;&#30475;&#19981;&#35265;&#30340;&#26597;&#35810;&#30340;AS2&#20219;&#21153;&#12290;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;&#23398;&#26415;&#22522;&#20934;&#21644;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;SOTA QA&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research studies graph-based approaches for Answer Sentence Selection (AS2), an essential component for retrieval-based Question Answering (QA) systems. During offline learning, our model constructs a small-scale relevant training graph per question in an unsupervised manner, and integrates with Graph Neural Networks. Graph nodes are question sentence to answer sentence pairs. We train and integrate state-of-the-art (SOTA) models for computing scores between question-question, question-answer, and answer-answer pairs, and use thresholding on relevance scores for creating graph edges. Online inference is then performed to solve the AS2 task on unseen queries. Experiments on two well-known academic benchmarks and a real-world dataset show that our approach consistently outperforms SOTA QA baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2201.02797</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#26159;&#21307;&#30103;&#36816;&#33829;&#21644;&#26381;&#21153;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#36890;&#36807;&#20174;&#20020;&#24202;&#25991;&#26723;&#20013;&#39044;&#27979;&#21307;&#30103;&#32534;&#30721;&#26469;&#31649;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#12290;&#20294;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#32570;&#20047;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#30340;&#32479;&#19968;&#35270;&#22270;&#12290;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#25552;&#20379;&#23545;&#21307;&#30103;&#32534;&#30721;&#27169;&#22411;&#32452;&#20214;&#30340;&#19968;&#33324;&#29702;&#35299;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#27492;&#26694;&#26550;&#19979;&#26368;&#36817;&#30340;&#39640;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#23558;&#21307;&#30103;&#32534;&#30721;&#20998;&#35299;&#20026;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#21363;&#29992;&#20110;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#12289;&#26500;&#24314;&#28145;&#24230;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#26426;&#21046;&#12289;&#29992;&#20110;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#25104;&#21307;&#30103;&#20195;&#30721;&#30340;&#35299;&#30721;&#22120;&#27169;&#22359;&#20197;&#21450;&#36741;&#21161;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning and natural language processing have been widely applied to this task. However, deep learning-based medical coding lacks a unified view of the design of neural network architectures. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we introduce the benchmarks and real-world usage and discuss key research challenges and future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#31264;&#23494;Passage&#26816;&#32034;&#21644;Passage&#37325;&#26032;&#25490;&#21517;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;listwise&#33976;&#39311;&#21644;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.07367</link><description>&lt;p&gt;
RocketQAv2&#65306;&#31264;&#23494;Passage&#26816;&#32034;&#21644;Passage&#37325;&#26032;&#25490;&#21517;&#30340;&#32852;&#21512;&#35757;&#32451;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#31264;&#23494;Passage&#26816;&#32034;&#21644;Passage&#37325;&#26032;&#25490;&#21517;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;listwise&#33976;&#39311;&#21644;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;Passage&#26816;&#32034;&#21644;Passage&#37325;&#26032;&#25490;&#21517;&#26159;&#23547;&#25214;&#21644;&#25490;&#24207;&#30456;&#20851;&#20449;&#24687;&#30340;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#30001;&#20110;&#20004;&#20010;&#27493;&#39588;&#37117;&#23545;&#26368;&#32456;&#24615;&#33021;&#26377;&#36129;&#29486;&#65292;&#22240;&#27492;&#32852;&#21512;&#20248;&#21270;&#23427;&#20204;&#20197;&#23454;&#29616;&#30456;&#20114;&#25552;&#39640;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31264;&#23494;Passage&#26816;&#32034;&#21644;Passage&#37325;&#26032;&#25490;&#21517;&#30340;&#32852;&#21512;&#35757;&#32451;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#21160;&#24577;listwise&#33976;&#39311;&#65292;&#22312;&#20854;&#20013;&#20026;&#26816;&#32034;&#22120;&#21644;&#37325;&#26032;&#25490;&#21517;&#22120;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;listwise&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#21160;&#24577;&#33976;&#39311;&#36807;&#31243;&#20013;&#65292;&#26816;&#32034;&#22120;&#21644;&#37325;&#26032;&#25490;&#21517;&#22120;&#21487;&#20197;&#26681;&#25454;&#24444;&#27492;&#30340;&#30456;&#20851;&#24615;&#20449;&#24687;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#26500;&#36896;&#20016;&#23500;&#30340;listwise&#35757;&#32451;&#23454;&#20363;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MSMARCO&#21644;&#33258;&#28982;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/DensePhrases&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage re-ranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other's relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.c
&lt;/p&gt;</description></item><item><title>PAIR&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#20013;&#21516;&#26102;&#32771;&#34385;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#65292;&#36890;&#36807;&#27491;&#24335;&#20844;&#24335;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.06027</link><description>&lt;p&gt;
PAIR&#65306;&#21033;&#29992;&#27573;&#33853;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#25913;&#36827;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06027
&lt;/p&gt;
&lt;p&gt;
PAIR&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#20013;&#21516;&#26102;&#32771;&#34385;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#65292;&#36890;&#36807;&#27491;&#24335;&#20844;&#24335;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#24050;&#25104;&#20026;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25214;&#21040;&#30456;&#20851;&#20449;&#24687;&#30340;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#24191;&#27867;&#37319;&#29992;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#26102;&#20165;&#32771;&#34385;&#20102;&#26597;&#35810;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#30456;&#20284;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#65288;&#31216;&#20026;PAIR&#65289;&#36827;&#34892;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#20284;&#20851;&#31995;&#30340;&#27491;&#24335;&#20844;&#24335;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#32422;&#26463;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#37329;&#34701;&#25991;&#26723;&#19978;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2108.04080</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#25991;&#20214;&#20013;&#30340;&#24212;&#29992;--&#20197;&#32463;&#27982;&#39044;&#27979;FOMC&#20250;&#35758;&#35760;&#24405;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis in Document -- FOMC Meeting Minutes on Economic Projection. (arXiv:2108.04080v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.04080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#37329;&#34701;&#25991;&#26723;&#19978;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#32852;&#20648;&#20869;&#30340;&#32852;&#37030;&#20844;&#24320;&#24066;&#22330;&#22996;&#21592;&#20250;&#36127;&#36131;&#31649;&#29702;&#36890;&#32960;&#12289;&#26368;&#22823;&#21270;&#23601;&#19994;&#21644;&#31283;&#23450;&#21033;&#29575;&#12290;&#20250;&#35758;&#35760;&#24405;&#22312;&#24066;&#22330;&#27874;&#21160;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#32463;&#27982;&#22797;&#26434;&#24615;&#19981;&#26029;&#37325;&#26032;&#26435;&#34913;&#30340;&#40479;&#30640;&#22270;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20174;&#22823;&#22411;&#37329;&#34701;&#25991;&#26412;&#20013;&#20998;&#26512;&#21644;&#25552;&#21462;&#21508;&#31181;&#26041;&#38754;&#30340;&#24773;&#24863;&#20197;&#36827;&#34892;&#32463;&#27982;&#39044;&#27979;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#37329;&#34701;&#25968;&#25454;&#20013;&#24182;&#19981;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#35757;&#32451;&#37329;&#34701;&#25991;&#20214;&#19978;&#30340;ABSA&#65292;&#24182;&#20998;&#26512;&#20854;&#23545;&#21508;&#31181;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Federal Open Market Committee within the Federal Reserve System is responsible for managing inflation, maximizing employment, and stabilizing interest rates. Meeting minutes play an important role for market movements because they provide the birds eye view of how this economic complexity is constantly re-weighed. Therefore, There has been growing interest in analyzing and extracting sentiments on various aspects from large financial texts for economic projection. However, Aspect-based Sentiment Analysis is not widely used on financial data due to the lack of large labeled dataset. In this paper, I propose a model to train ABSA on financial documents under weak supervision and analyze its predictive power on various macroeconomic indicators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38405;&#35835;&#29702;&#35299;&#20013;&#20154;&#31867;&#20851;&#27880;&#21147;&#20998;&#37197;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25191;&#34892;&#30456;&#21516;&#30340;&#38405;&#35835;&#20219;&#21153;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#21333;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#65292;&#35835;&#32773;&#22312;&#31532;&#19968;&#36941;&#38405;&#35835;&#21644;&#37325;&#26032;&#38405;&#35835;&#36807;&#31243;&#20013;&#20998;&#21035;&#20851;&#27880;&#22522;&#26412;&#25991;&#26412;&#29305;&#24449;&#21644;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#25991;&#26412;&#29305;&#24449;&#21644;&#38382;&#39064;&#30456;&#20851;&#24615;&#20250;&#20998;&#21035;&#35843;&#33410;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2107.05799</link><description>&lt;p&gt;
&#20154;&#30340;&#27880;&#24847;&#21147;&#22312;&#30446;&#26631;&#23450;&#21521;&#38405;&#35835;&#29702;&#35299;&#26102;&#20381;&#36182;&#20110;&#20219;&#21153;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human Attention during Goal-directed Reading Comprehension Relies on Task Optimization. (arXiv:2107.05799v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38405;&#35835;&#29702;&#35299;&#20013;&#20154;&#31867;&#20851;&#27880;&#21147;&#20998;&#37197;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25191;&#34892;&#30456;&#21516;&#30340;&#38405;&#35835;&#20219;&#21153;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#21333;&#35789;&#30340;&#38405;&#35835;&#26102;&#38388;&#65292;&#35835;&#32773;&#22312;&#31532;&#19968;&#36941;&#38405;&#35835;&#21644;&#37325;&#26032;&#38405;&#35835;&#36807;&#31243;&#20013;&#20998;&#21035;&#20851;&#27880;&#22522;&#26412;&#25991;&#26412;&#29305;&#24449;&#21644;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#25991;&#26412;&#29305;&#24449;&#21644;&#38382;&#39064;&#30456;&#20851;&#24615;&#20250;&#20998;&#21035;&#35843;&#33410;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#20219;&#21153;&#20013;&#20851;&#27880;&#21147;&#20998;&#37197;&#30340;&#35745;&#31639;&#21407;&#21017;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#30446;&#26631;&#23450;&#21521;&#38405;&#35835;&#65292;&#21363;&#38405;&#35835;&#19968;&#31687;&#25991;&#31456;&#20197;&#22238;&#31572;&#33041;&#28023;&#20013;&#30340;&#38382;&#39064;&#65292;&#26159;&#19968;&#31181;&#24378;&#28872;&#24341;&#21457;&#27880;&#24847;&#21147;&#30340;&#24120;&#35265;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20160;&#20040;&#35745;&#31639;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#36825;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#20851;&#27880;&#21147;&#20998;&#37197;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20248;&#21270;&#25191;&#34892;&#30456;&#21516;&#38405;&#35835;&#20219;&#21153;&#30340;&#20851;&#27880;&#26435;&#37325;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#21333;&#35789;&#19978;&#30340;&#38405;&#35835;&#26102;&#38388;&#12290;&#30524;&#21160;&#36319;&#36394;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#35835;&#32773;&#22312;&#31532;&#19968;&#36941;&#38405;&#35835;&#21644;&#37325;&#26032;&#38405;&#35835;&#36807;&#31243;&#20013;&#20998;&#21035;&#20851;&#27880;&#22522;&#26412;&#25991;&#26412;&#29305;&#24449;&#21644;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#31867;&#20284;&#22320;&#65292;&#25991;&#26412;&#29305;&#24449;&#21644;&#38382;&#39064;&#30456;&#20851;&#24615;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;DNN&#23618;&#20013;&#20998;&#21035;&#35843;&#33410;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#24403;&#35835;&#32773;&#22312;&#33041;&#28023;&#20013;&#27809;&#26377;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25195;&#25551;&#19968;&#31687;&#25991;&#31456;&#26102;&#65292;&#20182;&#20204;&#30340;&#38405;&#35835;&#26102;&#38388;&#21487;&#20197;&#30001;&#20026;&#21333;&#35789;&#39044;&#27979;&#20219;&#21153;&#20248;&#21270;&#30340;DNN&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38405;&#35835;&#20013;&#20851;&#27880;&#21147;&#20998;&#37197;&#20381;&#36182;&#20110;&#20219;&#21153;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational principles underlying attention allocation in complex goal-directed tasks remain elusive. Goal-directed reading, i.e., reading a passage to answer a question in mind, is a common real-world task that strongly engages attention. Here, we investigate what computational models can explain attention distribution in this complex task. We show that the reading time on each word is predicted by the attention weights in transformer-based deep neural networks (DNNs) optimized to perform the same reading task. Eye-tracking further reveals that readers separately attend to basic text features and question-relevant information during first-pass reading and rereading, respectively. Similarly, text features and question relevance separately modulate attention weights in shallow and deep DNN layers. Furthermore, when readers scan a passage without a question in mind, their reading time is predicted by DNNs optimized for a word prediction task. Therefore, attention during real-world 
&lt;/p&gt;</description></item></channel></rss>