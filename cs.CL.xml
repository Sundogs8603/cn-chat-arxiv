<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36716;&#24405;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#31070;&#32463;&#21322;&#39532;&#23572;&#21487;&#22827;&#26465;&#20214;&#38543;&#26426;&#22330;&#65292;&#33021;&#22815;&#22312;PodcastFillers&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;6.4&#65285;&#65288;&#20998;&#27573;&#32423;&#21035;&#65289;&#21644;3.1&#65285;&#65288;&#20107;&#20214;&#32423;&#21035;&#65289;&#30340;&#32477;&#23545;F1&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.06475</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#21322;&#26465;&#20214;&#38543;&#26426;&#22330;&#30340;&#26080;&#38656;&#36716;&#24405;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transcription free filler word detection with Neural semi-CRFs. (arXiv:2303.06475v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36716;&#24405;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#31070;&#32463;&#21322;&#39532;&#23572;&#21487;&#22827;&#26465;&#20214;&#38543;&#26426;&#22330;&#65292;&#33021;&#22815;&#22312;PodcastFillers&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;6.4&#65285;&#65288;&#20998;&#27573;&#32423;&#21035;&#65289;&#21644;3.1&#65285;&#65288;&#20107;&#20214;&#32423;&#21035;&#65289;&#30340;&#32477;&#23545;F1&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a transcription-free filler word detection system that uses structured state space sequence model and neural semi-Markov conditional random fields, achieving an absolute F1 improvement of 6.4% (segment level) and 3.1% (event level) on the PodcastFillers dataset.
&lt;/p&gt;
&lt;p&gt;
&#38750;&#35821;&#35328;&#22635;&#20805;&#35789;&#65292;&#22914;&#8220;&#21999;&#8221;&#25110;&#8220;&#21834;&#8221;&#65292;&#22312;&#33258;&#21457;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#29992;&#20110;&#34920;&#36798;&#29369;&#35947;&#25110;&#19981;&#30830;&#23450;&#24615;&#12290;&#20197;&#21069;&#26816;&#27979;&#26576;&#20123;&#38750;&#35821;&#35328;&#22635;&#20805;&#35789;&#30340;&#24037;&#20316;&#39640;&#24230;&#20381;&#36182;&#20110;&#26469;&#33258;&#25104;&#29087;&#21830;&#19994;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#36716;&#24405;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;ASR&#31995;&#32479;&#22312;&#35768;&#22810;&#26041;&#38754;&#65288;&#20363;&#22914;&#39044;&#31639;&#12289;&#30446;&#26631;&#35821;&#35328;&#21644;&#35745;&#31639;&#33021;&#21147;&#65289;&#24182;&#19981;&#26222;&#36941;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#20110;ASR&#31995;&#32479;&#30340;&#22635;&#20805;&#35789;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#21644;&#31070;&#32463;&#21322;&#39532;&#23572;&#21487;&#22827;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;semi-CRFs&#65289;&#65292;&#25105;&#20204;&#22312;PodcastFillers&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;6.4&#65285;&#65288;&#20998;&#27573;&#32423;&#21035;&#65289;&#21644;3.1&#65285;&#65288;&#20107;&#20214;&#32423;&#21035;&#65289;&#30340;&#32477;&#23545;F1&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23545;&#26816;&#27979;&#32467;&#26524;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#20998;&#26512;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-linguistic filler words, such as "uh" or "um", are prevalent in spontaneous speech and serve as indicators for expressing hesitation or uncertainty. Previous works for detecting certain non-linguistic filler words are highly dependent on transcriptions from a well-established commercial automatic speech recognition (ASR) system. However, certain ASR systems are not universally accessible from many aspects, e.g., budget, target languages, and computational power. In this work, we investigate filler word detection system that does not depend on ASR systems. We show that, by using the structured state space sequence model (S4) and neural semi-Markov conditional random fields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment level) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a qualitative analysis on the detected results to analyze the limitations of our proposed system.
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06333</link><description>&lt;p&gt;
&#38477;&#33853;&#20254;&#65306;&#35780;&#20272;&#20132;&#20114;&#24335;&#20154;&#26426;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#39134;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#20026;&#20849;&#21516;&#30340;&#20889;&#20316;&#25104;&#26524;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#20110;&#20132;&#20114;&#24335;&#29615;&#22659;&#19979;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;Parachute&#23637;&#31034;&#20102;&#20132;&#20114;&#35780;&#20272;&#30340;&#32508;&#21512;&#35270;&#35282;&#65292;&#20854;&#20013;&#27599;&#20010;&#35780;&#20272;&#26041;&#38754;&#37117;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;Parachute&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>AVTALER&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;UI&#65292;&#32467;&#21512;&#20102;&#20154;&#31867;&#30340;&#29420;&#29305;&#25216;&#33021;&#21644;&#33258;&#21160;&#21270;&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#29992;&#25143;&#23545;&#21487;&#27604;&#36739;&#30340;&#25991;&#26412;&#25688;&#24405;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21644;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.06264</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#25345;&#23545;&#24179;&#34892;&#25991;&#26412;&#38598;&#21512;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#30340;&#20132;&#20114;&#24335;UI
&lt;/p&gt;
&lt;p&gt;
An Interactive UI to Support Sensemaking over Collections of Parallel Texts. (arXiv:2303.06264v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06264
&lt;/p&gt;
&lt;p&gt;
AVTALER&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;UI&#65292;&#32467;&#21512;&#20102;&#20154;&#31867;&#30340;&#29420;&#29305;&#25216;&#33021;&#21644;&#33258;&#21160;&#21270;&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#29992;&#25143;&#23545;&#21487;&#27604;&#36739;&#30340;&#25991;&#26412;&#25688;&#24405;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21644;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
AVTALER is an interactive UI that combines human skills and the advantages of automation to support users in sensemaking and contrasting comparable text excerpts.
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23478;&#21644;&#31185;&#23398;&#35760;&#32773;&#31561;&#20154;&#32463;&#24120;&#38656;&#35201;&#29702;&#35299;&#22823;&#37327;&#35770;&#25991;&#21450;&#20854;&#22312;&#33539;&#22260;&#12289;&#37325;&#28857;&#12289;&#21457;&#29616;&#25110;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#37327;&#30340;&#35770;&#25991;&#65292;&#36880;&#19968;&#36827;&#34892;&#27604;&#36739;&#21644;&#23545;&#27604;&#26159;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23436;&#20840;&#33258;&#21160;&#21270;&#36825;&#20010;&#23457;&#26597;&#36807;&#31243;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#29702;&#35299;&#23457;&#26597;&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#34429;&#28982;&#26377;&#29616;&#26377;&#30340;&#24037;&#20855;&#26469;&#24110;&#21161;&#32452;&#32455;&#21644;&#27880;&#37322;&#25991;&#29486;&#32508;&#36848;&#30340;&#35770;&#25991;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#20154;&#20204;&#36880;&#20010;&#38405;&#35835;&#35770;&#25991;&#24182;&#25163;&#21160;&#29702;&#35299;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AVTALER&#65292;&#23427;&#32467;&#21512;&#20102;&#20154;&#20204;&#29420;&#29305;&#30340;&#25216;&#33021;&#12289;&#19978;&#19979;&#25991;&#24847;&#35782;&#21644;&#30693;&#35782;&#65292;&#20197;&#21450;&#33258;&#21160;&#21270;&#30340;&#20248;&#21183;&#12290;&#32473;&#23450;&#19968;&#32452;&#21487;&#27604;&#36739;&#30340;&#25991;&#26412;&#25688;&#24405;&#65292;&#23427;&#25903;&#25345;&#29992;&#25143;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21644;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientists and science journalists, among others, often need to make sense of a large number of papers and how they compare with each other in scope, focus, findings, or any other important factors. However, with a large corpus of papers, it's cognitively demanding to pairwise compare and contrast them all with each other. Fully automating this review process would be infeasible, because it often requires domain-specific knowledge, as well as understanding what the context and motivations for the review are. While there are existing tools to help with the process of organizing and annotating papers for literature reviews, at the core they still rely on people to serially read through papers and manually make sense of relevant information.  We present AVTALER, which combines peoples' unique skills, contextual awareness, and knowledge, together with the strength of automation. Given a set of comparable text excerpts from a paper corpus, it supports users in sensemaking and contrasting pa
&lt;/p&gt;</description></item><item><title>AUTODIAL&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#26469;&#25191;&#34892;&#23545;&#35805;&#20219;&#21153;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;AUTODIAL&#22312;&#19977;&#20010;&#23545;&#35805;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;3-6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;11&#20493;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;&#36825;&#34920;&#26126;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06245</link><description>&lt;p&gt;
AUTODIAL: &#39640;&#25928;&#24322;&#27493;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06245
&lt;/p&gt;
&lt;p&gt;
AUTODIAL&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#26469;&#25191;&#34892;&#23545;&#35805;&#20219;&#21153;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;AUTODIAL&#22312;&#19977;&#20010;&#23545;&#35805;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;3-6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;11&#20493;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;&#36825;&#34920;&#26126;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
AUTODIAL is a multi-task dialogue model that significantly reduces memory footprint and achieves faster inference times by using parallel decoders to perform dialogue tasks. Compared to existing generative approach, AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks. This suggests that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#23545;&#35805;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#26222;&#36941;&#65292;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTODIAL&#65292;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#23545;&#35805;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;AUTODIAL&#21033;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#25191;&#34892;&#35832;&#22914;&#23545;&#35805;&#34892;&#20026;&#39044;&#27979;&#12289;&#39046;&#22495;&#39044;&#27979;&#12289;&#24847;&#22270;&#39044;&#27979;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31561;&#20219;&#21153;&#12290;&#20351;&#29992;&#20998;&#31867;&#35299;&#30721;&#22120;&#32780;&#19981;&#26159;&#29983;&#25104;&#35299;&#30721;&#22120;&#20351;AUTODIAL&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#19978;&#23454;&#29616;&#27604;&#29616;&#26377;&#29983;&#25104;&#26041;&#27861;&#65288;&#21363;SimpleTOD&#65289;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large dialogue models become commonplace in practice, the problems surrounding high compute requirements for training, inference and larger memory footprint still persists. In this work, we present AUTODIAL, a multi-task dialogue model that addresses the challenges of deploying dialogue model. AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act prediction, domain prediction, intent prediction, and dialogue state tracking. Using classification decoders over generative decoders allows AUTODIAL to significantly reduce memory footprint and achieve faster inference times compared to existing generative approach namely SimpleTOD. We demonstrate that AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks compared to SimpleTOD. Our results show that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20351;&#29992;&#24494;&#35843;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36793;&#38469;&#26368;&#22823;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#26041;&#27861;&#30452;&#25509;&#20174;&#26032;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#27493;&#39588;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06230</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#24494;&#35843;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generating Query Focused Summaries without Fine-tuning the Transformer-based Pre-trained Models. (arXiv:2303.06230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20351;&#29992;&#24494;&#35843;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36793;&#38469;&#26368;&#22823;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#26041;&#27861;&#30452;&#25509;&#20174;&#26032;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#27493;&#39588;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the method of generating query-focused summaries without fine-tuning transformer-based pre-trained models, using the Marginal Maximum Relevance (MMR) approach to obtain query-focused summaries directly from a new dataset without fine-tuning, reducing computational time and cost.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#27599;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24494;&#35843;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#38656;&#35201;&#26356;&#39640;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#20276;&#38543;&#30528;&#22686;&#21152;&#30340;&#30899;&#36275;&#36857;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26377;&#21161;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#25105;&#20204;&#36991;&#20813;&#24494;&#35843;&#27493;&#39588;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23581;&#35797;&#29983;&#25104;&#25688;&#35201;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#20250;&#24590;&#26679;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#30465;&#30053;&#24494;&#35843;&#27493;&#39588;&#65292;&#24182;&#35843;&#26597;&#36793;&#38469;&#26368;&#22823;&#30456;&#20851;&#24615;&#65288;MMR&#65289;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#20174;&#26410;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#22312;&#32500;&#22522;&#30334;&#31185;&#24403;&#21069;&#20107;&#20214;&#38376;&#25143;&#65288;WCEP&#65289;&#21644;Debatepedia&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#25688;&#35201;&#20219;&#21153;&#30340;&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;MMR&#65292;&#25105;&#20204;&#26681;&#25454;&#26597;&#35810;&#23545;&#25991;&#26723;&#30340;&#21477;&#23376;&#36827;&#34892;&#25490;&#21517;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#21518;&#30340;&#21477;&#23376;&#20256;&#36882;&#32473;&#19971;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25191;&#34892;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning the Natural Language Processing (NLP) models for each new data set requires higher computational time associated with increased carbon footprint and cost. However, fine-tuning helps the pre-trained models adapt to the latest data sets; what if we avoid the fine-tuning steps and attempt to generate summaries using just the pre-trained models to reduce computational time and cost. In this paper, we tried to omit the fine-tuning steps and investigate whether the Marginal Maximum Relevance (MMR)-based approach can help the pre-trained models to obtain query-focused summaries directly from a new data set that was not used to pre-train the models. First, we used topic modelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to generate queries for summarization tasks. Then, using MMR, we ranked the sentences of the documents according to the queries. Next, we passed the ranked sentences to seven transformer-based pre-trained models to perform the summarization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06182</link><description>&lt;p&gt;
&#36808;&#21521;MoE&#37096;&#32626;&#65306;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#25512;&#29702;&#20013;&#30340;&#20302;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;MoE&#24037;&#20316;&#36127;&#36733;&#30340;&#29305;&#24449;&#21270;&#65292;&#21363;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#20302;&#25928;&#29575;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#20302;&#25928;&#29575;&#30340;&#26469;&#28304;&#65292;&#21363;&#65288;1&#65289;&#21160;&#24577;&#38376;&#25511;&#65292;&#65288;2&#65289;&#19987;&#23478;&#32531;&#20914;&#21644;&#65288;3&#65289;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#38376;&#25511;&#21487;&#20197;&#20351;LM&#30340;&#25191;&#34892;&#26102;&#38388;&#25552;&#39640;1.25-4&#20493;&#65292;MT&#32534;&#30721;&#22120;&#25552;&#39640;2-5&#20493;&#65292;MT&#35299;&#30721;&#22120;&#25552;&#39640;1.09-1.5&#20493;&#12290;&#23427;&#36824;&#21487;&#20197;&#23558;LM&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.36&#20493;&#65292;MT&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#26131;&#35835;&#24615;&#23545;&#20505;&#36873;&#25200;&#21160;&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20445;&#25345;&#26131;&#35835;&#24615;&#30340;&#25991;&#26412;&#25200;&#21160;&#30340;&#31995;&#32479;&#24615;&#34920;&#24449;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.05077</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#25991;&#26412;&#25200;&#21160;&#30340;&#26131;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Legibility of Visual Text Perturbations. (arXiv:2303.05077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#26131;&#35835;&#24615;&#23545;&#20505;&#36873;&#25200;&#21160;&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20445;&#25345;&#26131;&#35835;&#24615;&#30340;&#25991;&#26412;&#25200;&#21160;&#30340;&#31995;&#32479;&#24615;&#34920;&#24449;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method to learn models that predict the legibility of a perturbed string and rank candidate perturbations based on their legibility, filling the gap in systematically characterizing the legibility of text perturbations while preserving it.
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;NLP&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#20250;&#25200;&#21160;&#36755;&#20837;&#20197;&#20135;&#29983;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#20294;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#36127;&#38754;&#24433;&#21709;&#30340;&#23383;&#31526;&#20018;&#65288;&#20363;&#22914;'ergo' $\rightarrow$ '$\epsilon$rgo'&#65289;&#65292;&#36825;&#20123;&#23383;&#31526;&#20018;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26131;&#35835;&#30340;&#12290;&#23613;&#31649;&#20445;&#25345;&#26131;&#35835;&#24615;&#26159;&#25991;&#26412;&#25200;&#21160;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#23545;&#20854;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#34920;&#24449;&#65307;&#30456;&#21453;&#65292;&#26131;&#35835;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25200;&#21160;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#30340;&#30452;&#35273;&#32422;&#26463;&#26469;&#23454;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#20445;&#25345;&#26131;&#35835;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25200;&#21160;&#22810;&#23569;&#36755;&#20837;&#65292;&#25110;&#22914;&#20309;&#37327;&#21270;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25200;&#21160;&#23383;&#31526;&#20018;&#30340;&#26131;&#35835;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#26131;&#35835;&#24615;&#23545;&#20505;&#36873;&#25200;&#21160;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;LEGIT&#65292;&#19968;&#20010;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#35270;&#35273;&#19978;&#25200;&#21160;&#25991;&#26412;&#30340;&#26131;&#35835;&#24615;&#12290;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;$0.91$&#30340;F1&#20998;&#25968;&#65292;&#20197;&#39044;&#27979;&#36755;&#20837;&#26159;&#21542;&#26131;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many adversarial attacks in NLP perturb inputs to produce visually similar strings ('ergo' $\rightarrow$ '$\epsilon$rgo') which are legible to humans but degrade model performance. Although preserving legibility is a necessary condition for text perturbation, little work has been done to systematically characterize it; instead, legibility is typically loosely enforced via intuitions around the nature and extent of perturbations. Particularly, it is unclear to what extent can inputs be perturbed while preserving legibility, or how to quantify the legibility of a perturbed string. In this work, we address this gap by learning models that predict the legibility of a perturbed string, and rank candidate perturbations based on their legibility. To do so, we collect and release LEGIT, a human-annotated dataset comprising the legibility of visually perturbed text. Using this dataset, we build both text- and vision-based models which achieve up to $0.91$ F1 score in predicting whether an input
&lt;/p&gt;</description></item><item><title>NASTyLinker&#26159;&#19968;&#31181;NIL&#24863;&#30693;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#26469;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#24182;&#22312;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#39640;&#38142;&#25509;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.04426</link><description>&lt;p&gt;
NASTyLinker&#65306;NIL&#24863;&#30693;&#21487;&#25193;&#23637;&#22522;&#20110;Transformer&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker. (arXiv:2303.04426v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04426
&lt;/p&gt;
&lt;p&gt;
NASTyLinker&#26159;&#19968;&#31181;NIL&#24863;&#30693;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#26469;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#24182;&#22312;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#39640;&#38142;&#25509;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
NASTyLinker is a NIL-aware entity linker that represents NIL entities by producing mention clusters and resolves conflicts while maintaining high linking performance for known entities.
&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#26816;&#27979;&#25991;&#26412;&#20013;&#23454;&#20307;&#25552;&#21450;&#24182;&#23558;&#20854;&#28040;&#27495;&#20026;&#21442;&#32771;&#30693;&#35782;&#24211;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;EL&#26041;&#27861;&#20551;&#23450;&#21442;&#32771;&#30693;&#35782;&#24211;&#26159;&#23436;&#25972;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#38656;&#35201;&#22788;&#29702;&#38142;&#25509;&#21040;&#19981;&#21253;&#21547;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#65288;NIL&#23454;&#20307;&#65289;&#30340;&#24773;&#20917;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32771;&#34385;&#25552;&#21450;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#12290;&#21516;&#26102;&#65292;&#25552;&#21450;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21487;&#20197;&#24110;&#21161;&#26174;&#33879;&#25552;&#39640;&#24050;&#30693;&#23454;&#20307;&#30340;&#38142;&#25509;&#24615;&#33021;&#12290;&#36890;&#36807;NASTyLinker&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;EL&#26041;&#27861;&#65292;&#23427;&#30693;&#36947;NIL&#23454;&#20307;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#25552;&#21450;&#31751;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#30340;&#39640;&#38142;&#25509;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#30340;&#23494;&#38598;&#34920;&#31034;&#23545;&#25552;&#21450;&#21644;&#23454;&#20307;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#35299;&#20915;&#20914;&#31361;&#65288;&#22914;&#26524;&#19968;&#20010;&#23454;&#20307;&#26377;&#22810;&#20010;&#25552;&#21450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is the task of detecting mentions of entities in text and disambiguating them to a reference knowledge base. Most prevalent EL approaches assume that the reference knowledge base is complete. In practice, however, it is necessary to deal with the case of linking to an entity that is not contained in the knowledge base (NIL entity). Recent works have shown that, instead of focusing only on affinities between mentions and entities, considering inter-mention affinities can be used to represent NIL entities by producing clusters of mentions. At the same time, inter-mention affinities can help to substantially improve linking performance for known entities. With NASTyLinker, we introduce an EL approach that is aware of NIL entities and produces corresponding mention clusters while maintaining high linking performance for known entities. The approach clusters mentions and entities based on dense representations from Transformers and resolves conflicts (if more than one en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02846</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#21644;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Contrastive Variational Information Bottleneck framework (CVIB) to reduce spurious correlations for aspect-based sentiment analysis (ABSA). The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning, which discards the superfluous patterns or spurious correlations between input features and prediction labels.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#25991;&#29486;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#22312;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#36825;&#20250;&#32473;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24102;&#26469;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;&#31216;&#20026;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;ABSA&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;CVIB&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;VIB&#65289;&#21407;&#21017;&#20174;&#21407;&#22987;&#32593;&#32476;&#20013;&#23398;&#20064;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#19988;&#21387;&#32553;&#30340;&#32593;&#32476;&#65288;&#33258;&#21098;&#26525;&#32593;&#32476;&#65289;&#65292;&#35813;&#32593;&#32476;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#21098;&#26525;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23558;&#20004;&#20010;&#32593;&#32476;&#25289;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have dominated the literature on aspect-based sentiment analysis (ABSA), yielding state-of-the-art results. However, these deep models generally suffer from spurious correlation problems between input features and output labels, which creates significant barriers to robustness and generalization capability. In this paper, we propose a novel Contrastive Variational Information Bottleneck framework (called CVIB) to reduce spurious correlations for ABSA. The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning. Concretely, we employ the Variational Information Bottleneck (VIB) principle to learn an informative and compressed network (self-pruned network) from the original network, which discards the superfluous patterns or spurious correlations between input features and prediction labels. Then, self-pruning contrastive learning is devised to pull together
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#30456;&#20851;&#24615;&#24314;&#31435;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.02563</link><description>&lt;p&gt;
FinXABSA: &#36890;&#36807;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37329;&#34701;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis. (arXiv:2303.02563v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#30456;&#20851;&#24615;&#24314;&#31435;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an aspect-based sentiment analysis approach to achieve explainability in financial analysis by establishing a relationship with stock prices using the Pearson correlation coefficient. The proposed methodology provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Pearson&#30456;&#20851;&#31995;&#25968;&#24314;&#31435;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20174;&#37329;&#34701;&#26032;&#38395;&#25991;&#31456;&#20013;&#26500;&#24314;&#26041;&#38754;&#21015;&#34920;&#65292;&#24182;&#20998;&#26512;&#27599;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#24378;&#24230;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;Pearson&#31995;&#25968;&#23558;&#36825;&#20123;&#24471;&#20998;&#19982;&#30456;&#20851;&#20844;&#21496;&#30340;&#32929;&#31080;&#20215;&#26684;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#20219;&#20309;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#35299;&#37322;&#24773;&#24863;&#20998;&#26512;&#32467;&#26524;&#21450;&#20854;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#24433;&#21709;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#24615;&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for explainability in financial analysis by utilizing the Pearson correlation coefficient to establish a relationship between aspect-based sentiment analysis and stock prices. The proposed methodology involves constructing an aspect list from financial news articles and analyzing sentiment intensity scores for each aspect. These scores are then compared to the stock prices for the relevant companies using the Pearson coefficient to determine any significant correlations. The results indicate that the proposed approach provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions. Additionally, this methodology offers a transparent and interpretable way to explain the sentiment analysis results and their impact on stock prices. Overall, the findings of this paper demonstrate the importance of explainability in f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21333;&#35843;&#27979;&#35797;&#38598;SiMuST-C&#65292;&#29992;&#20110;&#35780;&#20272;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SimulMT&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27979;&#35797;&#38598;&#21487;&#20197;&#32531;&#35299;&#20302;&#20272;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#33258;&#21160;&#25552;&#21462;&#30340;&#21333;&#35843;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#23558;SimulMT&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;3&#20010;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.00969</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27979;&#35797;&#38598;&#30340;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Reasonability of the Test Set for Simultaneous Machine Translation. (arXiv:2303.00969v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21333;&#35843;&#27979;&#35797;&#38598;SiMuST-C&#65292;&#29992;&#20110;&#35780;&#20272;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SimulMT&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27979;&#35797;&#38598;&#21487;&#20197;&#32531;&#35299;&#20302;&#20272;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#33258;&#21160;&#25552;&#21462;&#30340;&#21333;&#35843;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#23558;SimulMT&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;3&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a manually annotated monotonic test set SiMuST-C for evaluating the performance of simultaneous machine translation (SimulMT) models, which can alleviate the underestimation problem and fine-tuning on an automatically extracted monotonic training set can improve SimulMT models by up to 3 BLEU points.
&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SimulMT&#65289;&#27169;&#22411;&#22312;&#28304;&#35821;&#21477;&#32467;&#26463;&#21069;&#24320;&#22987;&#32763;&#35793;&#65292;&#20351;&#24471;&#32763;&#35793;&#19982;&#28304;&#35821;&#21477;&#21333;&#35843;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#30340;&#23436;&#25972;&#21477;&#23376;&#32763;&#35793;&#27979;&#35797;&#38598;&#26159;&#36890;&#36807;&#31163;&#32447;&#32763;&#35793;&#25972;&#20010;&#28304;&#35821;&#21477;&#33719;&#24471;&#30340;&#65292;&#36825;&#19981;&#26159;&#20026;SimulMT&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#65292;&#36825;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#36825;&#26159;&#21542;&#20250;&#20302;&#20272;SimulMT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;MuST-C&#33521;&#27721;&#27979;&#35797;&#38598;&#25163;&#21160;&#27880;&#37322;&#20102;&#19968;&#20010;&#21333;&#35843;&#27979;&#35797;&#38598;&#65292;&#31216;&#20026;SiMuST-C&#12290;&#25105;&#20204;&#30340;&#20154;&#24037;&#35780;&#20272;&#30830;&#35748;&#20102;&#25105;&#20204;&#27880;&#37322;&#30340;&#27979;&#35797;&#38598;&#30340;&#21487;&#25509;&#21463;&#24615;&#12290;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;SimulMT&#27169;&#22411;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27979;&#35797;&#38598;&#21487;&#20197;&#32531;&#35299;&#20302;&#20272;&#38382;&#39064;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#33258;&#21160;&#25552;&#21462;&#30340;&#21333;&#35843;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#23558;SimulMT&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;3&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SimulMT) models start translation before the end of the source sentence, making the translation monotonically aligned with the source sentence. However, the general full-sentence translation test set is acquired by offline translation of the entire source sentence, which is not designed for SimulMT evaluation, making us rethink whether this will underestimate the performance of SimulMT models. In this paper, we manually annotate a monotonic test set based on the MuST-C English-Chinese test set, denoted as SiMuST-C. Our human evaluation confirms the acceptability of our annotated test set. Evaluations on three different SimulMT models verify that the underestimation problem can be alleviated on our test set. Further experiments show that finetuning on an automatically extracted monotonic training set improves SimulMT models by up to 3 BLEU points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20107;&#20214;&#39537;&#21160;&#26032;&#38395;&#21465;&#20107;&#25552;&#21462;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#31579;&#36873;&#36229;&#36807;900&#31687;&#25991;&#31456;&#65292;&#24471;&#21040;&#20102;54&#31687;&#30456;&#20851;&#25991;&#31456;&#65292;&#36825;&#20123;&#25991;&#31456;&#36890;&#36807;&#34920;&#31034;&#27169;&#22411;&#12289;&#25552;&#21462;&#26631;&#20934;&#21644;&#35780;&#20272;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#32508;&#21512;&#21644;&#32452;&#32455;&#12290;</title><link>http://arxiv.org/abs/2302.08351</link><description>&lt;p&gt;
&#20107;&#20214;&#39537;&#21160;&#26032;&#38395;&#21465;&#20107;&#25552;&#21462;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Event-based News Narrative Extraction. (arXiv:2302.08351v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20107;&#20214;&#39537;&#21160;&#26032;&#38395;&#21465;&#20107;&#25552;&#21462;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#31579;&#36873;&#36229;&#36807;900&#31687;&#25991;&#31456;&#65292;&#24471;&#21040;&#20102;54&#31687;&#30456;&#20851;&#25991;&#31456;&#65292;&#36825;&#20123;&#25991;&#31456;&#36890;&#36807;&#34920;&#31034;&#27169;&#22411;&#12289;&#25552;&#21462;&#26631;&#20934;&#21644;&#35780;&#20272;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#32508;&#21512;&#21644;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey presents an extensive study of research in the area of event-based news narrative extraction, screening over 900 articles and synthesizing 54 relevant articles organized by representation model, extraction criteria, and evaluation application.
&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#26159;&#25105;&#20204;&#29702;&#35299;&#19990;&#30028;&#30340;&#22522;&#30784;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#26102;&#38388;&#30693;&#35782;&#34920;&#31034;&#32467;&#26500;&#12290;&#35745;&#31639;&#26426;&#21465;&#20107;&#25552;&#21462;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#23427;&#22823;&#37327;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#21465;&#20107;&#25552;&#21462;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#21644;&#31574;&#21010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#38754;&#65292;&#23398;&#26415;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29305;&#21035;&#26159;&#65292;&#26412;&#25991;&#20391;&#37325;&#20110;&#20174;&#20107;&#20214;&#20013;&#24515;&#30340;&#35282;&#24230;&#25552;&#21462;&#26032;&#38395;&#21465;&#20107;&#12290;&#20174;&#26032;&#38395;&#25968;&#25454;&#20013;&#25552;&#21462;&#21465;&#20107;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#20449;&#24687;&#26223;&#35266;&#26041;&#38754;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#12290;&#26412;&#32508;&#36848;&#23545;&#20107;&#20214;&#39537;&#21160;&#26032;&#38395;&#21465;&#20107;&#25552;&#21462;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31579;&#36873;&#20102;&#36229;&#36807;900&#31687;&#25991;&#31456;&#65292;&#24471;&#21040;&#20102;54&#31687;&#30456;&#20851;&#25991;&#31456;&#12290;&#36825;&#20123;&#25991;&#31456;&#36890;&#36807;&#34920;&#31034;&#27169;&#22411;&#12289;&#25552;&#21462;&#26631;&#20934;&#21644;&#35780;&#20272;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#32508;&#21512;&#21644;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narratives are fundamental to our understanding of the world, providing us with a natural structure for knowledge representation over time. Computational narrative extraction is a subfield of artificial intelligence that makes heavy use of information retrieval and natural language processing techniques. Despite the importance of computational narrative extraction, relatively little scholarly work exists on synthesizing previous research and strategizing future research in the area. In particular, this article focuses on extracting news narratives from an event-centric perspective. Extracting narratives from news data has multiple applications in understanding the evolving information landscape. This survey presents an extensive study of research in the area of event-based news narrative extraction. In particular, we screened over 900 articles that yielded 54 relevant articles. These articles are synthesized and organized by representation model, extraction criteria, and evaluation app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdapterSoup&#65292;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#19978;&#25191;&#34892;&#26435;&#37325;&#31354;&#38388;&#24179;&#22343;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07027</link><description>&lt;p&gt;
AdapterSoup&#65306;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdapterSoup&#65292;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#19978;&#25191;&#34892;&#26435;&#37325;&#31354;&#38388;&#24179;&#22343;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
AdapterSoup is a method that uses weight averaging to improve the generalization ability of pretrained language models. It performs weight-space averaging of adapters trained on different domains, and can improve performance to new domains without extra training.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#29305;&#21270;&#12290;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#36866;&#24212;&#26041;&#27861;&#24314;&#35758;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#19968;&#20010;&#36866;&#37197;&#22120;&#12290;&#36825;&#23548;&#33268;&#20102;&#33391;&#22909;&#30340;&#39046;&#22495;&#20869;&#24471;&#20998;&#65292;&#20294;&#22312;&#39046;&#22495;&#25110;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#30456;&#20851;&#39046;&#22495;&#36866;&#37197;&#22120;&#26469;&#22788;&#29702;&#26032;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AdapterSoup&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#19978;&#25191;&#34892;&#26435;&#37325;&#31354;&#38388;&#24179;&#22343;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#30340;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#32452;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65307;&#28982;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#30830;&#23450;&#22312;&#27979;&#35797;&#26102;&#24212;&#24179;&#22343;&#21738;&#20123;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;AdapterSoup&#22987;&#32456;&#25552;&#39640;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#19979;&#35757;&#32451;&#30340;&#30456;&#21516;&#39046;&#22495;&#36866;&#37197;&#22120;&#30340;&#26435;&#37325;&#24179;&#22343;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain- or resource-restricted settings. A solution is to use a related-domain adapter for the novel domain at test time. In this paper, we introduce AdapterSoup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25913;&#36827;&#23454;&#26102;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.13294</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25913;&#36827;&#23454;&#26102;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates how to use in-context learning of large language models to improve real-time adaptive machine translation, and the experimental results show promising effects.
&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#26159;&#39640;&#36136;&#37327;&#32763;&#35793;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#39033;&#30446;&#20013;&#65292;&#36981;&#24490;&#39044;&#20808;&#25209;&#20934;&#30340;&#26415;&#35821;&#24182;&#36866;&#24212;&#26356;&#27491;&#30340;&#32763;&#35793;&#23588;&#20026;&#37325;&#35201;&#12290;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#36866;&#24212;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#36259;&#33021;&#21147;&#65292;&#23427;&#20204;&#23398;&#20064;&#22797;&#21046;&#26576;&#20123;&#36755;&#20837;-&#36755;&#20986;&#25991;&#26412;&#29983;&#25104;&#27169;&#24335;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#38388;&#23558;LLM&#25552;&#20379;&#32473;&#30001;&#32763;&#35793;&#23545;&#21015;&#34920;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#39046;&#22495;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25913;&#36827;&#23454;&#26102;&#33258;&#36866;&#24212;MT&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#22312;&#32763;&#35793;&#26102;&#38388;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;GPT-3.5&#21487;&#20197;&#22312;&#32763;&#35793;&#26032;&#21477;&#23376;&#26102;&#36866;&#24212;&#19968;&#32452;&#39046;&#22495;&#20869;&#30340;&#21477;&#23376;&#23545;&#21644;/&#25110;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.02623</link><description>&lt;p&gt;
&#32479;&#19968;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the Universal Document Processing (UDOP) model, which unifies text, image, and layout modalities together with varied task formats, and achieves pretraining and multi-domain downstream tasks unification through a novel Transformer model. It also achieves high-quality neural document editing and content customization.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#25991;&#26723;AI&#27169;&#22411;&#65292;&#23427;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#65288;&#21253;&#25324;&#25991;&#26723;&#29702;&#35299;&#21644;&#29983;&#25104;&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;UDOP&#21033;&#29992;&#25991;&#26412;&#20869;&#23481;&#21644;&#25991;&#26723;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Vision-Text-Layout Transformer&#65292;UDOP&#23558;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#32479;&#19968;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#24207;&#21015;&#29983;&#25104;&#26041;&#26696;&#20013;&#12290;UDOP&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#22810;&#26679;&#21270;&#26631;&#35760;&#25968;&#25454;&#19978;&#20351;&#29992;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;UDOP&#36824;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#37325;&#24314;&#23398;&#20064;&#20174;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#29983;&#25104;&#25991;&#26723;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#25991;&#26723;AI&#39046;&#22495;&#20013;&#31532;&#19968;&#27425;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;8&#20010;&#25991;&#26723;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Docu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.16270</link><description>&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#65306;&#37319;&#29992;&#36880;&#26679;&#26412;&#35745;&#31639;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation. (arXiv:2211.16270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a memory-efficient training method for neural transducer, which computes the transducer loss and gradients sample by sample, significantly reducing memory usage and performing at competitive speed compared to the default batched computation.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#34429;&#28982;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#27969;&#24335;ASR&#65292;&#20294;&#35757;&#32451;&#36807;&#31243;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20869;&#23384;&#38656;&#27714;&#21487;&#33021;&#20250;&#36805;&#36895;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;GPU&#30340;&#23481;&#37327;&#65292;&#38480;&#21046;&#25209;&#37327;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20856;&#22411;&#36716;&#24405;&#22120;&#35757;&#32451;&#35774;&#32622;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#36880;&#26679;&#26412;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#24182;&#34892;&#24615;&#12290;&#22312;&#19968;&#32452;&#24443;&#24213;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36880;&#26679;&#26412;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;&#20316;&#20026;&#20142;&#28857;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;&#20165;6 GB&#30340;&#20869;&#23384;&#35745;&#31639;&#20102;&#25209;&#37327;&#22823;&#23567;&#20026;1024&#65292;&#38899;&#39057;&#38271;&#24230;&#20026;40&#31186;&#30340;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural transducer is an end-to-end model for automatic speech recognition (ASR). While the model is well-suited for streaming ASR, the training process remains challenging. During training, the memory requirements may quickly exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence lengths. In this work, we analyze the time and space complexity of a typical transducer training setup. We propose a memory-efficient training method that computes the transducer loss and gradients sample by sample. We present optimizations to increase the efficiency and parallelism of the sample-wise method. In a set of thorough benchmarks, we show that our sample-wise method significantly reduces memory usage, and performs at competitive speed when compared to the default batched computation. As a highlight, we manage to compute the transducer loss and gradients for a batch size of 1024, and audio length of 40 seconds, using only 6 GB of memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;ESAL&#65292;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#26469;&#26816;&#32034;&#19981;&#21516;&#31867;&#21035;&#30340;&#35821;&#20041;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ESAL&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#30103;&#20449;&#24687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15544</link><description>&lt;p&gt;
&#33258;&#21160;&#25552;&#21462;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#20449;&#24687;&#65306;&#19987;&#23478;&#31995;&#32479;&#21644;&#26631;&#27880;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Automatically Extracting Information in Medical Dialogue: Expert System And Attention for Labelling. (arXiv:2211.15544v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;ESAL&#65292;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#26469;&#26816;&#32034;&#19981;&#21516;&#31867;&#21035;&#30340;&#35821;&#20041;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ESAL&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#30103;&#20449;&#24687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel model ESAL, which uses mixture of experts and pre-trained BERT to retrieve the semantics of different categories, enabling the model to fuse the differences between them. The experimental results indicate that ESAL significantly improves the performance of Medical Information Classification.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#27491;&#22312;&#25104;&#20026;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#30005;&#23376;&#30149;&#21382;&#65288;EMR&#65289;&#25968;&#37327;&#24222;&#22823;&#65292;&#20174;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26159;&#22256;&#38590;&#30340;&#12290;&#20197;&#21069;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26469;&#20174;EMR&#20013;&#26816;&#32034;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#26080;&#27861;&#35782;&#21035;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21363;&#19987;&#23478;&#31995;&#32479;&#21644;&#26631;&#27880;&#27880;&#24847;&#21147;&#65288;ESAL&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#26469;&#26816;&#32034;&#19981;&#21516;&#31867;&#21035;&#30340;&#35821;&#20041;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ESAL&#34987;&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ESAL&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#30103;&#20449;&#24687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue information extraction is becoming an increasingly significant problem in modern medical care. It is difficult to extract key information from electronic medical records (EMRs) due to their large numbers. Previously, researchers proposed attention-based models for retrieving features from EMRs, but their limitations were reflected in their inability to recognize different categories in medical dialogues. In this paper, we propose a novel model, Expert System and Attention for Labelling (ESAL). We use mixture of experts and pre-trained BERT to retrieve the semantics of different categories, enabling the model to fuse the differences between them. In our experiment, ESAL was applied to a public dataset and the experimental results indicated that ESAL significantly improved the performance of Medical Information Classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#19988;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2211.05103</link><description>&lt;p&gt;
&#24847;&#22806;&#23398;&#20064;&#32773;&#65306;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#19988;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper extends previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. The pre-trained speech models optimally encode language discriminatory information in lower layers, and the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, the authors achieve results similar to current state-of-the-art systems for language identification, with 5x less parameters. The model is open-sourced through the NVIDIA NeMo toolkit.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;&#25105;&#20204;&#36890;&#36807;NVIDIA NeMo&#24037;&#20855;&#21253;&#24320;&#28304;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extend previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. We find that pre-trained speech models optimally encode language discriminatory information in lower layers. Further, we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, we achieve results similar to current state-of-the-art systems for language identification. More, our model accomplishes this with 5x less parameters. We open-source the model through the NVIDIA NeMo toolkit.
&lt;/p&gt;</description></item><item><title>BLOOM&#26159;&#19968;&#20010;&#30001;&#25968;&#30334;&#21517;&#30740;&#31350;&#20154;&#21592;&#21512;&#20316;&#35774;&#35745;&#21644;&#26500;&#24314;&#30340;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36827;&#34892;&#22810;&#20219;&#21153;&#25552;&#31034;&#24494;&#35843;&#21518;&#34920;&#29616;&#26356;&#24378;&#12290;&#35813;&#27169;&#22411;&#30340;&#21457;&#24067;&#26377;&#21161;&#20110;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.05100</link><description>&lt;p&gt;
BLOOM: &#19968;&#20010;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05100
&lt;/p&gt;
&lt;p&gt;
BLOOM&#26159;&#19968;&#20010;&#30001;&#25968;&#30334;&#21517;&#30740;&#31350;&#20154;&#21592;&#21512;&#20316;&#35774;&#35745;&#21644;&#26500;&#24314;&#30340;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36827;&#34892;&#22810;&#20219;&#21153;&#25552;&#31034;&#24494;&#35843;&#21518;&#34920;&#29616;&#26356;&#24378;&#12290;&#35813;&#27169;&#22411;&#30340;&#21457;&#24067;&#26377;&#21161;&#20110;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
BLOOM is an open-access language model with 176B parameters, designed and built by hundreds of researchers. It achieves competitive performance on various benchmarks and shows even stronger results after multitask prompted finetuning. The release of this model facilitates the democratization of large language model technology.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26681;&#25454;&#23569;&#37327;&#28436;&#31034;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;LLMs&#37117;&#26159;&#30001;&#36164;&#28304;&#20016;&#23500;&#30340;&#32452;&#32455;&#24320;&#21457;&#30340;&#65292;&#24182;&#32463;&#24120;&#34987;&#20445;&#23494;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#31181;&#24378;&#22823;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BLOOM&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#25968;&#30334;&#21517;&#30740;&#31350;&#20154;&#21592;&#21512;&#20316;&#35774;&#35745;&#21644;&#26500;&#24314;&#30340;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;BLOOM&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;ROOTS&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;46&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;13&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#25968;&#30334;&#20010;&#26469;&#28304;&#65288;&#24635;&#20849;59&#31181;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;BLOOM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#22312;&#36827;&#34892;&#22810;&#20219;&#21153;&#25552;&#31034;&#24494;&#35843;&#21518;&#34920;&#29616;&#26356;&#24378;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#20351;&#29992;LLMs&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#35768;&#21487;&#19979;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.00171</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#24863;&#23884;&#20837;&#22312;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to build a single model that can transition between different emotions, languages, and annotation formats by leveraging multilingual models and Demux, to achieve knowledge sharing and reduce training costs.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#31185;&#23558;&#24773;&#24863;&#34701;&#20837;&#20854;&#29702;&#35770;&#21644;&#24212;&#29992;&#20013;&#65292;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#24773;&#24863;&#30340;&#38656;&#27714;&#19981;&#26029;&#22810;&#26679;&#21270;&#12290;&#36825;&#20123;&#38656;&#27714;&#21253;&#25324;&#25512;&#26029;&#19981;&#21516;&#31867;&#22411;&#30340;&#24773;&#24863;&#12289;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26684;&#24335;&#12290;&#19981;&#21516;&#37197;&#32622;&#20043;&#38388;&#30340;&#20849;&#20139;&#27169;&#22411;&#23558;&#20351;&#30693;&#35782;&#20849;&#20139;&#21644;&#35757;&#32451;&#25104;&#26412;&#38477;&#20302;&#65292;&#24182;&#31616;&#21270;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#37197;&#32622;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;Demux&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#36755;&#20837;&#21253;&#25324;&#24863;&#20852;&#36259;&#30340;&#24773;&#24863;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21160;&#24577;&#22320;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#30340;&#24773;&#24863;&#12290;Demux&#36824;&#20135;&#29983;&#24773;&#24863;&#23884;&#20837;&#65292;&#23545;&#23427;&#20204;&#25191;&#34892;&#25805;&#20316;&#21487;&#20197;&#36890;&#36807;&#27719;&#38598;&#27599;&#20010;&#31751;&#30340;&#23884;&#20837;&#26469;&#36807;&#28193;&#21040;&#24773;&#24863;&#31751;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Demux&#21487;&#20197;&#21516;&#26102;&#20256;&#36755;k
&lt;/p&gt;
&lt;p&gt;
The need for emotional inference from text continues to diversify as more and more disciplines integrate emotions into their theories and applications. These needs include inferring different emotion types, handling multiple languages, and different annotation formats. A shared model between different configurations would enable the sharing of knowledge and a decrease in training costs, and would simplify the process of deploying emotion recognition models in novel environments. In this work, we study how we can build a single model that can transition between these different configurations by leveraging multilingual models and Demux, a transformer-based model whose input includes the emotions of interest, enabling us to dynamically change the emotions predicted by the model. Demux also produces emotion embeddings, and performing operations on them allows us to transition to clusters of emotions by pooling the embeddings of each cluster. We show that Demux can simultaneously transfer k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#24182;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.15842</link><description>&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#24773;&#24863;&#35782;&#21035;&#20013;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#30740;&#31350;&#65306;&#20197;&#24773;&#24863;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#24182;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection, develops two modeling approaches to capture word associations of the emotion words themselves, and integrates pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models, demonstrating state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#34920;&#36798;&#30340;&#24773;&#24863;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#26631;&#31614;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#19968;&#31181;&#26159;&#23558;&#24773;&#24863;&#21253;&#21547;&#22312;&#36755;&#20837;&#20013;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#39033;&#20998;&#20026;&#20004;&#31867;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#12290;&#21069;&#32773;&#26681;&#25454;&#37329;&#26631;&#31614;&#21160;&#24577;&#21464;&#21270;&#65292;&#32780;&#21518;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting emotions expressed in text has become critical to a range of fields. In this work, we investigate ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection. First, we develop two modeling approaches to the problem in order to capture word associations of the emotion words themselves, by either including the emotions in the input, or by leveraging Masked Language Modeling (MLM). Second, we integrate pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models. We split these terms into two categories, local and global. The former dynamically change based on the gold labels, while the latter remain static during training. We demonstrate state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models. On top of better performance, we also demonstrate improved robustness. Code is available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.15173</link><description>&lt;p&gt;
Articulation GAN: &#26080;&#30417;&#30563;&#24314;&#27169;&#20851;&#33410;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new unsupervised generative model that learns to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner, which more closely mimics human speech production and better simulates the process of human speech production.
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#27874;&#24418;&#25110;&#39057;&#35889;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#36807;&#25511;&#21046;&#20851;&#33410;&#26469;&#20135;&#29983;&#35821;&#38899;&#65292;&#36825;&#36890;&#36807;&#22768;&#38899;&#20256;&#25773;&#30340;&#29289;&#29702;&#29305;&#24615;&#23548;&#33268;&#35821;&#38899;&#22768;&#38899;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#33410;&#29983;&#25104;&#22120;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33539;&#20363;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#20135;&#29983;/&#21512;&#25104;&#12290;&#20851;&#33410;&#29983;&#25104;&#22120;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#39044;&#35757;&#32451;&#29289;&#29702;&#27169;&#22411;&#65288;ema2wav&#65289;&#23558;&#29983;&#25104;&#30340;EMA&#34920;&#31034;&#36716;&#25442;&#20026;&#35821;&#38899;&#27874;&#24418;&#65292;&#36825;&#20123;&#27874;&#24418;&#34987;&#21457;&#36865;&#21040;&#37492;&#21035;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#20851;&#33410;&#20998;&#26512;&#34920;&#26126;&#65292;&#32593;&#32476;&#23398;&#20064;&#25511;&#21046;&#20851;&#33410;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#26041;&#24335;&#12290;&#36755;&#20986;&#30340;&#22768;&#23398;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Generative deep neural networks are widely used for speech synthesis, but most existing models directly generate waveforms or spectral outputs. Humans, however, produce speech by controlling articulators, which results in the production of speech sounds through physical properties of sound propagation. We introduce the Articulatory Generator to the Generative Adversarial Network paradigm, a new unsupervised generative model of speech production/synthesis. The Articulatory Generator more closely mimics human speech production by learning to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner. A separate pre-trained physical model (ema2wav) then transforms the generated EMA representations to speech waveforms, which get sent to the Discriminator for evaluation. Articulatory analysis suggests that the network learns to control articulators in a similar manner to humans during speech production. Acoustic analysis of the outputs sugge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#23383;&#20856;&#26469;&#25913;&#36827;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23454;&#39564;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;31%&#30340;&#20154;&#21517;&#38169;&#35823;&#26469;&#25552;&#39640;&#32763;&#35793;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.11981</link><description>&lt;p&gt;
&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#26816;&#27979;&#21644;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Named Entity Detection and Injection for Direct Speech Translation. (arXiv:2210.11981v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#23383;&#20856;&#26469;&#25913;&#36827;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23454;&#39564;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;31%&#30340;&#20154;&#21517;&#38169;&#35823;&#26469;&#25552;&#39640;&#32763;&#35793;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how to leverage named entity dictionaries to improve speech-to-text translation model outputs, and experiments show that it can improve named entity accuracy in translation with a 31% reduction in person name errors.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#21477;&#35805;&#20013;&#65292;&#26576;&#20123;&#35789;&#23545;&#20854;&#35821;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#65292;&#21629;&#21517;&#23454;&#20307;&#65288;NE&#65289;&#23545;&#31070;&#32463;&#27169;&#22411;&#26469;&#35828;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#65288;S2T&#65289;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#22788;&#29702;&#19968;&#30452;&#34987;&#24573;&#35270;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;S2T&#27169;&#22411;&#22312;&#20301;&#32622;&#21644;&#29305;&#21035;&#26159;&#20154;&#21517;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#38500;&#38750;&#20107;&#20808;&#30693;&#36947;&#20854;&#25340;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#24050;&#30693;&#21487;&#33021;&#20986;&#29616;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;NE&#23383;&#20856;&#26469;&#25913;&#36827;S2T&#27169;&#22411;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#22312;&#35805;&#35821;&#20013;&#30340;NE&#65292;&#20174;S2T&#32534;&#30721;&#22120;&#36755;&#20986;&#24320;&#22987;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#26816;&#27979;&#36136;&#37327;&#36275;&#20197;&#36890;&#36807;&#20943;&#23569;31&#65285;&#30340;&#20154;&#21517;&#38169;&#35823;&#26469;&#25552;&#39640;&#32763;&#35793;&#20013;&#30340;NE&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a sentence, certain words are critical for its semantic. Among them, named entities (NEs) are notoriously challenging for neural models. Despite their importance, their accurate handling has been neglected in speech-to-text (S2T) translation research, and recent work has shown that S2T models perform poorly for locations and notably person names, whose spelling is challenging unless known in advance. In this work, we explore how to leverage dictionaries of NEs known to likely appear in a given context to improve S2T model outputs. Our experiments show that we can reliably detect NEs likely present in an utterance starting from S2T encoder outputs. Indeed, we demonstrate that the current detection quality is sufficient to improve NE accuracy in the translation with a 31% reduction in person name errors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AugPro&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#20559;&#31227;&#20915;&#31574;&#36793;&#30028;&#65292;&#35745;&#31639;&#24320;&#38144;&#23567;&#12290;</title><link>http://arxiv.org/abs/2210.11768</link><description>&lt;p&gt;
&#25237;&#24433;&#22686;&#24378;&#65306;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AugPro&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#20559;&#31227;&#20915;&#31574;&#36793;&#30028;&#65292;&#35745;&#31639;&#24320;&#38144;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
AugPro is an effective and efficient data augmentation method for distillation, which avoids shifting decision boundaries and has little computational overhead.
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#20174;&#22823;&#22411;&#27169;&#22411;&#21521;&#23567;&#22411;&#27169;&#22411;&#36716;&#31227;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#34920;&#31034;&#25554;&#20540;&#12289;&#26631;&#35760;&#26367;&#25442;&#25110;&#27169;&#22411;&#22686;&#24378;&#31561;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20915;&#31574;&#36793;&#30028;&#30340;&#20559;&#31227;&#65288;&#34920;&#31034;&#25554;&#20540;&#65289;&#65292;&#19981;&#22815;&#34920;&#36798;&#65288;&#26631;&#35760;&#26367;&#25442;&#65289;&#25110;&#24341;&#20837;&#36807;&#22810;&#30340;&#35745;&#31639;&#24320;&#38144;&#65288;&#27169;&#22411;&#22686;&#24378;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AugPro&#65288;&#25237;&#24433;&#22686;&#24378;&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#34920;&#31034;&#25554;&#20540;&#22686;&#24378;&#26041;&#27861;&#20043;&#19978;&#65292;&#20197;&#20445;&#25345;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23558;&#22686;&#24378;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#26631;&#35760;&#20197;&#36991;&#20813;&#20559;&#31227;&#20915;&#31574;&#36793;&#30028;&#12290;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#25805;&#20316;&#65292;&#35745;&#31639;&#24320;&#38144;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not expressive enough (token replacement), or introduce too much computational overhead (augmentation with models). To this end, we propose AugPro (Augmentation with Projection), an effective and efficient data augmentation method for distillation. Our method builds on top of representation interpolation augmentation methods to maintain the diversity of expressions and converts the augmented data to tokens to avoid shifting decision boundaries. It uses simple operations that come with little computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#22312;&#20316;&#32773;&#24402;&#23646;&#21644;&#28151;&#28102;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#38656;&#35201;&#24320;&#21457;&#26032;&#22411;AA / AO&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#31070;&#32463;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10488</link><description>&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#20316;&#32773;&#24402;&#23646;&#21644;&#28151;&#28102;&#65306;&#25968;&#25454;&#25366;&#25496;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#22312;&#20316;&#32773;&#24402;&#23646;&#21644;&#28151;&#28102;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#38656;&#35201;&#24320;&#21457;&#26032;&#22411;AA / AO&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#31070;&#32463;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey investigates the application of neural text generation techniques in authorship attribution and obfuscation, and highlights the need for developing novel AA/AO solutions to deal with neural texts.
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#65288;AA&#65289;&#21644;&#20316;&#32773;&#28151;&#28102;&#65288;AO&#65289;&#26159;&#38544;&#31169;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#21644;&#37325;&#35201;&#30340;&#20004;&#20010;&#20132;&#32455;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#20316;&#32773;&#30340;&#27010;&#24565;&#21450;&#20854;&#38543;&#20043;&#32780;&#26469;&#30340;&#38544;&#31169;&#20851;&#27880;&#20165;&#38024;&#23545;&#20154;&#31867;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#65288;NTG&#65289;&#25216;&#26415;&#30340;&#29190;&#28856;&#24615;&#36827;&#23637;&#65292;&#29616;&#22312;&#24517;&#39035;&#32771;&#34385;&#20154;&#31867;&#12289;&#26426;&#22120;&#25110;&#23427;&#20204;&#30340;&#32452;&#21512;&#30340;&#20316;&#32773;&#36523;&#20221;&#12290;&#30001;&#20110;&#31070;&#32463;&#25991;&#26412;&#22312;&#24694;&#24847;&#20351;&#29992;&#26102;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#20102;&#35299;&#20256;&#32479;AA / AO&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#24182;&#24320;&#21457;&#22788;&#29702;&#31070;&#32463;&#25991;&#26412;&#30340;&#26032;&#22411;AA / AO&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two interlocking research questions of growing interest and importance in privacy research are Authorship Attribution (AA) and Authorship Obfuscation (AO). Given an artifact, especially a text t in question, an AA solution aims to accurately attribute t to its true author out of many candidate authors while an AO solution aims to modify t to hide its true authorship. Traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. However, in recent years, due to the explosive advancements in Neural Text Generation (NTG) techniques in NLP, capable of synthesizing human-quality open-ended texts (so-called "neural texts"), one has to now consider authorships by humans, machines, or their combination. Due to the implications and potential threats of neural texts when used maliciously, it has become critical to understand the limitations of traditional AA/AO solutions and develop novel AA/AO solutions in dealing with neural texts. In this survey, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EDU&#32423;&#21035;&#30340;&#21487;&#21464;&#38271;&#24230;&#25688;&#35201;&#25552;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35777;&#26126;EDU&#27604;&#21477;&#23376;&#20855;&#26377;&#26356;&#39640;&#30340;&#33258;&#21160;&#35780;&#20272;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2210.04029</link><description>&lt;p&gt;
EDU&#32423;&#21035;&#30340;&#21487;&#21464;&#38271;&#24230;&#25688;&#35201;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
EDU-level Extractive Summarization with Varying Summary Lengths. (arXiv:2210.04029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EDU&#32423;&#21035;&#30340;&#21487;&#21464;&#38271;&#24230;&#25688;&#35201;&#25552;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35777;&#26126;EDU&#27604;&#21477;&#23376;&#20855;&#26377;&#26356;&#39640;&#30340;&#33258;&#21160;&#35780;&#20272;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an EDU-level extractive model with varying summary lengths and provides evidence from both theoretical and experimental perspectives to justify and quantify that EDUs make summaries with higher automatic evaluation scores than sentences.
&lt;/p&gt;
&lt;p&gt;
&#25277;&#21462;&#24335;&#27169;&#22411;&#36890;&#24120;&#23558;&#25991;&#26412;&#25688;&#35201;&#21046;&#23450;&#20026;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#22266;&#23450;&#30340;&#21069;k&#20010;&#26174;&#33879;&#21477;&#23376;&#20316;&#20026;&#25688;&#35201;&#12290;&#24456;&#23569;&#26377;&#24037;&#20316;&#25506;&#32034;&#32454;&#31890;&#24230;&#30340;&#22522;&#26412;&#35805;&#35821;&#21333;&#20803;&#65288;EDU&#65289;&#30340;&#25552;&#21462;&#65292;&#32780;&#19988;&#23545;&#20110;&#25277;&#21462;&#21333;&#20803;&#30340;&#36873;&#25321;&#32570;&#20047;&#20998;&#26512;&#21644;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#22266;&#23450;&#30340;&#21069;k&#20010;&#26174;&#33879;&#21477;&#23376;&#30340;&#36873;&#25321;&#31574;&#30053;&#19981;&#36866;&#21512;&#25688;&#35201;&#38656;&#27714;&#65292;&#22240;&#20026;&#19981;&#21516;&#25991;&#26723;&#20013;&#26174;&#33879;&#21477;&#23376;&#30340;&#25968;&#37327;&#19981;&#21516;&#65292;&#22240;&#27492;&#22312;&#29616;&#23454;&#20013;&#19981;&#23384;&#22312;&#20849;&#21516;&#25110;&#26368;&#20339;&#30340;k&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#22522;&#20110;EDU&#21644;&#21477;&#23376;&#30340;oracle&#25688;&#35201;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#30340;&#35282;&#24230;&#25552;&#20379;&#35777;&#25454;&#65292;&#35777;&#26126;&#21644;&#37327;&#21270;EDU&#27604;&#21477;&#23376;&#20855;&#26377;&#26356;&#39640;&#30340;&#33258;&#21160;&#35780;&#20272;&#20998;&#25968;&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;EDU&#30340;&#36825;&#31181;&#20248;&#28857;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;EDU&#32423;&#21035;&#30340;&#21487;&#21464;&#38271;&#24230;&#25688;&#35201;&#25552;&#21462;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;EDU-VL
&lt;/p&gt;
&lt;p&gt;
Extractive models usually formulate text summarization as extracting fixed top-$k$ salient sentences from the document as a summary. Few works exploited extracting finer-grained Elementary Discourse Unit (EDU) with little analysis and justification for the extractive unit selection. Further, the selection strategy of the fixed top-$k$ salient sentences fits the summarization need poorly, as the number of salient sentences in different documents varies and therefore a common or best $k$ does not exist in reality. To fill these gaps, this paper first conducts the comparison analysis of oracle summaries based on EDUs and sentences, which provides evidence from both theoretical and experimental perspectives to justify and quantify that EDUs make summaries with higher automatic evaluation scores than sentences. Then, considering this merit of EDUs, this paper further proposes an EDU-level extractive model with Varying summary Lengths and develops the corresponding learning algorithm. EDU-VL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#38750;&#30830;&#23450;&#24615;&#22534;&#26632;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24778;&#20154;&#35745;&#31639;&#33021;&#21147;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65292;&#36824;&#21487;&#20197;&#35782;&#21035;&#35768;&#22810;&#38750;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65292;&#24182;&#19988;&#21487;&#20197;&#35782;&#21035;&#20855;&#26377;&#27604;&#20854;&#22534;&#26632;&#23383;&#27597;&#34920;&#22823;&#23567;&#26356;&#22823;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2210.01343</link><description>&lt;p&gt;
&#38750;&#30830;&#23450;&#24615;&#22534;&#26632;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24778;&#20154;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Surprising Computational Power of Nondeterministic Stack RNNs. (arXiv:2210.01343v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#38750;&#30830;&#23450;&#24615;&#22534;&#26632;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24778;&#20154;&#35745;&#31639;&#33021;&#21147;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65292;&#36824;&#21487;&#20197;&#35782;&#21035;&#35768;&#22810;&#38750;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65292;&#24182;&#19988;&#21487;&#20197;&#35782;&#21035;&#20855;&#26377;&#27604;&#20854;&#22534;&#26632;&#23383;&#27597;&#34920;&#22823;&#23567;&#26356;&#22823;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates the surprising computational power of nondeterministic stack recurrent neural networks, which can not only recognize context-free languages, but also many non-context-free languages and languages with larger alphabet sizes than their stack alphabet.
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20855;&#26377;&#22266;&#23450;&#30340;&#12289;&#26377;&#38480;&#30340;&#35760;&#24518;&#21333;&#20803;&#12290;&#22312;&#29702;&#35770;&#19978;&#65288;&#20551;&#35774;&#26377;&#30028;&#30340;&#33539;&#22260;&#21644;&#31934;&#24230;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#33021;&#21147;&#20026;&#27491;&#21017;&#35821;&#35328;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;RNN&#26080;&#27861;&#23398;&#20064;&#35768;&#22810;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#65288;CFL&#65289;&#12290;&#20026;&#20102;&#25193;&#23637;RNN&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#22534;&#26632;&#25968;&#25454;&#32467;&#26500;&#22686;&#24378;&#20102;RNN&#65292;&#20351;&#23427;&#20204;&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#30456;&#24403;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#35821;&#35328;&#35782;&#21035;&#33021;&#21147;&#22686;&#21152;&#21040;CFL&#12290;&#38750;&#30830;&#23450;&#24615;&#26159;&#35782;&#21035;&#25152;&#26377;CFL&#25152;&#24517;&#38656;&#30340;&#65288;&#19981;&#20165;&#20165;&#26159;&#30830;&#23450;&#24615;CFL&#65289;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#30830;&#23450;&#24615;&#21644;&#31070;&#32463;&#25511;&#21046;&#22120;&#30456;&#20114;&#20316;&#29992;&#20135;&#29983;&#20102;&#21478;&#22806;&#20004;&#31181;&#24847;&#24819;&#19981;&#21040;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#38750;&#30830;&#23450;&#24615;&#22534;&#26632;RNN&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;CFL&#65292;&#36824;&#21487;&#20197;&#35782;&#21035;&#35768;&#22810;&#38750;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#35328;&#12290;&#20854;&#27425;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#20855;&#26377;&#27604;&#20854;&#22534;&#26632;&#23383;&#27597;&#34920;&#22823;&#23567;&#26356;&#22823;&#30340;&#35821;&#35328;&#65292;&#36825;&#19968;&#28857;&#21487;&#33021;&#36229;&#20986;&#20102;&#20154;&#20204;&#30340;&#39044;&#26399;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#22686;&#21152;&#20854;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional recurrent neural networks (RNNs) have a fixed, finite number of memory cells. In theory (assuming bounded range and precision), this limits their formal language recognition power to regular languages, and in practice, RNNs have been shown to be unable to learn many context-free languages (CFLs). In order to expand the class of languages RNNs recognize, prior work has augmented RNNs with a nondeterministic stack data structure, putting them on par with pushdown automata and increasing their language recognition power to CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic CFLs), but in this paper, we show that nondeterminism and the neural controller interact to produce two more unexpected abilities. First, the nondeterministic stack RNN can recognize not only CFLs, but also many non-context-free languages. Second, it can recognize languages with much larger alphabet sizes than one might expect given the size of its stack alphabet. Finally, to inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Twitter&#19978;&#23186;&#20307;&#26426;&#26500;&#26032;&#38395;&#24086;&#23376;&#30340;&#29992;&#25143;&#21709;&#24212;&#30340;&#19978;&#19979;&#25991;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26032;&#35821;&#26009;&#24211;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#26041;&#27861;&#32570;&#20047;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.00465</link><description>&lt;p&gt;
&#35780;&#20272;&#19978;&#19979;&#25991;&#20449;&#24687;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the impact of contextual information in hate speech detection. (arXiv:2210.00465v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Twitter&#19978;&#23186;&#20307;&#26426;&#26500;&#26032;&#38395;&#24086;&#23376;&#30340;&#29992;&#25143;&#21709;&#24212;&#30340;&#19978;&#19979;&#25991;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26032;&#35821;&#26009;&#24211;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#26041;&#27861;&#32570;&#20047;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a novel corpus for contextualized hate speech detection based on user responses to news posts from media outlets on Twitter, to address the limitation of current automatic hate speech detection methods lacking context.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#24378;&#24230;&#21644;&#19982;&#21463;&#20445;&#25252;&#32676;&#20307;&#25104;&#21592;&#30340;&#26292;&#21147;&#34892;&#20026;&#30340;&#20851;&#31995;&#65292;&#20167;&#24680;&#35328;&#35770;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20854;&#20182;&#34394;&#25311;&#23186;&#20307;&#20013;&#33719;&#24471;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#25968;&#37327;&#24040;&#22823;&#65292;&#22240;&#27492;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#33258;&#21160;&#24037;&#20855;&#20197;&#24110;&#21161;&#20998;&#26512;&#21644;&#31649;&#29702;&#36825;&#31181;&#35328;&#35770;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#33267;&#23569;&#22312;&#20854;&#26368;&#20855;&#23041;&#32961;&#24615;&#30340;&#24418;&#24335;&#20013;&#12290;&#24403;&#21069;&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#26041;&#27861;&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#32570;&#20047;&#19978;&#19979;&#25991;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#21644;&#36164;&#28304;&#26159;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23396;&#31435;&#30340;&#28040;&#24687;&#27809;&#26377;&#20219;&#20309;&#31867;&#22411;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#25110;&#27491;&#22312;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;&#36825;&#38480;&#21046;&#20102;&#21487;&#29992;&#20449;&#24687;&#26469;&#23450;&#20041;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#24086;&#23376;&#26159;&#21542;&#20855;&#26377;&#20167;&#24680;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Twitter&#19978;&#23186;&#20307;&#26426;&#26500;&#26032;&#38395;&#24086;&#23376;&#30340;&#29992;&#25143;&#21709;&#24212;&#30340;&#19978;&#19979;&#25991;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26032;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, hate speech has gained great relevance in social networks and other virtual media because of its intensity and its relationship with violent acts against members of protected groups. Due to the great amount of content generated by users, great effort has been made in the research and development of automatic tools to aid the analysis and moderation of this speech, at least in its most threatening forms. One of the limitations of current approaches to automatic hate speech detection is the lack of context. Most studies and resources are performed on data without context; that is, isolated messages without any type of conversational context or the topic being discussed. This restricts the available information to define if a post on a social network is hateful or not. In this work, we provide a novel corpus for contextualized hate speech detection based on user responses to news posts from media outlets on Twitter. This corpus was collected in the Rioplatense dialectal v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27599;&#31181;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05735</link><description>&lt;p&gt;
&#23398;&#20064;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27599;&#31181;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a sparse multilingual ASR model, which explicitly learns the parameters for each language by activating language-specific sub-networks, and enables knowledge transfer for lower-resource languages via joint multilingual training. The proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26377;&#25928;&#22320;&#21387;&#32553;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;ASR&#20013;&#65292;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#21487;&#33021;&#20250;&#23548;&#33268;&#26576;&#20123;&#35821;&#35328;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#20026;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#25513;&#30721;&#21487;&#33021;&#19981;&#36866;&#21512;&#25152;&#26377;&#35821;&#35328;&#24182;&#19988;&#20002;&#24323;&#37325;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASR&#36335;&#24452;&#65292;&#19968;&#31181;&#31232;&#30095;&#30340;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#65292;&#23427;&#28608;&#27963;&#35821;&#35328;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#65288;&#8220;&#36335;&#24452;&#8221;&#65289;&#65292;&#20197;&#20415;&#20026;&#27599;&#31181;&#35821;&#35328;&#26174;&#24335;&#22320;&#23398;&#20064;&#21442;&#25968;&#12290;&#36890;&#36807;&#37325;&#21472;&#30340;&#23376;&#32593;&#32476;&#65292;&#20849;&#20139;&#21442;&#25968;&#36824;&#21487;&#20197;&#36890;&#36807;&#32852;&#21512;&#22810;&#35821;&#35328;&#35757;&#32451;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;ASR&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#27969;&#24335;RNN-T&#27169;&#22411;&#22312;4&#31181;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ASR&#36335;&#24452;&#27169;&#22411;&#20248;&#20110;&#23494;&#38598;&#27169;&#22411;&#21644;&#35821;&#35328;&#19981;&#21487;&#30693;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;&#21333;&#35821;&#31232;&#30095;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks ("pathways"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;DailyTalk&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2207.01063</link><description>&lt;p&gt;
DailyTalk&#65306;&#38754;&#21521;&#23545;&#35805;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#21475;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;DailyTalk&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a high-quality conversational speech dataset DailyTalk designed for conversational TTS. DailyTalk can be used as a general TTS dataset, and the baseline can represent contextual information from DailyTalk.
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25968;&#25454;&#38598;&#37117;&#26159;&#30001;&#21333;&#20010;&#35805;&#35821;&#32452;&#25104;&#65292;&#32570;&#20047;&#23545;&#35805;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DailyTalk&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;&#25105;&#20204;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;DailyDialog&#20013;&#25277;&#26679;&#12289;&#20462;&#25913;&#21644;&#24405;&#21046;&#20102;2,541&#20010;&#23545;&#35805;&#65292;&#24182;&#32487;&#25215;&#20102;&#20854;&#27880;&#37322;&#23646;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#20316;&#20026;&#25105;&#20204;&#30340;&#22522;&#32447;&#65292;&#20854;&#20013;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;TTS&#22312;&#23545;&#35805;&#20013;&#30340;&#21382;&#21490;&#20449;&#24687;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;&#36890;&#36807;&#22522;&#32447;&#23454;&#39564;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;DailyTalk&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#20195;&#30721;&#21487;&#20379;&#23398;&#26415;&#29992;&#36884;&#20813;&#36153;&#20351;&#29992;&#65292;&#37319;&#29992;CC-BY-SA 4.0&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of current Text-to-Speech (TTS) datasets, which are collections of individual utterances, contain few conversational aspects. In this paper, we introduce DailyTalk, a high-quality conversational speech dataset designed for conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the open-domain dialogue dataset DailyDialog inheriting its annotated attributes. On top of our dataset, we extend prior work as our baseline, where a non-autoregressive TTS is conditioned on historical information in a dialogue. From the baseline experiment with both general and our novel metrics, we show that DailyTalk can be used as a general TTS dataset, and more than that, our baseline can represent contextual information from DailyTalk. The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license.
&lt;/p&gt;</description></item><item><title>EasyNLP&#26159;&#19968;&#27454;&#20840;&#38754;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#20840;&#38754;&#30340;NLP&#31639;&#27861;&#22871;&#20214;&#65292;&#20855;&#26377;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#21151;&#33021;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;PTMs&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#27169;&#22411;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2205.00258</link><description>&lt;p&gt;
EasyNLP&#65306;&#19968;&#27454;&#20840;&#38754;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing. (arXiv:2205.00258v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00258
&lt;/p&gt;
&lt;p&gt;
EasyNLP&#26159;&#19968;&#27454;&#20840;&#38754;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#20840;&#38754;&#30340;NLP&#31639;&#27861;&#22871;&#20214;&#65292;&#20855;&#26377;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#21151;&#33021;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;PTMs&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#27169;&#22411;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
EasyNLP is a comprehensive and easy-to-use natural language processing toolkit that supports a comprehensive suite of NLP algorithms. It features knowledge-enhanced pre-training, knowledge distillation and few-shot learning functionalities for large-scale PTMs, and provides a unified framework of model training, inference and deployment for real-world applications.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#25104;&#21151;&#37325;&#22609;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24037;&#19994;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#33719;&#24471;&#39640;&#24615;&#33021;&#27169;&#22411;&#24182;&#23558;&#20854;&#37096;&#32626;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#24182;&#19981;&#23481;&#26131;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;EasyNLP&#26088;&#22312;&#20351;&#26500;&#24314;NLP&#24212;&#29992;&#31243;&#24207;&#21464;&#24471;&#23481;&#26131;&#65292;&#25903;&#25345;&#20840;&#38754;&#30340;NLP&#31639;&#27861;&#22871;&#20214;&#12290;&#23427;&#36824;&#20855;&#26377;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#21151;&#33021;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;PTMs&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#27169;&#22411;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#30446;&#21069;&#65292;EasyNLP&#24050;&#32463;&#20026;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#21313;&#22810;&#20010;&#19994;&#21153;&#37096;&#38376;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#26080;&#32541;&#38598;&#25104;&#21040;&#38463;&#37324;&#20113;&#30340;AI&#24179;&#21488;&#65288;PAI&#65289;&#20135;&#21697;&#20013;&#12290;&#25105;&#20204;&#30340;EasyNLP&#24037;&#20855;&#21253;&#30340;&#28304;&#20195;&#30721;&#24050;&#22312;GitHub&#65288;https://github.com/alibaba/EasyNLP&#65289;&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Pre-Trained Models (PTMs) has reshaped the development of Natural Language Processing (NLP). Yet, it is not easy to obtain high-performing models and deploy them online for industrial practitioners. To bridge this gap, EasyNLP is designed to make it easy to build NLP applications, which supports a comprehensive suite of NLP algorithms. It further features knowledge-enhanced pre-training, knowledge distillation and few-shot learning functionalities for large-scale PTMs, and provides a unified framework of model training, inference and deployment for real-world applications. Currently, EasyNLP has powered over ten business units within Alibaba Group and is seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud. The source code of our EasyNLP toolkit is released at GitHub (https://github.com/alibaba/EasyNLP).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;ConSLT&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#32423;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;SLT&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#20844;&#24320;&#21487;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#35821;&#26009;&#24211;&#38750;&#24120;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.04916</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Token-level Contrastive Framework for Sign Language Translation. (arXiv:2204.04916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04916
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;ConSLT&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#32423;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;SLT&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#20844;&#24320;&#21487;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#35821;&#26009;&#24211;&#38750;&#24120;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposed a token-level contrastive framework for sign language translation, ConSLT, which incorporates token-level contrastive learning into the SLT decoding process to learn effective token representations and alleviate the issue of limited publicly available SLT corpus.
&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#26159;&#19968;&#39033;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24357;&#21512;&#32843;&#20154;&#21644;&#21548;&#21147;&#20154;&#20043;&#38388;&#30340;&#27807;&#36890;&#38548;&#38402;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#25163;&#35821;&#32763;&#35793;&#65292;&#20294;&#20844;&#24320;&#21487;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#35821;&#26009;&#24211;&#38750;&#24120;&#26377;&#38480;&#65292;&#36825;&#23548;&#33268;&#20102;&#26631;&#35760;&#34920;&#31034;&#30340;&#23849;&#28291;&#21644;&#29983;&#25104;&#26631;&#35760;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConSLT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#32423;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;SLT&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#26631;&#35760;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ConSLT&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27599;&#20010;&#26631;&#35760;&#21450;&#20854;&#30001;&#19981;&#21516;&#20002;&#22833;&#25513;&#30721;&#29983;&#25104;&#30340;&#23545;&#24212;&#26631;&#35760;&#35270;&#20026;&#27491;&#23545;&#65292;&#28982;&#21518;&#38543;&#26426;&#20174;&#24403;&#21069;&#21477;&#23376;&#20013;&#19981;&#22312;&#35789;&#27719;&#34920;&#20013;&#30340;$K$&#20010;&#26631;&#35760;&#20013;&#25277;&#26679;&#26500;&#24314;&#36127;&#20363;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;ConSLT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign Language Translation (SLT) is a promising technology to bridge the communication gap between the deaf and the hearing people. Recently, researchers have adopted Neural Machine Translation (NMT) methods, which usually require large-scale corpus for training, to achieve SLT. However, the publicly available SLT corpus is very limited, which causes the collapse of the token representations and the inaccuracy of the generated tokens. To alleviate this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation , which learns effective token representations by incorporating token-level contrastive learning into the SLT decoding process. Concretely, ConSLT treats each token and its counterpart generated by different dropout masks as positive pairs during decoding, and then randomly samples $K$ tokens in the vocabulary that are not in the current sentence to construct negative examples. We conduct comprehen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;&#26041;&#27861;&#65292;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#26085;&#35821;ASR&#20013;&#30340;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2204.00175</link><description>&lt;p&gt;
&#26085;&#35821;ASR&#20013;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Alternate Intermediate Conditioning with Syllable-level and Character-level Targets for Japanese ASR. (arXiv:2204.00175v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;&#26041;&#27861;&#65292;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#26085;&#35821;ASR&#20013;&#30340;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an alternate intermediate conditioning method with syllable-level and character-level targets to deal with the many-to-one and one-to-many mapping problems in Japanese ASR, and achieves better performance than conventional multi-task and Self-conditioned CTC methods in experiments.
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30452;&#25509;&#23558;&#36755;&#20837;&#35821;&#38899;&#26144;&#23556;&#21040;&#23383;&#31526;&#12290;&#28982;&#32780;&#65292;&#24403;&#22810;&#20010;&#19981;&#21516;&#30340;&#21457;&#38899;&#24212;&#35813;&#26144;&#23556;&#21040;&#19968;&#20010;&#23383;&#31526;&#25110;&#19968;&#20010;&#21457;&#38899;&#34987;&#22810;&#20010;&#19981;&#21516;&#30340;&#23383;&#31526;&#20849;&#20139;&#26102;&#65292;&#26144;&#23556;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#30001;&#20110;&#26085;&#35821;&#27721;&#23383;&#30340;&#23384;&#22312;&#65292;&#26085;&#35821;ASR&#26368;&#23481;&#26131;&#36973;&#21463;&#36825;&#31181;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23383;&#31526;&#21644;&#38899;&#33410;&#20043;&#38388;&#30340;&#26174;&#24335;&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#25105;&#26465;&#20214;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#65292;&#20854;&#20013;&#19978;&#23618;&#8220;&#33258;&#25105;&#26465;&#20214;&#8221;&#20110;&#19979;&#23618;&#30340;&#20013;&#38388;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#23383;&#31526;&#21644;&#38899;&#33410;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#33258;&#21457;&#26085;&#35821;&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#21644;&#33258;&#25105;&#26465;&#20214;CTC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition directly maps input speech to characters. However, the mapping can be problematic when several different pronunciations should be mapped into one character or when one pronunciation is shared among many different characters. Japanese ASR suffers the most from such many-to-one and one-to-many mapping problems due to Japanese kanji characters. To alleviate the problems, we introduce explicit interaction between characters and syllables using Self-conditioned connectionist temporal classification (CTC), in which the upper layers are ``self-conditioned'' on the intermediate predictions from the lower layers. The proposed method utilizes character-level and syllable-level intermediate predictions as conditioning features to deal with mutual dependency between characters and syllables. Experimental results on Corpus of Spontaneous Japanese show that the proposed method outperformed the conventional multi-task and Self-conditioned CTC methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;I-Tuning&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#23558;&#19981;&#21487;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#35299;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#65292;&#20351;&#24471;&#27169;&#22411;&#21253;&#21547;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#23569;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#21516;&#26102;&#22312;&#19977;&#20010;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#19982;&#22823;&#35268;&#27169;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#37117;&#23569;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2202.06574</link><description>&lt;p&gt;
I-Tuning: &#21033;&#29992;&#22270;&#20687;&#23545;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36731;&#37327;&#32423;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;I-Tuning&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#23558;&#19981;&#21487;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#35299;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#65292;&#20351;&#24471;&#27169;&#22411;&#21253;&#21547;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#23569;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#21516;&#26102;&#22312;&#19977;&#20010;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#19982;&#22823;&#35268;&#27169;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#37117;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a lightweight image captioning framework (I-Tuning) that connects the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT with a novel I-Tuning cross-attention module. The framework contains fewer trainable parameters and achieves comparable or better performance than large-scale baseline systems on three image captioning benchmarks, while requiring much fewer trainable parameters and training data compared with state-of-the-art baselines.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#20256;&#32479;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#35821;&#35328;&#25551;&#36848;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#19982;&#36825;&#20123;&#39640;&#25104;&#26412;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;I-Tuning&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;I-Tuning&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23558;&#19981;&#21487;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#35299;&#30721;&#22120;GPT2&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;CLIP-ViT&#36830;&#25509;&#36215;&#26469;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#21442;&#25968;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#26356;&#26032;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26694;&#26550;&#36731;&#24039;&#24555;&#36895;&#12290;&#22312;&#19977;&#20010;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#19982;&#22823;&#35268;&#27169;&#22522;&#32447;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#22810;&#36798;10&#20493;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning is a traditional vision-and-language task that aims to generate the language description of an image. Recent studies focus on scaling up the model size and the number of training data, which significantly increase the cost of model training. Different to these heavy-cost models, we introduce a lightweight image captioning framework (I-Tuning), which contains a small number of trainable parameters. We design a novel I-Tuning cross-attention module to connect the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT. Since most parameters are not required to be updated during training, our framework is lightweight and fast. Experimental results conducted on three image captioning benchmarks reveal that our framework achieves comparable or better performance than the large-scale baseline systems. But our models contain up to 10 times fewer trainable parameters and require much fewer data for training compared with state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.08071</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey summarizes the fundamental concepts and current research status of temporal sentence grounding in videos (TSGV), also known as natural language video localization (NLVL) or video moment retrieval (VMR), as well as future research directions. TSGV aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video, connecting computer vision and natural language, and has drawn significant attention from researchers in both communities.
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#65292;&#21448;&#31216;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#25110;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#65288;VMR&#65289;&#65292;&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#12290;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;TSGV&#24341;&#36215;&#20102;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35270;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25552;&#20379;TSGV&#20013;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20316;&#20026;&#32972;&#26223;&#65292;&#25105;&#20204;&#20197;&#25945;&#31243;&#30340;&#24418;&#24335;&#20171;&#32461;&#20102;TSGV&#20013;&#21151;&#33021;&#32452;&#20214;&#30340;&#24120;&#35265;&#32467;&#26500;&#65306;&#20174;&#21407;&#22987;&#35270;&#39057;&#21644;&#35821;&#35328;&#26597;&#35810;&#30340;&#29305;&#24449;&#25552;&#21462;&#21040;&#30446;&#26631;&#26102;&#21051;&#30340;&#31572;&#26696;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#25216;&#26415;&#65292;&#36825;&#26159;TSGV&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#26377;&#25928;&#23545;&#40784;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;TSGV&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal sentence grounding in videos (TSGV), \aka natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. La
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WGE&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.09231</link><description>&lt;p&gt;
&#20004;&#35270;&#35282;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WGE&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a graph neural network model named WGE, which learns vector representations of entities and relations from two single entity- and relation-focused graphs, and achieves excellent performance on knowledge graph completion task.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;WGE&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#32467;&#26500;&#12290;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;WGE&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#21521;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#22270;&#65292;&#23558;&#23454;&#20307;&#35270;&#20026;&#33410;&#28857;&#12290;WGE&#36824;&#20174;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#32422;&#26463;&#26465;&#20214;&#26500;&#24314;&#21478;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#21521;&#22270;&#65292;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#35270;&#20026;&#33410;&#28857;&#12290;&#28982;&#21518;&#65292;WGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26550;&#26500;&#65292;&#20174;&#36825;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#20013;&#26356;&#22909;&#22320;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;WGE&#23558;&#23398;&#20064;&#21040;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#39304;&#36865;&#21040;&#21152;&#26435;&#24471;&#20998;&#20989;&#25968;&#20013;&#65292;&#20197;&#36820;&#22238;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#19977;&#20803;&#32452;&#24471;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WGE&#22312;&#19971;&#20010;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an effective graph neural network (GNN)-based knowledge graph embedding model, which we name WGE, to capture entity- and relation-focused graph structures. Given a knowledge graph, WGE builds a single undirected entity-focused graph that views entities as nodes. WGE also constructs another single undirected graph from relation-focused constraints, which views entities and relations as nodes. WGE then proposes a GNN-based architecture to better learn vector representations of entities and relations from these two single entity- and relation-focused graphs. WGE feeds the learned entity and relation representations into a weighted score function to return the triple scores for knowledge graph completion. Experimental results show that WGE outperforms strong baselines on seven benchmark datasets for knowledge graph completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.05006</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper systematically surveys pre-trained language models in the biomedical domain, summarizes their recent progress and applications, and proposes a taxonomy.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#36825;&#20063;&#26377;&#30410;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65306;&#26469;&#33258;&#20449;&#24687;&#23398;&#12289;&#21307;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;CS&#65289;&#31038;&#21306;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;PLMs&#65292;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#34507;&#30333;&#36136;&#21644;DNA&#24207;&#21015;&#65292;&#29992;&#20110;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#21307;&#23398;PLMs&#30340;&#36328;&#23398;&#31185;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31038;&#21306;&#20043;&#38388;&#30340;&#20256;&#25773;&#65307;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#30456;&#20114;&#23396;&#31435;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;&#26399;&#26395;&#19968;&#39033;&#35843;&#26597;&#65292;&#19981;&#20165;&#31995;&#32479;&#22320;&#23457;&#26597;&#29983;&#29289;&#21307;&#23398;PLMs&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32780;&#19988;&#26631;&#20934;&#21270;&#26415;&#35821;&#21644;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21160;&#26426;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;&#29983;&#29289;&#21307;&#23398;PLMs&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing (NLP) tasks. This also benefits biomedical domain: researchers from informatics, medicine, and computer science (CS) communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It expects a survey that not only systematically reviews recent advances of biomedical PLMs and their applications but also standardizes terminology and benchmarks. In this paper, we summarize the recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks. Particularly, we discuss the motivations and propose a taxonomy of existing b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#28145;&#24230;&#21463;&#38480;&#30340;$\mathsf{Dyck}_{k}$&#23376;&#38598;$\mathsf{Dyck}_{k,D}$&#65292;&#36825;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#26377;&#30028;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2105.11115</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#26377;&#30028;&#23618;&#27425;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Networks Can Process Bounded Hierarchical Languages. (arXiv:2105.11115v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#28145;&#24230;&#21463;&#38480;&#30340;$\mathsf{Dyck}_{k}$&#23376;&#38598;$\mathsf{Dyck}_{k,D}$&#65292;&#36825;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#26377;&#30028;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that self-attention networks can process the depth-bounded subset of $\mathsf{Dyck}_{k}$, $\mathsf{Dyck}_{k,D}$, which better captures the bounded hierarchical structure of natural language.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#35777;&#26126;&#23427;&#20204;&#22312;&#22788;&#29702;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#24418;&#24335;&#35821;&#35328;&#65288;&#20363;&#22914;$\mathsf{Dyck}_k$&#65292;&#30001;$k$&#31181;&#23884;&#22871;&#25324;&#21495;&#32452;&#25104;&#30340;&#35821;&#35328;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#34920;&#26126;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#29992;&#27169;&#22411;&#24456;&#22909;&#22320;&#36817;&#20284;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#24418;&#24335;&#35821;&#35328;&#26469;&#35828;&#36807;&#20110;&#24369;&#65292;&#25110;&#32773;&#35828;&#23618;&#27425;&#32467;&#26500;&#21644;&#36882;&#24402;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#21487;&#33021;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;$\mathsf{Dyck}_{k,D}$&#26469;&#38480;&#23450;&#36825;&#19968;&#21547;&#20041;&#65292;&#20854;&#20013;$\mathsf{Dyck}_{k,D}$&#26159;&#28145;&#24230;&#21463;&#38480;&#30340;$\mathsf{Dyck}_{k}$&#23376;&#38598;&#65292;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#26377;&#30028;&#23618;&#27425;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;$D+1$&#23618;&#21644;$O(\log k)$&#20869;&#23384;&#22823;&#23567;&#65288;&#27599;&#20010;&#20196;&#29260;&#27599;&#23618;&#65289;&#30340;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35782;&#21035;$\mathsf{Dyck}_{k,D}$&#65292;&#20197;&#21450;&#19968;&#20010;&#20855;&#26377;&#20004;&#23618;&#21644;$O(\log k)$&#20869;&#23384;&#22823;&#23567;&#30340;&#36719;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;$\mathsf{Dyck}_{k,D}$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36719;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22312;$\mathsf{Dyck}_{k,D}$&#19978;&#29983;&#25104;&#27491;&#30830;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested parentheses of $k$ types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process $\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by $D$, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with $D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes $\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and $O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show that s
&lt;/p&gt;</description></item></channel></rss>