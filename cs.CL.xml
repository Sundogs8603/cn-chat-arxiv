<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Point-Bind&#21644;Point-LLM&#26159;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;&#27169;&#22411;&#65292;&#33021;&#23454;&#29616;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;Point-LLM&#33021;&#23454;&#29616;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00615</link><description>&lt;p&gt;
Point-Bind&#21644;Point-LLM&#65306;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. (arXiv:2309.00615v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00615
&lt;/p&gt;
&lt;p&gt;
Point-Bind&#21644;Point-LLM&#26159;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;&#27169;&#22411;&#65292;&#33021;&#23454;&#29616;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;Point-LLM&#33021;&#23454;&#29616;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Point-Bind&#65292;&#19968;&#20010;&#23558;&#28857;&#20113;&#19982;2D&#22270;&#20687;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#23545;&#40784;&#30340;3D&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22312;ImageBind&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;3D&#21644;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Point-LLM&#65292;&#31532;&#19968;&#20010;&#36981;&#24490;3D&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#25216;&#26415;&#65292;Point-LLM&#23558;Point-Bind&#30340;&#35821;&#20041;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#65292;&#20363;&#22914;LLaMA&#65292;&#19981;&#38656;&#35201;3D&#25351;&#20196;&#25968;&#25454;&#20294;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;&#23558;3D&#28857;&#20113;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#21551;&#31034;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ZiyuGuo99/Point-Bind_Point-LLM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.00614</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#22522;&#32447;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00614
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20854;&#23433;&#20840;&#28431;&#27934;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25991;&#26412;&#20248;&#21270;&#22120;&#21487;&#20197;&#29983;&#25104;&#32469;&#36807;&#23457;&#26597;&#21644;&#23545;&#40784;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#20511;&#37492;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#20016;&#23500;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#38382;&#39064;&#20837;&#25163;&#65306;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20160;&#20040;&#26679;&#30340;&#23041;&#32961;&#27169;&#22411;&#26159;&#23454;&#29992;&#30340;&#65311;&#22522;&#32447;&#38450;&#24481;&#25216;&#26415;&#22312;&#36825;&#20010;&#26032;&#39046;&#22495;&#20013;&#34920;&#29616;&#22914;&#20309;&#65311;LLM&#23433;&#20840;&#24615;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#26377;&#20309;&#19981;&#21516;&#65311;&#25105;&#20204;&#23545;&#20027;&#23548;&#23545;&#25239;LLM&#25915;&#20987;&#30340;&#20960;&#31181;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65292;&#35752;&#35770;&#20102;&#27599;&#31181;&#31574;&#30053;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#65306;&#26816;&#27979;&#65288;&#22522;&#20110;&#22256;&#24785;&#24230;&#65289;&#12289;&#36755;&#20837;&#39044;&#22788;&#29702;&#65288;&#25913;&#20889;&#21644;&#37325;&#26032;&#26631;&#35760;&#21270;&#65289;&#21644;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30333;&#30418;&#21644;&#28784;&#30418;&#35774;&#32622;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#32771;&#34385;&#30340;&#38450;&#24481;&#31574;&#30053;&#22312;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#22810;&#21464;&#37327;TPTL&#19982;&#21333;&#36793;&#21306;&#38388;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#26159;PSPACE-Complete&#30340;&#65292;&#19982;Metric Interval Temporal Logic&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26356;&#23481;&#26131;&#30340;&#35745;&#31639;&#26816;&#26597;&#26041;&#27861;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#23450;&#26102;&#35789;&#27809;&#26377;&#38480;&#21046;&#30340;&#22810;&#21464;&#37327;TPTL&#30340;&#21487;&#21028;&#23450;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#30340;&#29255;&#27573;&#12290;</title><link>http://arxiv.org/abs/2309.00386</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;TPTL&#19982;&#21333;&#36793;&#21306;&#38388;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#26159;PSPACE-Complete&#30340;
&lt;/p&gt;
&lt;p&gt;
Satisfiability Checking of Multi-Variable TPTL with Unilateral Intervals Is PSPACE-Complete. (arXiv:2309.00386v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00386
&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;TPTL&#19982;&#21333;&#36793;&#21306;&#38388;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#26159;PSPACE-Complete&#30340;&#65292;&#19982;Metric Interval Temporal Logic&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26356;&#23481;&#26131;&#30340;&#35745;&#31639;&#26816;&#26597;&#26041;&#27861;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#23450;&#26102;&#35789;&#27809;&#26377;&#38480;&#21046;&#30340;&#22810;&#21464;&#37327;TPTL&#30340;&#21487;&#21028;&#23450;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26102;&#38388;&#21629;&#39064;&#26102;&#24577;&#36923;&#36753;&#65288;TPTL&#65289;&#30340;{0, &#8734;}&#29255;&#27573;&#30340;&#21487;&#20915;&#23450;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;TPTL&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#22312;TPTL$^{0, &#8734;}$&#20013;&#26159;PSPACE-Complete&#30340;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#23427;&#30340;1&#21464;&#37327;&#29255;&#27573;&#65288;1-TPTL$^{0, &#8734;}$&#65289;&#65292;&#19982;&#20854;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#26159;EXPSPACE-Complete&#30340;Metric Interval Temporal Logic&#65288;MITL&#65289;&#30456;&#27604;&#65292;&#26356;&#21152;&#34920;&#36798;&#21147;&#24378;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#19978;&#26356;&#23481;&#26131;&#26816;&#26597;&#21487;&#28385;&#36275;&#24615;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#26159;&#26356;&#21152;&#34920;&#36798;&#21147;&#24378;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;TPTL$^{0, &#8734;}$&#26159;&#31532;&#19968;&#20010;&#19981;&#23545;&#23450;&#26102;&#35789;&#65288;&#20363;&#22914;&#26377;&#30028;&#21464;&#24322;&#24230;&#65292;&#26377;&#30028;&#26102;&#38388;&#31561;&#65289;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;/&#32422;&#26463;&#21363;&#21487;&#21028;&#23450;&#21487;&#28385;&#36275;&#24615;&#30340;TPTL&#30340;&#22810;&#21464;&#37327;&#29255;&#27573;&#12290;&#36890;&#36807;&#23558;&#20854;&#35268;&#32422;&#21040;&#19968;&#31181;&#31216;&#20026;&#21333;&#36793;&#38750;&#26102;&#24577;&#20132;&#26367;&#33258;&#21160;&#26426;&#30340;&#31354;&#31867;&#21035;&#30340;&#31354;&#26816;&#26597;&#38382;&#39064;&#65292;&#25105;&#20204;&#24471;&#21040;PSPACE&#30340;&#20250;&#21592;&#12290;&#35813;&#35268;&#32422;&#28041;&#21450;&#22810;&#20010;&#26102;&#38047;&#30340;&#20132;&#26367;&#26102;&#25511;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the decidability of the ${0,\infty}$ fragment of Timed Propositional Temporal Logic (TPTL). We show that the satisfiability checking of TPTL$^{0,\infty}$ is PSPACE-complete. Moreover, even its 1-variable fragment (1-TPTL$^{0,\infty}$) is strictly more expressive than Metric Interval Temporal Logic (MITL) for which satisfiability checking is EXPSPACE complete. Hence, we have a strictly more expressive logic with computationally easier satisfiability checking. To the best of our knowledge, TPTL$^{0,\infty}$ is the first multi-variable fragment of TPTL for which satisfiability checking is decidable without imposing any bounds/restrictions on the timed words (e.g. bounded variability, bounded time, etc.). The membership in PSPACE is obtained by a reduction to the emptiness checking problem for a new "non-punctual" subclass of Alternating Timed Automata with multiple clocks called Unilateral Very Weak Alternating Timed Automata (VWATA$^{0,\infty}$) which we prove to be in PSP
&lt;/p&gt;</description></item><item><title>BatchPrompt&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#25552;&#39640;LLM&#30340;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;&#20196;&#29260;&#35745;&#25968;&#24046;&#24322;&#23548;&#33268;&#30340;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#30340;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00384</link><description>&lt;p&gt;
BatchPrompt: &#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#23454;&#29616;&#26356;&#22810;&#20219;&#21153;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
BatchPrompt: Accomplish more with less. (arXiv:2309.00384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00384
&lt;/p&gt;
&lt;p&gt;
BatchPrompt&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#25552;&#39640;LLM&#30340;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;&#20196;&#29260;&#35745;&#25968;&#24046;&#24322;&#23548;&#33268;&#30340;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;LLM&#65288;Language Model&#65289;&#34987;&#35757;&#32451;&#26469;&#20351;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#23454;&#29616;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#25512;&#29702;&#12290;&#20026;&#36825;&#20123;LLM&#21046;&#20316;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#21644;&#23436;&#25104;&#31034;&#20363;&#20197;&#21450;&#25512;&#29702;&#19978;&#19979;&#25991;&#30340;&#21333;&#20010;&#31034;&#20363;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#24120;&#35268;&#25552;&#31034;&#22522;&#20934;&#31216;&#20026;SinglePrompt&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#25968;&#25454;&#28857;&#19981;&#19968;&#23450;&#24456;&#38271;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#20196;&#29260;&#35745;&#25968;&#21487;&#33021;&#27604;&#25968;&#25454;&#28857;&#30340;&#20196;&#29260;&#35745;&#25968;&#22823;&#24471;&#22810;&#65292;&#19982;Fine-tuned BERT&#31561;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#23548;&#33268;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#29575;&#38477;&#20302;&#12290;&#36825;&#20010;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#24433;&#21709;&#20102;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#25269;&#28040;&#20102;LLM&#25152;&#33021;&#25552;&#20379;&#30340;&#35768;&#22810;&#22909;&#22788;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#25552;&#31034;&#31574;&#30053;&#31216;&#20026;BatchPrompt&#12290;&#36825;&#31181;&#31574;&#30053;&#22686;&#21152;&#20102;&#25968;&#25454;&#28857;&#30340;&#23494;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many LLMs are trained to perform zero-shot or few-shot inference using instruction-based prompts. Crafting prompts for these LLMs typically requires the user to provide a detailed task description, examples of context and completion, and single example of context for inference. This regular prompt baseline is referred to as SinglePrompt in this paper. However, for NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization compared with encoder-based models like fine-tuned BERT. This cost-efficiency issue, affecting inference speed and compute budget, counteracts the many benefits LLMs have to offer. This paper aims to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as BatchPrompt. This strategy increases the density of data points, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#23545;&#35805;&#35821;&#36830;&#25509;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;&#36830;&#25509;&#35789;&#31181;&#31867;&#30340;&#22788;&#29702;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00368</link><description>&lt;p&gt;
&#35770;&#35770;&#25991;&#26102;&#20505;&#20250;&#24433;&#21709;&#35745;&#31639;&#21477;&#23376;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
When Do Discourse Markers Affect Computational Sentence Understanding?. (arXiv:2309.00368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#23545;&#35805;&#35821;&#36830;&#25509;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;&#36830;&#25509;&#35789;&#31181;&#31867;&#30340;&#22788;&#29702;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#33021;&#21147;&#21644;&#20351;&#29992;&#24773;&#20917;&#26174;&#33879;&#22686;&#38271;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#29702;&#35299;&#20154;&#31867;&#22914;&#20309;&#22788;&#29702;&#35805;&#35821;&#36830;&#25509;&#35789;&#65292;&#20294;&#23545;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#36825;&#19968;&#29616;&#35937;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22240;&#27492;&#65292;&#24456;&#37325;&#35201;&#30340;&#26159;&#23558;NLP&#27169;&#22411;&#32622;&#20110;&#26174;&#24494;&#38236;&#19979;&#65292;&#26816;&#26597;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#29702;&#35299;&#12289;&#22788;&#29702;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#20171;&#32461;&#20102;&#33258;&#21160;&#21477;&#23376;&#22788;&#29702;&#31995;&#32479;&#30340;&#20027;&#35201;&#26426;&#21046;&#65292;&#28982;&#21518;&#30528;&#37325;&#35780;&#20272;&#20102;&#35805;&#35821;&#36830;&#25509;&#35789;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20061;&#20010;&#27969;&#34892;&#30340;&#31995;&#32479;&#22312;&#29702;&#35299;&#33521;&#35821;&#35805;&#35821;&#36830;&#25509;&#35789;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;&#36830;&#25509;&#35789;&#29702;&#35299;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NLP&#31995;&#32479;&#19981;&#33021;&#21516;&#26679;&#22909;&#22320;&#22788;&#29702;&#25152;&#26377;&#30340;&#35805;&#35821;&#36830;&#25509;&#35789;&#65292;&#24182;&#19988;&#19981;&#21516;&#36830;&#25509;&#35789;&#31181;&#31867;&#30340;&#35745;&#31639;&#22788;&#29702;&#22797;&#26434;&#24615;&#24182;&#19981;&#24635;&#26159;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities and use cases of automatic natural language processing (NLP) have grown significantly over the last few years. While much work has been devoted to understanding how humans deal with discourse connectives, this phenomenon is understudied in computational systems. Therefore, it is important to put NLP models under the microscope and examine whether they can adequately comprehend, process, and reason within the complexity of natural language. In this chapter, we introduce the main mechanisms behind automatic sentence processing systems step by step and then focus on evaluating discourse connective processing. We assess nine popular systems in their ability to understand English discourse connectives and analyze how context and language understanding tasks affect their connective comprehension. The results show that NLP systems do not process all discourse connectives equally well and that the computational processing complexity of different connective kinds is not always 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;</title><link>http://arxiv.org/abs/2309.00359</link><description>&lt;p&gt;
&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#29992;&#20110;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39321;&#20892;&#22312;&#24341;&#20837;&#20449;&#24687;&#29702;&#35770;&#30340;&#32463;&#20856;&#35770;&#25991;&#20013;&#23558;&#36890;&#20449;&#20998;&#20026;&#19977;&#20010;&#23618;&#27425;&#65306;&#25216;&#26415;&#23618;&#12289;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#12290;&#25216;&#26415;&#23618;&#20851;&#27880;&#30340;&#26159;&#20934;&#30830;&#37325;&#26500;&#20256;&#36755;&#30340;&#31526;&#21495;&#65292;&#32780;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#21017;&#28041;&#21450;&#25512;&#26029;&#20986;&#30340;&#24847;&#20041;&#21450;&#20854;&#23545;&#25509;&#25910;&#32773;&#30340;&#24433;&#21709;&#12290;&#24471;&#30410;&#20110;&#30005;&#20449;&#25216;&#26415;&#65292;&#31532;&#19968;&#23618;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#22823;&#30340;&#36827;&#27493;&#65292;&#22914;&#20114;&#32852;&#32593;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31532;&#20108;&#20010;&#30446;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31532;&#19977;&#23618;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#35302;&#21450;&#12290;&#31532;&#19977;&#20010;&#38382;&#39064;&#28041;&#21450;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#12290;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#20043;&#19968;&#21487;&#33021;&#26159;LLM&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;&#34892;&#20026;&#26631;&#35760;&#23450;&#20041;&#20102;&#22312;&#19968;&#27425;&#36890;&#20449;&#20013;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#65292;&#22914;&#20998;&#20139;&#12289;&#28857;&#36190;&#12289;&#28857;&#20987;&#12289;&#36141;&#20080;&#12289;&#36716;&#25512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.00312</link><description>&lt;p&gt;
&#29992;&#20110;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#30340;&#19981;&#21516;&#25253;&#21578;&#32467;&#26524;&#30340;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies. (arXiv:2309.00312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#39064;&#24314;&#27169;&#21644;&#25991;&#26412;&#25366;&#25496;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#38598;&#65292;&#36866;&#29992;&#20110;&#36827;&#34892;&#20803;&#20998;&#26512;&#21644;&#31995;&#32479;&#23457;&#26597;&#12290;&#23545;&#20110;&#35777;&#25454;&#32508;&#36848;&#65292;&#19978;&#36848;NLP&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#25991;&#29486;&#25628;&#32034;&#25110;&#20174;&#25253;&#21578;&#20013;&#25552;&#21462;&#20540;&#20197;&#33258;&#21160;&#21270;SR&#21644;MA&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21516;&#19968;&#24191;&#20041;&#30740;&#31350;&#38382;&#39064;&#19978;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#26681;&#25454;&#20854;&#27604;&#20363;&#21457;&#29983;&#21644;&#22312;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#20013;&#30340;&#19968;&#33268;&#24615;&#20998;&#24067;&#23545;&#20854;&#36827;&#34892;&#25490;&#21517;&#65292;&#25214;&#21040;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26174;&#33879;&#30456;&#20851;&#30340;&#35805;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;&#34917;&#20805;&#33829;&#20859;&#21270;&#21512;&#29289;&#26159;&#21542;&#26174;&#33879;&#26377;&#30410;&#20110;&#40644;&#26001;&#21464;&#24615;(MD)&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30830;&#23450;&#20102;&#20843;&#31181;&#21270;&#21512;&#29289;&#19982;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30340;&#29305;&#23450;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modeling and text mining are subsets of Natural Language Processing with relevance for conducting meta-analysis (MA) and systematic review (SR). For evidence synthesis, the above NLP methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of SR and MA. Instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. Specifically, the objective is to find topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence and consistency of distribution across reports of significant results. The proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (MD). Eight compounds were identified as having a particular association with reports of significant r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26059;&#24459;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#27468;&#25163;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#21333;&#22768;&#36947;&#30340;&#38899;&#22495;&#65292;&#22312;&#19981;&#38477;&#20302;&#38899;&#33394;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22810;&#27468;&#25163;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#38899;&#39640;&#21644;&#38899;&#32032;&#23450;&#26102;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#21333;&#22768;&#36947;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00284</link><description>&lt;p&gt;
&#29992;&#26080;&#26059;&#24459;&#30417;&#30563;&#39044;&#35757;&#32451;&#22686;&#24378;&#21333;&#22768;&#36947;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#30340;&#38899;&#22495;
&lt;/p&gt;
&lt;p&gt;
Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training. (arXiv:2309.00284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26059;&#24459;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#27468;&#25163;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#21333;&#22768;&#36947;&#30340;&#38899;&#22495;&#65292;&#22312;&#19981;&#38477;&#20302;&#38899;&#33394;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22810;&#27468;&#25163;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#38899;&#39640;&#21644;&#38899;&#32032;&#23450;&#26102;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#21333;&#22768;&#36947;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#22768;&#36947;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#65288;SVS&#65289;&#36890;&#24120;&#22312;&#27468;&#25163;&#38899;&#22495;&#20043;&#22806;&#25110;&#22522;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#38899;&#39640;&#19978;&#25928;&#26524;&#19981;&#20339;&#12290;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27468;&#25163;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26080;&#26059;&#24459;&#30417;&#30563;&#22810;&#22768;&#36947;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21333;&#22768;&#36947;&#30340;&#38899;&#22495;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#38899;&#33394;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22810;&#27468;&#25163;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20165;&#21253;&#21547;&#38899;&#39057;&#21644;&#27468;&#35789;&#37197;&#23545;&#65292;&#27809;&#26377;&#38899;&#32032;&#23450;&#26102;&#20449;&#24687;&#21644;&#38899;&#39640;&#26631;&#27880;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39044;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38899;&#32032;&#39044;&#27979;&#22120;&#26469;&#29983;&#25104;&#24103;&#32423;&#38899;&#32032;&#27010;&#29575;&#21521;&#37327;&#20316;&#20026;&#38899;&#32032;&#23450;&#26102;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#35828;&#35805;&#20154;&#32534;&#30721;&#22120;&#26469;&#24314;&#27169;&#19981;&#21516;&#21809;&#27468;&#32773;&#30340;&#38899;&#33394;&#21464;&#21270;&#65292;&#24182;&#30452;&#25509;&#20174;&#38899;&#39057;&#20013;&#20272;&#35745;&#24103;&#32423;f0&#20540;&#26469;&#25552;&#20379;&#38899;&#39640;&#20449;&#24687;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#22312;&#24494;&#35843;&#27493;&#39588;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The single-speaker singing voice synthesis (SVS) usually underperforms at pitch values that are out of the singer's vocal range or associated with limited training samples. Based on our previous work, this work proposes a melody-unsupervised multi-speaker pre-training method conducted on a multi-singer dataset to enhance the vocal range of the single-speaker, while not degrading the timbre similarity. This pre-training method can be deployed to a large-scale multi-singer dataset, which only contains audio-and-lyrics pairs without phonemic timing information and pitch annotation. Specifically, in the pre-training step, we design a phoneme predictor to produce the frame-level phoneme probability vectors as the phonemic timing information and a speaker encoder to model the timbre variations of different singers, and directly estimate the frame-level f0 values from the audio to provide the pitch information. These pre-trained model parameters are delivered into the fine-tuning step as prio
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20960;&#20309;&#35270;&#35282;&#35299;&#37322;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21457;&#29616;&#36890;&#29992;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#21487;&#33021;&#26159;&#23884;&#20837;&#21521;&#37327;&#65292;&#20165;&#20165;&#36817;&#20284;&#20102;&#20854;&#23545;&#25239;&#35757;&#32451;&#21306;&#22495;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.00254</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#65306;&#20960;&#20309;&#21487;&#33021;&#26159;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Why do universal adversarial attacks work on large language models?: Geometry might be the answer. (arXiv:2309.00254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20960;&#20309;&#35270;&#35282;&#35299;&#37322;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21457;&#29616;&#36890;&#29992;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#21487;&#33021;&#26159;&#23884;&#20837;&#21521;&#37327;&#65292;&#20165;&#20165;&#36817;&#20284;&#20102;&#20854;&#23545;&#25239;&#35757;&#32451;&#21306;&#22495;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26032;&#21457;&#33021;&#21147;&#65292;&#22312;&#31038;&#20250;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#23427;&#20204;&#30340;&#20869;&#37096;&#24037;&#20316;&#20173;&#28982;&#22522;&#26412;&#26410;&#35299;&#20915;&#12290;&#24050;&#32463;&#35777;&#26126;&#22522;&#20110;&#26799;&#24230;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#65292;&#30001;&#20110;&#23427;&#20204;&#23545;&#36755;&#20837;&#19981;&#25935;&#24863;&#30340;&#29305;&#24615;&#65292;&#21487;&#33021;&#20855;&#26377;&#28508;&#22312;&#30340;&#21361;&#38505;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#35299;&#37322;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36890;&#36807;&#23545;&#25915;&#20987;117M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#35777;&#25454;&#34920;&#26126;&#36890;&#29992;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#21487;&#33021;&#26159;&#23884;&#20837;&#21521;&#37327;&#65292;&#20165;&#20165;&#36817;&#20284;&#20102;&#20854;&#23545;&#25239;&#35757;&#32451;&#21306;&#22495;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20010;&#20551;&#35774;&#24471;&#21040;&#20102;&#36890;&#36807;&#30333;&#30418;&#27169;&#22411;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#21253;&#25324;&#23545;&#38544;&#34255;&#34920;&#31034;&#30340;&#38477;&#32500;&#21644;&#30456;&#20284;&#24230;&#27979;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#20851;&#20110;&#39537;&#21160;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28508;&#22312;&#26426;&#21046;&#30340;&#26032;&#20960;&#20309;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving univer
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#26816;&#27979;&#38463;&#25289;&#20271;&#25512;&#25991;&#20013;&#33258;&#26432;&#24605;&#32500;&#30340;&#26041;&#27861;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#26432;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00246</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#38463;&#25289;&#20271;&#25512;&#25991;&#20013;&#30340;&#33258;&#26432;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Detecting Suicidality in Arabic Tweets Using Machine Learning and Deep Learning Techniques. (arXiv:2309.00246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00246
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#26816;&#27979;&#38463;&#25289;&#20271;&#25512;&#25991;&#20013;&#33258;&#26432;&#24605;&#32500;&#30340;&#26041;&#27861;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#26432;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#36890;&#36807;&#20351;&#20840;&#29699;&#20154;&#27665;&#33021;&#22815;&#21363;&#26102;&#12289;&#20844;&#24320;&#21644;&#39057;&#32321;&#22320;&#36830;&#25509;&#36215;&#26469;&#65292;&#25913;&#21464;&#20102;&#20256;&#32479;&#30340;&#27807;&#36890;&#25216;&#26415;&#12290;&#20154;&#20204;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#20010;&#20154;&#25925;&#20107;&#24182;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#12290;&#36127;&#38754;&#24773;&#32490;&#65292;&#27604;&#22914;&#27515;&#20129;&#12289;&#33258;&#27531;&#21644;&#22256;&#38590;&#24605;&#32500;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26222;&#36941;&#23384;&#22312;&#65292;&#23588;&#20854;&#26159;&#22312;&#24180;&#36731;&#19968;&#20195;&#20013;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#33258;&#26432;&#24605;&#32500;&#23558;&#26377;&#21161;&#20110;&#25552;&#20379;&#36866;&#24403;&#30340;&#24178;&#39044;&#65292;&#26368;&#32456;&#38459;&#27490;&#20182;&#20154;&#33258;&#27531;&#21644;&#33258;&#26432;&#65292;&#24182;&#38459;&#27490;&#33258;&#26432;&#24605;&#32500;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20256;&#25773;&#12290;&#20026;&#20102;&#30740;&#31350;&#33258;&#21160;&#26816;&#27979;&#38463;&#25289;&#20271;&#25512;&#25991;&#20013;&#30340;&#33258;&#26432;&#24605;&#32500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#26432;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#35789;&#39057;&#21644;&#35789;&#23884;&#20837;&#29305;&#24449;&#35757;&#32451;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;KNN&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65292;&#24182;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms have revolutionized traditional communication techniques by enabling people globally to connect instantaneously, openly, and frequently. People use social media to share personal stories and express their opinion. Negative emotions such as thoughts of death, self-harm, and hardship are commonly expressed on social media, particularly among younger generations. As a result, using social media to detect suicidal thoughts will help provide proper intervention that will ultimately deter others from self-harm and committing suicide and stop the spread of suicidal ideation on social media. To investigate the ability to detect suicidal thoughts in Arabic tweets automatically, we developed a novel Arabic suicidal tweets dataset, examined several machine learning models, including Na\"ive Bayes, Support Vector Machine, K-Nearest Neighbor, Random Forest, and XGBoost, trained on word frequency and word embedding features, and investigated the ability of pre-trained deep lea
&lt;/p&gt;</description></item><item><title>NeuroSurgeon&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;Huggingface Transformers&#24211;&#20013;&#21457;&#29616;&#21644;&#25805;&#20316;&#27169;&#22411;&#23376;&#32593;&#32476;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#25512;&#36827;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00244</link><description>&lt;p&gt;
NeuroSurgeon: &#19968;&#31181;&#29992;&#20110;&#23376;&#32593;&#32476;&#20998;&#26512;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
NeuroSurgeon: A Toolkit for Subnetwork Analysis. (arXiv:2309.00244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00244
&lt;/p&gt;
&lt;p&gt;
NeuroSurgeon&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;Huggingface Transformers&#24211;&#20013;&#21457;&#29616;&#21644;&#25805;&#20316;&#27169;&#22411;&#23376;&#32593;&#32476;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#25512;&#36827;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#30340;&#31639;&#27861;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#24050;&#35757;&#32451;&#27169;&#22411;&#20998;&#35299;&#20026;&#21151;&#33021;&#30005;&#36335;&#26469;&#29702;&#35299;&#23427;&#20204;(&#21442;&#32771;Csord\'as&#31561;&#20154;&#30340;&#30740;&#31350;&#65292;2020&#65307;Lepori&#31561;&#20154;&#65292;2023)&#12290;&#20026;&#20102;&#25512;&#36827;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;NeuroSurgeon&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#21644;&#25805;&#20316;Huggingface Transformers&#24211;&#20013;&#30340;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;(Wolf&#31561;&#20154;&#65292;2019)&#12290;NeuroSurgeon&#21487;&#20197;&#22312;https://github.com/mlepori1/NeuroSurgeon &#20813;&#36153;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#26816;&#32034;&#26469;&#22686;&#24378;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#65292;&#24182;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20107;&#23454;&#26680;&#26597;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00240</link><description>&lt;p&gt;
FactLLaMA: &#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#20248;&#21270;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking. (arXiv:2309.00240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#26816;&#32034;&#26469;&#22686;&#24378;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#65292;&#24182;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20107;&#23454;&#26680;&#26597;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#22312;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25351;&#20196;&#36861;&#36394;&#21464;&#31181;&#65292;&#22914;InstructGPT&#21644;Alpaca&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30693;&#35782;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26368;&#26032;&#25110;&#20805;&#20998;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20107;&#23454;&#26680;&#26597;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#35777;&#25454;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#19982;&#32473;&#23450;&#36755;&#20837;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#12290;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#21487;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35777;&#25454;&#23545;&#19968;&#20010;&#21517;&#20026;LLaMA&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36755;&#20837;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic fact-checking plays a crucial role in combating the spread of misinformation. Large Language Models (LLMs) and Instruction-Following variants, such as InstructGPT and Alpaca, have shown remarkable performance in various natural language processing tasks. However, their knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking. To address this limitation, we propose combining the power of instruction-following language models with external evidence retrieval to enhance fact-checking performance. Our approach involves leveraging search engines to retrieve relevant evidence for a given input claim. This external evidence serves as valuable supplementary information to augment the knowledge of the pretrained language model. Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately. To evaluate our method, we conducted experiments 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#39044;&#27979;&#38463;&#25289;&#20271;&#20010;&#20154;&#36523;&#20221;&#26696;&#20214;&#27861;&#24459;&#21028;&#20915;&#30340;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27861;&#23448;&#21644;&#24459;&#24072;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20943;&#23569;&#21028;&#20915;&#24046;&#24322;&#65292;&#24182;&#24110;&#21161;&#35785;&#35772;&#24403;&#20107;&#20154;&#20107;&#20808;&#20998;&#26512;&#26696;&#20214;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00238</link><description>&lt;p&gt;
ALJP: &#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#38463;&#25289;&#20271;&#20010;&#20154;&#36523;&#20221;&#26696;&#20214;&#30340;&#27861;&#24459;&#21028;&#20915;
&lt;/p&gt;
&lt;p&gt;
ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine Learning Models. (arXiv:2309.00238v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00238
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#39044;&#27979;&#38463;&#25289;&#20271;&#20010;&#20154;&#36523;&#20221;&#26696;&#20214;&#27861;&#24459;&#21028;&#20915;&#30340;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27861;&#23448;&#21644;&#24459;&#24072;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20943;&#23569;&#21028;&#20915;&#24046;&#24322;&#65292;&#24182;&#24110;&#21161;&#35785;&#35772;&#24403;&#20107;&#20154;&#20107;&#20808;&#20998;&#26512;&#26696;&#20214;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#26088;&#22312;&#22522;&#20110;&#26696;&#24773;&#25551;&#36848;&#39044;&#27979;&#21028;&#20915;&#32467;&#26524;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#27979;&#27861;&#24459;&#32844;&#19994;&#30340;&#32467;&#26524;&#26469;&#24110;&#21161;&#28508;&#22312;&#30340;&#23458;&#25143;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#31181;&#25552;&#20986;&#30340;&#25216;&#26415;&#26159;&#29992;&#38463;&#25289;&#20271;&#35821;&#23454;&#29616;&#30340;&#65292;&#21482;&#26377;&#23569;&#25968;&#23581;&#35797;&#37319;&#29992;&#33521;&#35821;&#12289;&#27721;&#35821;&#21644;&#21360;&#22320;&#35821;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#20174;&#38463;&#25289;&#20271;&#35821;&#26696;&#24773;&#33050;&#26412;&#20013;&#39044;&#27979;&#21028;&#20915;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#25242;&#20859;&#26435;&#21644;&#23130;&#23035;&#26080;&#25928;&#30340;&#26696;&#20214;&#20013;&#12290;&#35813;&#31995;&#32479;&#23558;&#24110;&#21161;&#27861;&#23448;&#21644;&#24459;&#24072;&#25552;&#39640;&#24037;&#20316;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#21028;&#20915;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#23457;&#21028;&#20043;&#21069;&#65292;&#23427;&#23558;&#24110;&#21161;&#35785;&#35772;&#24403;&#20107;&#20154;&#12289;&#24459;&#24072;&#21644;&#27861;&#23398;&#29983;&#20998;&#26512;&#20219;&#20309;&#32473;&#23450;&#26696;&#20214;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal Judgment Prediction (LJP) aims to predict judgment outcomes based on case description. Several researchers have developed techniques to assist potential clients by predicting the outcome in the legal profession. However, none of the proposed techniques were implemented in Arabic, and only a few attempts were implemented in English, Chinese, and Hindi. In this paper, we develop a system that utilizes deep learning (DL) and natural language processing (NLP) techniques to predict the judgment outcome from Arabic case scripts, especially in cases of custody and annulment of marriage. This system will assist judges and attorneys in improving their work and time efficiency while reducing sentencing disparity. In addition, it will help litigants, lawyers, and law students analyze the probable outcomes of any given case before trial. We use a different machine and deep learning models such as Support Vector Machine (SVM), Logistic regression (LR), Long Short Term Memory (LSTM), and Bidir
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>JoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#23545;&#35805;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#21709;&#24212;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#22788;&#29702;&#26497;&#31471;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00230</link><description>&lt;p&gt;
JoTR: &#19968;&#31181;&#22522;&#20110;&#32852;&#21512;Transformer&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JoTR: A Joint Transformer and Reinforcement Learning Framework for Dialog Policy Learning. (arXiv:2309.00230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00230
&lt;/p&gt;
&lt;p&gt;
JoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#23545;&#35805;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#21709;&#24212;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#22788;&#29702;&#26497;&#31471;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#26159;&#23545;&#35805;&#24314;&#27169;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#20027;&#35201;&#20316;&#29992;&#26159;&#30830;&#23450;&#21512;&#36866;&#30340;&#25277;&#35937;&#22238;&#24212;&#65292;&#36890;&#24120;&#31216;&#20026;"&#23545;&#35805;&#21160;&#20316;"&#12290;&#20256;&#32479;&#30340;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20351;&#29992;&#20174;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#30340;&#39044;&#23450;&#20041;&#21160;&#20316;&#20505;&#36873;&#39033;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19981;&#23436;&#25972;&#30340;&#20505;&#36873;&#39033;&#21487;&#33021;&#20250;&#26174;&#33879;&#38480;&#21046;&#21709;&#24212;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#22788;&#29702;&#26497;&#31471;&#25805;&#20316;&#21442;&#25968;&#19979;&#20986;&#29616;&#25361;&#25112;&#24615;&#24773;&#20917;&#26102;&#36896;&#25104;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;JoTR&#12290;&#35813;&#26694;&#26550;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;Transformer&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#23545;&#35805;&#21160;&#20316;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;JoTR&#21046;&#23450;&#20102;&#19968;&#20010;&#35789;&#32423;&#31574;&#30053;&#65292;&#20801;&#35768;&#26356;&#21160;&#24577;&#21644;&#36866;&#24212;&#24615;&#30340;&#23545;&#35805;&#21160;&#20316;&#29983;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#21160;&#20316;&#27169;&#26495;&#12290;&#36825;&#31181;&#35774;&#32622;&#22686;&#24378;&#20102;&#21709;&#24212;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#31995;&#32479;&#22788;&#29702;&#26497;&#31471;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue policy learning (DPL) is a crucial component of dialogue modelling. Its primary role is to determine the appropriate abstract response, commonly referred to as the "dialogue action". Traditional DPL methodologies have treated this as a sequential decision problem, using pre-defined action candidates extracted from a corpus. However, these incomplete candidates can significantly limit the diversity of responses and pose challenges when dealing with edge cases, which are scenarios that occur only at extreme operating parameters. To address these limitations, we introduce a novel framework, JoTR. This framework is unique as it leverages a text-to-text Transformer-based model to generate flexible dialogue actions. Unlike traditional methods, JoTR formulates a word-level policy that allows for a more dynamic and adaptable dialogue action generation, without the need for any action templates. This setting enhances the diversity of responses and improves the system's ability to handl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#31579;&#36873;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#28155;&#21152;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2309.00223</link><description>&lt;p&gt;
FruitShell&#27861;&#35821;&#21512;&#25104;&#31995;&#32479;&#22312;Blizzard 2023&#25361;&#25112;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The FruitShell French synthesis system at the Blizzard 2023 Challenge. (arXiv:2309.00223v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#31579;&#36873;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#28155;&#21152;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#20174;&#22899;&#24615;&#28436;&#35762;&#32773;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21644;&#29983;&#25104;&#19982;&#29305;&#23450;&#20010;&#20307;&#30456;&#20284;&#30340;&#35821;&#38899;&#12290;&#20851;&#20110;&#27604;&#36187;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31579;&#36873;&#36807;&#31243;&#65292;&#21435;&#38500;&#20102;&#32570;&#22833;&#25110;&#38169;&#35823;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#38500;&#38899;&#32032;&#20197;&#22806;&#30340;&#25152;&#26377;&#31526;&#21495;&#36827;&#34892;&#20102;&#25972;&#29702;&#65292;&#24182;&#28040;&#38500;&#20102;&#27809;&#26377;&#21457;&#38899;&#25110;&#25345;&#32493;&#26102;&#38388;&#20026;&#38646;&#30340;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#25991;&#26412;&#20013;&#28155;&#21152;&#20102;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#65292;&#26681;&#25454;&#25105;&#20204;&#20043;&#21069;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#12290;&#23545;&#20110;Spoke&#20219;&#21153;&#65292;&#25105;&#20204;&#26681;&#25454;&#27604;&#36187;&#35268;&#21017;&#36827;&#34892;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;G2P&#27169;&#22411;&#23558;&#27861;&#35821;&#25991;&#26412;&#36716;&#24405;&#20026;&#38899;&#32032;&#12290;&#30001;&#20110;G2P&#27169;&#22411;&#20351;&#29992;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#65292;&#25105;&#20204;&#23545;&#25552;&#20379;&#30340;&#27604;&#36187;&#25968;&#25454;&#24212;&#29992;&#20102;&#30456;&#21516;&#30340;&#36716;&#24405;&#36807;&#31243;&#65292;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32534;&#35793;&#22120;&#23545;&#26576;&#20123;&#25216;&#26415;&#38480;&#21046;&#30340;&#35782;&#21035;&#33021;&#21147;&#26377;&#38480;&#65292;&#25152;&#20197;&#25105;&#20204;&#20026;&#20102;&#20445;&#25345;&#31454;&#20105;&#30340;&#20844;&#27491;&#65292;&#23558;&#25968;&#25454;&#25353;&#38899;&#26631;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#29255;&#27573;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a French text-to-speech synthesis system for the Blizzard Challenge 2023. The challenge consists of two tasks: generating high-quality speech from female speakers and generating speech that closely resembles specific individuals. Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience. For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#20041;&#24341;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#23545;&#35937;&#25552;&#35758;&#35780;&#20272;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#38408;&#20540;&#36873;&#25321;&#27880;&#37322;&#37325;&#35201;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#23545;&#35937;&#25552;&#35758;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19982;&#22270;&#20687;&#23383;&#24149;&#24230;&#37327;&#21644;&#20154;&#24037;&#27880;&#37322;&#36873;&#25321;&#30340;&#27880;&#37322;&#19978;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#25913;&#36827;&#24615;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2309.00215</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24341;&#22320;&#26469;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#23545;&#35937;&#25552;&#35758;&#35780;&#20272;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding. (arXiv:2309.00215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#20041;&#24341;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#23545;&#35937;&#25552;&#35758;&#35780;&#20272;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#38408;&#20540;&#36873;&#25321;&#27880;&#37322;&#37325;&#35201;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#23545;&#35937;&#25552;&#35758;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19982;&#22270;&#20687;&#23383;&#24149;&#24230;&#37327;&#21644;&#20154;&#24037;&#27880;&#37322;&#36873;&#25321;&#30340;&#27880;&#37322;&#19978;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#25913;&#36827;&#24615;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#25552;&#35758;&#29983;&#25104;&#20316;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#22270;&#20687;&#23383;&#24149;&#12289;&#35270;&#35273;&#38382;&#31572;&#31561;&#65289;&#30340;&#26631;&#20934;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#29983;&#25104;&#30340;&#23545;&#35937;&#25552;&#35758;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#25152;&#26377;&#21487;&#29992;&#30340;&#27880;&#37322;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064; - &#26356;&#39640;&#30340;&#20998;&#25968;&#19981;&#19968;&#23450;&#23545;&#24212;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#20041;&#24341;&#22320;&#30340;&#26377;&#25928;&#24615;&#20197;&#20943;&#36731;&#36825;&#19968;&#24433;&#21709;&#65292;&#24182;&#24314;&#35758;&#20165;&#38024;&#23545;&#19968;&#37096;&#20998;&#36890;&#36807;&#38408;&#20540;&#36873;&#25321;&#27880;&#37322;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#21487;&#29992;&#27880;&#37322;&#26469;&#35780;&#20272;&#23545;&#35937;&#25552;&#35758;&#12290;&#36890;&#36807;&#20174;&#25551;&#36848;&#22270;&#20687;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#26469;&#37327;&#21270;&#23545;&#35937;&#27880;&#37322;&#23545;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19982;&#22270;&#20687;&#23383;&#24149;&#24230;&#37327;&#21644;&#20154;&#24037;&#27880;&#37322;&#36873;&#25321;&#30340;&#27880;&#37322;&#19978;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#25913;&#36827;&#24615;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object proposal generation serves as a standard pre-processing step in Vision-Language (VL) tasks (image captioning, visual question answering, etc.). The performance of object proposals generated for VL tasks is currently evaluated across all available annotations, a protocol that we show is misaligned - higher scores do not necessarily correspond to improved performance on downstream VL tasks. Our work serves as a study of this phenomenon and explores the effectiveness of semantic grounding to mitigate its effects. To this end, we propose evaluating object proposals against only a subset of available annotations, selected by thresholding an annotation importance score. Importance of object annotations to VL tasks is quantified by extracting relevant semantic information from text describing the image. We show that our method is consistent and demonstrates greatly improved alignment with annotations selected by image captioning metrics and human annotation when compared against existi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#30340;GPT-3.5-turbo&#21644;GPT-4&#22312;&#38889;&#22269;&#19978;&#24066;&#20844;&#21496;&#25259;&#38706;&#20013;&#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00208</link><description>&lt;p&gt;
&#29992;&#20110;&#35821;&#20041;&#30417;&#27979;&#20844;&#21496;&#25259;&#38706;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38889;&#22269;KOSPI&#21069;50&#23478;&#20844;&#21496;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea's Top 50 KOSPI Companies. (arXiv:2309.00208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#30340;GPT-3.5-turbo&#21644;GPT-4&#22312;&#38889;&#22269;&#19978;&#24066;&#20844;&#21496;&#25259;&#38706;&#20013;&#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;OpenAI&#30340;GPT-3.5-turbo&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#33258;&#21160;&#21270;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38889;&#22269;&#24773;&#22659;&#19979;&#35821;&#20041;&#20998;&#26512;&#20844;&#21496;&#25259;&#38706;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#21450;&#26102;&#25259;&#38706;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38889;&#22269;KOSPI&#19978;&#24066;&#30340;&#24066;&#20540;&#21069;50&#23478;&#19978;&#24066;&#20844;&#21496;&#65292;&#24182;&#22312;17&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#35814;&#32454;&#23457;&#26597;&#20102;&#23427;&#20204;&#30340;&#26376;&#24230;&#25259;&#38706;&#25688;&#35201;&#12290;&#27599;&#20010;&#25688;&#35201;&#37117;&#25353;&#29031;&#20174;1&#65288;&#38750;&#24120;&#36127;&#38754;&#65289;&#21040;5&#65288;&#38750;&#24120;&#27491;&#38754;&#65289;&#30340;&#27604;&#20363;&#36827;&#34892;&#20102;&#24773;&#24863;&#35780;&#32423;&#12290;&#20026;&#20102;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#24773;&#24863;&#35780;&#32423;&#19982;&#20154;&#24037;&#19987;&#23478;&#29983;&#25104;&#30340;&#35780;&#32423;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;GPT-3.5-turbo&#21644;GPT-4&#20043;&#38388;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21518;&#32773;&#22312;&#20154;&#31867;&#35780;&#20272;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of artificial intelligence, state-of-the-art language models such as OpenAI's GPT-3.5-turbo and GPT-4 offer unprecedented opportunities for automating complex tasks. This research paper delves into the capabilities of these models for semantically analyzing corporate disclosures in the Korean context, specifically for timely disclosure. The study focuses on the top 50 publicly traded companies listed on the Korean KOSPI, based on market capitalization, and scrutinizes their monthly disclosure summaries over a period of 17 months. Each summary was assigned a sentiment rating on a scale ranging from 1(very negative) to 5(very positive). To gauge the effectiveness of the language models, their sentiment ratings were compared with those generated by human experts. Our findings reveal a notable performance disparity between GPT-3.5-turbo and GPT-4, with the latter demonstrating significant accuracy in human evaluation tests. The Spearman correlation coefficie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#25991;&#26412;&#22320;&#29702;&#20449;&#24687;&#30340;&#35268;&#24459;&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#35821;&#35328;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#22320;&#29702;&#20449;&#24687;&#30340;&#25968;&#37327;&#12289;&#38271;&#24230;&#21644;&#36317;&#31163;&#31526;&#21512;Gamma&#20998;&#24067;&#65292;&#24182;&#19988;&#25490;&#38500;&#20102;&#39640;&#26031;&#20998;&#24067;&#21644;Zipf&#23450;&#24459;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00180</link><description>&lt;p&gt;
&#25506;&#32034;&#25991;&#26412;&#22320;&#29702;&#20449;&#24687;&#30340;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Exploring the law of text geographic information. (arXiv:2309.00180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00180
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#25991;&#26412;&#22320;&#29702;&#20449;&#24687;&#30340;&#35268;&#24459;&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#35821;&#35328;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#22320;&#29702;&#20449;&#24687;&#30340;&#25968;&#37327;&#12289;&#38271;&#24230;&#21644;&#36317;&#31163;&#31526;&#21512;Gamma&#20998;&#24067;&#65292;&#24182;&#19988;&#25490;&#38500;&#20102;&#39640;&#26031;&#20998;&#24067;&#21644;Zipf&#23450;&#24459;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22320;&#29702;&#20449;&#24687;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#19988;&#34987;&#24191;&#27867;&#20381;&#36182;&#12290;&#30001;&#20110;&#22320;&#29702;&#20449;&#24687;&#20998;&#24067;&#19981;&#28165;&#26224;&#65292;&#26377;&#25928;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#27492;&#39537;&#20351;&#25105;&#20204;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#35748;&#20026;&#22320;&#29702;&#20449;&#24687;&#21463;&#21040;&#20154;&#31867;&#34892;&#20026;&#12289;&#35748;&#30693;&#12289;&#34920;&#36798;&#21644;&#24605;&#32500;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#22522;&#20110;&#25105;&#20204;&#23545;&#33258;&#28982;&#31995;&#32479;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#25105;&#20204;&#20551;&#35774;&#20854;&#31526;&#21512;Gamma&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#21253;&#21547;&#19981;&#21516;&#35821;&#35328;&#21644;&#31867;&#22411;&#30340;24&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#25581;&#31034;&#20102;&#22320;&#29702;&#20449;&#24687;&#20013;&#25968;&#37327;&#12289;&#38271;&#24230;&#21644;&#36317;&#31163;&#30340;&#22522;&#26412;&#35268;&#24459;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#19982;&#39640;&#26031;&#20998;&#24067;&#21644;Zipf&#23450;&#24459;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#36825;&#20123;&#27861;&#21017;&#30340;&#19981;&#20381;&#36182;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20154;&#31867;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#30340;&#19978;&#38480;&#65292;&#25351;&#21521;t
&lt;/p&gt;
&lt;p&gt;
Textual geographic information is indispensable and heavily relied upon in practical applications. The absence of clear distribution poses challenges in effectively harnessing geographic information, thereby driving our quest for exploration. We contend that geographic information is influenced by human behavior, cognition, expression, and thought processes, and given our intuitive understanding of natural systems, we hypothesize its conformity to the Gamma distribution. Through rigorous experiments on a diverse range of 24 datasets encompassing different languages and types, we have substantiated this hypothesis, unearthing the underlying regularities governing the dimensions of quantity, length, and distance in geographic information. Furthermore, theoretical analyses and comparisons with Gaussian distributions and Zipf's law have refuted the contingency of these laws. Significantly, we have estimated the upper bounds of human utilization of geographic information, pointing towards t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;SCDA&#65292;&#36890;&#36807;&#21033;&#29992;&#20122;&#25991;&#21270;&#34920;&#36798;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#20845;&#20010;&#22686;&#24378;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#38754;&#20020;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;SCDA&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00178</link><description>&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#21542;&#38656;&#35201;&#20122;&#25991;&#21270;&#65311;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Will Sentiment Analysis Need Subculture? A New Data Augmentation Approach. (arXiv:2309.00178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;SCDA&#65292;&#36890;&#36807;&#21033;&#29992;&#20122;&#25991;&#21270;&#34920;&#36798;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#20845;&#20010;&#22686;&#24378;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#38754;&#20020;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;SCDA&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33879;&#21517;&#35866;&#35821;&#8220;&#31508;&#33021;&#32988;&#36807;&#21073;&#8221;&#24378;&#35843;&#20102;&#25991;&#23383;&#34920;&#36798;&#22312;&#22609;&#36896;&#24773;&#24863;&#26041;&#38754;&#25152;&#20855;&#26377;&#30340;&#24378;&#22823;&#24433;&#21709;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;&#31934;&#24515;&#25171;&#36896;&#30340;&#25991;&#23383;&#21487;&#20197;&#22312;&#25991;&#21270;&#20013;&#20135;&#29983;&#28145;&#36828;&#20849;&#40483;&#65292;&#20256;&#36798;&#28145;&#21051;&#30340;&#24773;&#24863;&#12290;&#22914;&#20170;&#65292;&#20114;&#32852;&#32593;&#30340;&#26222;&#21450;&#20419;&#25104;&#20102;&#22260;&#32469;&#24403;&#20195;&#31038;&#20250;&#29615;&#22659;&#32858;&#38598;&#30340;&#20122;&#25991;&#21270;&#12290;&#20122;&#25991;&#21270;&#36890;&#36807;&#28909;&#34935;&#36861;&#27714;&#26032;&#22855;&#26469;&#24039;&#22937;&#22320;&#34920;&#36798;&#20154;&#31867;&#24773;&#24863;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#20107;&#23454;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20122;&#25991;&#21270;&#30340;&#35270;&#35282;&#20016;&#23500;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#38754;&#20020;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20122;&#25991;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65288;SCDA&#65289;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20845;&#31181;&#19981;&#21516;&#20122;&#25991;&#21270;&#34920;&#36798;&#29983;&#25104;&#22120;&#65292;&#20026;&#27599;&#20010;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#20845;&#20010;&#22686;&#24378;&#25991;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;SCDA&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The renowned proverb that "The pen is mightier than the sword" underscores the formidable influence wielded by text expressions in shaping sentiments. Indeed, well-crafted written can deeply resonate within cultures, conveying profound sentiments. Nowadays, the omnipresence of the Internet has fostered a subculture that congregates around the contemporary milieu. The subculture artfully articulates the intricacies of human feelings by ardently pursuing the allure of novelty, a fact that cannot be disregarded in the sentiment analysis. This paper strives to enrich data through the lens of subculture, to address the insufficient training data faced by sentiment analysis. To this end, a new approach of subculture-based data augmentation (SCDA) is proposed, which engenders six enhanced texts for each training text by leveraging the creation of six diverse subculture expression generators. The extensive experiments attest to the effectiveness and potential of SCDA. The results also shed lig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00155</link><description>&lt;p&gt;
LLM&#22312;Shell&#20013;&#30340;&#24212;&#29992;&#65306;&#29983;&#25104;&#24335;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#32592;&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34588;&#32592;&#65288;&#21363;&#20351;&#26159;&#39640;&#20132;&#20114;&#24335;&#30340;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#30495;&#23454;&#24863;&#26469;&#27450;&#39575;&#25915;&#20987;&#32773;&#12290;&#36825;&#20010;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#33021;&#22815;&#21019;&#24314;&#21487;&#20449;&#19988;&#21160;&#24577;&#30340;&#34588;&#32592;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#22914;&#30830;&#23450;&#24615;&#21709;&#24212;&#12289;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#38656;&#35201;&#21028;&#26029;&#34588;&#32592;&#22238;&#24212;&#26159;&#21542;&#34394;&#20551;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27599;&#20010;&#21629;&#20196;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34588;&#32592;&#65292;&#31216;&#20026;shelLM&#65292;&#36798;&#21040;&#20102;0.92&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
&lt;/p&gt;</description></item><item><title>&#24314;&#26500;&#35821;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27934;&#35265;&#21644;&#25216;&#26415;&#23545;&#20110;&#25805;&#20316;&#21270;&#24314;&#26500;&#20027;&#20041;&#26041;&#27861;&#20197;&#21450;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.00135</link><description>&lt;p&gt;
&#24314;&#26500;&#35821;&#27861;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Construction Grammar and Artificial Intelligence. (arXiv:2309.00135v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00135
&lt;/p&gt;
&lt;p&gt;
&#24314;&#26500;&#35821;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27934;&#35265;&#21644;&#25216;&#26415;&#23545;&#20110;&#25805;&#20316;&#21270;&#24314;&#26500;&#20027;&#20041;&#26041;&#27861;&#20197;&#21450;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#24403;&#20195;&#30340;&#24314;&#26500;&#35821;&#27861;&#23398;&#32773;&#26469;&#35828;&#65292;&#28145;&#20837;&#29702;&#35299;&#24314;&#26500;&#35821;&#27861;&#19982;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#38750;&#24120;&#26377;&#30410;&#12290;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21382;&#21490;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20851;&#31995;&#26681;&#26893;&#20110;&#23545;&#20154;&#31867;&#27807;&#36890;&#21644;&#35821;&#35328;&#30340;&#20849;&#21516;&#24577;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#31532;&#19968;&#20010;&#24433;&#21709;&#26041;&#21521;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27934;&#35265;&#21644;&#25216;&#26415;&#22312;&#25805;&#20316;&#21270;&#12289;&#39564;&#35777;&#21644;&#25193;&#23637;&#35821;&#35328;&#24314;&#26500;&#20027;&#20041;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#31532;&#20108;&#20010;&#24433;&#21709;&#26041;&#21521;&#65292;&#24378;&#35843;&#24314;&#26500;&#35821;&#27861;&#27934;&#35265;&#21644;&#20998;&#26512;&#23545;&#20110;&#26500;&#24314;&#30495;&#27491;&#26234;&#33021;&#20195;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#21162;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#29992;&#21508;&#31181;&#20363;&#23376;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#31181;&#20851;&#31995;&#21313;&#20998;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this chapter, we argue that it is highly beneficial for the contemporary construction grammarian to have a thorough understanding of the strong relationship between the research fields of construction grammar and artificial intelligence. We start by unravelling the historical links between the two fields, showing that their relationship is rooted in a common attitude towards human communication and language. We then discuss the first direction of influence, focussing in particular on how insights and techniques from the field of artificial intelligence play an important role in operationalising, validating and scaling constructionist approaches to language. We then proceed to the second direction of influence, highlighting the relevance of construction grammar insights and analyses to the artificial intelligence endeavour of building truly intelligent agents. We support our case with a variety of illustrative examples and conclude that the further elaboration of this relationship wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;QS-TTS&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;TTS&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#26080;&#26631;&#31614;&#35821;&#38899;&#38899;&#39057;&#25552;&#39640;&#21512;&#25104;&#36136;&#37327;&#24182;&#38477;&#20302;&#23545;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.00126</link><description>&lt;p&gt;
QS-TTS: &#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#21322;&#30417;&#30563;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning. (arXiv:2309.00126v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00126
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;QS-TTS&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;TTS&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#26080;&#26631;&#31614;&#35821;&#38899;&#38899;&#39057;&#25552;&#39640;&#21512;&#25104;&#36136;&#37327;&#24182;&#38477;&#20302;&#23545;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;TTS&#26694;&#26550;QS-TTS&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#26080;&#26631;&#31614;&#35821;&#38899;&#38899;&#39057;&#30340;&#21521;&#37327;&#37327;&#21270;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;VQ-S3RL&#65289;&#26469;&#25552;&#39640;TTS&#36136;&#37327;&#65292;&#24182;&#38477;&#20302;&#23545;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;VQ-S3R&#23398;&#20064;&#22120;&#65306;&#39318;&#20808;&#65292;&#20027;&#35201;&#23398;&#20064;&#22120;&#36890;&#36807;&#22810;&#38454;&#27573;&#22810;&#30721;&#26412;&#65288;MSMC&#65289;VQ-S3R&#19982;&#23545;&#27604;&#24335;S3RL&#30456;&#32467;&#21512;&#30340;MSMC-VQ-GAN&#29983;&#25104;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#28982;&#21518;&#35299;&#30721;&#22238;&#21407;&#38899;&#39057;&#65307;&#21516;&#26102;&#65292;&#21103;&#23398;&#20064;&#22120;&#36890;&#36807;VQ-VAE&#23558;MSMC&#34920;&#31034;&#36827;&#19968;&#27493;&#25277;&#35937;&#20026;&#39640;&#24230;&#32039;&#20945;&#30340;VQ&#34920;&#31034;&#12290;&#36825;&#20004;&#20010;&#29983;&#25104;&#24335;VQ-S3R&#23398;&#20064;&#22120;&#20026;TTS&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#35821;&#38899;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21512;&#25104;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#23545;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;QS-TTS&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#27979;&#35797;&#23454;&#39564;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#12290;LLMs&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20219;&#21153;&#65292;&#20294;&#22312;&#20351;&#29992;&#26102;&#23384;&#22312;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;LLMs&#30340;&#21457;&#23637;&#12289;&#24212;&#29992;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#20174;&#19994;&#32773;&#29702;&#35299;&#21644;&#24212;&#23545;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.00087</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65306;&#28508;&#21147;&#19982;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Large language models in medicine: the potentials and pitfalls. (arXiv:2309.00087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00087
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#12290;LLMs&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20219;&#21153;&#65292;&#20294;&#22312;&#20351;&#29992;&#26102;&#23384;&#22312;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;LLMs&#30340;&#21457;&#23637;&#12289;&#24212;&#29992;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#20174;&#19994;&#32773;&#29702;&#35299;&#21644;&#24212;&#23545;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21307;&#30103;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#21040;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#12290;&#38543;&#30528;&#29983;&#20135;LLMs&#30340;&#20844;&#21496;&#19982;&#21307;&#30103;&#31995;&#32479;&#20043;&#38388;&#30340;&#26426;&#26500;&#21512;&#20316;&#22686;&#21152;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#20020;&#24202;&#24212;&#29992;&#27491;&#36880;&#28176;&#25104;&#20026;&#29616;&#23454;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21307;&#30103;&#20174;&#19994;&#32773;&#20102;&#35299;LLMs&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#20197;&#21450;&#22312;&#21307;&#23398;&#20013;&#30340;&#24403;&#21069;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20351;&#29992;LLMs&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#21644;&#37197;&#22871;&#25945;&#31243;&#26088;&#22312;&#20026;&#21307;&#30103;&#20174;&#19994;&#32773;&#25552;&#20379;&#20851;&#20110;&#36825;&#20123;&#20027;&#39064;&#30340;&#27010;&#36848;&#65292;&#20197;&#24110;&#21161;&#20182;&#20204;&#29702;&#35299;LLMs&#22312;&#21307;&#23398;&#20013;&#24212;&#29992;&#30340;&#24555;&#36895;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been applied to tasks in healthcare, ranging from medical exam questions to responding to patient questions. With increasing institutional partnerships between companies producing LLMs and healthcare systems, real world clinical application is coming closer to reality. As these models gain traction, it is essential for healthcare practitioners to understand what LLMs are, their development, their current and potential applications, and the associated pitfalls when utilized in medicine. This review and accompanying tutorial aim to give an overview of these topics to aid healthcare practitioners in understanding the rapidly changing landscape of LLMs as applied to medicine.
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15906</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#20307;&#31995;&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15906
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35843;&#26597;&#20102;&#32654;&#22269;&#27861;&#24459;&#22312;&#38754;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#30740;&#35752;&#20250;&#26399;&#38388;&#21046;&#23450;&#30340;&#22810;&#31181;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#33258;&#20027;&#26435;&#12289;&#38544;&#31169;&#26435;&#12289;&#23562;&#20005;&#12289;&#22810;&#26679;&#24615;&#12289;&#24179;&#31561;&#20197;&#21450;&#36523;&#24515;&#20581;&#24247;&#31561;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#23466;&#27861;&#21644;&#27665;&#26435;&#27861;&#20284;&#20046;&#26080;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#27495;&#35270;&#24615;&#20135;&#20986;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#25105;&#20204;&#25490;&#38500;&#31532;230&#26465;&#27454;&#25552;&#20379;&#30340;&#36131;&#20219;&#20445;&#25252;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#35777;&#26126;&#35837;&#35876;&#21644;&#20135;&#21697;&#36131;&#20219;&#32034;&#36180;&#30340;&#22240;&#26524;&#20851;&#31995;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#29420;&#29305;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#33021;&#22815;&#36866;&#24212;&#26032;&#23041;&#32961;&#24182;&#20026;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#30340;&#27861;&#24459;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32763;&#35793;SQuAD 2.0&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#30340;&#26368;&#22909;&#34920;&#29616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.09862</link><description>&lt;p&gt;
&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi. (arXiv:2308.09862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32763;&#35793;SQuAD 2.0&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#30340;&#26368;&#22909;&#34920;&#29616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#39640;&#24230;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#23545;&#25968;&#25454;&#26377;&#30528;&#26080;&#27490;&#22659;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26469;&#35828;&#65292;&#26500;&#24314;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37325;&#28857;&#26159;&#20026;&#20004;&#31181;&#36825;&#26679;&#30340;&#35821;&#35328;-&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;-&#24320;&#21457;&#19968;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#21360;&#22320;&#35821;&#26159;&#20840;&#29699;&#31532;&#19977;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;3.45&#20159;&#35828;&#35805;&#32773;&#65292;&#32780;&#39532;&#25289;&#22320;&#35821;&#21017;&#26159;&#20840;&#29699;&#31532;11&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;8.32&#21315;&#19975;&#35828;&#35805;&#32773;&#65292;&#20294;&#36825;&#20004;&#31181;&#35821;&#35328;&#22312;&#26500;&#24314;&#39640;&#25928;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#36164;&#28304;&#19978;&#37117;&#38754;&#20020;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23558;SQuAD 2.0&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#20013;&#26368;&#22823;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;28,000&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26550;&#26500;&#19978;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#22312;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in deep-learning have led to the development of highly sophisticated systems with an unquenchable appetite for data. On the other hand, building good deep-learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most spoken language worldwide, with 345 million speakers, and Marathi being the 11th most spoken language globally, with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To tackle the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15484</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#65306;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#33021;&#22815;&#37319;&#29992;&#26368;&#23567;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#26469;&#35299;&#32806;TTS&#12290;&#20026;&#20102;&#35299;&#20915;&#31163;&#25955;&#34920;&#31034;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#27874;&#24418;&#22833;&#30495;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-LM-Speech&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23558;&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#20026;&#22522;&#20110;mel&#39057;&#35889;&#22270;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#38901;&#24459;&#29942;&#39048;&#30340;&#25552;&#31034;&#32534;&#30721;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#34920;&#31034;&#33021;&#21147;&#12290;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#36935;&#21040;&#32570;&#22833;&#21644;&#37325;&#22797;&#21333;&#35789;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#30001;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#23384;&#22312;&#23548;&#33268;&#34920;&#36798;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tetra-Diff-Speech&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38271;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#38901;&#24459;&#34920;&#36798;&#12290;&#25105;&#20204;&#26399;&#26395;&#35821;&#20041;&#32534;&#30721;&#30340;&#20449;&#24687;&#20869;&#23481;&#20171;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between t
&lt;/p&gt;</description></item><item><title>Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.11991</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#24515;&#29702;&#26381;&#21153;&#30340;Psy-LLM
&lt;/p&gt;
&lt;p&gt;
Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. (arXiv:2307.11991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11991
&lt;/p&gt;
&lt;p&gt;
Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24515;&#29702;&#21672;&#35810;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20840;&#29699;COVID-19&#30340;&#29190;&#21457;&#65292;&#36825;&#21152;&#24378;&#20102;&#21450;&#26102;&#21644;&#19987;&#19994;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#38656;&#27714;&#12290;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#38656;&#27714;&#30340;&#20027;&#35201;&#26381;&#21153;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Psy-LLM&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#20013;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20174;&#24515;&#29702;&#23398;&#23478;&#21644;&#24191;&#27867;&#25910;&#38598;&#30340;&#24515;&#29702;&#25991;&#31456;&#20013;&#33719;&#21462;&#30340;&#30495;&#23454;&#19990;&#30028;&#19987;&#19994;&#38382;&#31572;&#12290;Psy-LLM&#26694;&#26550;&#20316;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#21069;&#31471;&#24037;&#20855;&#65292;&#20801;&#35768;&#20182;&#20204;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#26469;&#32531;&#35299;&#24739;&#32773;&#21387;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#21327;&#21161;&#30340;&#32039;&#24613;&#26696;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#22256;&#24785;&#24230;&#31561;&#20869;&#22312;&#24230;&#37327;&#26631;&#20934;&#21644;&#22806;&#37096;&#24230;&#37327;&#26631;&#20934;&#23545;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for psychological counseling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counseling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based system leveraging Large Language Models (LLMs) for question-answering in online psychological consultation. Our framework combines pre-trained LLMs with real-world professional Q&amp;A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;GPT-2&#21644;GPT-3.5&#20026;&#20363;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#24615;&#21035;&#21270;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#20135;&#29983;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09162</link><description>&lt;p&gt;
&#25581;&#31034;&#22312;LLM&#20013;&#32844;&#19994;&#24615;&#21035;&#20559;&#35265;&#65306;&#20998;&#26512;&#21644;&#35299;&#20915;&#31038;&#20250;&#23398;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications. (arXiv:2307.09162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;GPT-2&#21644;GPT-3.5&#20026;&#20363;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#24615;&#21035;&#21270;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#20135;&#29983;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#21644;&#20559;&#35265;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#31687;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;GPT-2&#21644;GPT-3.5&#36825;&#20123;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#24433;&#21709;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;&#29616;&#26377;&#20851;&#20110;AI&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#24403;&#21069;&#30693;&#35782;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#26041;&#27861;&#21253;&#25324;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;GPT-2&#21644;GPT-3.5&#30340;&#25968;&#25454;&#65292;&#24182;&#36816;&#29992;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25216;&#26415;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#20855;&#26377;&#24615;&#21035;&#33394;&#24425;&#30340;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#12290;&#35752;&#35770;&#37096;&#20998;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#30340;&#20262;&#29702;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#31038;&#20250;&#35748;&#30693;&#30340;&#28508;&#22312;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender bias in artificial intelligence (AI) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. This research paper aims to analyze gender bias in Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent language models, to better understand its implications. Through a comprehensive literature review, the study examines existing research on gender bias in AI language models and identifies gaps in the current knowledge. The methodology involves collecting and preprocessing data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models. The discussion explores the ethical implications of gender bias and its potential consequences on social percepti
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2307.01540</link><description>&lt;p&gt;
&#22312;&#35838;&#22530;&#19978;&#23398;&#20064;&#25552;&#31034;&#20197;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01540
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#22312;&#24110;&#21161;&#31038;&#20250;&#35299;&#20915;&#32039;&#36843;&#30340;&#31038;&#20250;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#27966;&#29983;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;AI&#31995;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#28818;&#20316;&#20063;&#20135;&#29983;&#20102;&#36127;&#38754;&#24773;&#32490;&#65292;&#21363;&#20351;&#22312;&#26032;&#39062;&#30340;AI&#26041;&#27861;&#21462;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#36129;&#29486;&#20043;&#21518;&#12290;&#36896;&#25104;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20294;&#20063;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26412;&#36523;&#65292;&#26159;&#36234;&#26469;&#36234;&#22810;&#20154;&#38169;&#35823;&#22320;&#35748;&#20026;&#33258;&#24049;&#33021;&#22815;&#36731;&#26494;&#35775;&#38382;&#21644;&#22788;&#29702;&#20219;&#20309;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20219;&#20309;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#26080;&#38656;&#23545;AI&#25110;&#38382;&#39064;&#39046;&#22495;&#26377;&#20219;&#20309;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#24573;&#35270;&#20102;&#24403;&#21069;LLMs&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#24187;&#35273;&#21644;&#25512;&#29702;&#38480;&#21046;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#23545;&#20110;&#35299;&#20915;&#30001;LLMs&#29983;&#25104;&#30340;&#21487;&#33021;&#38169;&#35823;&#24314;&#35758;&#21487;&#33021;&#20135;&#29983;&#30340;&#30450;&#30446;&#36807;&#24230;&#33258;&#20449;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#36825;&#21487;&#20197;&#20943;&#23569;&#24656;&#24807;&#21644;&#20854;&#20182;&#36127;&#38754;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitude
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.16805</link><description>&lt;p&gt;
CLIPAG: &#36208;&#21521;&#26080;&#38656;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#23545;&#40784;&#26799;&#24230; (Perceptually Aligned Gradients, PAG) &#26159;&#22312;&#20581;&#22766;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#26377;&#36259;&#23646;&#24615;&#65292;&#20854;&#20013;&#23427;&#20204;&#30340;&#36755;&#20837;&#28176;&#21464;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#24182;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#12290;&#34429;&#28982;&#36825;&#19968;&#29616;&#35937;&#24341;&#36215;&#20102;&#26174;&#30528;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20294;&#20165;&#20165;&#22312;&#21333;&#27169;&#24577;&#32431;&#35270;&#35273;&#26550;&#26500;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558; PAG &#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#36825;&#26159;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#40065;&#26834;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102; PAG&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102; CLIPAG &#22312;&#20960;&#31181;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#32541;&#38598;&#25104; CLIPAG &#30340; "&#21363;&#25554;&#21363;&#29992;" &#26041;&#24335;&#26174;&#33879;&#25913;&#36827;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20854; PAG &#23646;&#24615;&#65292;CLIPAG &#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#27169;&#22411;-&#26080;&#20851;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#26367;&#25442;&#35780;&#20998;&#22120;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15245</link><description>&lt;p&gt;
C-PMI: &#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#29992;&#20110;&#23545;&#35805;&#35780;&#20272;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation. (arXiv:2306.15245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#27169;&#22411;-&#26080;&#20851;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#26367;&#25442;&#35780;&#20998;&#22120;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;chatbot&#30340;&#26080;&#21442;&#32771;&#32423;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#25429;&#25417;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#65288;C-PMI&#65289;&#26469;&#24230;&#37327;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#22522;&#20110;&#32473;&#23450;&#35780;&#20272;&#32500;&#24230;&#30340;&#23545;&#35805;&#20132;&#20114;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;FED&#23545;&#35805;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#35780;&#20272;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#36127;&#23545;&#25968;&#20284;&#28982;&#30340;&#35780;&#20998;&#22120;&#26367;&#25442;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;C-PMI&#35780;&#20998;&#22120;&#65292;&#25105;&#20204;&#22312;FED&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;Spearman&#30456;&#20851;&#24615;&#24179;&#22343;&#30456;&#23545;&#25552;&#39640;&#20102;60.5%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/renll/C-PMI&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing reference-free turn-level evaluation metrics for chatbots inadequately capture the interaction between the user and the system. Consequently, they often correlate poorly with human evaluations. To address this issue, we propose a novel model-agnostic approach that leverages Conditional Pointwise Mutual Information (C-PMI) to measure the turn-level interaction between the system and the user based on a given evaluation dimension. Experimental results on the widely used FED dialogue evaluation dataset demonstrate that our approach significantly improves the correlation with human judgment compared with existing evaluation systems. By replacing the negative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve a relative 60.5% higher Spearman correlation on average for the FED evaluation metric. Our code is publicly available at https://github.com/renll/C-PMI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;PubMed&#25688;&#35201;&#20013;&#20351;&#29992;SNOMED CT&#29256;&#26412;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#25968;&#25454;&#38598;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#21270;&#22320;&#21457;&#29616;&#21644;&#25918;&#32622;&#26032;&#27010;&#24565;&#21040;&#30693;&#35782;&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.14704</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#20016;&#23500;&#26412;&#20307;&#30693;&#35782;&#65306;&#19968;&#31181;&#29992;&#20110;&#27010;&#24565;&#21457;&#29616;&#21644;&#25918;&#32622;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement. (arXiv:2306.14704v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;PubMed&#25688;&#35201;&#20013;&#20351;&#29992;SNOMED CT&#29256;&#26412;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#25968;&#25454;&#38598;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#21270;&#22320;&#21457;&#29616;&#21644;&#25918;&#32622;&#26032;&#27010;&#24565;&#21040;&#30693;&#35782;&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#27010;&#24565;&#30340;&#25552;&#21450;&#32463;&#24120;&#20986;&#29616;&#22312;&#25991;&#26412;&#20013;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#23558;&#20854;&#25910;&#38598;&#24182;&#25918;&#32622;&#21040;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#26412;&#20307;&#21644;&#20998;&#31867;&#31995;&#32479;&#65289;&#20013;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;&#19968;&#65289;&#22823;&#37096;&#20998;&#20551;&#35774;&#26032;&#27010;&#24565;&#24050;&#32463;&#34987;&#21457;&#29616;&#65292;&#19981;&#33021;&#25903;&#25345;&#27010;&#24565;&#21457;&#29616;&#65307;&#65288;&#20108;&#65289;&#21482;&#20351;&#29992;&#27010;&#24565;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65307;&#65288;&#19977;&#65289;&#20027;&#35201;&#20851;&#27880;&#19982;&#21407;&#23376;&#27010;&#24565;&#30340;&#20998;&#31867;&#65292;&#32780;&#19981;&#26159;&#22797;&#26434;&#27010;&#24565;&#65288;&#21253;&#21547;&#36923;&#36753;&#36816;&#31639;&#31526;&#65289;&#30340;&#25918;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20351;&#29992;2014&#24180;&#21644;2017&#24180;&#30340;SNOMED CT&#29256;&#26412;&#36866;&#37197;MedMentions&#25968;&#25454;&#38598;&#65288;PubMed&#25688;&#35201;&#65289;&#65292;&#28085;&#30422;&#30142;&#30149;&#23376;&#31867;&#21035;&#21644;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#21457;&#29616;&#12289;&#36807;&#31243;&#20197;&#21450;&#21046;&#33647;/&#29983;&#29289;&#20135;&#21697;&#31867;&#21035;&#12290;&#25105;&#20204;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#27010;&#24565;&#21457;&#29616;&#21644;&#25918;&#32622;&#30340;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20351;&#29992;&#26041;&#24335;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based m
&lt;/p&gt;</description></item><item><title>Lingua Manga&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#36890;&#29992;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#26631;&#31614;&#25928;&#29575;&#65292;&#21516;&#26102;&#20419;&#36827;&#28789;&#27963;&#21644;&#24555;&#36895;&#24320;&#21457;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#21327;&#21161;&#29087;&#32451;&#30340;&#31243;&#24207;&#21592;&#21644;&#20302;&#20195;&#30721;&#29978;&#33267;&#26080;&#20195;&#30721;&#29992;&#25143;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.11702</link><description>&lt;p&gt;
Lingua Manga: &#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#36890;&#29992;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Lingua Manga: A Generic Large Language Model Centric System for Data Curation. (arXiv:2306.11702v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11702
&lt;/p&gt;
&lt;p&gt;
Lingua Manga&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#36890;&#29992;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#26631;&#31614;&#25928;&#29575;&#65292;&#21516;&#26102;&#20419;&#36827;&#28789;&#27963;&#21644;&#24555;&#36895;&#24320;&#21457;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#21327;&#21161;&#29087;&#32451;&#30340;&#31243;&#24207;&#21592;&#21644;&#20302;&#20195;&#30721;&#29978;&#33267;&#26080;&#20195;&#30721;&#29992;&#25143;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#21253;&#21547;&#35768;&#22810;&#20851;&#38190;&#20294;&#32791;&#26102;&#30340;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#24320;&#21457;&#36890;&#29992;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lingua Manga&#65292;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#19988;&#22810;&#21151;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;Lingua Manga&#36890;&#36807;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#21644;&#25216;&#26415;&#27700;&#24179;&#30340;&#29992;&#25143;&#30340;&#31034;&#20363;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21327;&#21161;&#29087;&#32451;&#30340;&#31243;&#24207;&#21592;&#21644;&#20302;&#20195;&#30721;&#29978;&#33267;&#26080;&#20195;&#30721;&#29992;&#25143;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks. However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system. To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models. Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development. Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KLDivS&#21644;JSDivS&#36825;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#21487;&#35299;&#37322;&#22320;&#35780;&#20272;MLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.07795</link><description>&lt;p&gt;
&#26500;&#24314;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#25972;&#20307;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Constructing Holistic Measures for Social Biases in Masked Language Models. (arXiv:2305.07795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KLDivS&#21644;JSDivS&#36825;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#21487;&#35299;&#37322;&#22320;&#35780;&#20272;MLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#65292;MLMs &#24456;&#21487;&#33021;&#21453;&#26144;&#29616;&#23454;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#12290;&#36807;&#21435;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#35780;&#20272;&#25351;&#26631;&#37319;&#29992;&#19981;&#21516;&#30340;&#25513;&#30721;&#31574;&#30053;&#65292;&#35774;&#35745;&#20102;MLMs &#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#12290;&#36825;&#20123;&#25351;&#26631;&#32570;&#20047;&#23545;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#21464;&#21270;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#23558;MLMs&#36755;&#20986;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#8212;&#8212;Kullback Leibler &#25955;&#24230;&#24471;&#20998;&#65288;KLDivS&#65289;&#21644;Jensen Shannon &#36317;&#31163;&#24471;&#20998;&#65288;JSDivS&#65289;&#65292;&#20197;&#35780;&#20272;MLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;StereoSet &#21644;CrowS-Pairs&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#36807;&#21435;&#25552;&#20986;&#30340;&#25351;&#26631;&#30456;&#27604;&#65292;KLDivS&#21644;JSDivS&#26356;&#21152;&#31283;&#23450;&#21644;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Models (MLMs) have been successful in many natural language processing tasks. However, real-world stereotype biases are likely to be reflected in MLMs due to their learning from large text corpora. Most of the evaluation metrics proposed in the past adopt different masking strategies, designed with the log-likelihood of MLMs. They lack holistic considerations such as variance for stereotype bias and anti-stereotype bias samples. In this paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs are considered Gaussian distributions. Two evaluation metrics, Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score (JSDivS) are proposed to evaluate social biases in MLMs The experimental results on the public datasets StereoSet and CrowS-Pairs demonstrate that KLDivS and JSDivS are more stable and interpretable compared to the metrics proposed in the past.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13301</link><description>&lt;p&gt;
&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Codex&#12289;ChatGPT&#21644;GPT-4&#65289;&#22312;AI&#31038;&#21306;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#12290;&#19968;&#20123;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20294;&#26159;&#23427;&#20204;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#65288;&#20363;&#22914;&#31616;&#21333;&#30340;&#32467;&#26500;&#25110;&#38543;&#26426;&#25277;&#26679;&#65289;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#25110;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CBR-ApSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26694;&#26550;&#65292;&#19982;GPT-3.5&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23545;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#20197;&#28789;&#27963;&#35843;&#25972;GPT-3.5&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#28041;&#21450;&#65288;1&#65289;&#36890;&#36807;&#21435;&#35821;&#20041;&#21270;&#36755;&#20837;&#38382;&#39064;&#26469;&#33258;&#36866;&#24212;&#26816;&#32034;&#26696;&#20363;&#65292;&#26681;&#25454;&#38382;&#39064;&#24847;&#22270;&#65292;&#20197;&#21450;&#65288;2&#65289;&#33258;&#36866;&#24212;&#22238;&#36864;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#25552;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#26696;&#20363;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#21435;&#35821;&#20041;&#21270;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Semantic D
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have significantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs' performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic D
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.08468</link><description>&lt;p&gt;
LEVER: &#20351;&#29992;&#25191;&#34892;&#36827;&#34892;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#23398;&#20064;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22312;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;code LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22312;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#27492;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#23558;LLM&#35299;&#30721;&#19982;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#25110;&#22522;&#20110;&#25191;&#34892;&#32467;&#26524;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26679;&#26412;&#20462;&#21098;&#21644;&#37325;&#26032;&#25490;&#24207;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#24212;&#29992;&#26469;&#35828;&#65292;&#33719;&#21462;&#27979;&#35797;&#29992;&#20363;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#21551;&#21457;&#24335;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#25429;&#25417;&#25191;&#34892;&#32467;&#26524;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#27604;&#22914;&#25968;&#25454;&#31867;&#22411;&#21644;&#20540;&#33539;&#22260;&#65292;&#36825;&#24448;&#24448;&#34920;&#26126;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEVER&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#20174;&#32780;&#25913;&#36827;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#20174;LLM&#20013;&#25277;&#26679;&#30340;&#31243;&#24207;&#26159;&#21542;&#27491;&#30830;&#12290;&#36890;&#36807;&#23558;&#39564;&#35777;&#20998;&#25968;&#19982;LLM&#29983;&#25104;&#20998;&#25968;&#30456;&#32467;&#21512;&#65292;&#23545;&#25277;&#26679;&#30340;&#31243;&#24207;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiLD&#30340;&#26694;&#26550;&#65292;&#23427;&#30001;&#22823;&#23567;&#19981;&#21516;&#30340;&#20004;&#20010;&#27169;&#22411;&#21327;&#20316;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#22823;&#22411;&#27169;&#22411;&#21017;&#22312;&#24517;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23545;&#23567;&#22411;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2302.07863</link><description>&lt;p&gt;
&#22823;&#23567;&#19981;&#21516;&#30340;Transformer&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiLD&#30340;&#26694;&#26550;&#65292;&#23427;&#30001;&#22823;&#23567;&#19981;&#21516;&#30340;&#20004;&#20010;&#27169;&#22411;&#21327;&#20316;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#22823;&#22411;&#27169;&#22411;&#21017;&#22312;&#24517;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23545;&#23567;&#22411;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#38271;&#26102;&#38388;&#30340;&#25512;&#29702;&#24310;&#36831;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#24182;&#19988;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#26102;&#24212;&#29992;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#27169;&#22411;&#38656;&#35201;&#36845;&#20195;&#22320;&#36816;&#34892;&#25165;&#33021;&#36880;&#20010;&#29983;&#25104;&#26631;&#35760;&#65292;&#22240;&#27492;&#25512;&#29702;&#24310;&#36831;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Big Little Decoder&#65288;BiLD&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#24212;&#29992;&#30340;&#25512;&#29702;&#25928;&#29575;&#21644;&#24310;&#36831;&#12290;BiLD&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#21327;&#20316;&#22320;&#29983;&#25104;&#25991;&#26412;&#12290;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#36816;&#34892;&#20197;&#20302;&#24310;&#36831;&#29983;&#25104;&#25991;&#26412;&#65292;&#22823;&#22411;&#27169;&#22411;&#21482;&#22312;&#38656;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#35843;&#25972;&#23567;&#22411;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25552;&#39640;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28176;&#36827;&#33976;&#39311;&#26426;&#21046;&#65292;&#20351;&#23567;&#22411;&#27169;&#22411;&#36880;&#28176;&#22320;&#20174;&#22823;&#22411;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;BiLD&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#19982;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Zipf's Law&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#20013;&#30340;&#35789;&#27719;&#20998;&#31867;&#20026;&#24120;&#35265;&#35789;&#21644;&#31232;&#32570;&#35789;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#23545;&#21477;&#23376;&#36827;&#34892;&#22788;&#29702;&#65292;&#36827;&#32780;&#35299;&#20915;&#23454;&#20307;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#20154;&#35774;&#35745;&#30340;&#35268;&#21017;&#23545;&#29983;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#31232;&#32570;&#23454;&#20307;&#36827;&#34892;&#26631;&#35760;&#65292;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.12636</link><description>&lt;p&gt;
&#22522;&#20110;Zipf's Law&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#35299;&#20915;&#23454;&#20307;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Zipf's Law-based Text Generation Approach for Addressing Imbalance in Entity Extraction. (arXiv:2205.12636v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Zipf's Law&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#20013;&#30340;&#35789;&#27719;&#20998;&#31867;&#20026;&#24120;&#35265;&#35789;&#21644;&#31232;&#32570;&#35789;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#23545;&#21477;&#23376;&#36827;&#34892;&#22788;&#29702;&#65292;&#36827;&#32780;&#35299;&#20915;&#23454;&#20307;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#20154;&#35774;&#35745;&#30340;&#35268;&#21017;&#23545;&#29983;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#31232;&#32570;&#23454;&#20307;&#36827;&#34892;&#26631;&#35760;&#65292;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#25277;&#21462;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26234;&#33021;&#21457;&#23637;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#21463;&#21040;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20449;&#24687;&#35266;&#23519;&#35813;&#38382;&#39064;&#65292;&#35748;&#35782;&#21040;&#23454;&#20307;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20849;&#24615;&#21644;&#31232;&#32570;&#24615;&#65292;&#36825;&#21487;&#20197;&#22312;&#35789;&#27719;&#30340;&#21487;&#37327;&#21270;&#20998;&#24067;&#20013;&#21453;&#26144;&#20986;&#26469;&#12290;Zipf's Law&#25104;&#20026;&#19968;&#20010;&#21512;&#36866;&#30340;&#37319;&#29992;&#26041;&#24335;&#65292;&#24182;&#19988;&#20026;&#20102;&#23558;&#35789;&#27719;&#36716;&#21464;&#20026;&#23454;&#20307;&#65292;&#23558;&#25991;&#26723;&#20013;&#30340;&#35789;&#27719;&#20998;&#31867;&#20026;&#24120;&#35265;&#35789;&#21644;&#31232;&#32570;&#35789;&#12290;&#38543;&#21518;&#65292;&#21477;&#23376;&#34987;&#20998;&#31867;&#20026;&#24120;&#35265;&#21477;&#21644;&#31232;&#32570;&#21477;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#20998;&#31867;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#31232;&#32570;&#23454;&#20307;&#28982;&#21518;&#20351;&#29992;&#20154;&#35774;&#35745;&#30340;&#35268;&#21017;&#36827;&#34892;&#26631;&#35760;&#65292;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20174;&#25216;&#26415;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#30340;&#26696;&#20363;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity extraction is critical in the intelligent advancement across diverse domains. Nevertheless, a challenge to its effectiveness arises from the data imbalance. This paper proposes a novel approach by viewing the issue through the quantitative information, recognizing that entities exhibit certain levels of commonality while others are scarce, which can be reflected in the quantifiable distribution of words. The Zipf's Law emerges as a well-suited adoption, and to transition from words to entities, words within the documents are classified as common and rare ones. Subsequently, sentences are classified into common and rare ones, and are further processed by text generation models accordingly. Rare entities within the generated sentences are then labeled using human-designed rules, serving as a supplement to the raw dataset, thereby mitigating the imbalance problem. The study presents a case of extracting entities from technical documents, and experimental results from two datasets p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#37325;&#20998;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#35835;&#25991;&#26412;&#20013;&#38544;&#34255;&#30340;&#22810;&#37325;&#20998;&#24418;&#23646;&#24615;&#65292;&#24182;&#32467;&#21512;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#23454;&#29616;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2111.13861</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#22810;&#37325;&#20998;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
A New Multifractal-based Deep Learning Model for Text Mining. (arXiv:2111.13861v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#37325;&#20998;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#35835;&#25991;&#26412;&#20013;&#38544;&#34255;&#30340;&#22810;&#37325;&#20998;&#24418;&#23646;&#24615;&#65292;&#24182;&#32467;&#21512;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#23454;&#29616;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#19981;&#30830;&#23450;&#24615;&#30340;&#19990;&#30028;&#20013;&#65292;&#23384;&#22312;&#30340;&#32441;&#29702;&#32534;&#32455;&#20986;&#22797;&#26434;&#30340;&#27169;&#24335;&#65292;&#22810;&#37325;&#20998;&#24418;&#25104;&#20026;&#27934;&#23519;&#21147;&#30340;&#26631;&#24535;&#65292;&#29031;&#20142;&#23427;&#20204;&#12290;&#24403;&#25105;&#20204;&#28145;&#20837;&#25506;&#32034;&#26500;&#25104;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#21644;&#26234;&#33021;&#26381;&#21153;&#30340;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#26102;&#65292;&#25105;&#20204;&#24847;&#35782;&#21040;&#22312;&#25991;&#26412;&#30340;&#38754;&#32433;&#21518;&#38754;&#38544;&#34255;&#30528;&#20154;&#31867;&#24605;&#24819;&#21644;&#35748;&#30693;&#30340;&#34920;&#29616;&#65292;&#19982;&#22797;&#26434;&#24615;&#32039;&#23494;&#30456;&#20114;&#20132;&#32455;&#12290;&#22312;&#23558;&#25991;&#26412;&#35270;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25581;&#31034;&#20854;&#20013;&#38544;&#34255;&#30340;&#23453;&#34255;&#65292;&#20511;&#21161;&#25552;&#20986;&#30340;&#22810;&#37325;&#20998;&#24418;&#26041;&#27861;&#35299;&#35835;&#23884;&#20837;&#22312;&#25991;&#26412;&#26223;&#35266;&#20013;&#30340;&#22810;&#37325;&#20998;&#24418;&#23646;&#24615;&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#23381;&#32946;&#20986;&#25105;&#20204;&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36824;&#21033;&#29992;&#20102;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;&#21147;&#37327;&#65292;&#22312;&#20854;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#23454;&#29616;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this world full of uncertainty, where the fabric of existence weaves patterns of complexity, multifractal emerges as beacons of insight, illuminating them. As we delve into the realm of text mining that underpins various natural language processing applications and powers a range of intelligent services, we recognize that behind the veil of text lies a manifestation of human thought and cognition, intricately intertwined with the complexities. Building upon the foundation of perceiving text as a complex system, this study embarks on a journey to unravel the hidden treasures within, armed with the proposed multifractal method that deciphers the multifractal attributes embedded within the text landscape. This endeavor culminates in the birth of our novel model, which also harnesses the power of the proposed activation function to facilitate nonlinear information transmission within its neural network architecture. The success on experiments anchored in real-world technical reports cov
&lt;/p&gt;</description></item></channel></rss>