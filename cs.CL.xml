<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#19982;&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;Librispeech&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08551</link><description>&lt;p&gt;
&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#20013;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22686;&#24378;&#26500;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmenting conformers with structured state space models for online speech recognition. (arXiv:2309.08551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#19982;&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;Librispeech&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#26159;&#19968;&#31181;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ASR&#31995;&#32479;&#24212;&#29992;&#22330;&#26223;&#65292;&#20854;&#20013;&#27169;&#22411;&#21482;&#33021;&#35775;&#38382;&#24038;&#20391;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#22686;&#24378;&#22312;&#32447;ASR&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;S4&#26159;&#19968;&#31867;&#25552;&#20379;&#20102;&#35775;&#38382;&#20219;&#24847;&#38271;&#24038;&#20391;&#19978;&#19979;&#25991;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#24335;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;S4&#27169;&#22411;&#30340;&#21508;&#20010;&#21464;&#31181;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#20854;&#19982;&#21367;&#31215;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#26377;&#25928;&#30340;&#35774;&#35745;&#26159;&#20351;&#29992;&#20855;&#26377;&#23454;&#20540;&#24490;&#29615;&#26435;&#37325;&#30340;&#23567;&#22411;S4&#27169;&#22411;&#19982;&#26412;&#22320;&#21367;&#31215;&#30456;&#21472;&#21152;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#20114;&#34917;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#26469;&#33258;Librispeech&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;4.01%/8.53%&#30340;WER&#65292;&#20248;&#20110;&#32463;&#36807;&#35814;&#32454;&#35843;&#20248;&#30340;Conformers&#12290;
&lt;/p&gt;
&lt;p&gt;
Online speech recognition, where the model only accesses context to the left, is an important and challenging use case for ASR systems. In this work, we investigate augmenting neural encoders for online ASR by incorporating structured state-space sequence models (S4), which are a family of models that provide a parameter-efficient way of accessing arbitrarily long left context. We perform systematic ablation studies to compare variants of S4 models and propose two novel approaches that combine them with convolutions. We find that the most effective design is to stack a small S4 using real-valued recurrent weights with a local convolution, allowing them to work complementarily. Our best model achieves WERs of 4.01%/8.53% on test sets from Librispeech, outperforming Conformers with extensively tuned convolution.
&lt;/p&gt;</description></item><item><title>PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2309.08140</link><description>&lt;p&gt;
PromptTTS++&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#25552;&#31034;&#24335;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08140
&lt;/p&gt;
&lt;p&gt;
PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PromptTTS++&#65292;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#20026;&#20102;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;TTS&#26694;&#26550;&#20013;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#35813;&#25552;&#31034;&#25551;&#36848;&#20102;&#35821;&#38899;&#29305;&#24449;&#65288;&#22914;&#20013;&#24615;&#12289;&#24180;&#36731;&#12289;&#32769;&#24180;&#21644;&#27785;&#38391;&#65289;&#65292;&#26088;&#22312;&#19982;&#35828;&#35805;&#39118;&#26684;&#22823;&#33268;&#29420;&#31435;&#12290;&#30001;&#20110;&#30446;&#21069;&#27809;&#26377;&#21253;&#21547;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LibriTTS-R&#35821;&#26009;&#24211;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22768;&#23398;&#27169;&#22411;&#19982;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#26469;&#24314;&#27169;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679;&#21270;&#35828;&#35805;&#32773;&#22240;&#32032;&#12290;&#19982;&#20043;&#21069;&#20165;&#20381;&#36182;&#26679;&#24335;&#25552;&#31034;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26679;&#24335;&#25552;&#31034;&#20165;&#25551;&#36848;&#20102;&#35828;&#35805;&#32773;&#20010;&#24615;&#21270;&#30340;&#26377;&#38480;&#26041;&#38754;&#65292;&#22914;&#38899;&#35843;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#33021;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39069;&#22806;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21040;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic f
&lt;/p&gt;</description></item><item><title>CoLLD&#26159;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#22797;&#21046;&#22823;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.07707</link><description>&lt;p&gt;
CoLLD: &#23545;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07707
&lt;/p&gt;
&lt;p&gt;
CoLLD&#26159;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#22797;&#21046;&#22823;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#30001;&#20110;&#24320;&#21457;&#36825;&#20123;&#22823;&#27169;&#22411;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#20026;&#26032;&#20219;&#21153;&#26500;&#24314;&#26032;&#32534;&#30721;&#22120;&#24182;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#24212;&#29992;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38024;&#23545;&#36739;&#23567;&#27169;&#22411;&#21644;&#19981;&#22826;&#23454;&#38469;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#23618;&#19982;&#23618;&#33976;&#39311;&#65288;CoLLD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25513;&#34109;&#39044;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#22797;&#21046;&#22823;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;CoLLD&#22312;&#22810;&#35821;&#31181;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#21644;&#35782;&#21035;&#22522;&#20934;&#19978;&#32988;&#36807;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#23567;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale self-supervised pre-trained speech encoders outperform conventional approaches in speech recognition and translation tasks. Due to the high cost of developing these large models, building new encoders for new tasks and deploying them to on-device applications are infeasible. Prior studies propose model compression methods to address this issue, but those works focus on smaller models and less realistic tasks. Thus, we propose Contrastive Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to compress pre-trained speech encoders by leveraging masked prediction and contrastive learning to train student models to copy the behavior of a large teacher model. CoLLD outperforms prior methods and closes the gap between small and large models on multilingual speech-to-text translation and recognition benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#30340;AdapT&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#26469;&#35299;&#20915;&#38590;&#20197;&#39044;&#27979;&#30340;&#20195;&#30721;&#26631;&#35760;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Dynamic Temperature Sampling. (arXiv:2309.02772v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#30340;AdapT&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#26469;&#35299;&#20915;&#38590;&#20197;&#39044;&#27979;&#30340;&#20195;&#30721;&#26631;&#35760;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#26159;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35774;&#35745;&#30340;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30001;&#20110;&#36825;&#20010;&#30095;&#24573;&#65292;&#22914;&#20309;&#35774;&#35745;&#26356;&#22909;&#30340;&#20195;&#30721;&#29983;&#25104;&#35299;&#30721;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#26631;&#35760;&#20002;&#22833;&#20998;&#24067;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#30721;&#26631;&#35760;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#38590;&#20197;&#39044;&#27979;&#30340;&#25361;&#25112;&#24615;&#26631;&#35760;&#21644;&#26131;&#20110;&#25512;&#26029;&#30340;&#33258;&#20449;&#26631;&#35760;&#12290;&#20854;&#20013;&#65292;&#25361;&#25112;&#24615;&#26631;&#35760;&#20027;&#35201;&#20986;&#29616;&#22312;&#20195;&#30721;&#22359;&#30340;&#24320;&#22836;&#12290;&#21463;&#21040;&#19978;&#36848;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65306;&#33258;&#36866;&#24212;&#28201;&#24230;&#65288;AdapT&#65289;&#37319;&#26679;&#65292;&#23427;&#22312;&#35299;&#30721;&#19981;&#21516;&#30340;&#26631;&#35760;&#26102;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#12290;&#25105;&#20204;&#22312;&#37319;&#26679;&#25361;&#25112;&#24615;&#26631;&#35760;&#26102;&#24212;&#29992;&#36739;&#22823;&#30340;&#28201;&#24230;&#20540;&#12290;&#21516;&#26102;&#65292;&#22312;&#37319;&#26679;&#33258;&#20449;&#26631;&#35760;&#26102;&#24212;&#29992;&#36739;&#23567;&#30340;&#28201;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have shown impressive results in code generation. However, existing decoding strategies are designed for Natural Language (NL) generation, overlooking the differences between NL and programming languages (PL). Due to this oversight, a better decoding strategy for code generation remains an open question. In this paper, we conduct the first systematic study to explore a decoding strategy specialized in code generation. With an analysis of loss distributions of code tokens, we find that code tokens can be divided into two categories: challenging tokens that are difficult to predict and confident tokens that can be easily inferred. Among them, the challenging tokens mainly appear at the beginning of a code block. Inspired by the above findings, we propose a simple yet effective method: Adaptive Temperature (AdapT) sampling, which dynamically adjusts the temperature coefficient when decoding different tokens. We apply a larger temperature when samplin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20013;&#31934;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLaMA&#30340;&#20013;&#25991;&#21307;&#23398;LLM&#65292;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#20197;&#21450;&#24341;&#20837;&#19968;&#20010;&#20013;&#25991;&#22810;&#36718;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;CMtMedQA&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35805;&#21644;&#20027;&#21160;&#35810;&#38382;&#21457;&#36215;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03549</link><description>&lt;p&gt;
Zhongjing: &#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#21644;&#30495;&#23454;&#30340;&#22810;&#36718;&#23545;&#35805;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#21307;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20013;&#31934;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLaMA&#30340;&#20013;&#25991;&#21307;&#23398;LLM&#65292;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#20197;&#21450;&#24341;&#20837;&#19968;&#20010;&#20013;&#25991;&#22810;&#36718;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;CMtMedQA&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35805;&#21644;&#20027;&#21160;&#35810;&#38382;&#21457;&#36215;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#22312;&#29702;&#35299;&#21644;&#22238;&#24212;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#20013;&#21307;&#23398;&#65289;&#20013;&#65292;&#23427;&#20204;&#22312;&#24120;&#35268;&#20351;&#29992;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#12290;&#30446;&#21069;&#23558;&#20013;&#21307;&#32435;&#20837;LLM&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20351;&#29992;&#21333;&#36718;&#21644;&#31934;&#31616;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#21307;&#29983;&#19968;&#26679;&#30340;&#20027;&#21160;&#35810;&#38382;&#21644;&#22810;&#36718;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#33021;&#22987;&#32456;&#19982;&#19987;&#23478;&#30340;&#23433;&#20840;&#21644;&#19987;&#19994;&#24615;&#23545;&#40784;&#22238;&#22797;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20013;&#21307;&#22522;&#20110;LLaMA&#30340;&#20013;&#25991;&#21307;&#23398;LLM&#8212;&#8212;&#20013;&#31934;&#65292;&#23427;&#23454;&#29616;&#20102;&#20174;&#39044;&#35757;&#32451;&#21040;&#24378;&#21270;&#23398;&#20064;&#30340;&#25972;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;70,000&#20010;&#30495;&#23454;&#21307;&#24739;&#23545;&#35805;&#30340;&#20013;&#25991;&#22810;&#36718;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;CMtMedQA&#65292;&#23427;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35805;&#21644;&#20027;&#21160;&#35810;&#38382;&#21457;&#36215;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;RoBERTa&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.03952</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#20154;&#26684;&#35782;&#21035;&#22120;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03952
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;RoBERTa&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#26684;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#20010;&#20154;&#22240;&#32032;&#65292;&#34987;&#32435;&#20837;&#20102;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#20135;&#21697;&#25512;&#33616;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35782;&#21035;&#20219;&#21153;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#25991;&#26412;&#35782;&#21035;&#20010;&#20307;&#30340;&#20154;&#26684;&#12290;&#32771;&#34385;&#21040;ChatGPT&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#20154;&#26684;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25506;&#32034;ChatGPT&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#25105;&#20204;&#35774;&#35745;&#30340;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25351;&#23548;ChatGPT&#22312;&#25351;&#23450;&#23618;&#27425;&#20998;&#26512;&#32473;&#23450;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#23558;ChatGPT&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#24494;&#35843;&#30340;RoBERTa&#20197;&#21450;&#30456;&#24212;&#30340;&#26368;&#26032;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, personality has been regarded as a valuable personal factor being incorporated into numerous tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition task, which aims to identify an individual's personality based on given text. Considering that ChatGPT has recently exhibited remarkable abilities on various natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition task for generating effective personality data. Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level. We compare the performance of ChatGPT on two representative real-world datasets with traditional neural network, fine-tuned RoBERTa, and corresponding state-of-the-art task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.04047</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#35270;&#21548;&#34701;&#21512;&#23548;&#33322;&#30340;&#20027;&#21160;&#31232;&#30095;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#19968;&#20010;&#21548;&#35273;&#30446;&#26631;&#65292;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#33258;&#20027;&#26435;&#30340;&#23454;&#20307;&#24517;&#39035;&#19981;&#20165;&#35201;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;, &#32780;&#19988;&#36824;&#35201;&#26377;&#33021;&#21147;&#22312;&#19981;&#29306;&#29298;&#33258;&#20027;&#24615;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#27714;&#20154;&#31867;/&#31070;&#35861;&#30340;&#24110;&#21161;&#65292;&#20363;&#22914;&#65292;&#24403;&#19981;&#30830;&#23450;&#23548;&#33322;&#21040;&#21738;&#37324;&#23547;&#25214;&#22024;&#26434;&#25110;&#38388;&#27463;&#24615;&#21548;&#35273;&#30446;&#26631;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAVEN-&#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#22312;CAVEN&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(RL)&#35774;&#32622;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20174;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#21363;&#65306;(i)&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#65292;&#25110;(ii)&#21521;&#31070;&#35861;&#25552;&#20986;&#38382;&#39064;&#24182;&#25509;&#25910;&#30701;&#25110;&#35814;&#32454;&#30340;&#22238;&#31572;&#65292;&#25110;(iii)&#25552;&#38382;&#26222;&#36941;&#38382;&#39064;(&#24403;&#19981;&#30830;&#23450;&#35813;&#38382;&#20160;&#20040;&#26102;)&#24182;&#33719;&#24471;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#29305;&#36136;&#29702;&#35770;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;ChatGPT&#22987;&#32456;&#34920;&#29616;&#20986;ENFJ&#22411;&#20154;&#26684;&#65292;&#26080;&#35770;&#25351;&#20196;&#25110;&#24773;&#22659;&#22914;&#20309;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#30340;&#20010;&#24615;&#21270;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#26356;&#22909;&#30340;&#27807;&#36890;&#21644;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.19926</link><description>&lt;p&gt;
ChatGPT&#26159;ENFJ&#65292;Bard&#26159;ISTJ&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models. (arXiv:2305.19926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#29305;&#36136;&#29702;&#35770;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;ChatGPT&#22987;&#32456;&#34920;&#29616;&#20986;ENFJ&#22411;&#20154;&#26684;&#65292;&#26080;&#35770;&#25351;&#20196;&#25110;&#24773;&#22659;&#22914;&#20309;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#30340;&#20010;&#24615;&#21270;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#26356;&#22909;&#30340;&#27807;&#36890;&#21644;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22823;&#22823;&#37325;&#22609;&#20102;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#19981;&#20165;&#20851;&#27880;LLMs&#30340;&#24615;&#33021;&#65292;&#36824;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#25506;&#32034;&#23427;&#20204;&#30340;&#29305;&#28857;&#65292;&#35748;&#35782;&#21040;&#20102;&#29702;&#35299;&#23427;&#20204;&#34892;&#20026;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#23398;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#29305;&#36136;&#29702;&#35770;&#30740;&#31350;LLMs&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#20851;&#27880;&#35780;&#20272;ChatGPT&#25152;&#23637;&#31034;&#30340;&#20154;&#26684;&#31867;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#28041;&#21450;&#19971;&#31181;&#38468;&#21152;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#24433;&#21709;&#65292;&#20197;&#21450;&#20845;&#31181;&#20854;&#20182;LLMs&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35843;&#26597;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#23637;&#31034;&#23545;&#25351;&#20196;&#25110;&#24773;&#22659;&#32447;&#32034;&#30340;&#20154;&#26684;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#25351;&#20196;&#25110;&#24773;&#22659;&#22914;&#20309;&#65292;ChatGPT&#22987;&#32456;&#20445;&#25345;&#20854;ENFJ&#20154;&#26684;&#12290;&#36890;&#36807;&#25581;&#31034;LLMs&#30340;&#20010;&#24615;&#21270;&#65292;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#26356;&#22909;&#30340;&#27807;&#36890;&#21644;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction. We not only focus on the performance of LLMs, but also explore their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics. Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework. We first focus on evaluating the consistency of personality types exhibited by ChatGPT. Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of six other LLMs. Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues. The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts. By shedding light on the personalization of LLMs, we anticipate that our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18365</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#21040;&#24213;&#26377;&#24590;&#26679;&#30340;&#24212;&#29992;&#65311;&#20843;&#20010;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;LLM &#26159;&#21542;&#26377;&#33021;&#21147;&#25512;&#21160;&#21270;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#21547; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21517;&#31216;&#39044;&#27979;&#12289;&#23646;&#24615;&#39044;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#12289;&#21453;&#24212;&#39044;&#27979;&#12289;&#21453;&#21512;&#25104;&#65288;&#20174;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#65289;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#12289;&#20998;&#23376;&#23383;&#24149;&#21644;&#35797;&#21058;&#36873;&#25321;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; BBBP&#12289;Tox21&#12289;PubChem&#12289;USPTO &#21644; ChEBI&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#31034;&#20363;&#20013;&#65292;&#23545;&#19977;&#31181; GPT &#27169;&#22411;&#65288;GPT-4&#12289;GPT-3.5 &#21644; DaVinci-003&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03236</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26497;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#32508;&#36848;&#20102;OOD&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;OOD&#26816;&#27979;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#30456;&#20851;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26368;&#36817;&#30340;&#31639;&#27861;&#20998;&#25104;&#19977;&#31867;&#65306;&#65288;1&#65289;&#21487;&#29992;OOD&#25968;&#25454;&#65292;&#65288;2&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26631;&#31614;&#21487;&#29992;&#65292;&#65288;3&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+ID&#26631;&#31614;&#19981;&#21487;&#29992;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#24635;&#32467;&#29616;&#26377;&#24037;&#20316;&#24182;&#25552;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2303.14582</link><description>&lt;p&gt;
&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#35782;&#21035;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#30456;&#20851;&#28304;&#20219;&#21153;&#26469;&#35757;&#32451;&#20302;&#36164;&#28304;&#30446;&#26631;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#25152;&#26377;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#31616;&#21333;&#32452;&#21512;&#24182;&#19981;&#24635;&#26159;&#33021;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#20026;&#20250;&#23384;&#22312;&#36127;&#36801;&#31227;&#12290;&#22240;&#27492;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35782;&#21035;&#21738;&#20123;&#28304;&#20219;&#21153;&#30340;&#23376;&#38598;&#20250;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#30410;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23376;&#38598;&#30340;&#25968;&#37327;&#38543;&#30528;&#28304;&#20219;&#21153;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#20195;&#29702;&#24314;&#27169;&#20013;&#65292;&#25105;&#20204;&#23545;&#28304;&#20219;&#21153;&#36827;&#34892;&#37319;&#26679;&#65288;&#38543;&#26426;&#65289;&#65292;&#24182;&#39044;&#20808;&#35745;&#31639;&#23427;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#34920;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36924;&#36817;&#39044;&#20808;&#35745;&#31639;&#30340;&#34920;&#29616;&#65292;&#35813;&#27169;&#22411;&#20063;&#21487;&#29992;&#20110;&#39044;&#27979;&#26410;&#37319;&#26679;&#30340;&#23376;&#38598;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#31034;&#20363;&#21644;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item></channel></rss>