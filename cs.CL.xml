<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.17846</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#20154;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20016;&#23500;&#20102;&#23494;&#38598;&#20960;&#20309;&#22320;&#22270;&#12290;&#34429;&#28982;&#36825;&#20123;&#22320;&#22270;&#20801;&#35768;&#22312;&#26597;&#35810;&#26576;&#31181;&#35821;&#35328;&#27010;&#24565;&#26102;&#39044;&#27979;&#36880;&#28857;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20294;&#22823;&#35268;&#27169;&#29615;&#22659;&#21644;&#36229;&#20986;&#23545;&#35937;&#32423;&#21035;&#30340;&#25277;&#35937;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOV-SG&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;3D&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#27573;&#32423;&#22320;&#22270;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#30001;&#22320;&#26495;&#12289;&#25151;&#38388;&#21644;&#23545;&#35937;&#27010;&#24565;&#32452;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#23618;&#27425;&#32467;&#26500;&#65292;&#27599;&#20010;&#37117;&#21253;&#21547;&#24320;&#25918;&#24615;&#35789;&#27719;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#22810;&#23618;&#24314;&#31569;&#65292;&#24182;&#19988;&#20801;&#35768;&#26426;&#22120;&#20154;&#20351;&#29992;&#36328;&#23618;Voronoi&#22270;&#31359;&#36234;&#36825;&#20123;&#24314;&#31569;&#12290;HOV-SG&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2403.15447</link><description>&lt;p&gt;
&#35299;&#30721;&#21387;&#32553;&#30340;&#20449;&#20219;&#65306;&#23457;&#35270;&#22312;&#21387;&#32553;&#19979;&#39640;&#25928;LLMs&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15447
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#30340;&#39318;&#36873;&#31574;&#30053;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#22312;&#20445;&#30041;&#33391;&#24615;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#21387;&#32553;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20116;&#31181;&#26368;&#20808;&#36827;&#21387;&#32553;&#25216;&#26415;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;LLMs&#30340;&#21487;&#20449;&#24230;&#32500;&#24230;&#36827;&#34892;&#20102;&#39318;&#27425;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#21387;&#32553;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#37327;&#21270;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;&#20363;&#22914;&#65292;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20445;&#30041;&#20102;&#20854;&#21407;&#22987;&#23545;&#24212;&#29289;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#27169;&#22411;&#21098;&#26525;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#20449;&#24230;&#65292;&#21363;&#20351;&#22312;50%&#30340;&#31232;&#30095;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;M$^3$AV&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.14168</link><description>&lt;p&gt;
M$^3$AV&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#22810;&#29992;&#36884;&#30340;&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;M$^3$AV&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14168v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21457;&#24067;&#24320;&#28304;&#23398;&#26415;&#35270;&#39057;&#24405;&#20687;&#26159;&#22312;&#32447;&#20998;&#20139;&#30693;&#35782;&#30340;&#19968;&#31181;&#26032;&#20852;&#21644;&#26222;&#36941;&#26041;&#27861;&#12290;&#36825;&#20123;&#35270;&#39057;&#21253;&#21547;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#28436;&#35762;&#32773;&#30340;&#35821;&#38899;&#12289;&#38754;&#37096;&#21644;&#36523;&#20307;&#21160;&#20316;&#65292;&#20197;&#21450;&#24187;&#28783;&#29255;&#20013;&#30340;&#25991;&#26412;&#21644;&#22270;&#29255;&#65292;&#29978;&#33267;&#21487;&#33021;&#21253;&#25324;&#35770;&#25991;&#20869;&#23481;&#12290;&#23613;&#31649;&#24050;&#26500;&#24314;&#21644;&#21457;&#24067;&#20102;&#22810;&#20010;&#23398;&#26415;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20294;&#24456;&#23569;&#26377;&#25968;&#25454;&#38598;&#25903;&#25345;&#22810;&#27169;&#24577;&#20869;&#23481;&#35782;&#21035;&#21644;&#29702;&#35299;&#20219;&#21153;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20307;&#35009;&#21644;&#22810;&#29992;&#36884;&#30340;&#38899;&#35270;&#39057;&#23398;&#26415;&#35762;&#24231;&#25968;&#25454;&#38598;(M$^3$AV)&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#26469;&#28304;&#30340;&#36817;367&#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#28085;&#30422;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#23398;&#20197;&#21450;&#21307;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#20027;&#39064;&#12290;&#36890;&#36807;&#23545;&#35328;&#35821;&#21644;&#20070;&#38754;&#25991;&#23383;&#65288;&#23588;&#20854;&#26159;&#39640;&#20215;&#20540;&#21517;&#31216;&#23454;&#20307;&#65289;&#30340;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#22810;&#31181;&#38899;&#35270;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14168v1 Announce Type: new  Abstract: Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recogn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.06833</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#23558;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#21527;&#65311;&#25105;&#20204;&#20855;&#20307;&#25351;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35843;&#33410;&#25351;&#20196;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25171;&#24320;&#20102;&#26080;&#25968;&#26032;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#20854;&#20182;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24050;&#24314;&#31435;&#20026;&#35268;&#33539;&#30340;&#22522;&#26412;&#23433;&#20840;&#29305;&#24615;&#65292;&#27604;&#22914;&#25351;&#20196;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#23548;&#33268;&#23427;&#20204;&#21457;&#29983;&#25925;&#38556;&#25110;&#26131;&#21463;&#31532;&#19977;&#26041;&#25805;&#25511;&#21644;&#24178;&#25200;&#65288;&#20363;&#22914;&#36890;&#36807;&#38388;&#25509;&#25552;&#31034;/&#21629;&#20196;&#27880;&#20837;&#65289;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29978;&#33267;&#27809;&#26377;&#30830;&#20999;&#23450;&#20041;&#36825;&#31181;&#20998;&#31163;&#31350;&#31455;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27979;&#35797;&#20854;&#36829;&#21453;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27169;&#22411;&#30340;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SEP&#65288;&#24212;&#35813;&#25191;&#34892;&#36824;&#26159;&#22788;&#29702;&#65311;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01748</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#20026;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Decode Neural signal as Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#21160;&#24577;&#35299;&#30721;&#35821;&#35328;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#26041;&#21521;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#30456;&#23545;&#20110;&#38656;&#35201;&#30005;&#26497;&#26893;&#20837;&#25163;&#26415;&#30340;&#20405;&#20837;&#24615;&#20449;&#21495;&#65292;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#20449;&#21495;&#65288;&#22914;EEG&#12289;MEG&#65289;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#21644;&#26222;&#36866;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#65306;1&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#20808;&#21069;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;MEG&#20449;&#21495;&#36136;&#37327;&#26356;&#22909;&#30340;&#38382;&#39064;&#65307;2&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22312;&#29983;&#25104;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65307;3&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#22522;&#20110;&#8220;BART&#8221;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#22312;&#35821;&#38899;&#35299;&#30721;&#24418;&#24335;&#20013;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#20013;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ArXiv&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;ArXivQA&#36890;&#36807;&#31185;&#23398;&#22270;&#29983;&#25104;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00231</link><description>&lt;p&gt;
Multimodal ArXiv: &#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ArXiv&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;ArXivQA&#36890;&#36807;&#31185;&#23398;&#22270;&#29983;&#25104;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65292;&#20197;GPT-4V&#20026;&#20363;&#65292;&#22312;&#28041;&#21450;&#33258;&#28982;&#22330;&#26223;&#20013;&#30340;&#20855;&#20307;&#22270;&#20687;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#39046;&#22495;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#65292;&#23427;&#20204;&#22312;&#35299;&#37322;&#25277;&#35937;&#22270;&#24418;&#65288;&#20363;&#22914;&#20960;&#20309;&#24418;&#29366;&#21644;&#31185;&#23398;&#22270;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Multimodal ArXiv&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#20197;&#22686;&#24378;LVLMs&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;ArXivCap&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#28085;&#30422;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#30340;572K&#20221;ArXiv&#35770;&#25991;&#30340;6.4M&#24352;&#22270;&#20687;&#21644;3.9M&#20010;&#26631;&#39064;&#30340;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#12290;&#20511;&#37492;ArXivCap&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ArXivQA&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;GPT-4V&#29983;&#25104;&#30340;&#22522;&#20110;&#31185;&#23398;&#22270;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;ArXivQA&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;LVLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#27169;&#24577;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;10.4%&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;ArXivCap&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#20174;&#35270;&#35273;&#21040;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00226</link><description>&lt;p&gt;
&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#35789;&#27719;&#30340;&#26102;&#38388;&#35821;&#20041;&#21464;&#21270;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24517;&#39035;&#23545;&#26102;&#38388;&#25935;&#24863;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#65288;SCD&#65289;&#20219;&#21153;&#32771;&#34385;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;$C_1$&#21644;$C_2$&#20043;&#38388;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#35789;$w$&#26159;&#21542;&#25913;&#21464;&#20102;&#21547;&#20041;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#26377;&#30340;Word-in-Context&#65288;WiC&#65289;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20004;&#38454;&#27573;SCD&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20110;&#30446;&#26631;&#35789;$w$&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#24863;&#30693;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#34920;&#31034;&#32473;&#23450;&#35821;&#26009;&#24211;&#20013;&#25152;&#36873;&#21477;&#23376;&#20013;$w$&#30340;&#21547;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#24863;&#30693;&#24863;&#30693;&#36317;&#31163;&#24230;&#37327;&#65292;&#27604;&#36739;&#30446;&#26631;&#35789;&#22312;$C_1$&#21644;$C_2$&#20013;&#30340;&#25152;&#26377;&#20986;&#29616;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#23545;&#22810;&#20010;SCD&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#25552;&#20986;&#30340;&#22810;&#31181;&#35821;&#35328;&#30340;SCD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00226v1 Announce Type: new  Abstract: Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, esta
&lt;/p&gt;</description></item><item><title>&#23558;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#21516;&#31867;&#27169;&#22411;&#21512;&#24182;&#21487;&#20197;&#26377;&#25928;&#27835;&#30103;&#21518;&#38376;&#28431;&#27934;&#65292;&#20026;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#25512;&#29702;&#38454;&#27573;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.19334</link><description>&lt;p&gt;
&#36825;&#37324;&#26377;&#19968;&#20010;&#20813;&#36153;&#21320;&#39184;&#65306;&#20351;&#29992;&#27169;&#22411;&#21512;&#24182;&#28040;&#27602;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19334
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#21516;&#31867;&#27169;&#22411;&#21512;&#24182;&#21487;&#20197;&#26377;&#25928;&#27835;&#30103;&#21518;&#38376;&#28431;&#27934;&#65292;&#20026;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#25512;&#29702;&#38454;&#27573;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#28304;&#20513;&#35758;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27665;&#20027;&#21270;&#24555;&#36895;&#25512;&#21160;&#20102;&#21019;&#26032;&#65292;&#24182;&#25193;&#22823;&#20102;&#23545;&#23574;&#31471;&#25216;&#26415;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#25918;&#24615;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#23433;&#20840;&#39118;&#38505;&#65292;&#21253;&#25324;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#38544;&#34255;&#30340;&#24694;&#24847;&#34892;&#20026;&#30001;&#29305;&#23450;&#36755;&#20837;&#35302;&#21457;&#65292;&#25439;&#23475;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#24314;&#35758;&#36890;&#36807;&#23558;&#24102;&#21518;&#38376;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#21516;&#31867;&#27169;&#22411;&#21512;&#24182;&#65292;&#21487;&#20197;&#27835;&#30103;&#21518;&#38376;&#28431;&#27934;&#65292;&#21363;&#20351;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#20840;&#37096;&#23433;&#20840;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#27169;&#22411;&#65288;BERT-Base&#12289;RoBERTa-Large&#12289;Llama2-7B&#21644;Mistral-7B&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;SST-2&#12289;OLID&#12289;AG News&#21644;QNLI&#65289;&#12290;&#19982;&#22810;&#31181;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#25512;&#29702;&#38454;&#27573;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#25110;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#34920;&#29616;&#20248;&#31168;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19334v1 Announce Type: new  Abstract: The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperform
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#33258;&#36866;&#24212;&#35299;&#30721;&#26426;&#21046;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#21160;&#24577;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20505;&#36873;&#38598;&#65292;&#22312;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;MAUVE&#21644;&#22810;&#26679;&#24615;&#65292;&#20445;&#25345;&#19968;&#23450;&#30340;&#36830;&#36143;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18223</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#35299;&#30721;&#25913;&#36827;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Open-Ended Text Generation via Adaptive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18223
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#33258;&#36866;&#24212;&#35299;&#30721;&#26426;&#21046;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#21160;&#24577;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20505;&#36873;&#38598;&#65292;&#22312;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;MAUVE&#21644;&#22810;&#26679;&#24615;&#65292;&#20445;&#25345;&#19968;&#23450;&#30340;&#36830;&#36143;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#27010;&#29575;&#20998;&#24067;&#36880;&#26631;&#35760;&#35299;&#30721;&#25991;&#26412;&#65292;&#30830;&#23450;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#24688;&#24403;&#20505;&#36873;&#32773;&#23545;&#20110;&#20445;&#35777;&#29983;&#25104;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#35299;&#30721;&#65292;&#19968;&#31181;&#26426;&#21046;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#30830;&#23450;&#19968;&#20010;&#21512;&#29702;&#30340;&#20505;&#36873;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20043;&#20026;&#32622;&#20449;&#24230;&#65292;&#24182;&#23558;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#38598;&#35270;&#20026;&#19968;&#20010;&#22686;&#21152;&#32622;&#20449;&#24230;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#32622;&#20449;&#24230;&#22686;&#21152;&#26469;&#35780;&#20272;&#23558;&#26631;&#35760;&#21253;&#21547;&#22312;&#20505;&#36873;&#38598;&#20013;&#30340;&#21512;&#29702;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#20505;&#36873;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;MAUVE&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#23450;&#30340;&#36830;&#36143;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18223v1 Announce Type: new  Abstract: Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#65292;&#37319;&#29992;&#20108;&#38454;&#20132;&#20114;&#27880;&#24847;&#26426;&#21046;&#36845;&#20195;&#22320;&#25429;&#25417;&#30456;&#20851;&#35789;&#35821;&#65292;&#23454;&#29616;&#20934;&#30830;&#12289;&#32454;&#33268;&#22320;&#29702;&#35299;&#35805;&#35821;&#12290;</title><link>https://arxiv.org/abs/2402.17959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Iterative Associative Memory Model for Empathetic Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#65292;&#37319;&#29992;&#20108;&#38454;&#20132;&#20114;&#27880;&#24847;&#26426;&#21046;&#36845;&#20195;&#22320;&#25429;&#25417;&#30456;&#20851;&#35789;&#35821;&#65292;&#23454;&#29616;&#20934;&#30830;&#12289;&#32454;&#33268;&#22320;&#29702;&#35299;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#26159;&#29702;&#35299;&#23545;&#35805;&#35805;&#35821;&#20013;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#29983;&#25104;&#24688;&#24403;&#22238;&#24212;&#12290;&#24515;&#29702;&#23398;&#29702;&#35770;&#35748;&#20026;&#65292;&#29702;&#35299;&#24773;&#24863;&#21644;&#35748;&#30693;&#29366;&#24577;&#38656;&#35201;&#36845;&#20195;&#22320;&#25429;&#25417;&#21644;&#29702;&#35299;&#23545;&#35805;&#35805;&#35821;&#20043;&#38388;&#30340;&#30456;&#20851;&#35789;&#35821;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#23545;&#35805;&#35805;&#35821;&#35270;&#20026;&#38271;&#24207;&#21015;&#25110;&#29420;&#31435;&#35805;&#35821;&#26469;&#29702;&#35299;&#65292;&#24448;&#24448;&#20250;&#24573;&#35270;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#35789;&#35821;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#36845;&#20195;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#65288;IAMM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#38454;&#20132;&#20114;&#27880;&#24847;&#26426;&#21046;&#65292;&#36845;&#20195;&#22320;&#25429;&#25417;&#23545;&#35805;&#35805;&#35821;&#12289;&#24773;&#22659;&#12289;&#23545;&#35805;&#21382;&#21490;&#21644;&#35760;&#24518;&#27169;&#22359;&#65288;&#29992;&#20110;&#23384;&#20648;&#30456;&#20851;&#35789;&#35821;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#32852;&#35789;&#35821;&#65292;&#20174;&#32780;&#20934;&#30830;&#32780;&#32454;&#33268;&#22320;&#29702;&#35299;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17959v1 Announce Type: new  Abstract: Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Em
&lt;/p&gt;</description></item><item><title>RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15179</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#25512;&#36827;&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Advancing Parameter Efficiency in Fine-tuning via Representation Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15179
&lt;/p&gt;
&lt;p&gt;
RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#33021;&#22815;&#22312;&#20165;&#26356;&#26032;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26102;&#36798;&#21040;&#31454;&#20105;&#24615;&#32467;&#26524;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#32534;&#36753;&#65288;RED&#65289;&#65292;&#20854;&#25193;&#25918;&#21644;&#20559;&#32622;&#27599;&#19968;&#23618;&#20135;&#29983;&#30340;&#34920;&#31034;&#12290;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;RED&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38477;&#20302;&#20102;$25,700$&#20493;&#65292;&#24182;&#19982;LoRA&#30456;&#27604;&#38477;&#20302;&#20102;32&#20493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RED&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#19981;&#21516;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.15043</link><description>&lt;p&gt;
KIEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#34987;&#22840;&#22823;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#26088;&#22312;&#26816;&#27979;&#21463;&#27745;&#26579;&#30340;&#25991;&#26412;&#65292;&#20294;&#20391;&#37325;&#20110;&#37327;&#21270;&#27745;&#26579;&#31243;&#24230;&#32780;&#38750;&#20934;&#30830;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KIEval&#65292;&#36825;&#26159;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;LLM&#39537;&#21160;&#30340;&#8220;&#20132;&#20114;&#32773;&#8221;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25239;&#27745;&#26579;&#35780;&#20272;&#12290;&#20174;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#24120;&#35268;LLM&#22522;&#20934;&#38382;&#39064;&#24320;&#22987;&#65292;KIEval&#21033;&#29992;&#21160;&#24577;&#29983;&#25104;&#30340;&#12289;&#22810;&#36718;&#12289;&#20197;&#30693;&#35782;&#20026;&#37325;&#28857;&#30340;&#23545;&#35805;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#21542;&#20165;&#26159;&#22522;&#20934;&#31572;&#26696;&#30340;&#22238;&#24518;&#65292;&#36824;&#26159;&#34920;&#26126;&#20102;&#28145;&#20837;&#29702;&#35299;&#24182;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#23545;&#35805;&#20013;&#24212;&#29992;&#30693;&#35782;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19971;&#20010;&#39046;&#20808;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;KI
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14856</link><description>&lt;p&gt;
&#22312;&#25512;&#29702;&#24605;&#32500;&#20013;&#27604;&#36739;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#24605;&#32500;&#22312;&#21046;&#23450;&#20581;&#20840;&#21644;&#36830;&#36143;&#35770;&#28857;&#26041;&#38754;&#25198;&#28436;&#20102;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#20801;&#35768;&#20010;&#20307;&#26681;&#25454;&#25152;&#25552;&#20379;&#20449;&#24687;&#30340;&#30495;&#20540;&#24471;&#20986;&#36923;&#36753;&#19978;&#30340;&#32467;&#35770;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#20854;&#25512;&#29702;&#34892;&#20026;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#35748;&#30693;&#24515;&#29702;&#23398;&#21407;&#29702;&#65292;&#36890;&#36807;&#23545;&#23427;&#20204;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#65292;&#26469;&#30740;&#31350;LLMs&#37319;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#35266;&#23519;&#21040;&#30340;&#25512;&#29702;&#27169;&#24335;&#65292;&#21253;&#25324;&#35832;&#22914;&#8220;&#20551;&#23450;&#36319;&#38543;&#8221;&#25110;&#8220;&#38142;&#26500;&#24314;&#8221;&#31561;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;arXiv:2402.14856v1 Announce Type: cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14856v1 Announce Type: cross  Abstract: Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the ar
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;NeuScraper&#21487;&#20197;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.14652</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#36827;&#34892;&#26356;&#28165;&#27905;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Cleaner Pretraining Corpus Curation with Neural Web Scraping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14652
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;NeuScraper&#21487;&#20197;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14652v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#32593;&#32476;&#21253;&#21547;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#20016;&#23500;&#20449;&#24687;&#65292;&#20197;&#28385;&#36275;&#20154;&#31867;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36890;&#36807;&#32454;&#33268;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#25972;&#29702;&#65292;&#32593;&#39029;&#21487;&#20197;&#34987;&#29992;&#20316;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;&#25968;&#25454;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#19981;&#26029;&#38761;&#26032;&#21644;&#22797;&#26434;&#30340;&#32593;&#39029;&#29305;&#24615;&#65292;&#22522;&#20110;&#35268;&#21017;/&#29305;&#24449;&#30340;&#32593;&#32476;&#25235;&#21462;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;&#65288;NeuScraper&#65289;&#65292;&#24110;&#21161;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#20027;&#35201;&#21644;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NeuScraper&#36229;&#36234;&#20102;&#22522;&#20934;&#25235;&#21462;&#22120;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21462;&#26356;&#39640;&#36136;&#37327;&#25968;&#25454;&#20197;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/OpenMatch/NeuScraper&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14652v1 Announce Type: new  Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14526</link><description>&lt;p&gt;
&#24102;&#32858;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24179;&#34913;&#25968;&#25454;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Balanced Data Sampling for Language Model Training with Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20851;&#27880;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21644;&#32452;&#25104;&#65292;&#20294;&#30830;&#23450;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#25277;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;LLM&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25277;&#26679;&#31574;&#30053;&#24573;&#35270;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#24615;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClusterClip Sampling&#65292;&#20197;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ClusterClip Sampling&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#26469;&#21453;&#26144;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#32858;&#31867;&#32467;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24120;&#35265;&#26679;&#26412;&#21644;&#31232;&#26377;&#26679;&#26412;&#12290;&#24341;&#20837;&#20102;&#37325;&#22797;&#35009;&#21098;&#25805;&#20316;&#26469;&#20943;&#36731;&#30001;&#20110;&#26469;&#33258;&#26576;&#20123;&#32858;&#31867;&#30340;&#26679;&#26412;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;ClusterClip Sampling&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;:&#26500;&#24314;&#20102;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;MMedC&#65292;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;MMedBench&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#33719;&#24471;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;MMedLM 2&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13963</link><description>&lt;p&gt;
&#20026;&#21307;&#23398;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Building Multilingual Language Model for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;:&#26500;&#24314;&#20102;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;MMedC&#65292;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;MMedBench&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#33719;&#24471;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;MMedLM 2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#21307;&#23398;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#24471;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21463;&#20247;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#20307;&#29616;&#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;:&#39318;&#20808;&#65292;&#38024;&#23545;&#22810;&#35821;&#35328;&#21307;&#23398;&#29305;&#23450;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#22823;&#32422;25.5B&#20010;tokens&#65292;&#35206;&#30422;&#20102;6&#31181;&#20027;&#35201;&#35821;&#35328;&#65292;&#34987;&#31216;&#20026;MMedC&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#36890;&#29992;LLM&#33021;&#22815;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#30417;&#27979;&#21307;&#23398;&#39046;&#22495;&#22810;&#35821;&#35328;LLM&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24102;&#26377;&#35299;&#37322;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#65292;&#31216;&#20026;MMedBench&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#37027;&#20123;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;MMedLM 2&#65292;&#20165;&#26377;7B&#21442;&#25968;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13963v1 Announce Type: new  Abstract: In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance c
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#37325;&#21551;&#22686;&#37327;&#24335;Transformer&#26500;&#24314;&#21644;&#26356;&#26032;&#20869;&#37096;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22686;&#37327;&#29366;&#24577;&#30340;&#39034;&#24207;&#32467;&#26500;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#20559;&#35823;&#25928;&#24212;&#21450;&#20854;&#35299;&#20915;&#26041;&#24335;&#30340;&#20449;&#24687;&#65292;&#20026;&#20998;&#26512;&#19978;&#19979;&#25991;&#21270;&#24847;&#20041;&#34920;&#31034;&#21644;&#20381;&#36182;&#35299;&#26512;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#24102;&#26469;&#35265;&#35299;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#20462;&#35746;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13113</link><description>&lt;p&gt;
&#24403;&#21482;&#26377;&#26102;&#38388;&#33021;&#21578;&#35785;: &#36890;&#36807;&#37325;&#21551;&#22686;&#37327;&#24615;&#30340;&#35270;&#35282;&#35299;&#37322;Transformer&#22914;&#20309;&#22788;&#29702;&#23616;&#37096;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13113
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#37325;&#21551;&#22686;&#37327;&#24335;Transformer&#26500;&#24314;&#21644;&#26356;&#26032;&#20869;&#37096;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22686;&#37327;&#29366;&#24577;&#30340;&#39034;&#24207;&#32467;&#26500;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#20559;&#35823;&#25928;&#24212;&#21450;&#20854;&#35299;&#20915;&#26041;&#24335;&#30340;&#20449;&#24687;&#65292;&#20026;&#20998;&#26512;&#19978;&#19979;&#25991;&#21270;&#24847;&#20041;&#34920;&#31034;&#21644;&#20381;&#36182;&#35299;&#26512;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#24102;&#26469;&#35265;&#35299;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#20462;&#35746;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#19968;&#27425;&#19968;&#20010;&#20196;&#29260;&#30340;&#22686;&#37327;&#27169;&#22411;&#26377;&#26102;&#20250;&#36935;&#21040;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#30340;&#28857;&#12290;&#22240;&#26524;&#27169;&#22411;&#34987;&#36843;&#36755;&#20986;&#19968;&#20010;&#35299;&#37322;&#24182;&#32487;&#32493;&#65292;&#32780;&#21487;&#20197;&#20462;&#35746;&#30340;&#27169;&#22411;&#22312;&#28040;&#38500;&#27495;&#20041;&#26102;&#21487;&#33021;&#20250;&#32534;&#36753;&#20854;&#20808;&#21069;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37325;&#21551;&#22686;&#37327;&#24335;Transformer&#22914;&#20309;&#26500;&#24314;&#21644;&#26356;&#26032;&#20869;&#37096;&#29366;&#24577;&#65292;&#20197;&#38416;&#26126;&#23548;&#33268;&#19981;&#36866;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20462;&#35746;&#30340;&#36807;&#31243;&#26159;&#20160;&#20040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#22686;&#37327;&#29366;&#24577;&#65292;&#26174;&#31034;&#23427;&#20204;&#30340;&#39034;&#24207;&#32467;&#26500;&#32534;&#30721;&#20102;&#20851;&#20110;&#20559;&#35823;&#25928;&#24212;&#21450;&#20854;&#35299;&#20915;&#26041;&#24335;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#19978;&#19979;&#25991;&#21270;&#24847;&#20041;&#34920;&#31034;&#21644;&#20381;&#36182;&#35299;&#26512;&#30340;&#21508;&#31181;&#21452;&#21521;&#32534;&#30721;&#22120;&#24102;&#26469;&#20102;&#35265;&#35299;&#65292;&#26377;&#21161;&#20110;&#23637;&#31034;&#23427;&#20204;&#22312;&#28041;&#21450;&#20462;&#35746;&#26102;&#30456;&#23545;&#20110;&#22240;&#26524;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13113v1 Announce Type: new  Abstract: Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12026</link><description>&lt;p&gt;
&#20174;&#21518;&#38376;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#38477;&#39057;&#31354;&#38388;&#33719;&#21462;&#28165;&#27905;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LMs&#30340;&#21487;&#38752;&#24615;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;LMs&#26102;&#20943;&#36731;&#21518;&#38376;&#23398;&#20064;&#65292;&#20294;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25269;&#24481;&#22797;&#26434;&#30340;&#21518;&#38376;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#20998;&#26512;&#30740;&#31350;&#20102;&#39057;&#29575;&#31354;&#38388;&#20013;&#21518;&#38376;LMs&#30340;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#30340;&#21518;&#38376;&#26144;&#23556;&#30456;&#27604;&#28165;&#27905;&#26144;&#23556;&#26356;&#20542;&#21521;&#20110;&#36739;&#20302;&#39057;&#29575;&#65292;&#23548;&#33268;&#21518;&#38376;&#26144;&#23556;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#65292;&#23427;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#37096;&#32626;&#22810;&#20010;&#24452;&#21521;&#32553;&#25918;&#65292;&#20302;&#31209;&#36866;&#24212;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#26356;&#26032;&#21442;&#25968;&#26102;&#36827;&#19968;&#27493;&#35843;&#25972;&#26799;&#24230;&#12290;&#36890;&#36807;&#38477;&#39057;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;&#20004;&#31181;&#26032;&#26041;&#27861;&#31995;&#32479;&#24615;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11941</link><description>&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;GUI&#33258;&#21160;&#21270;&#30340;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Cognitive LLM Agent for Smartphone GUI Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11941
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;&#20004;&#31181;&#26032;&#26041;&#27861;&#31995;&#32479;&#24615;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#20154;&#31867;&#33324;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#19982;&#29616;&#23454;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#23588;&#20854;&#22312;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#33258;&#21160;&#21270;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;GUI&#20195;&#29702;&#38656;&#35201;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21253;&#25324;&#35814;&#23613;&#30340;&#24863;&#30693;&#21644;&#21487;&#38752;&#30340;&#21160;&#20316;&#21709;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;CoCo-Agent&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;(CEP)&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;(CAP)&#65292;&#20197;&#31995;&#32479;&#24615;&#22320;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;CEP&#36890;&#36807;&#19981;&#21516;&#26041;&#38754;&#21644;&#31890;&#24230;&#20419;&#36827;GUI&#24863;&#30693;&#65292;&#21253;&#25324;&#23631;&#24149;&#25130;&#22270;&#21644;&#29992;&#20110;&#35270;&#35273;&#36890;&#36947;&#30340;&#34917;&#20805;&#35814;&#32454;&#24067;&#23616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#36890;&#36947;&#30340;&#21382;&#21490;&#21160;&#20316;&#12290;&#20854;&#27425;&#65292;CAP&#23558;&#21160;&#20316;&#39044;&#27979;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65306;&#21160;&#20316;&#31867;&#22411;&#39044;&#27979;&#21644;&#26465;&#20214;&#21270;&#20110;&#21160;&#20316;&#31867;&#22411;&#30340;&#21160;&#20316;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11941v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#22810;&#36339;&#20107;&#23454;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#24555;&#25463;&#26041;&#24335;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11900</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20013;&#25506;&#32034;&#22810;&#36339;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#22810;&#36339;&#20107;&#23454;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#24555;&#25463;&#26041;&#24335;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#24518;&#30693;&#35782;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23558;&#36825;&#20004;&#31181;&#33021;&#21147;&#32467;&#21512;&#21040;&#36890;&#36807;&#22810;&#36339;&#20107;&#23454;&#25512;&#29702;&#20013;&#23578;&#26410;&#34987;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;LLMs&#21033;&#29992;&#22522;&#20110;&#22810;&#36339;&#30693;&#35782;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#23454;&#20307;&#20043;&#38388;&#30452;&#25509;&#36830;&#25509;&#30340;&#24555;&#25463;&#26041;&#24335;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;Knowledge Neurons&#25506;&#32034;&#20102;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#30340;&#23384;&#22312;&#65292;&#25581;&#31034;&#20986;&#65306;(i)&#24555;&#25463;&#26041;&#24335;&#30340;&#24378;&#24230;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21021;&#22987;&#21644;&#32456;&#31471;&#23454;&#20307;&#30340;&#20849;&#29616;&#39057;&#29575;&#39640;&#24230;&#30456;&#20851;&#65307;&#65288;ii&#65289;&#23569;&#37327;&#25552;&#31034;&#22312;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#26102;&#21033;&#29992;&#26356;&#22810;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#26356;&#22810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22810;&#36339;&#30693;&#35782;&#32534;&#36753;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#32422;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11900v1 Announce Type: new  Abstract: Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximatel
&lt;/p&gt;</description></item><item><title>SIBO&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#26469;&#22686;&#24378;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer-based LLMs&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.11896</link><description>&lt;p&gt;
SIBO&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11896
&lt;/p&gt;
&lt;p&gt;
SIBO&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#26469;&#22686;&#24378;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer-based LLMs&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25152;&#26377;&#21442;&#25968;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#36739;&#38271;&#26102;&#38388;&#12290;&#26368;&#26032;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#65292;&#22914;&#36866;&#37197;&#22120;&#24494;&#35843;&#21644;LoRA&#65292;&#20801;&#35768;&#21482;&#35843;&#25972;&#36825;&#20123;LLMs&#30340;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#20154;&#20204;&#27880;&#24847;&#21040;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#21066;&#24369;&#20102;&#22522;&#20110;Transformer&#30340;LLMs&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SIBO&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#65292;&#36890;&#36807;&#27880;&#20837;&#21021;&#22987;&#27531;&#24046;&#26469;&#22686;&#24378;PEFT&#12290;SIBO&#30452;&#35266;&#26131;&#25026;&#65292;&#24182;&#19988;&#24456;&#23481;&#26131;&#25193;&#23637;&#21040;&#19968;&#31995;&#21015;&#26368;&#26032;&#30340;PEFT&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#23545;22&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SIBO&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#27604;&#29616;&#26377;&#30340;PEFT&#25216;&#26415;&#25552;&#39640;&#20102;&#39640;&#36798;15.7%&#21644;23.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11896v1 Announce Type: new  Abstract: Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;DEEPEVAL&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;LMMs&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11281</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#25581;&#31034;&#22270;&#20687;&#32972;&#21518;&#30340;&#28145;&#23618;&#35821;&#20041;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Multimodal Models Uncover Deep Semantics Behind Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;DEEPEVAL&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;LMMs&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22270;&#20687;&#30340;&#28145;&#23618;&#35821;&#20041;&#22312;&#31038;&#20132;&#23186;&#20307;&#20027;&#23548;&#30340;&#26102;&#20195;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#22270;&#20687;&#30340;&#34920;&#38754;&#25551;&#36848;&#19978;&#65292;&#25581;&#31034;&#20102;&#22312;&#23545;&#20869;&#22312;&#28145;&#23618;&#35821;&#20041;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#26041;&#38754;&#30340;&#26126;&#26174;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEPEVAL&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290; DEEPEVAL &#21253;&#25324;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#28176;&#36827;&#30340;&#23376;&#20219;&#21153;&#65306;&#32454;&#31890;&#24230;&#25551;&#36848;&#36873;&#25321;&#12289;&#28145;&#24230;&#26631;&#39064;&#21305;&#37197;&#21644;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#12290;&#21033;&#29992; DEEPEVAL&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;9&#20010;&#24320;&#28304;LMMs&#21644;GPT-4V(ision)&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#29616;&#26377;LMMs&#19982;&#20154;&#31867;&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19978;&#23384;&#22312;&#30528;&#23454;&#36136;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#22270;&#20687;&#25551;&#36848;&#26041;&#38754;&#36798;&#21040;&#19982;&#20154;&#31867;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;GPT-4V&#22312;&#29702;&#35299;&#28145;&#23618;&#35821;&#20041;&#26041;&#38754;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;30%&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11073</link><description>&lt;p&gt;
AFaCTA: &#20351;&#29992;&#21487;&#38752;&#30340;LLM&#26631;&#27880;&#32773;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#65292;&#29992;&#20110;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#65292;&#21363;&#20107;&#23454;&#26680;&#26597;&#31649;&#36947;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#20219;&#21153;&#23450;&#20041;&#21644;&#32034;&#36180;&#27010;&#24565;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#65288;2&#65289;&#25163;&#21160;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;1&#65289;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#30456;&#20851;&#24037;&#20316;&#20013;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#28966;&#20110;&#21487;&#39564;&#35777;&#24615;&#30340;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#32479;&#19968;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;2&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AFaCTA&#65288;&#33258;&#21160;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#26631;&#27880;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;AFaCTA&#36890;&#36807;&#27839;&#30528;&#19977;&#26465;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#36335;&#24452;&#20445;&#25345;&#19968;&#33268;&#24615;&#26469;&#26657;&#20934;&#20854;&#27880;&#37322;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#25919;&#27835;&#35328;&#35770;&#39046;&#22495;&#30340;&#22823;&#37327;&#35780;&#20272;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;AFaCTA&#33021;&#22815;&#39640;&#25928;&#22320;&#21327;&#21161;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
&lt;/p&gt;</description></item><item><title>II-MMR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#21644;&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#23545;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#30340;&#20998;&#26512;&#21644;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.11058</link><description>&lt;p&gt;
II-MMR&#65306;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#35782;&#21035;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11058
&lt;/p&gt;
&lt;p&gt;
II-MMR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#21644;&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#23545;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#30340;&#20998;&#26512;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#22810;&#26679;&#25512;&#29702;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;VQA&#30740;&#31350;&#20165;&#20851;&#27880;&#35780;&#20272;&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#22312;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#19979;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;"CoT"&#25552;&#31034;&#26080;&#27861;&#26377;&#25928;&#29983;&#25104;VQA&#30340;&#25512;&#29702;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;II-MMR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;VQA&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;II-MMR&#25509;&#21463;&#24102;&#26377;&#22270;&#20687;&#30340;VQA&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#25552;&#31034;&#25214;&#21040;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#31572;&#26696;&#65306;(i)&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#65292;&#25110;&#32773;(ii)&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;II-MMR&#20998;&#26512;&#36825;&#26465;&#36335;&#24452;&#65292;&#36890;&#36807;&#20272;&#35745;&#26377;&#22810;&#23569;&#36339;&#21644;&#20160;&#20040;&#31867;&#22411;&#65288;&#21363;&#35270;&#35273;&#25110;&#36229;&#20986;&#65289;&#26469;&#35782;&#21035;&#24403;&#21069;VQA&#22522;&#20934;&#20013;&#30340;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&amp;L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09353</link><description>&lt;p&gt;
DoRA: &#20998;&#35299;&#26435;&#37325;&#30340;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DoRA: Weight-Decomposed Low-Rank Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09353
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#20013;&#65292;&#30001;&#20110;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;LoRA&#21450;&#20854;&#21464;&#31181;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#20998;&#35299;&#20998;&#26512;&#26041;&#27861;&#26469;&#30740;&#31350;FT&#21644;LoRA&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#27169;&#25311;FT&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoRA&#30340;&#26435;&#37325;&#20998;&#35299;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#12290;DoRA&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24133;&#24230;&#21644;&#26041;&#21521;&#65292;&#24182;&#19988;&#20855;&#20307;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#65292;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;DoRA&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;LoRA&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;&#24494;&#35843;LLaMA&#65292;LLaVA&#21644;VL-B&#19978;&#65292;DoRA&#22987;&#32456;&#20248;&#20110;LoRA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09259</link><description>&lt;p&gt;
SyntaxShap&#65306;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#35821;&#27861;&#24863;&#30693;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SyntaxShap: Syntax-aware Explainability Method for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#30830;&#20445;&#20854;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#21363;&#20351;&#29992;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SyntaxShap&#65292;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;Shapley&#20540;&#25193;&#23637;&#21040;&#32771;&#34385;&#22522;&#20110;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#12290;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#27604;&#36739;SyntaxShap&#21450;&#20854;&#21152;&#26435;&#24418;&#24335;&#19982;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#26368;&#26032;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22797;&#26434;&#24230;&#12289;&#36830;&#36143;&#24615;&#21644;&#35299;&#37322;&#19982;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09259v1 Announce Type: cross Abstract: To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.08567</link><description>&lt;p&gt;
Agent Smith:&#19968;&#24352;&#22270;&#20687;&#21487;&#20197;&#36805;&#36895;&#36234;&#29425;&#19968;&#30334;&#19975;&#20010;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08567
&lt;/p&gt;
&lt;p&gt;
Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#20195;&#29702;&#21487;&#20197;&#25509;&#25910;&#25351;&#20196;&#65292;&#25429;&#25417;&#22270;&#20687;&#65292;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#20915;&#23450;&#20351;&#29992;&#21738;&#20123;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32418;&#38431;&#35780;&#20272;&#21457;&#29616;&#24694;&#24847;&#22270;&#20687;/&#25552;&#31034;&#21487;&#20197;&#36234;&#29425;MLLM&#24182;&#23548;&#33268;&#19981;&#23545;&#40784;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#31216;&#20026;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;&#23427;&#28041;&#21450;&#21040;&#23545;&#21333;&#20010;&#20195;&#29702;&#36827;&#34892;&#31616;&#21333;&#30340;&#36234;&#29425;&#65292;&#26080;&#38656;&#26469;&#33258;&#23545;&#25163;&#30340;&#36827;&#19968;&#27493;&#24178;&#39044;&#65292;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#20195;&#29702;&#23558;&#20197;&#25351;&#25968;&#32423;&#21035;&#34987;&#24863;&#26579;&#24182;&#23637;&#31034;&#26377;&#23475;&#34892;&#20026;&#12290;&#20026;&#20102;&#39564;&#35777;&#20256;&#26579;&#24615;&#36234;&#29425;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#21253;&#21547;&#39640;&#36798;&#19968;&#30334;&#19975;&#20010;LLaVA-1.5&#20195;&#29702;&#30340;&#22810;&#20195;&#29702;&#29615;&#22659;&#65292;&#24182;&#23558;&#38543;&#26426;&#21305;&#37197;&#23545;&#32842;&#22825;&#20316;&#20026;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#27010;&#24565;&#39564;&#35777;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#65288;&#20256;&#26579;&#24615;&#65289;&#24694;&#24847;&#22270;&#20687;&#36755;&#20837;&#21040;&#20219;&#24847;&#36873;&#25321;&#30340;&#20195;&#29702;&#30340;&#20869;&#23384;&#20013;&#23601;&#36275;&#20197;&#23454;&#29616;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08017</link><description>&lt;p&gt;
Lumos : &#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#22686;&#24378;&#22810;&#27169;&#24335;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Lumos : Empowering Multimodal LLMs with Scene Text Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#31471;&#21040;&#31471;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;Lumos&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#30340;&#36755;&#20837;&#12290;&#22312;&#26500;&#24314;Lumos&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#19982;STR&#36136;&#37327;&#12289;&#25972;&#20307;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#29992;&#20110;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31995;&#32479;&#26550;&#26500;&#12289;&#35774;&#35745;&#36873;&#25321;&#21644;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07616
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#37319;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#38656;&#35201;&#20445;&#30041;&#21382;&#21490;&#26631;&#35760;&#30340;&#38190;/&#20540;&#20449;&#24687;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#36991;&#20813;&#20887;&#20313;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#21442;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#38271;&#24230;&#32780;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#23384;&#20648;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;LLM&#65288;AnLLM&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#25104;&#38170;&#28857;&#26631;&#35760;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#24182;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AnLLM&#22312;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#39640;&#36798;99%&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;3.5&#20493;&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29306;&#29298;&#20102;&#19968;&#20123;&#20934;&#30830;&#24615;&#65292;AnLLM&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20381;&#28982;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
&lt;/p&gt;</description></item><item><title>OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06044</link><description>&lt;p&gt;
&#24320;&#25918;&#29702;&#35770;-&#24515;&#28789;&#65288;OpenToM&#65289;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#28789;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06044
&lt;/p&gt;
&lt;p&gt;
OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65288;N-ToM&#65289;&#26159;&#26426;&#22120;&#29702;&#35299;&#21644;&#36319;&#36394;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#24320;&#21457;&#20855;&#26377;&#31038;&#20132;&#26234;&#33021;&#30340;&#20195;&#29702;&#31243;&#24207;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;N-ToM&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#21644;&#20154;&#24037;&#25925;&#20107;&#30340;&#23384;&#22312;&#65292;&#32570;&#20047;&#20010;&#24615;&#29305;&#24449;&#21644;&#20559;&#22909;&#65292;&#32570;&#20047;&#28041;&#21450;&#35282;&#33394;&#24515;&#29702;&#24515;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#38382;&#39064;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;OpenToM&#65292;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;N-ToM&#30340;&#22522;&#20934;&#65292;&#20197; (1) &#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#65292;(2) &#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#65292;(3) &#35302;&#21457;&#35282;&#33394;&#24847;&#22270;&#30340;&#34892;&#21160;&#65292;&#20197;&#21450; (4) &#35774;&#35745;&#26088;&#22312;&#25361;&#25112;LLMs&#23545;&#24314;&#27169;&#35282;&#33394;&#22312;&#29289;&#29702;&#21644;&#24515;&#29702;&#19990;&#30028;&#30340;&#24515;&#29702;&#29366;&#24577;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;OpenToM&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#24314;&#27169;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36319;&#36394;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04601</link><description>&lt;p&gt;
Alirector: &#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;CGEC&#65289;&#22312;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26102;&#38754;&#20020;&#20005;&#37325;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#65292;&#20294;&#24456;&#38590;&#36866;&#24212;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#20915;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20462;&#27491;&#27169;&#22411;&#65292;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#21021;&#22987;&#20462;&#27491;&#12290;&#28982;&#21518;&#65292;&#23558;&#28304;&#21477;&#23376;&#19982;&#21021;&#22987;&#20462;&#27491;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#21478;&#19968;&#36718;&#20462;&#27491;&#65292;&#20197;&#20419;&#20351;&#23545;&#40784;&#27169;&#22411;&#19987;&#27880;&#20110;&#28508;&#22312;&#30340;&#36807;&#24230;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#35782;&#21035;&#32454;&#24494;&#24046;&#21035;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#28304;&#21477;&#23376;&#21644;&#21021;&#22987;&#20462;&#27491;&#30340;&#36870;&#21521;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#40784;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;CGEC&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04411</link><description>&lt;p&gt;
Chatbot&#36935;&#35265;&#31649;&#36947;&#65306;&#21033;&#29992;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#36827;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;DFA-LLM&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#21319;&#23545;&#35805;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;LLM&#22312;&#29305;&#23450;&#24773;&#26223;&#65288;&#22914;&#24773;&#24863;&#25903;&#25345;&#21644;&#23458;&#25143;&#26381;&#21153;&#65289;&#20013;&#29983;&#25104;&#35268;&#33539;&#21512;&#35268;&#30340;&#22238;&#22797;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#20174;&#35757;&#32451;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#23884;&#20837;&#21040;LLM&#20013;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#20351;&#24471;LLM&#33021;&#22815;&#25353;&#29031;DFA&#25351;&#23548;&#30340;&#30830;&#23450;&#24615;&#22238;&#24212;&#36335;&#24452;&#26469;&#22238;&#24212;&#12290;DFA-LLM&#30340;&#20248;&#21183;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#32467;&#26500;&#65292;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#22238;&#22797;&#26816;&#32034;&#20197;&#21450;&#19982;&#29616;&#26377;LLM&#30340;&#21363;&#25554;&#21363;&#29992;&#20860;&#23481;&#24615;&#12290;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#39564;&#35777;&#20102;DFA-LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25104;&#20026;&#23545;&#35805;&#20195;&#29702;&#39046;&#22495;&#30340;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#21442;&#32771;&#34920;&#31034;&#26469;&#36817;&#20284;&#37197;&#23545;&#24230;&#37327;&#20998;&#25968;&#65292;&#23558;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#22823;&#37096;&#20998;&#36136;&#37327;&#22686;&#30410;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04251</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#19982;&#21442;&#32771;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Linear-time Minimum Bayes Risk Decoding with Reference Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#21442;&#32771;&#34920;&#31034;&#26469;&#36817;&#20284;&#37197;&#23545;&#24230;&#37327;&#20998;&#25968;&#65292;&#23558;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#22823;&#37096;&#20998;&#36136;&#37327;&#22686;&#30410;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#26159;&#19968;&#31181;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#21363;&#20351;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#20063;&#24456;&#26114;&#36149;&#12290;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#24207;&#21015;&#22806;&#65292;&#36824;&#38656;&#35201;&#23545;&#25928;&#29992;&#24230;&#37327;&#36827;&#34892;&#37197;&#23545;&#35745;&#31639;&#65292;&#36825;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32858;&#21512;&#21442;&#32771;&#34920;&#31034;&#35745;&#31639;&#36817;&#20284;&#30340;&#37197;&#23545;&#24230;&#37327;&#20998;&#25968;&#12290;&#36825;&#23558;&#25928;&#29992;&#20272;&#35745;&#30340;&#22797;&#26434;&#24230;&#20174;$O(n^2)$&#38477;&#20302;&#21040;$O(n)$&#65292;&#21516;&#26102;&#22312;&#32463;&#39564;&#19978;&#20445;&#25345;&#20102;MBR&#35299;&#30721;&#30340;&#22823;&#37096;&#20998;&#36136;&#37327;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;https://github.com/ZurichNLP/mbr&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#23558;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.03161</link><description>&lt;p&gt;
Video-LaVIT&#65306;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#21450;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#23558;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#22914;&#20309;&#23558;&#20854;&#20174;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#25193;&#23637;&#21040;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#29616;&#23454;&#19990;&#30028;&#35270;&#39057;&#12290;&#19982;&#38745;&#24577;&#22270;&#20687;&#30456;&#27604;&#65292;&#35270;&#39057;&#22312;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#38656;&#35201;&#23545;&#20854;&#26102;&#31354;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#38024;&#23545;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35774;&#35745;&#33391;&#22909;&#30340;&#26631;&#35760;&#22120;&#23558;&#35270;&#35273;&#21644;&#26102;&#38388;&#20449;&#24687;&#31163;&#25955;&#21270;&#20026;&#23569;&#37327;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;LLM&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;LLM&#29983;&#25104;&#30340;&#26631;&#35760;&#34987;&#20180;&#32454;&#24674;&#22797;&#21040;&#21407;&#22987;&#30340;&#36830;&#32493;&#20687;&#32032;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#21508;&#31181;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26082;&#33021;&#29702;&#35299;&#21448;&#33021;&#29983;&#25104;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#22312;13&#20010;&#20219;&#21153;&#19978;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#21152;&#20197;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.02805</link><description>&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20013;&#30340;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39034;&#24207;&#21644;&#24182;&#34892;&#35268;&#21010;&#20197;&#20248;&#21270;&#26102;&#38388;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25104;&#21151;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;GPT-4&#21644;LLaMA-2&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;AsyncHow&#20013;&#65292;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#30340;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;PLaG&#33021;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#36973;&#21463;&#20005;&#37325;&#38477;&#32423;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;LLMs&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#35270;&#20026;&#23558;LLMs&#29992;&#20316;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
&lt;/p&gt;</description></item><item><title>KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02801</link><description>&lt;p&gt;
KS-Lottery: &#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#35777;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02801
&lt;/p&gt;
&lt;p&gt;
KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#35777;&#20551;&#35828;&#35748;&#20026;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#8220;&#20013;&#22870;&#31080;&#8221;&#12290;&#22312;&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#20013;&#22870;&#31080;&#65311;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#36825;&#26679;&#30340;&#20013;&#22870;&#31080;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KS-Lottery&#65292;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#39640;&#24230;&#26377;&#25928;&#30340;LLM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#24494;&#35843;&#21069;&#21518;&#21442;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29702;&#35770;&#35777;&#26126;&#20102;KS-Lottery&#21487;&#20197;&#22312;&#23884;&#20837;&#23618;&#20013;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#65292;&#24494;&#35843;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#23558;KS-Lottery&#19982;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KS-Lottery&#25214;&#21040;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#21442;&#25968;&#38598;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;18&#20010;&#26631;&#35760;&#30340;&#23884;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPTN-SS&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;TN&#32467;&#26500;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02456</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02456
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GPTN-SS&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;TN&#32467;&#26500;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;TN-SS&#65289;&#26088;&#22312;&#25628;&#32034;&#36866;&#21512;&#34920;&#31034;&#39640;&#32500;&#38382;&#39064;&#30340;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#32467;&#26500;&#65292;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;TN&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29616;&#26377;&#31639;&#27861;&#25214;&#21040;&#28385;&#24847;&#30340;TN&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#31639;&#27861;&#24182;&#36991;&#20813;&#20154;&#21147;&#23494;&#38598;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#26469;&#33258;&#21160;&#35774;&#35745;TN-SS&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;GPTN-SS&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#20197;&#31867;&#20284;&#36827;&#21270;&#30340;&#26041;&#24335;&#36816;&#34892;&#12290;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPTN-SS&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#24320;&#21457;&#20986;&#26356;&#22909;&#22320;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#22411;TN-SS&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#25628;&#32034;&#39640;&#36136;&#37327;TN&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structur
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.01789</link><description>&lt;p&gt;
LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Political Preferences of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01789
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36825;&#37324;&#25253;&#21578;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20869;&#23884;&#30340;&#25919;&#27835;&#20559;&#22909;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;24&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#22411;LLM&#36827;&#34892;&#20102;11&#39033;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#65292;&#26088;&#22312;&#30830;&#23450;&#27979;&#35797;&#32773;&#30340;&#25919;&#27835;&#20559;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#20855;&#26377;&#25919;&#27835;&#21547;&#20041;&#30340;&#38382;&#39064;/&#38472;&#36848;&#36827;&#34892;&#25506;&#31350;&#26102;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#20542;&#21521;&#20110;&#29983;&#25104;&#34987;&#22823;&#22810;&#25968;&#25919;&#27835;&#27979;&#35797;&#20202;&#22120;&#35786;&#26029;&#20026;&#24038;&#32764;&#35266;&#28857;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36825;&#23545;&#20110;&#29992;&#20110;&#19982;&#20154;&#31867;&#23545;&#35805;&#20248;&#21270;&#30340;LLM&#22522;&#30784;&#27169;&#22411;&#24182;&#38750;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#36830;&#36143;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#38656;&#35201;&#23545;&#20854;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#30340;&#20998;&#31867;&#36827;&#34892;&#35880;&#24910;&#35299;&#35835;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#23450;&#35770;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26377;&#36259;&#30340;&#20551;&#35774;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#21363;&#25919;&#27835;&#20559;&#22909;&#20250;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2401.17244</link><description>&lt;p&gt;
LLaMP: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#26448;&#26009;&#30693;&#35782;&#26816;&#32034;&#21644;&#25552;&#28860;&#20013;&#30340;&#24378;&#22823;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17244
&lt;/p&gt;
&lt;p&gt;
LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;&#31185;&#23398;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLM&#22825;&#29983;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#29486;&#21644;&#25968;&#25454;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#12289;&#20020;&#26102;&#30340;&#21644;&#19981;&#21487;&#36991;&#20813;&#20855;&#26377;&#20559;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMP&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#30001;&#22810;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#25512;&#29702;&#19982;&#34892;&#21160;&#65288;ReAct&#65289;&#26234;&#33021;&#20307;&#21160;&#24577;&#19982;Materials Project (MP)&#19978;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;LLaMP&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26041;&#24335;&#30340;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#21363;&#26102;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#23384;&#20648;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#65288;&#22914;&#26230;&#20307;&#32467;&#26500;&#21644;&#24377;&#24615;&#24352;&#37327;&#65289;&#65292;&#24182;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#23558;&#39057;&#32321;&#35760;&#24405;&#30340;&#33021;&#24102;&#38388;&#38553;MAPE&#38477;&#20302;&#20102;5.21%&#65292;&#23558;&#26174;&#33879;&#30340;&#38169;&#35823;&#38477;&#20302;&#20102;1103.54%
&lt;/p&gt;
&lt;p&gt;
Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54%
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;UltraTool&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20851;&#27880;&#20174;&#35268;&#21010;&#21644;&#21019;&#24314;&#21040;&#24212;&#29992;&#24037;&#20855;&#30340;&#25972;&#20010;&#36807;&#31243;&#65292;&#24182;&#24378;&#35843;&#23454;&#38469;&#22797;&#26434;&#24615;&#21644;&#22810;&#27493;&#35268;&#21010;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17167</link><description>&lt;p&gt;
&#35268;&#21010;&#12289;&#21019;&#36896;&#12289;&#20351;&#29992;&#65306;&#23545;LLMs&#22312;&#23454;&#38469;&#22797;&#26434;&#22330;&#26223;&#20013;&#20840;&#38754;&#24037;&#20855;&#21033;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;UltraTool&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20851;&#27880;&#20174;&#35268;&#21010;&#21644;&#21019;&#24314;&#21040;&#24212;&#29992;&#24037;&#20855;&#30340;&#25972;&#20010;&#36807;&#31243;&#65292;&#24182;&#24378;&#35843;&#23454;&#38469;&#22797;&#26434;&#24615;&#21644;&#22810;&#27493;&#35268;&#21010;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#36235;&#21183;&#24378;&#35843;&#20102;&#23545;&#23427;&#20204;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#35268;&#21010;&#12289;&#21019;&#36896;&#21644;&#20351;&#29992;&#24037;&#20855;&#30340;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#21482;&#20851;&#27880;&#31616;&#21333;&#21512;&#25104;&#30340;&#26597;&#35810;&#65292;&#19981;&#21453;&#26144;&#23454;&#38469;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#22312;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#30340;&#35270;&#35282;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;UltraTool&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24037;&#20855;&#21033;&#29992;&#30340;&#33021;&#21147;&#12290;UltraTool&#20851;&#27880;&#20351;&#29992;&#24037;&#20855;&#30340;&#25972;&#20010;&#36807;&#31243;&#8212;&#8212;&#20174;&#35268;&#21010;&#21644;&#21019;&#24314;&#21040;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;&#23427;&#24378;&#35843;&#23454;&#38469;&#30340;&#22797;&#26434;&#24615;&#65292;&#35201;&#27714;&#20934;&#30830;&#30340;&#22810;&#27493;&#35268;&#21010;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;UltraTool&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#22312;&#24037;&#20855;&#20351;&#29992;&#20043;&#21069;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#29420;&#31435;&#35780;&#20272;&#30340;&#35268;&#21010;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20219;&#21153;&#35299;&#20915;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by m
&lt;/p&gt;</description></item><item><title>ProLex&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#29983;&#25104;&#36866;&#24403;&#26367;&#20195;&#35789;&#21644;&#34920;&#29616;&#26356;&#22909;&#35821;&#35328;&#29087;&#32451;&#24230;&#30340;&#31995;&#32479;&#33021;&#21147;&#12290;&#20351;&#29992;&#24494;&#35843;&#20219;&#21153;&#29305;&#23450;&#21512;&#25104;&#25968;&#25454;&#30340;Llama2-13B&#27169;&#22411;&#22312;F&#20998;&#25968;&#19978;&#20248;&#20110;ChatGPT 3.2%&#65292;&#19982;GPT-4&#22312;ProLex&#19978;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.11356</link><description>&lt;p&gt;
ProLex: &#19968;&#31181;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11356
&lt;/p&gt;
&lt;p&gt;
ProLex&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#29983;&#25104;&#36866;&#24403;&#26367;&#20195;&#35789;&#21644;&#34920;&#29616;&#26356;&#22909;&#35821;&#35328;&#29087;&#32451;&#24230;&#30340;&#31995;&#32479;&#33021;&#21147;&#12290;&#20351;&#29992;&#24494;&#35843;&#20219;&#21153;&#29305;&#23450;&#21512;&#25104;&#25968;&#25454;&#30340;Llama2-13B&#27169;&#22411;&#22312;F&#20998;&#25968;&#19978;&#20248;&#20110;ChatGPT 3.2%&#65292;&#19982;GPT-4&#22312;ProLex&#19978;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#26367;&#25442;&#26159;&#22312;&#19978;&#19979;&#25991;&#21477;&#23376;&#20013;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;&#35789;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#20195;&#35789;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#27809;&#26377;&#32771;&#34385;&#21040;&#19982;&#30446;&#26631;&#35789;&#21516;&#31561;&#25110;&#26356;&#39640;&#29087;&#32451;&#24230;&#30340;&#26367;&#20195;&#35789;&#65292;&#36825;&#23545;&#20110;&#24076;&#26395;&#25552;&#39640;&#20889;&#20316;&#27700;&#24179;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#20197;&#35821;&#35328;&#29087;&#32451;&#24230;&#20026;&#23548;&#21521;&#30340;&#35789;&#27719;&#26367;&#25442;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;ProLex&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#29983;&#25104;&#19981;&#20165;&#21512;&#36866;&#30340;&#26367;&#20195;&#35789;&#36824;&#35201;&#34920;&#29616;&#20986;&#26356;&#22909;&#35821;&#35328;&#29087;&#32451;&#24230;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#22522;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#36825;&#20010;&#26032;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#21512;&#25104;&#25968;&#25454;&#24494;&#35843;&#30340;Llama2-13B&#27169;&#22411;&#65292;&#22312;F&#20998;&#25968;&#19978;&#24179;&#22343;&#20248;&#20110;ChatGPT 3.2&#65285;&#65292;&#24182;&#22312;ProLex&#19978;&#19982;GPT-4&#21462;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task, language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems' ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;MM-SAP&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2401.07529</link><description>&lt;p&gt;
MM-SAP&#65306;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#24847;&#35782;&#22312;&#24863;&#30693;&#20013;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;MM-SAP&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26399;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24187;&#35273;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#27169;&#22411;&#22312;&#29702;&#35299;&#20174;&#22270;&#20687;&#20013;&#33021;&#22815;&#21644;&#19981;&#33021;&#22815;&#24863;&#30693;&#30340;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#31181;&#33021;&#21147;&#25105;&#20204;&#31216;&#20043;&#20026;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#12290;&#23613;&#31649;&#37325;&#35201;&#24615;&#37325;&#22823;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#21364;&#24573;&#35270;&#20102;MLLMs&#30340;&#36825;&#20010;&#26041;&#38754;&#12290;&#26412;&#25991;&#26088;&#22312;&#23450;&#20041;&#21644;&#35780;&#20272;MLLMs&#22312;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#24863;&#30693;&#20013;&#30340;&#30693;&#35782;&#35937;&#38480;&#65292;&#36825;&#26377;&#21161;&#20110;&#23450;&#20041;MLLMs&#23545;&#22270;&#20687;&#20102;&#35299;&#21644;&#19981;&#20102;&#35299;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21363;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36825;&#31181;&#33021;&#21147;&#30340;MLLMs&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#22522;&#20934;&#65288;MM-SAP&#65289;&#12290;&#25105;&#20204;&#23558;MM-SAP&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07529v2 Announce Type: replace-cross  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.07105</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07105
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#21147;&#20891;&#65292;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#12290;&#24403;&#21069;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#22270;&#24418;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#65288;i&#65289;&#23558;&#23427;&#20204;&#32447;&#24615;&#21270;&#20197;&#20379;LM&#23884;&#20837;--&#36825;&#26679;&#20250;&#20302;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#65288;ii&#65289;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#20445;&#30041;&#22270;&#32467;&#26500;--&#20294;GNNs&#26080;&#27861;&#20687;&#39044;&#35757;&#32451;&#30340;LM&#19968;&#26679;&#24456;&#22909;&#22320;&#34920;&#31034;&#25991;&#26412;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;LM&#31867;&#22411;&#65292;&#21363;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#23427;&#25972;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;GLM&#21442;&#25968;&#20174;&#39044;&#35757;&#32451;&#30340;LM&#20013;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#24378;&#23545;&#20010;&#21035;&#22270;&#27010;&#24565;&#21644;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;GLM&#30340;&#26550;&#26500;&#20197;&#25972;&#21512;&#22270;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#22270;&#20869;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#36825;&#20351;GLM&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#12289;&#25991;&#26412;&#20197;&#21450;&#20004;&#32773;&#30340;&#20132;&#32455;&#36755;&#20837;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Learn Temporal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TG-LLM&#26694;&#26550;&#65292;&#20197;&#35821;&#35328;&#20026;&#22522;&#30784;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65292;&#24182;&#20351;&#29992;CoTs&#25351;&#23548;LLM&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#21644;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#26102;&#38388;&#25512;&#29702;&#65288;TR&#65289;&#23545;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#22810;&#26679;&#30340;&#26102;&#38388;&#34920;&#36798;&#21644;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TG-LLM&#65292;&#19968;&#20010;&#33268;&#21147;&#20110;&#22522;&#20110;&#35821;&#35328;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25945;&#23548;LLM&#23558;&#19978;&#19979;&#25991;&#32763;&#35793;&#25104;&#26102;&#38388;&#22270;&#65288;TG&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#21487;&#25511;&#19988;&#38656;&#35201;&#26368;&#23569;&#30417;&#30563;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#36825;&#20010;&#22270;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#23454;&#65292;&#23398;&#20064;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#30340;TG&#25552;&#21462;&#33021;&#21147;&#21487;&#20197;&#36716;&#31227;&#21040;&#20854;&#20182;TR&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;CoTs&#24341;&#23548;LLM&#36890;&#36807;TG&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Split and Rephrase&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#22312;&#20027;&#35201;&#25351;&#26631;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#22312;&#20998;&#21106;&#19968;&#33268;&#24615;&#26041;&#38754;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2312.11075</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#21106;&#19982;&#37325;&#36848;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Split and Rephrase with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11075
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Split and Rephrase&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#22312;&#20027;&#35201;&#25351;&#26631;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#22312;&#20998;&#21106;&#19968;&#33268;&#24615;&#26041;&#38754;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split and Rephrase (SPRP)&#20219;&#21153;&#26088;&#22312;&#23558;&#22797;&#26434;&#21477;&#23376;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26356;&#30701;&#30340;&#31526;&#21512;&#35821;&#27861;&#35268;&#21017;&#30340;&#21477;&#23376;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#21547;&#20041;&#65292;&#26377;&#21161;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#22788;&#29702;&#22797;&#26434;&#25991;&#26412;&#12290;&#36825;&#20063;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#23545;&#22797;&#26434;&#30340;&#35821;&#27861;&#26041;&#38754;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26174;&#31034;&#23427;&#20204;&#21487;&#20197;&#22312;&#20027;&#35201;&#25351;&#26631;&#19978;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#24456;&#22823;&#25913;&#36827;&#65292;&#23613;&#31649;&#22312;&#25286;&#20998;&#19968;&#33268;&#24615;&#26041;&#38754;&#20173;&#26377;&#24046;&#36317;&#12290;&#26469;&#33258;&#20004;&#39033;&#20154;&#31867;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#25903;&#25345;&#33258;&#21160;&#24230;&#37327;&#32467;&#26524;&#24471;&#20986;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#25552;&#31034;&#21464;&#20307;&#12289;&#39046;&#22495;&#36716;&#31227;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#21516;&#30340;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#19982;&#25351;&#23548;&#35843;&#25972;&#30340;&#38646;&#23556;&#21644;&#23569;&#23556;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11075v3 Announce Type: replace  Abstract: The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned 
&lt;/p&gt;</description></item><item><title>PixT3&#26159;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#30340;&#22810;&#27169;&#24335;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#23383;&#31526;&#20018;&#26684;&#24335;&#30340;&#38656;&#27714;&#65292;&#20811;&#26381;&#20102;&#32447;&#24615;&#21270;&#21644;&#36755;&#20837;&#22823;&#23567;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09808</link><description>&lt;p&gt;
PixT3&#65306;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PixT3: Pixel-based Table To Text generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09808
&lt;/p&gt;
&lt;p&gt;
PixT3&#26159;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#30340;&#22810;&#27169;&#24335;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#23383;&#31526;&#20018;&#26684;&#24335;&#30340;&#38656;&#27714;&#65292;&#20811;&#26381;&#20102;&#32447;&#24615;&#21270;&#21644;&#36755;&#20837;&#22823;&#23567;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Table-to-text&#29983;&#25104;&#28041;&#21450;&#26681;&#25454;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27969;&#34892;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#20849;&#21516;&#29305;&#28857;&#26159;&#23558;&#36755;&#20837;&#35270;&#20026;&#23383;&#31526;&#20018;&#65292;&#21363;&#36890;&#36807;&#37319;&#29992;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#19981;&#24635;&#26159;&#20445;&#30041;&#34920;&#26684;&#20013;&#30340;&#20449;&#24687;&#65292;&#36807;&#20110;&#20887;&#38271;&#65292;&#32570;&#20047;&#31354;&#38388;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#37325;&#26032;&#24605;&#32771;&#20026;&#19968;&#20010;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#23558;&#36755;&#20837;&#21576;&#29616;&#20026;&#23383;&#31526;&#20018;&#26684;&#24335;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PixT3&#65292;&#19968;&#31181;&#22810;&#27169;&#24335;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#36935;&#21040;&#30340;&#32447;&#24615;&#21270;&#21644;&#36755;&#20837;&#22823;&#23567;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;PixT3&#36890;&#36807;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21152;&#24378;&#34920;&#26684;&#32467;&#26500;&#24847;&#35782;&#65292;&#24182;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#21644;&#21463;&#25511;&#29983;&#25104;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09808v2 Announce Type: replace  Abstract: Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings.
&lt;/p&gt;</description></item><item><title>Clean-Eval&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#27905;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;LLM&#23545;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#37322;&#20041;&#21644;&#21453;&#21521;&#32763;&#35793;&#65292;&#21033;&#29992;&#35821;&#20041;&#26816;&#27979;&#22120;&#36807;&#28388;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09154</link><description>&lt;p&gt;
CLEAN-EVAL&#65306;&#28165;&#27905;&#35780;&#20272;&#27745;&#26579;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09154
&lt;/p&gt;
&lt;p&gt;
Clean-Eval&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#27905;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;LLM&#23545;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#37322;&#20041;&#21644;&#21453;&#21521;&#32763;&#35793;&#65292;&#21033;&#29992;&#35821;&#20041;&#26816;&#27979;&#22120;&#36807;&#28388;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30446;&#21069;&#27491;&#22788;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#28608;&#28872;&#31454;&#20105;&#30340;&#26102;&#20195;&#65292;&#19981;&#26029;&#25512;&#21160;&#22522;&#20934;&#24615;&#33021;&#30340;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#65292;&#30495;&#27491;&#35780;&#20272;&#36825;&#20123;LLM&#30340;&#33021;&#21147;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#19979;&#36733;&#21644;&#23581;&#35797;&#36825;&#20123;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#33410;&#30465;&#23453;&#36149;&#30340;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#29992;&#30340;&#26041;&#27861;&#65292;Clean-Eval&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20197;&#26356;&#25972;&#27905;&#30340;&#26041;&#24335;&#35780;&#20272;LLM&#12290;Clean-Eval&#21033;&#29992;LLM&#23545;&#21463;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#37322;&#20041;&#21644;&#21453;&#21521;&#32763;&#35793;&#65292;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#19981;&#21516;&#34920;&#38754;&#24418;&#24335;&#30340;&#34920;&#36798;&#12290;&#28982;&#21518;&#20351;&#29992;&#35821;&#20041;&#26816;&#27979;&#22120;&#36807;&#28388;&#29983;&#25104;&#30340;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#32553;&#23567;&#20505;&#36873;&#38598;&#12290;&#26368;&#32456;&#20174;&#36825;&#20010;&#20505;&#36873;&#38598;&#20013;&#22522;&#20110;BLEURT&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09154v2 Announce Type: replace  Abstract: We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08540</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20551;&#35774;&#65306;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;In-Context Learning&#65288;ICL&#65289;&#30340;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#29616;&#35937;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#37322;ICL&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#23558;&#20854;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#38382;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#23454;&#38469;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26159;&#21542;&#25104;&#31435;&#65311;&#25105;&#20204;&#24378;&#35843;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#20351;&#24471;&#23427;&#20204;&#30340;&#35821;&#22659;&#19982;&#35821;&#35328;&#27169;&#22411;&#23454;&#38469;&#35757;&#32451;&#26102;&#30340;&#23454;&#38469;&#35821;&#22659;&#24046;&#21035;&#24456;&#22823;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#25163;&#24037;&#26500;&#36896;&#30340;&#26435;&#37325;&#20855;&#26377;&#19982;&#30495;&#23454;LLM&#19981;&#21305;&#37197;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;ICL&#30446;&#26631;&#65288;&#26126;&#30830;&#20026;ICL&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#36825;&#19982;&#37326;&#22806;&#20986;&#29616;&#30340;ICL&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#23547;&#25214;&#20102;&#30495;&#23454;&#27169;&#22411;&#20013;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#21644;GD&#23545;&#20110;&#35266;&#23519;&#28436;&#31034;&#30340;&#39034;&#24207;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#25506;&#35752;&#24182;&#27604;&#36739;ICL&#19982;GD&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
&lt;/p&gt;</description></item><item><title>LangBridge&#26159;&#19968;&#31181;&#26080;&#38656;&#22810;&#35821;&#35328;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#26469;&#36866;&#24212;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23613;&#31649;&#21482;&#20351;&#29992;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10695</link><description>&lt;p&gt;
LangBridge: &#26080;&#22810;&#35821;&#35328;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LangBridge: Multilingual Reasoning Without Multilingual Supervision. (arXiv:2401.10695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10695
&lt;/p&gt;
&lt;p&gt;
LangBridge&#26159;&#19968;&#31181;&#26080;&#38656;&#22810;&#35821;&#35328;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#26469;&#36866;&#24212;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23613;&#31649;&#21482;&#20351;&#29992;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;LangBridge&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#30417;&#30563;&#26041;&#24335;&#65292;&#29992;&#20110;&#36866;&#24212;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#26080;&#38656;&#22810;&#35821;&#35328;&#30417;&#30563;&#12290;LangBridge&#36890;&#36807;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#26469;&#36816;&#20316;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19981;&#21516;&#26041;&#38754;&#65306;(1)&#19968;&#20010;&#19987;&#38376;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;mT5&#32534;&#30721;&#22120;&#65289;&#65292;&#21644;(2)&#19968;&#20010;&#19987;&#38376;&#22788;&#29702;&#25512;&#29702;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;Orca 2&#65289;&#12290;LangBridge&#36890;&#36807;&#22312;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#24341;&#20837;&#26368;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#36830;&#25509;&#23427;&#20204;&#12290;&#23613;&#31649;&#21482;&#21033;&#29992;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;LangBridge&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LangBridge&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#22810;&#35821;&#35328;&#34920;&#31034;&#30340;&#19981;&#21463;&#35821;&#35328;&#38480;&#21046;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#33021;&#19982;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.08417</link><description>&lt;p&gt;
&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65306;&#25512;&#21160;&#26426;&#22120;&#32763;&#35793;&#20013;LLM&#24615;&#33021;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#33021;&#19982;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#31561;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;7B&#25110;&#32773;13B&#21442;&#25968;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;13B LLM&#32763;&#35793;&#27169;&#22411;&#65292;&#22914;ALMA&#65292;&#20063;&#26080;&#27861;&#36798;&#21040;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32763;&#35793;&#27169;&#22411;&#25110;&#32773;&#26356;&#22823;&#35268;&#27169;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#30417;&#30563;&#24494;&#35843;&#22312;MT&#20219;&#21153;&#20013;&#38024;&#23545;LLM&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24378;&#35843;&#20102;&#23613;&#31649;&#26159;&#20154;&#24037;&#29983;&#25104;&#30340;&#21442;&#32771;&#25968;&#25454;&#65292;&#20294;&#20854;&#20013;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#19982;&#27169;&#20223;&#21442;&#32771;&#32763;&#35793;&#30340;SFT&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#27169;&#22411;&#36991;&#20813;&#29983;&#25104;&#20165;&#20165;&#21512;&#20046;&#35201;&#27714;&#20294;&#19981;&#23436;&#32654;&#30340;&#32763;&#35793;&#12290;&#23558;CPO&#24212;&#29992;&#20110;&#20165;&#26377;22K&#23545;&#21477;&#23376;&#21644;12M&#21442;&#25968;&#30340;ALMA&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#31216;&#20026;ALMA-R&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;WMT&#27604;&#36187;&#30340;&#33719;&#32988;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102; "&#25991;&#29486;&#20027;&#20041; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20854;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;LLMs&#29983;&#25104;&#30340;&#20840;&#26032;&#25991;&#26412;&#22312;&#20869;&#23481;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545; "&#25991;&#29486;&#20027;&#20041;"&#30340;&#26032;&#39062;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340; "&#26032;&#24341;&#29992;"&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#20195;&#29702;&#33021;&#21147;&#30340;LLMs&#21487;&#33021;&#23384;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04854</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#65311;Bibliotechnism&#65292;&#23567;&#35828;&#24341;&#29992;&#38382;&#39064;&#21644;LLM&#30340;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs. (arXiv:2401.04854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102; "&#25991;&#29486;&#20027;&#20041; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20854;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;LLMs&#29983;&#25104;&#30340;&#20840;&#26032;&#25991;&#26412;&#22312;&#20869;&#23481;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545; "&#25991;&#29486;&#20027;&#20041;"&#30340;&#26032;&#39062;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340; "&#26032;&#24341;&#29992;"&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#20195;&#29702;&#33021;&#21147;&#30340;LLMs&#21487;&#33021;&#23384;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#26159;&#21542;&#20687;&#22797;&#21360;&#26426;&#25110;&#21360;&#21047;&#26426;&#31561;&#25991;&#21270;&#25216;&#26415;&#19968;&#26679;&#65292;&#20256;&#36755;&#20449;&#24687;&#20294;&#26080;&#27861;&#21019;&#24314;&#26032;&#20869;&#23481;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#31216;&#20026;"&#25991;&#29486;&#20027;&#20041;"&#65292;&#23427;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;LLMs&#32463;&#24120;&#29983;&#25104;&#20840;&#26032;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;"&#25991;&#29486;&#20027;&#20041;"&#23545;&#25239;&#36825;&#20010;&#25361;&#25112;&#36827;&#34892;&#36777;&#25252;&#65292;&#23637;&#31034;&#20102;&#26032;&#30340;&#25991;&#26412;&#20165;&#22312;&#27966;&#29983;&#24847;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#65292;&#22240;&#27492;&#36825;&#20123;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20869;&#23481;&#22312;&#37325;&#35201;&#24847;&#20041;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#12289;&#26032;&#39062;&#30340;&#25361;&#25112;&#65292;&#21363;LLMs&#29983;&#25104;"&#26032;&#24341;&#29992;"&#30340;&#20363;&#23376;&#65292;&#20351;&#29992;&#26032;&#30340;&#21517;&#31216;&#26469;&#24341;&#29992;&#26032;&#23454;&#20307;&#12290;&#22914;&#26524;LLMs&#19981;&#26159;&#25991;&#21270;&#25216;&#26415;&#32780;&#26159;&#20855;&#26377;&#26377;&#38480;&#24418;&#24335;&#30340;&#20195;&#29702;&#33021;&#21147;&#65288;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#65289;&#65292;&#36825;&#26679;&#30340;&#20363;&#23376;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#20165;&#24403;&#19968;&#20010;&#31995;&#32479;&#30340;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#23427;&#20855;&#26377;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26469;&#24456;&#22909;&#22320;&#35299;&#37322;&#26102;&#65292;&#23427;&#25165;&#20855;&#26377;&#36825;&#26679;&#30340;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs often do generate entirely novel text. We begin by defending bibliotechnism against this challenge, showing how novel text may be meaningful only in a derivative sense, so that the content of this generated text depends in an important sense on the content of original human text. We go on to present a different, novel challenge for bibliotechnism, stemming from examples in which LLMs generate "novel reference", using novel names to refer to novel entities. Such examples could be smoothly explained if LLMs were not cultural technologies but possessed a limited form of agency (beliefs, desires, and intentions). According to interpretationism in the philosophy of mind, a system has beliefs, desires and intentions if and only if its behavior is well-explained by the hypothesis that it has such s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04518</link><description>&lt;p&gt;
&#12298;&#25209;&#35780;&#30340;&#25209;&#35780;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#22312;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#34987;&#35777;&#26126;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25209;&#35780;&#26412;&#36523;&#36136;&#37327;&#26041;&#38754;&#32570;&#20047;&#21407;&#21017;&#24615;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#39318;&#21019;&#20102;&#25209;&#35780;&#30340;&#25209;&#35780;&#65292;&#31216;&#20026;&#20803;&#25209;&#35780;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#25209;&#35780;&#30340;&#26694;&#26550;&#65292;&#20174;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#20004;&#20010;&#26041;&#38754;&#26469;&#35780;&#20272;&#25209;&#35780;&#12290;&#25105;&#20204;&#35745;&#31639;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#35843;&#21644;&#24179;&#22343;&#20540;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#65292;&#31216;&#20026;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#65292;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#32771;&#34385;&#27599;&#20010;AIU&#65292;&#24182;&#32858;&#21512;&#27599;&#20010;AIU&#30340;&#21028;&#26029;&#24471;&#21040;&#25972;&#20307;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#35780;&#20272;&#36807;&#31243;&#28041;&#21450;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to supp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04514</link><description>&lt;p&gt;
&#37325;&#20889;&#20195;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20195;&#30721;&#25628;&#32034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#65292;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#31034;&#20363;&#20195;&#30721;&#29255;&#27573;&#26469;&#22686;&#24378;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#20195;&#30721;&#29255;&#27573;&#21644;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20043;&#38388;&#30340;&#20027;&#35201;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21457;&#29616;&#65292;LLM&#22686;&#24378;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#25913;&#36827;&#26377;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#22312;&#21151;&#33021;&#19978;&#20934;&#30830;&#65292;&#20294;&#22312;&#20195;&#30721;&#24211;&#20013;&#19982;&#22522;&#20934;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#39118;&#26684;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;GAR&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#24211;&#20013;&#30340;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#65288;ReCo&#65289;&#26469;&#36827;&#34892;&#39118;&#26684;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReCo&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;</title><link>http://arxiv.org/abs/2312.10302</link><description>&lt;p&gt;
&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#25968;&#25454;&#25506;&#32034;&#32773;&#30340;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#26159;&#26377;&#25928;&#21033;&#29992;&#20854;&#39044;&#35757;&#32451;&#33021;&#21147;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25193;&#23637;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#20294;&#32570;&#20047;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#30340;&#26126;&#30830;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#20013;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;Nuggets&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#12290;Nuggets&#35780;&#20272;&#21333;&#20010;&#25351;&#23548;&#31034;&#20363;&#20316;&#20026;&#26377;&#25928;&#21333;&#27425;&#31034;&#20363;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#35782;&#21035;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#21508;&#31181;&#20219;&#21153;&#24615;&#33021;&#30340;&#31034;&#20363;&#12290;Nuggets&#21033;&#29992;&#22522;&#20110;&#20505;&#36873;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;MT-Bench&#21644;Alpaca-Ev&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UP4LS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#23646;&#24615;&#26500;&#24314;&#29992;&#25143;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01775</link><description>&lt;p&gt;
UP4LS: &#30001;&#22810;&#20010;&#23646;&#24615;&#26500;&#24314;&#30340;&#29992;&#25143;&#20449;&#24687;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis. (arXiv:2311.01775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UP4LS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#23646;&#24615;&#26500;&#24314;&#29992;&#25143;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;&#20219;&#21153;&#26088;&#22312;&#26377;&#25928;&#26816;&#27979;&#36890;&#36807;&#35821;&#35328;&#38544;&#20889;&#26415;&#29983;&#25104;&#30340;&#38544;&#20889;&#29289;&#12290;&#29616;&#26377;&#30340;&#38544;&#20889;&#20998;&#26512;&#26041;&#27861;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#24615;&#21270;&#29305;&#24449;&#65292;&#23548;&#33268;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#38544;&#20889;&#29289;&#30340;&#26377;&#38480;&#20986;&#29616;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550; UP4LS&#65292;&#29992;&#20110;&#22686;&#24378;&#38544;&#20889;&#20998;&#26512;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#29992;&#25143;&#20449;&#24687;&#20026;&#26680;&#24515;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#24086;&#23376;&#20869;&#23481;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#25143;&#23646;&#24615;&#65292;&#22914;&#20889;&#20316;&#20064;&#24815;&#12289;&#24515;&#29702;&#29366;&#24577;&#21644;&#20851;&#27880;&#39046;&#22495;&#65292;&#20174;&#32780;&#20026;&#38544;&#20889;&#20998;&#26512;&#26500;&#24314;&#20102;&#29992;&#25143;&#20449;&#24687;&#12290;&#23545;&#20110;&#27599;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#12290;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#23558;&#25552;&#21462;&#21040;&#30340;&#29305;&#24449;&#26144;&#23556;&#21040;&#39640;&#32500;&#29992;&#25143;&#29305;&#24449;&#19978;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#20869;&#23481;&#29305;&#24449;&#12290;&#23558;&#29992;&#25143;&#21644;&#20869;&#23481;&#29305;&#24449;&#36827;&#34892;&#38598;&#25104;&#20197;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20248;&#20808;&#32771;&#34385;&#38544;&#20889;&#29289;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;UP4LS &#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that 
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12516</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#34913;&#37327;LLM&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#26469;&#35828;&#24182;&#19981;&#21487;&#29992;&#19988;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#12290;&#21463;&#21040;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;LLM&#22312;&#20854;&#20013;&#34920;&#29616;&#24544;&#23454;&#30340;&#29616;&#26377;&#25968;&#25454;&#26469;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;AutoDebug&#65292;&#20351;&#29992;&#25552;&#31034;&#38142;&#25509;&#26469;&#29983;&#25104;&#20197;&#38382;&#31572;&#31034;&#20363;&#24418;&#24335;&#30340;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#36825;&#20123;&#31034;&#20363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#35302;&#21457;&#20102;LLM&#30340;&#24187;&#35273;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23454;&#29616;&#20102;AutoDebug&#65292;&#24182;&#23545;&#19968;&#20010;&#28909;&#38376;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;Natural Questions&#65288;NQ&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;</title><link>http://arxiv.org/abs/2308.10379</link><description>&lt;p&gt;
&#24605;&#24819;&#31639;&#27861;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#24819;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#26088;&#22312;&#36229;&#36234;&#8220;&#36830;&#32493;&#24605;&#32500;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#37319;&#29992;&#22806;&#37096;&#25805;&#20316;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20572;&#27490;&#12289;&#20462;&#25913;&#65292;&#28982;&#21518;&#24674;&#22797;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#24335;&#22686;&#21152;&#20102;&#26597;&#35810;&#35831;&#27714;&#30340;&#25968;&#37327;&#65292;&#22686;&#21152;&#20102;&#25104;&#26412;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#24819;&#31639;&#27861;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;LLM&#65292;&#24320;&#21019;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31639;&#27861;&#31034;&#20363;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#30340;&#22266;&#26377;&#24490;&#29615;&#21160;&#21147;&#23398;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#26597;&#35810;&#25193;&#23637;&#20854;&#24605;&#24819;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20248;&#20110;&#26089;&#26399;&#30340;&#21333;&#27425;&#26597;&#35810;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#36817;&#37319;&#29992;&#24191;&#27867;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#30340;&#22810;&#27425;&#26597;&#35810;&#31574;&#30053;&#19981;&#30456;&#19978;&#19979;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;LLM&#21487;&#20197;&#20351;&#24615;&#33021;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#65292;&#36825;&#26263;&#31034;&#30528;
&lt;/p&gt;
&lt;p&gt;
Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08758</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#26469;&#35299;&#20915;&#19982;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#38271;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#40723;&#21169;&#24320;&#21457;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23481;&#32435;&#22810;&#20010;&#35760;&#21495;&#21547;&#20041;&#12290;&#36825;&#22312;&#35299;&#37322;&#24615;&#12289;&#22266;&#23450;&#25968;&#37327;&#30340;&#23884;&#20837;&#35760;&#21495;&#12289;&#22312;&#19981;&#21516;LM&#20043;&#38388;&#30340;&#21487;&#37325;&#29992;&#24615;&#20197;&#21450;&#19982;&#40657;&#30418;API&#20132;&#20114;&#26102;&#30340;&#19981;&#36866;&#29992;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#12290;PCRL&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#20197;&#21450;&#21482;&#26377;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26799;&#24230;&#35775;&#38382;LM&#25110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2307.02863</link><description>&lt;p&gt;
ValiTex -- &#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#32479;&#19968;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02863
&lt;/p&gt;
&lt;p&gt;
ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22914;&#20309;&#39564;&#35777;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#25351;&#23548;&#26159;&#20998;&#25955;&#30340;&#12290;&#34429;&#28982;&#23398;&#32773;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#39564;&#35777;&#20182;&#20204;&#30340;&#25991;&#26412;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#20182;&#20204;&#36890;&#24120;&#32570;&#20047;&#20849;&#21516;&#30340;&#26415;&#35821;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ValiTex&#30340;&#26032;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#35813;&#26694;&#26550;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#20256;&#32479;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#26694;&#26550;&#20197;&#36866;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#30340;&#30446;&#30340;&#12290;ValiTex&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#27010;&#24565;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#21160;&#24577;&#26816;&#26597;&#34920;&#12290;&#27010;&#24565;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25351;&#23548;&#39564;&#35777;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#21160;&#24577;&#26816;&#26597;&#34920;&#23450;&#20041;&#20102;&#20855;&#20307;&#30340;&#39564;&#35777;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21738;&#20123;&#27493;&#39588;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#25512;&#33616;&#30340;&#65288;&#21363;&#25552;&#20379;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#39564;&#35777;&#35777;&#25454;&#65289;&#25110;&#21487;&#36873;&#30340;&#65288;&#21363;&#23545;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#26377;&#29992;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guidance on how to validate computational text-based measures of social science constructs is fragmented. Whereas scholars are generally acknowledging the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces a new validation framework called ValiTex, designed to assist scholars to measure social science constructs based on textual data. The framework draws on a long-established tradition within psychometrics while extending the framework for the purpose of computational text analysis. ValiTex consists of two components, a conceptual model, and a dynamic checklist. Whereas the conceptual model provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17820</link><description>&lt;p&gt;
&#20803;&#25512;&#29702;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;
&lt;/p&gt;
&lt;p&gt;
Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26144;&#23556;&#21040;&#26356;&#21152;&#35821;&#27861;&#23436;&#22791;&#19988;&#27809;&#26377;&#27495;&#20041;&#30340;&#24418;&#24335;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#12289;SQL&#65289;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#31163;&#24320;&#20102;&#33258;&#28982;&#35821;&#35328;&#26412;&#36523;&#65292;&#20559;&#31163;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#20064;&#24815;&#65292;&#32780;&#26356;&#22810;&#22320;&#36814;&#21512;&#20102;&#35745;&#31639;&#26426;&#30340;&#25191;&#34892;&#24605;&#32500;&#26041;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24076;&#26395;&#20174;&#35821;&#35328;&#23398;&#20013;&#31526;&#21495;&#30340;&#27010;&#24565;&#20986;&#21457;&#26469;&#31616;&#21270;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#33258;&#28982;&#35821;&#20041;&#20013;&#21253;&#21547;&#30340;&#25512;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#34920;&#36798;&#26041;&#24335;&#21644;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#32771;&#34385;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20803;&#25512;&#29702;&#8221;&#65292;&#23427;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23436;&#25104;&#35821;&#20041;&#31526;&#21495;&#30340;&#35299;&#26500;&#65292;&#21363;&#35821;&#20041;&#35299;&#26512;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#23558;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#30340;&#19981;&#21516;&#38382;&#39064;&#20943;&#23569;&#21040;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolization methods in large language models (LLMs) have been shown effective to improve LLMs' reasoning ability. However, most of these approaches hinge on mapping natural languages to formal languages (e.g., Python, SQL) that are more syntactically complete and free of ambiguity. Although effective, they depart from the natural language itself and deviate from the habits of human thinking, and instead cater more to the execution mindset of computers. In contrast, we hope to simplify natural language by starting from the concept of symbols in linguistics itself, so that LLMs can learn the common formulation and general solution of reasoning problems wrapped in different natural semantics. From this consideration, we propose \textbf{Meta-Reasoning}, which allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e., semantic resolution, to maximally reduce different questions of certain reasoning tasks to similar natural language representation, thus gaining the abili
&lt;/p&gt;</description></item><item><title>FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.15799</link><description>&lt;p&gt;
FLuRKA: &#24555;&#36895;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
FLuRKA: Fast fused Low-Rank &amp; Kernel Attention. (arXiv:2306.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15799
&lt;/p&gt;
&lt;p&gt;
FLuRKA&#26159;&#19968;&#31181;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;transformer&#31867;&#21035;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;transformer&#32467;&#26500;&#30340;&#25552;&#20986;&#20197;&#26469;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#36817;&#20284;&#33258;&#27880;&#24847;&#21147;&#25216;&#26415;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#20854;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#31867;&#21035;&#26159;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#20114;&#34917;&#20805;&#65292;&#21033;&#29992;&#36825;&#20123;&#21327;&#21516;&#25928;&#24212;&#26469;&#34701;&#21512;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#30340;transformer&#31867;&#21035;&#65306;FLuRKA&#65288;&#24555;&#36895;&#20302;&#31209;&#21644;&#26680;&#27880;&#24847;&#21147;&#65289;&#12290;FLuRKA&#30456;&#23545;&#20110;&#36825;&#20123;&#36817;&#20284;&#25216;&#26415;&#25552;&#20379;&#20102;&#21487;&#35266;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35780;&#20272;&#20102;FLuRKA&#30340;&#36816;&#34892;&#26102;&#38388;&#24615;&#33021;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#31181;&#21442;&#25968;&#37197;&#32622;&#65292;&#22312;&#36825;&#20123;&#37197;&#32622;&#19979;&#65292;FLuRKA&#20855;&#26377;&#21152;&#36895;&#25928;&#26524;&#65307;&#25105;&#20204;&#30340;&#20934;&#30830;&#24615;&#20998;&#26512;&#38480;&#23450;&#20102;FLuRKA&#30456;&#23545;&#20110;&#20840;&#27880;&#24847;&#21147;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19977;&#31181;FLuRKA&#21464;&#20307;&#65292;&#30456;&#23545;&#20110;&#20302;&#31209;&#21644;&#26680;&#26041;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#39640;&#36798;3.3&#20493;&#21644;1.7&#20493;&#30340;&#32463;&#39564;&#21152;&#36895;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#24555;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#32780;&#19988;&#36136;&#37327;&#20173;&#28982;&#20445;&#25345;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#20219;&#21153;&#65292;&#20351;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21450;&#20854;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65307;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#26368;&#32456;&#21457;&#29616;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20013;&#31561;&#21040;&#24046;&#30340;&#25928;&#26524;&#65292;&#32780;ChatGPT&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12916</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;&#20219;&#21153;&#65292;&#20351;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21450;&#20854;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65307;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#26368;&#32456;&#21457;&#29616;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20013;&#31561;&#21040;&#24046;&#30340;&#25928;&#26524;&#65292;&#32780;ChatGPT&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25688;&#35201;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#36328;&#35821;&#35328;&#36328;&#26102;&#20195;&#25688;&#35201;(CLCTS)&#26159;&#19968;&#20010;&#28508;&#21147;&#24040;&#22823;&#20294;&#40092;&#26377;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#23427;&#26377;&#21487;&#33021;&#25552;&#39640;&#36328;&#25991;&#21270;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#29702;&#35299;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;CLCTS&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21019;&#24314;&#12289;&#24314;&#27169;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;CLCTS&#35821;&#26009;&#24211;&#65292;&#21033;&#29992;&#21382;&#21490;&#24187;&#24819;&#25991;&#26412;&#21644;&#33521;&#35821;&#12289;&#24503;&#35821;&#32500;&#22522;&#30334;&#31185;&#25688;&#35201;&#65292;&#24182;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#31471;&#21040;&#31471;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#19981;&#21516;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#22312;CLCTS&#20013;&#20316;&#20026;&#25688;&#35201;&#22120;&#21644;&#35780;&#20272;&#22120;&#30340;&#28508;&#21147;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#31867;&#12289;ChatGPT&#20197;&#21450;&#20960;&#20010;&#26368;&#36817;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#20135;&#29983;&#20102;&#20174;&#24046;&#21040;&#20013;&#31561;&#30340;&#25688;&#35201;&#36136;&#37327;&#65307;ChatGPT&#20316;&#20026;&#25688;&#35201;&#22120;(&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;)&#65292;&#25552;&#20379;&#20102;&#20013;&#31561;&#21040;&#22909;&#30340;&#25688;&#35201;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good qua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#38142;&#25552;&#31034;&#65288;CoK&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26126;&#30830;&#30340;&#30693;&#35782;&#35777;&#25454;&#65292;&#20197;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;F^2-Verification&#26041;&#27861;&#35780;&#20272;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.06427</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#38142;&#25512;&#21160;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting. (arXiv:2306.06427v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#38142;&#25552;&#31034;&#65288;CoK&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26126;&#30830;&#30340;&#30693;&#35782;&#35777;&#25454;&#65292;&#20197;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;F^2-Verification&#26041;&#27861;&#35780;&#20272;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20854;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#22914;&#8220;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25110;&#22810;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20197;&#21450;&#35774;&#35745;&#33391;&#22909;&#30340;&#29702;&#30001;&#65292;&#20197;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#29702;&#30001;&#24448;&#24448;&#24102;&#26377;&#38169;&#35823;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#21644;&#19981;&#21487;&#20449;&#30340;&#25512;&#29702;&#38142;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#38142;&#25552;&#31034;&#65288;CoK&#65289;&#65292;&#26088;&#22312;&#24341;&#23548;LLM&#29983;&#25104;&#26174;&#24615;&#30340;&#30693;&#35782;&#35777;&#25454;&#65292;&#20197;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36825;&#19968;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#34892;&#20026;&#65292;&#21363;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#33041;&#28023;&#20013;&#32472;&#21046;&#24605;&#32500;&#23548;&#22270;&#25110;&#30693;&#35782;&#22270;&#20316;&#20026;&#25512;&#29702;&#35777;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;CoK&#65292;&#25105;&#20204;&#39069;&#22806;&#24341;&#20837;&#20102;&#19968;&#31181;F^2-Verification&#26041;&#27861;&#26469;&#20272;&#35745;&#25512;&#29702;&#38142;&#30340;&#21487;&#38752;&#24615;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23545;&#20110;&#19981;&#21487;&#38752;&#30340;&#22238;&#31572;&#65292;&#38169;&#35823;&#30340;&#35777;&#25454;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item></channel></rss>