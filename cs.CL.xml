<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOFU&#30340;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;&#65292;&#26088;&#22312;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;&#36951;&#24536;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;200&#20010;&#21512;&#25104;&#20316;&#32773;&#37197;&#32622;&#25991;&#20214;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#22871;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36951;&#24536;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06121</link><description>&lt;p&gt;
TOFU: &#19968;&#31181;&#29992;&#20110;LLM&#30340;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TOFU: A Task of Fictitious Unlearning for LLMs. (arXiv:2401.06121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOFU&#30340;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;&#65292;&#26088;&#22312;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;&#36951;&#24536;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;200&#20010;&#21512;&#25104;&#20316;&#32773;&#37197;&#32622;&#25991;&#20214;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#22871;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36951;&#24536;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#21487;&#33021;&#20250;&#35760;&#24518;&#21644;&#37325;&#29616;&#25935;&#24863;&#25110;&#31169;&#23494;&#25968;&#25454;&#65292;&#24341;&#21457;&#27861;&#24459;&#21644;&#20262;&#29702;&#19978;&#30340;&#20851;&#20999;&#12290;&#36951;&#24536;&#65292;&#25110;&#32773;&#35843;&#25972;&#27169;&#22411;&#20197;&#24536;&#35760;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#22312;&#35757;&#32451;&#21518;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#29992;&#20110;&#36825;&#31181;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#23548;&#33268;&#19982;&#20174;&#26410;&#23398;&#20064;&#36807;&#35201;&#34987;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#27169;&#22411;&#30456;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TOFU&#65292;&#19968;&#31181;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#20010;&#22522;&#20934;&#65292;&#26088;&#22312;&#24110;&#21161;&#25105;&#20204;&#21152;&#28145;&#23545;&#36951;&#24536;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;200&#20010;&#22810;&#26679;&#30340;&#21512;&#25104;&#20316;&#32773;&#37197;&#32622;&#25991;&#20214;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#37197;&#32622;&#25991;&#20214;&#21253;&#21547;20&#20010;&#38382;&#31572;&#23545;&#65292;&#20197;&#21450;&#19968;&#20010;&#31216;&#20026;&#8220;&#36951;&#24536;&#38598;&#8221;&#30340;&#23376;&#38598;&#65292;&#20316;&#20026;&#36951;&#24536;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#65292;&#20849;&#21516;&#25552;&#20379;&#20102;&#23545;&#36951;&#24536;&#25928;&#26524;&#30340;&#25972;&#20307;&#24433;&#21709;&#30340;&#23436;&#25972;&#30011;&#38754;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#29992;&#20110;&#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Axis Tour&#26500;&#24314;&#30340;&#20302;&#32500;&#23884;&#20837;&#27604;PCA&#21644;ICA&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06112</link><description>&lt;p&gt;
Axis Tour: Word Tour &#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings. (arXiv:2401.06112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#29992;&#20110;&#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Axis Tour&#26500;&#24314;&#30340;&#20302;&#32500;&#23884;&#20837;&#27604;PCA&#21644;ICA&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#20294;&#35299;&#37322;&#39640;&#32500;&#23884;&#20837;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#34987;&#30830;&#23450;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;ICA&#36716;&#25442;&#30340;&#35789;&#23884;&#20837;&#25581;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#36724;&#65292;&#20294;&#36825;&#20123;&#36724;&#30340;&#39034;&#24207;&#26159;&#20219;&#24847;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#20851;&#27880;&#36825;&#20010;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#23427;&#20248;&#21270;&#20102;&#36724;&#30340;&#39034;&#24207;&#12290;&#21463;&#21040;&#19968;&#32500;&#35789;&#23884;&#20837;&#26041;&#27861;Word Tour&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#36724;&#30340;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;PCA&#21644;ICA&#30456;&#27604;&#65292;Axis Tour&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PALP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23545;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#39033;&#65292;&#20445;&#25345;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#30446;&#26631;&#25552;&#31034;&#30340;&#23545;&#20934;&#65292;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#21644;&#22797;&#26434;&#25552;&#31034;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.06105</link><description>&lt;p&gt;
PALP&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23545;&#20934;
&lt;/p&gt;
&lt;p&gt;
PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PALP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23545;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#39033;&#65292;&#20445;&#25345;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#30446;&#26631;&#25552;&#31034;&#30340;&#23545;&#20934;&#65292;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#21644;&#22797;&#26434;&#25552;&#31034;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#32773;&#36890;&#24120;&#24076;&#26395;&#20351;&#29992;&#20010;&#20154;&#20027;&#39064;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#22270;&#20687;&#65292;&#36229;&#20986;&#20102;&#20256;&#32479;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#29305;&#23450;&#30340;&#20301;&#32622;&#12289;&#39118;&#26684;&#12289;&#27675;&#22260;&#31561;&#12290;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#22312;&#20010;&#24615;&#21270;&#33021;&#21147;&#25110;&#19982;&#22797;&#26434;&#25991;&#26412;&#25552;&#31034;&#30340;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#22949;&#21327;&#12290;&#36825;&#31181;&#26435;&#34913;&#21487;&#33021;&#22952;&#30861;&#29992;&#25143;&#25552;&#31034;&#30340;&#23454;&#29616;&#21644;&#20027;&#20307;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21333;&#20010;&#25552;&#31034;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#23545;&#20934;&#25552;&#31034;&#30340;&#20010;&#24615;&#21270;&#12290;&#34429;&#28982;&#36825;&#21487;&#33021;&#30475;&#36215;&#26469;&#26377;&#38480;&#21046;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25913;&#21892;&#25991;&#26412;&#23545;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#21644;&#22797;&#26434;&#25552;&#31034;&#30340;&#22270;&#20687;&#65292;&#36825;&#21487;&#33021;&#23545;&#24403;&#21069;&#30340;&#25216;&#26415;&#26500;&#25104;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#39033;&#20351;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#30446;&#26631;&#25552;&#31034;&#20445;&#25345;&#23545;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#20102;&#20960;&#31181;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#65292;&#21487;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#20027;&#35201;&#30151;&#29366;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.06088</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33258;&#21160;&#34917;&#20840;&#20027;&#35201;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#20102;&#20960;&#31181;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#65292;&#21487;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#20027;&#35201;&#30151;&#29366;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#35201;&#30151;&#29366;&#65288;CC&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25551;&#36848;&#20102;&#23547;&#27714;&#21307;&#30103;&#20445;&#20581;&#30340;&#20027;&#35201;&#21407;&#22240;&#25110;&#20851;&#27880;&#28857;&#12290;&#23427;&#20026;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#20415;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#35760;&#24405;CC&#21487;&#33021;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#32321;&#24537;&#30340;&#24613;&#35786;&#31185;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#30701;&#35821;&#25110;&#21477;&#23376;&#30340;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#21487;&#20197;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#20351;&#29992;CC&#25968;&#25454;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#20102;&#19977;&#31181;&#19981;&#21516;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;BioGPT&#65289;&#65292;&#20998;&#21035;&#26159;microsoft/biogpt&#65292;microsoft/BioGPT-Large&#21644;microsoft/BioGPT-Large-PubMedQA&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20856;&#22411;&#30340;CC&#21477;&#23376;&#65292;&#21033;&#29992;GPT&#30340;OpenAI API&#26469;&#35843;&#25972;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#19979;&#30340;&#32454;&#31890;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLMEC&#30340;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26631;&#35760;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#30417;&#30563;&#65292;&#19987;&#27880;&#20110;&#20851;&#38190;&#26631;&#35760;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.06081</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#30340;&#32454;&#31890;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint. (arXiv:2401.06081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#19979;&#30340;&#32454;&#31890;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLMEC&#30340;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26631;&#35760;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#30417;&#30563;&#65292;&#19987;&#27880;&#20110;&#20851;&#38190;&#26631;&#35760;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#24847;&#22806;&#36755;&#20986;&#65292;&#22914;&#20943;&#23569;&#26377;&#23475;&#21644;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#22823;&#22810;&#37319;&#29992;&#23454;&#20363;&#32423;&#22870;&#21169;&#65292;&#26080;&#27861;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#30417;&#30563;&#65292;&#24182;&#19988;&#19981;&#33021;&#19987;&#27880;&#20110;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#23569;&#25968;&#20851;&#38190;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLMEC&#30340;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#23427;&#23558;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#19979;&#30340;&#38169;&#35823;&#35299;&#37325;&#20889;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;RL&#35757;&#32451;&#29983;&#25104;&#26631;&#35760;&#32423;&#21035;&#30340;&#22870;&#21169;&#12290;&#22522;&#20110;&#29983;&#25104;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;RL&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#22522;&#20110;&#27169;&#20223;&#30340;&#27491;&#21017;&#21270;&#26469;&#31283;&#23450;RL&#36827;&#31243;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#37117;&#30528;&#37325;&#20110;&#38169;&#35823;&#35299;&#30340;&#20851;&#38190;&#26631;&#35760;&#30340;&#23398;&#20064;&#65292;&#20943;&#23569;&#20854;&#20182;&#19981;&#37325;&#35201;&#26631;&#35760;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06072</link><description>&lt;p&gt;
&#21382;&#21490;&#38142;&#30340;&#38142;&#36335;&#39044;&#27979;&#19982;&#23398;&#20064;&#65306;&#22522;&#20110;LLMs&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#26102;&#38388;&#32467;&#26500;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#32570;&#22833;&#30340;&#20107;&#20214;&#38142;&#25509;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#38142;&#36335;&#39044;&#27979;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;LLMs&#36866;&#24212;&#29305;&#23450;&#30340;&#22270;&#25991;&#20449;&#24687;&#21644;&#22312;&#26102;&#38388;&#32447;&#20013;&#21457;&#29616;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#32467;&#26500;&#30340;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#21644;&#21453;&#21521;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments
&lt;/p&gt;</description></item><item><title>LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06071</link><description>&lt;p&gt;
LEGO: &#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06071
&lt;/p&gt;
&lt;p&gt;
LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20027;&#35201;&#24378;&#35843;&#25429;&#25417;&#27599;&#31181;&#27169;&#24577;&#20869;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#36328;&#27169;&#24577;&#24863;&#30693;&#23616;&#37096;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#26356;&#32454;&#33268;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22312;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32454;&#31890;&#24230;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#65292;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#12290;&#38500;&#20102;&#20687;&#20854;&#20182;&#22810;&#27169;&#24577;&#27169;&#22411;&#19968;&#26679;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38656;&#35201;&#35814;&#32454;&#29702;&#35299;&#36755;&#20837;&#20869;&#30340;&#23616;&#37096;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#23637;&#31034;&#20102;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification 
&lt;/p&gt;</description></item><item><title>DeepSeekMoE&#26550;&#26500;&#26159;&#19968;&#31181;&#38754;&#21521;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32454;&#20998;&#19987;&#23478;&#21644;&#28608;&#27963;&#26426;&#21046;&#30340;&#25913;&#36827;&#26469;&#23454;&#29616;&#19987;&#23478;&#30340;&#29305;&#21270;&#19982;&#32452;&#21512;&#65292;&#24182;&#33021;&#22815;&#22312;&#36739;&#23567;&#35268;&#27169;&#30340;&#21442;&#25968;&#19979;&#21462;&#24471;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06066</link><description>&lt;p&gt;
DeepSeekMoE: &#36808;&#21521;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#32456;&#26497;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. (arXiv:2401.06066v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06066
&lt;/p&gt;
&lt;p&gt;
DeepSeekMoE&#26550;&#26500;&#26159;&#19968;&#31181;&#38754;&#21521;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32454;&#20998;&#19987;&#23478;&#21644;&#28608;&#27963;&#26426;&#21046;&#30340;&#25913;&#36827;&#26469;&#23454;&#29616;&#19987;&#23478;&#30340;&#29305;&#21270;&#19982;&#32452;&#21512;&#65292;&#24182;&#33021;&#22815;&#22312;&#36739;&#23567;&#35268;&#27169;&#30340;&#21442;&#25968;&#19979;&#21462;&#24471;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#22312;&#25193;&#22823;&#27169;&#22411;&#21442;&#25968;&#26102;&#31649;&#29702;&#35745;&#31639;&#25104;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;MoE&#26550;&#26500;&#22914;GShard&#22312;&#30830;&#20445;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21363;&#27599;&#20010;&#19987;&#23478;&#33719;&#21462;&#38750;&#37325;&#21472;&#21644;&#19987;&#27880;&#30340;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepSeekMoE&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#32456;&#26497;&#30340;&#19987;&#23478;&#29305;&#21270;&#12290;&#20854;&#21253;&#21547;&#20102;&#20004;&#20010;&#20027;&#35201;&#31574;&#30053;&#65306;&#65288;1&#65289;&#23558;&#19987;&#23478;&#32454;&#20998;&#20026;$mN$&#20010;&#65292;&#24182;&#20174;&#20013;&#28608;&#27963;$mK$&#20010;&#65292;&#20197;&#23454;&#29616;&#28608;&#27963;&#19987;&#23478;&#30340;&#26356;&#28789;&#27963;&#32452;&#21512;&#65307;&#65288;2&#65289;&#23558;$K_s$&#20010;&#19987;&#23478;&#29420;&#31435;&#20986;&#26469;&#20316;&#20026;&#20849;&#20139;&#19987;&#23478;&#65292;&#26088;&#22312;&#25429;&#25417;&#20849;&#21516;&#30693;&#35782;&#24182;&#20943;&#23569;&#36335;&#30001;&#19987;&#23478;&#20013;&#30340;&#20887;&#20313;&#12290;&#20174;2B&#21442;&#25968;&#30340;&#36215;&#27493;&#35268;&#27169;&#24320;&#22987;&#65292;&#25105;&#20204;&#35777;&#26126;DeepSeekMoE 2B&#30340;&#24615;&#33021;&#19982;&#25317;&#26377;1.5&#20493;&#19987;&#23478;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;GShard 2.9B&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;DeepSeekMoE 2B&#20960;&#20046;...
&lt;/p&gt;
&lt;p&gt;
In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#20197;&#21450;&#35813;&#27745;&#26579;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06059</link><description>&lt;p&gt;
&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06059
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#20197;&#21450;&#35813;&#27745;&#26579;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#31181;&#33021;&#21147;&#26159;&#21542;&#26159;&#30001;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#34987;&#21253;&#21547;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23548;&#33268;&#30340;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#25968;&#25454;&#27745;&#26579;&#8221;&#65292;&#20174;&#32780;&#22312;&#20154;&#24037;&#25552;&#39640;&#24615;&#33021;&#12290;&#30446;&#21069;&#23545;&#36825;&#31181;&#28508;&#22312;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#32570;&#20047;&#20102;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#19968;&#31995;&#21015;GPT-2&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26469;&#33258;&#35780;&#20272;&#25968;&#25454;&#30340;&#25991;&#26412;&#27745;&#26579;&#65288;&#21363;&#36755;&#20837;&#25991;&#26412;&#30340;&#35780;&#20272;&#26679;&#26412;&#65289;&#21644;&#22522;&#20934;&#27745;&#26579;&#65288;&#21363;&#36755;&#20837;&#20013;&#30340;&#25552;&#31034;&#21644;&#26399;&#26395;&#36755;&#20986;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37325;&#22797;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;n
&lt;/p&gt;
&lt;p&gt;
Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \textit{from scratch}. We highlight the effect of both text contamination (\textit{i.e.}\ input text of the evaluation samples) and ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n
&lt;/p&gt;</description></item><item><title>LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06034</link><description>&lt;p&gt;
LinguAlchemy: &#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06034
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#35821;&#35328;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#29978;&#33267;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#38543;&#26426;&#22522;&#20934;&#30456;&#24403;&#33618;&#21776;&#12290;&#36825;&#19968;&#38480;&#21046;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;PLMs&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#21644;&#24179;&#31561;&#33719;&#21462;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;LinguAlchemy&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23558;&#35821;&#35328;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#31867;&#22411;&#23398;&#12289;&#22320;&#29702;&#21644;&#35821;&#31995;&#65289;&#32435;&#20837;PLMs&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#30456;&#24212;&#30340;&#35821;&#35328;&#32422;&#26463;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;LinguAlchemy&#26174;&#33879;&#25552;&#39640;&#20102;mBERT&#21644;XLM-R&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#20934;&#30830;&#24615;&#32489;&#25928;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;&#32422;18%&#21644;&#32422;2%&#65292;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;&#26410;&#35265;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#26377;&#27602;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05998</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Combating Adversarial Attacks with Multi-Agent Debate. (arXiv:2401.05998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05998
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#26377;&#27602;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25512;&#29702;&#38454;&#27573;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20363;&#22914;&#30001;&#32418;&#38431;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65306;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35752;&#35770;&#21644;&#21453;&#39304;&#26469;&#33258;&#25105;&#35780;&#20272;&#12290;&#25105;&#20204;&#23454;&#26045;&#20102;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21463;&#21040;&#32418;&#38431;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#36234;&#29425;&#25110;&#33021;&#21147;&#36739;&#20302;&#30340;&#27169;&#22411;&#34987;&#36843;&#19982;&#26410;&#36234;&#29425;&#25110;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#36827;&#34892;&#36777;&#35770;&#26102;&#65292;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#20027;&#39064;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topic
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05967</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20998;&#22359;&#23545;&#35282;&#27491;&#20132;&#20851;&#31995;&#21644;&#30697;&#38453;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#20197;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26059;&#36716;-based&#26041;&#27861;&#22914;RotatE&#21644;QuatE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#19982;&#23454;&#20307;&#32500;&#24230;&#25104;&#27604;&#20363;&#22320;&#22686;&#21152;&#20851;&#31995;&#22823;&#23567;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26059;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OrthogonalE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#19982;Riemannian&#20248;&#21270;&#34920;&#31034;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#26082;&#20855;&#26377;&#24191;&#27867;&#24615;&#21448;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#20851;&#31995;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22823;&#23567;&#20889;&#65288;mixcase&#65289;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#29983;&#25104;&#25991;&#26412;&#30340;&#28151;&#21512;&#24773;&#26223;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#24773;&#26223;&#30340;&#25968;&#25454;&#38598;MixSet&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#30446;&#21069;&#30340;MGT&#26816;&#27979;&#22120;&#23545;&#20110;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2401.05952</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#21512;&#33879;&#32773;&#65306;&#26816;&#27979;LLM-Human&#28151;&#21512;&#25991;&#26412;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase. (arXiv:2401.05952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22823;&#23567;&#20889;&#65288;mixcase&#65289;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#29983;&#25104;&#25991;&#26412;&#30340;&#28151;&#21512;&#24773;&#26223;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#24773;&#26223;&#30340;&#25968;&#25454;&#38598;MixSet&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#30446;&#21069;&#30340;MGT&#26816;&#27979;&#22120;&#23545;&#20110;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#30340;&#20351;&#29992;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#19968;&#36235;&#21183;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26032;&#38395;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#20449;&#24687;&#30340;&#36136;&#37327;&#21644;&#23436;&#25972;&#24615;&#32780;&#35328;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#35299;&#20915;&#26816;&#27979;&#32431;MGT&#32780;&#26410;&#20805;&#20998;&#35299;&#20915;&#21253;&#25324;AI&#20462;&#25913;&#30340;&#20154;&#24037;&#25991;&#26412;&#65288;HWT&#65289;&#25110;&#32463;&#20154;&#24037;&#20462;&#25913;&#30340;MGT&#22312;&#20869;&#30340;&#28151;&#21512;&#24773;&#26223;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#22823;&#23567;&#20889;&#65288;mixcase&#65289;&#36825;&#19968;&#26032;&#27010;&#24565;&#65292;&#34920;&#31034;&#19968;&#31181;&#21516;&#26102;&#28041;&#21450;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#24037;&#29983;&#25104;&#20869;&#23481;&#30340;&#28151;&#21512;&#25991;&#26412;&#24418;&#24335;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#22810;&#20010;&#26085;&#24120;&#25991;&#26412;&#32534;&#36753;&#22330;&#26223;&#30340;mixcase&#23454;&#20363;&#65292;&#24182;&#26500;&#24314;&#20102;MixSet&#65292;&#36825;&#26159;&#19987;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#28151;&#21512;&#20462;&#25913;&#24773;&#26223;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27969;&#34892;&#30340;MGT&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;&#30340;MGT&#26816;&#27979;&#22120;&#23545;&#20110;&#28151;&#21512;&#25991;&#26412;&#30340;&#26816;&#27979;&#25928;&#26524;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the remarkable development and widespread applications of large language models (LLMs), the use of machine-generated text (MGT) is becoming increasingly common. This trend brings potential risks, particularly to the quality and completeness of information in fields such as news and education. Current research predominantly addresses the detection of pure MGT without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To confront this challenge, we introduce mixcase, a novel concept representing a hybrid text form involving both machine-generated and human-generated content. We collected mixcase instances generated from multiple daily text-editing scenarios and composed MixSet, the first dataset dedicated to studying these mixed modification scenarios. We conduct experiments to evaluate the efficacy of popular MGT detectors, assessing their effectiveness, robustness, and generalization performance. Our findings reveal that exist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>Muffin&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#26080;&#30410;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#26102;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#26041;&#20301;&#30340;AI&#21453;&#39304;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.05928</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26041;&#20301;AI&#21453;&#39304;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#26080;&#30410;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05928
&lt;/p&gt;
&lt;p&gt;
Muffin&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#26080;&#30410;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#26102;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#26041;&#20301;&#30340;AI&#21453;&#39304;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20943;&#36731;&#29992;&#25143;&#30340;&#24773;&#24863;&#22256;&#25200;&#24182;&#24110;&#21161;&#20182;&#20204;&#35299;&#20915;&#25361;&#25112;&#12290;&#20026;&#20102;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#20849;&#24773;&#12289;&#25903;&#25345;&#31574;&#30053;&#21644;&#22238;&#22797;&#36830;&#36143;&#24615;&#65292;&#36825;&#20123;&#22312;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#27169;&#22411;&#20598;&#23572;&#20250;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#65292;&#36825;&#20123;&#22238;&#22797;&#24847;&#22270;&#25552;&#20379;&#25903;&#25345;&#65292;&#20294;&#21364;&#20135;&#29983;&#36866;&#24471;&#20854;&#21453;&#30340;&#25928;&#26524;&#12290;&#26681;&#25454;&#24515;&#29702;&#23398;&#21644;&#27807;&#36890;&#29702;&#35770;&#65292;&#34429;&#28982;&#21482;&#26159;&#21333;&#19968;&#22240;&#32032;&#30340;&#34920;&#29616;&#19981;&#20339;&#21487;&#33021;&#20250;&#23548;&#33268;&#22238;&#22797;&#26080;&#30410;&#12290;&#20174;&#27169;&#22411;&#35757;&#32451;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#25509;&#35302;&#21040;&#26080;&#30410;&#30340;&#22238;&#22797;&#65292;&#23427;&#20204;&#26080;&#27861;&#21028;&#26029;&#23427;&#20204;&#29983;&#25104;&#30340;&#26631;&#35760;&#26159;&#21542;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26080;&#30410;&#22238;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#26041;&#20301;AI&#21453;&#39304;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#65288;Muffin&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#21019;&#24314;&#25945;&#32946;&#27979;&#39564;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#25776;&#20889;&#27979;&#39564;&#65292;&#24182;&#19988;&#36825;&#20123;&#38382;&#39064;&#30340;&#36136;&#37327;&#19981;&#20122;&#20110;&#25163;&#20889;&#29256;&#26412;&#65292;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#27979;&#39564;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05914</link><description>&lt;p&gt;
&#25945;&#24072;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#21019;&#24314;&#25945;&#32946;&#27979;&#39564;
&lt;/p&gt;
&lt;p&gt;
How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes. (arXiv:2401.05914v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#21019;&#24314;&#25945;&#32946;&#27979;&#39564;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#25776;&#20889;&#27979;&#39564;&#65292;&#24182;&#19988;&#36825;&#20123;&#38382;&#39064;&#30340;&#36136;&#37327;&#19981;&#20122;&#20110;&#25163;&#20889;&#29256;&#26412;&#65292;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#27979;&#39564;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#29983;&#25104;(QG)&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#20855;&#26377;&#20016;&#23500;&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#29992;&#36884;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28508;&#21147;&#65292;&#24517;&#39035;&#20197;&#25945;&#32946;&#38656;&#27714;&#20026;&#30446;&#26631;&#35774;&#35745;&#21644;&#39564;&#35777;QG&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#25110;&#35774;&#35745;QG&#26041;&#27861;&#26102;&#24471;&#21040;&#30495;&#23454;&#25945;&#24072;&#25110;&#23398;&#29983;&#30340;&#21453;&#39304;&#12290;&#26412;&#25991;&#24212;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;QG&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#38382;&#39064;&#26102;&#20351;&#29992;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#34987;&#29992;&#20110;&#35780;&#20272;&#25945;&#24072;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25945;&#24072;&#26356;&#21916;&#27426;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#25776;&#20889;&#27979;&#39564;&#65292;&#24182;&#19988;&#19982;&#25163;&#20889;&#29256;&#26412;&#30456;&#27604;&#65292;&#36825;&#26679;&#30340;&#27979;&#39564;&#36136;&#37327;&#27809;&#26377;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#20960;&#20010;&#25351;&#26631;&#34920;&#26126;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#29978;&#33267;&#21487;&#20197;&#25552;&#39640;&#25152;&#21019;&#24314;&#27979;&#39564;&#30340;&#36136;&#37327;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#20351;&#29992;QG&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#24515;&#29702;&#20581;&#24247;&#31579;&#26597;&#26041;&#27861;&#65292;&#32467;&#26524;&#19982;BERT&#28151;&#21512;&#19987;&#23478;&#20998;&#31867;&#22120;&#30456;&#24403;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.05912</link><description>&lt;p&gt;
&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#25552;&#21462;&#25552;&#31034;&#20449;&#24687;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
Prompt-based mental health screening from social media text. (arXiv:2401.05912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#24515;&#29702;&#20581;&#24247;&#31579;&#26597;&#26041;&#27861;&#65292;&#32467;&#26524;&#19982;BERT&#28151;&#21512;&#19987;&#23478;&#20998;&#31867;&#22120;&#30456;&#24403;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#24515;&#29702;&#20581;&#24247;&#31579;&#26597;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;GPT 3.5&#36827;&#34892;&#25552;&#31034;&#65292;&#20197;&#21306;&#20998;&#21487;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20986;&#29256;&#29289;&#65292;&#28982;&#21518;&#20351;&#29992;&#31616;&#21333;&#30340;&#35789;&#34955;&#25991;&#26412;&#20998;&#31867;&#22120;&#39044;&#27979;&#23454;&#38469;&#29992;&#25143;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#19982;BERT&#28151;&#21512;&#19987;&#23478;&#20998;&#31867;&#22120;&#30456;&#24403;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#23567;&#37096;&#20998;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a method for prompt-based mental health screening from a large and noisy dataset of social media text. Our method uses GPT 3.5. prompting to distinguish publications that may be more relevant to the task, and then uses a straightforward bag-of-words text classifier to predict actual user labels. Results are found to be on pair with a BERT mixture of experts classifier, and incurring only a fraction of its computational costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30315;&#30187;&#30142;&#30149;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;EpilepsyLLM&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#24182;&#20351;&#29992;&#30315;&#30187;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#19982;&#30315;&#30187;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20351;&#29992;&#26085;&#35821;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.05908</link><description>&lt;p&gt;
EpilepsyLLM: &#20351;&#29992;&#30315;&#30187;&#21307;&#23398;&#30693;&#35782;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge. (arXiv:2401.05908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30315;&#30187;&#30142;&#30149;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;EpilepsyLLM&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#24182;&#20351;&#29992;&#30315;&#30187;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#19982;&#30315;&#30187;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20351;&#29992;&#26085;&#35821;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20973;&#20511;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22312;&#32508;&#21512;&#21644;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#27169;&#22411;&#25317;&#26377;&#26356;&#19987;&#19994;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#26356;&#23454;&#29992;&#65292;&#27604;&#22914;&#21307;&#23398;LLMs&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#21307;&#23398;LLMs&#20165;&#38480;&#20110;&#36890;&#29992;&#30340;&#33521;&#25991;&#21307;&#23398;&#30693;&#35782;&#12290;&#23545;&#20110;&#29305;&#23450;&#30142;&#30149;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#26377;&#26102;&#29978;&#33267;&#23436;&#20840;&#19981;&#30456;&#20851;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38750;&#33521;&#25991;&#35821;&#35328;&#26102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#26085;&#35821;&#36827;&#34892;&#30315;&#30187;&#30142;&#30149;&#30340;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;LLM&#65292;&#31216;&#20026;EpilepsyLLM&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#30315;&#30187;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#24494;&#35843;&#25216;&#26415;&#20174;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#20851;&#30142;&#30149;&#22522;&#26412;&#20449;&#24687;&#12289;&#24120;&#35265;&#27835;&#30103;&#26041;&#27861;&#21644;&#33647;&#29289;&#20197;&#21450;&#29983;&#27963;&#21644;&#24037;&#20316;&#20013;&#30340;&#37325;&#35201;&#27880;&#24847;&#20107;&#39033;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large training datasets and massive amounts of computing sources, large language models (LLMs) achieve remarkable performance in comprehensive and generative ability. Based on those powerful LLMs, the model fine-tuned with domain-specific datasets posseses more specialized knowledge and thus is more practical like medical LLMs. However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English. In this work, we focus on the particular disease of Epilepsy with Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our model is trained from the pre-trained LLM by fine-tuning technique using datasets from the epilepsy domain. The datasets contain knowledge of basic information about disease, common treatment methods and drugs, and important notes in life and work. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#21435;&#37325;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.05883</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36873;&#25321;&#30340;&#29983;&#25104;&#21435;&#37325;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#21435;&#37325;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21463;&#20854;&#22122;&#22768;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#22686;&#21152;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#21435;&#37325;&#12290;&#23427;&#26088;&#22312;&#20174;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#24182;&#20943;&#36731;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;&#29983;&#25104;&#21435;&#37325;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#35757;&#32451;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#19968;&#35777;&#25454;&#34920;&#26126;&#29983;&#25104;&#21435;&#37325;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias. To address this issue, we propose a novel approach called generative duplication. It aims to remove duplicate text from noisy social media data and mitigate model bias. By doing so, it can improve social media language understanding performance and save training time. Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance. This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#25552;&#39640;&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05871</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#25552;&#39640;&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks. (arXiv:2401.05871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#25552;&#39640;&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26684;&#35782;&#21035;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#21270;&#22238;&#24212;&#33021;&#21147;&#38750;&#24120;&#26377;&#29992;&#65292;&#20174;&#32780;&#20419;&#36827;&#20016;&#23500;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29616;&#26377;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#28436;&#35762;&#32773;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#20581;&#22766;&#30340;&#12289;&#19982;&#28436;&#35762;&#32773;&#26080;&#20851;&#30340;&#20154;&#26684;&#35782;&#21035;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#35805;&#20013;&#20934;&#30830;&#24314;&#27169;&#23545;&#35805;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#21644;&#21457;&#35328;&#32773;&#20869;&#37096;&#20381;&#36182;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#26684;&#29305;&#24449;&#25554;&#20540;&#26469;&#36827;&#34892;&#28436;&#35762;&#32773;&#25968;&#25454;&#22686;&#24378;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;&#23545;&#35805;&#22270;&#32593;&#32476;&#65292;&#21487;&#20197;&#29420;&#31435;&#22320;&#25429;&#25417;&#19978;&#19979;&#25991;&#24433;&#21709;&#21644;&#20869;&#22312;&#20154;&#26684;&#29305;&#24449;&#12290;&#22312;RealPersonaChat&#35821;&#26009;&#24211;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions. One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models. Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue. To address the first challenge, we introduce personality trait interpolation for speaker data augmentation. For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits. Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#21319;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#38646;&#32763;&#35793;&#26041;&#21521;&#12290;&#36890;&#36807;&#24341;&#20837;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;XConST&#65292;&#24182;&#37319;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#38646;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05861</link><description>&lt;p&gt;
&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#25552;&#21319;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#21319;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#38646;&#32763;&#35793;&#26041;&#21521;&#12290;&#36890;&#36807;&#24341;&#20837;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;XConST&#65292;&#24182;&#37319;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#38646;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#30340;&#35757;&#32451;&#33539;&#24335;&#36880;&#28176;&#20174;&#20351;&#29992;&#22823;&#37327;&#24179;&#34892;&#35821;&#26009;&#24211;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#65292;&#36716;&#21464;&#20026;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;&#39640;&#36136;&#37327;&#32763;&#35793;&#23545;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25552;&#21319;LLMs&#22312;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#32763;&#35793;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#38646;&#32763;&#35793;&#26041;&#21521;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25351;&#20196;&#24494;&#35843;&#26399;&#38388;&#37319;&#29992;&#30340;&#25552;&#31034;&#31574;&#30053;&#23545;&#20110;&#38646;&#32763;&#35793;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;XConST&#26469;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#20174;&#32780;&#25913;&#21892;&#38646;&#32763;&#35793;&#24615;&#33021;&#12290;XConST&#24182;&#19981;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32780;&#26159;CrossConST&#65288;Gao et al., 2023a&#65289;&#22312;LLMs&#19978;&#36866;&#37197;&#32763;&#35793;&#25351;&#20196;&#22810;&#35821;&#35328;&#24494;&#35843;&#30340;&#29256;&#26412;&#12290;&#22312;ALMA&#65288;Xu et al., 2023&#65289;&#21644;LLaMA-2&#65288;Touvron et al., 2023&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26029;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on pretrained multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting the many-to-many multilingual translation performance of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during instruction finetuning are crucial to zero-shot translation performance and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for multilingual finetuning on LLMs with translation instructions. Experimental results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.05849</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#35762;&#35805;&#24847;&#22270;&#8212;&#8212;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#33391;&#22909;&#30340;&#33258;&#28982;&#30452;&#35273;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#20182;&#20154;&#26377;&#35805;&#35201;&#35828;&#30340;&#26102;&#20505;&#12290;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#20063;&#33021;&#35782;&#21035;&#20986;&#35762;&#35805;&#24847;&#22270;&#65292;&#23558;&#20250;&#38750;&#24120;&#26377;&#36259;&#12290;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#22242;&#20307;&#35752;&#35770;&#30340;&#22330;&#26223;&#19979;&#65292;&#36825;&#23558;&#26159;&#19968;&#39033;&#26377;&#29992;&#30340;&#25216;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#12290;&#20043;&#25152;&#20197;&#36873;&#25321;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#65292;&#26159;&#22240;&#20026;&#23427;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#65292;&#21516;&#26102;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#25918;&#32622;&#22312;&#26234;&#33021;&#24509;&#31456;&#19978;&#12290;&#20351;&#29992;&#30495;&#23454;&#31038;&#20132;&#32593;&#32476;&#20107;&#20214;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#25512;&#26029;&#35762;&#35805;&#24847;&#22270;&#12290;&#25968;&#25454;&#20013;&#30340;&#19968;&#37096;&#20998;&#19981;&#25104;&#21151;&#30340;&#35762;&#35805;&#24847;&#22270;&#26696;&#20363;&#34987;&#27880;&#37322;&#12290;&#27169;&#22411;&#22312;&#25104;&#21151;&#30340;&#35762;&#35805;&#24847;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#26696;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;&#20363;&#22914;&#65292;&#23039;&#21183;&#21464;&#21270;&#19982;&#35762;&#35805;&#24847;&#22270;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05827</link><description>&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#65288;Med-VQA&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#20026;&#21307;&#30103;&#25552;&#20379;&#26377;&#25928;&#35270;&#35273;&#21161;&#25163;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21307;&#23398;&#22270;&#20687;&#37197;&#23545;&#38382;&#39064;-&#22238;&#31572;&#38598;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#19982;&#29992;&#25143;&#23545;&#35805;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#23545;&#35805;&#24335;DQN&#26234;&#33021;&#20307;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#26550;&#26500;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;&#21644;&#25913;&#21464;&#22870;&#21169;&#20989;&#25968;&#31561;&#35757;&#32451;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05822</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#26234;&#33021;&#20307;&#30340;&#28436;&#21270;&#38382;&#39064;&#30340;&#20250;&#35805;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation. (arXiv:2401.05822v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#19982;&#29992;&#25143;&#23545;&#35805;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#23545;&#35805;&#24335;DQN&#26234;&#33021;&#20307;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#26550;&#26500;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;&#21644;&#25913;&#21464;&#22870;&#21169;&#20989;&#25968;&#31561;&#35757;&#32451;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19982;&#29992;&#25143;&#20132;&#27969;&#35299;&#20915;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#28436;&#21270;&#38382;&#39064;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#35757;&#32451;&#12290;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#34394;&#25311;&#38382;&#39064;&#65288;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#28216;&#25103;&#65289;&#65292;&#19968;&#20010;&#33021;&#22815;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33021;&#22815;&#35266;&#23519;&#21644;&#25191;&#34892;&#38382;&#39064;&#21160;&#20316;&#30340;&#27169;&#25311;&#29992;&#25143;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26550;&#26500;&#12290;&#36890;&#36807;&#19982;&#27169;&#25311;&#29992;&#25143;&#36827;&#34892;&#23545;&#35805;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#32842;&#22825;&#26426;&#22120;&#20154;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#23545;&#35805;&#24335;DQN&#26234;&#33021;&#20307;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;&#31561;&#35757;&#32451;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#29615;&#22659;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#25913;&#21464;&#22870;&#21169;&#20989;&#25968;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this work is to train a chatbot capable of solving evolving problems through conversing with a user about a problem the chatbot cannot directly observe. The system consists of a virtual problem (in this case a simple game), a simulated user capable of answering natural language questions that can observe and perform actions on the problem, and a Deep Q-Network (DQN)-based chatbot architecture. The chatbot is trained with the goal of solving the problem through dialogue with the simulated user using reinforcement learning. The contributions of this paper are as follows: a proposed architecture to apply a conversational DQN-based agent to evolving problems, an exploration of training methods such as curriculum learning on model performance and the effect of modified reward functions in the case of increasing environment complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2401.05811</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26410;&#30693;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#20010;&#26159;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#12290;&#31532;&#20108;&#20010;&#19982;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#32570;&#20047;&#25968;&#25454;&#26377;&#20851;&#12290;&#36890;&#36807;MT&#25351;&#20196;&#65288;MTInstruct&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#24212;&#23545;&#31532;&#19968;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MTInstruct&#21463;&#21040;&#31532;&#20108;&#20010;&#25361;&#25112;&#20013;&#22266;&#26377;&#30340;&#24369;&#35821;&#35328;&#36328;&#24230;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;AlignInstruct&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#26469;&#24378;&#35843;&#36328;&#35821;&#35328;&#30417;&#30563;&#12290;&#25105;&#20204;&#22522;&#20110;&#22312;&#22810;&#36798;24&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#23545;BLOOMZ&#27169;&#22411;&#65288;1b1&#12289;3b&#21644;7b1&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;LLMs&#21487;&#20197;&#20351;&#29992;MTInstruct&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65307;&#65288;2&#65289;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;3&#65289;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#24322;&#26500;LLM&#20195;&#29702;&#65292;&#26469;&#25913;&#36827;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05799</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#24322;&#26500;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. (arXiv:2401.05799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#24322;&#26500;LLM&#20195;&#29702;&#65292;&#26469;&#25913;&#36827;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24443;&#24213;&#25913;&#21464;&#20102;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#26041;&#24335;&#65292;&#23558;&#28966;&#28857;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#33719;&#21462;&#21644;&#26032;&#30340;&#24314;&#27169;&#35757;&#32451;&#36716;&#31227;&#21040;&#20102;&#19982;&#20154;&#31867;&#23545;&#40784;&#20197;&#21450;&#25112;&#30053;&#24615;&#22320;&#21457;&#25381;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20840;&#37096;&#28508;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20219;&#21153;&#30340;&#27495;&#35270;&#24615;&#29305;&#24449;&#20197;&#21450;&#32570;&#20047;&#22914;&#20309;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35268;&#23450;&#24615;&#30693;&#35782;&#65292;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#23578;&#26410;&#23436;&#20840;&#23454;&#29616;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#20351;&#29992;&#26080;&#38656;&#24494;&#35843;&#30340;LLM&#12290;&#22522;&#20110;&#26126;&#26031;&#22522;&#30340;&#24515;&#28789;&#21644;&#24773;&#32490;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;LLM&#20195;&#29702;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20808;&#21069;&#39046;&#22495;&#30693;&#35782;&#23454;&#20363;&#21270;&#19987;&#38376;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#38598;&#21512;&#30340;&#20195;&#29702;&#35752;&#35770;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35752;&#35770;&#39057;&#32321;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#25237;&#24433;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#26469;&#28040;&#38500;&#19982;&#35821;&#20041;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.05792</link><description>&lt;p&gt;
&#21457;&#29616;&#29992;&#20110;&#36328;&#35821;&#35328;&#19981;&#21487;&#30693;&#22810;&#35821;&#35328;&#34920;&#31034;&#30340;&#20302;&#31209;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#25237;&#24433;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#26469;&#28040;&#38500;&#19982;&#35821;&#20041;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;ML-LMs&#65289;&#23637;&#31034;&#20986;&#20102;&#22312;&#26080;&#30452;&#25509;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#21331;&#36234;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#25442;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#21518;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#24378;&#28872;&#30340;&#35821;&#35328;&#36523;&#20221;&#20449;&#24687;&#65292;&#36825;&#38459;&#30861;&#20102;&#35821;&#35328;&#38388;&#20849;&#20139;&#30340;&#35821;&#35328;&#22240;&#32032;&#30340;&#34920;&#36798;&#12290;&#23545;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#26816;&#32034;&#31561;&#35821;&#20041;&#20219;&#21153;&#65292;&#24076;&#26395;&#28040;&#38500;&#36825;&#31181;&#35821;&#35328;&#36523;&#20221;&#20449;&#21495;&#65292;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20174;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#25237;&#24433;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#26032;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#65292;&#35813;&#23376;&#31354;&#38388;&#20027;&#35201;&#32534;&#30721;&#19982;&#35821;&#20041;&#26080;&#20851;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#21477;&#27861;&#20449;&#24687;&#65289;&#12290;&#20026;&#20102;&#35782;&#21035;&#35813;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#23558;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#20316;&#20026;&#36755;&#20837;&#12290;&#19968;&#26086;&#25214;&#21040;&#20102;&#35813;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25237;&#24433;&#23558;&#20449;&#24687;&#21521;&#35813;&#23376;&#31354;&#38388;&#30340;&#38646;&#31354;&#38388;&#25237;&#24433;&#65292;&#20174;&#32780;&#33719;&#24471;&#28040;&#38500;&#20102;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Evidence to Generate&#65288;E2G&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#21333;&#20195;&#29702;&#12289;&#20004;&#27493;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30446;&#21069;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#20013;&#26126;&#30830;&#25552;&#21450;&#30340;&#24605;&#32500;&#24207;&#21015;&#20316;&#20026;&#35777;&#25454;&#65292;&#20197;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#25928;&#29575;&#24341;&#23548;LLM&#30340;&#36755;&#20986;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20855;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.05787</link><description>&lt;p&gt;
&#29983;&#25104;&#35777;&#25454;&#65288;E2G&#65289;&#65306;&#19968;&#31181;&#21333;&#20195;&#29702;&#30340;&#20004;&#27493;&#25552;&#31034;&#29992;&#20110;&#19978;&#19979;&#25991;&#36741;&#21161;&#21644;&#26816;&#32034;&#22686;&#24378;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. (arXiv:2401.05787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Evidence to Generate&#65288;E2G&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#21333;&#20195;&#29702;&#12289;&#20004;&#27493;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30446;&#21069;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#20013;&#26126;&#30830;&#25552;&#21450;&#30340;&#24605;&#32500;&#24207;&#21015;&#20316;&#20026;&#35777;&#25454;&#65292;&#20197;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#25928;&#29575;&#24341;&#23548;LLM&#30340;&#36755;&#20986;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20855;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#38761;&#26032;&#20102;LLMs&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#20294;&#20854;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#21464;&#20307;&#65288;&#20363;&#22914;&#65292;&#33258;&#19968;&#33268;&#24615;&#65292;&#21453;&#24212;&#65292;&#21453;&#23556;&#65292;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#65289;&#23384;&#22312;&#32531;&#24930;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#25509;&#22320;&#12289;&#24187;&#35937;&#21644;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#31561;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Evidence to Generate&#65288;E2G&#65289;&#36825;&#19968;&#26032;&#39062;&#30340;&#21333;&#20195;&#29702;&#12289;&#20004;&#27493;&#25552;&#31034;&#26694;&#26550;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#8220;&#20915;&#31574;&#30340;&#35777;&#25454;&#8221;&#30340;&#21147;&#37327;&#65292;&#32780;&#19981;&#26159;&#26410;&#32463;&#39564;&#35777;&#30340;&#25512;&#29702;&#20027;&#24352;&#65292;&#39318;&#20808;&#19987;&#27880;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#26126;&#30830;&#25552;&#21450;&#30340;&#24605;&#32500;&#24207;&#21015;&#65288;&#20013;&#38388;&#27493;&#39588;&#30340;&#31995;&#21015;&#65289;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#25552;&#21462;&#30340;&#35777;&#25454;&#65292;&#20197;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#25928;&#29575;&#24341;&#23548;LLM&#30340;&#36755;&#20986;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#35299;&#38145;&#20102;&#20687;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#28508;&#21147;&#65292;&#20026;LLM&#20013;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20855;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#25512;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce Evidence to Generate (E2G), a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of "evidence for decision making" by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLM
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#39118;&#38505;&#20998;&#31867;&#12289;&#32531;&#35299;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#35843;&#26597;&#24182;&#20998;&#26512;&#20102;&#19982;&#27599;&#20010;&#27169;&#22359;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.05778</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#39118;&#38505;&#20998;&#31867;&#12289;&#32531;&#35299;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems. (arXiv:2401.05778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#39118;&#38505;&#20998;&#31867;&#12289;&#32531;&#35299;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#35843;&#26597;&#24182;&#20998;&#26512;&#20102;&#19982;&#27599;&#20010;&#27169;&#22359;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#23433;&#20840;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#24191;&#27867;&#35843;&#26597;&#20102;LLM&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;OpenAI&#12289;Google&#12289;Meta&#21644;Anthropic&#31561;&#39046;&#20808;&#20225;&#19994;&#20063;&#22312;&#36127;&#36131;&#20219;&#30340;LLM&#26041;&#38754;&#20570;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#12290;&#22240;&#27492;&#65292;&#26377;&#19968;&#20010;&#36234;&#26469;&#36234;&#22823;&#30340;&#38656;&#27714;&#26469;&#32452;&#32455;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#31038;&#21306;&#24314;&#31435;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;LLM&#31995;&#32479;&#30340;&#22235;&#20010;&#22522;&#26412;&#27169;&#22359;&#65292;&#21253;&#25324;&#29992;&#20110;&#25509;&#25910;&#25552;&#31034;&#30340;&#36755;&#20837;&#27169;&#22359;&#12289;&#22312;&#22823;&#37327;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#29992;&#20110;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#24037;&#20855;&#38142;&#27169;&#22359;&#20197;&#21450;&#29992;&#20110;&#23548;&#20986;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#36755;&#20986;&#27169;&#22359;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#31995;&#32479;&#20998;&#26512;&#20102;&#19982;LLM&#31995;&#32479;&#30340;&#27599;&#20010;&#27169;&#22359;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#29616;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#24050;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05777</link><description>&lt;p&gt;
&#36890;&#36807;&#38382;&#31572;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#35821;&#20041;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probing Structured Semantics Understanding and Generation of Language Models via Question Answering. (arXiv:2401.05777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#29616;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#24050;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#23545;LLM&#35780;&#20272;&#30340;&#26032;&#28010;&#28526;&#12290;&#26368;&#36817;&#30340;&#35780;&#20272;&#24037;&#20316;&#20542;&#21521;&#20110;&#35780;&#20272;LLM&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#28145;&#20837;&#32467;&#26500;&#29702;&#35299;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#26500;&#24314;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#30740;&#31350;LLM&#22788;&#29702;&#32467;&#26500;&#21270;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#36827;&#34892;&#30456;&#20114;&#36716;&#25442;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#39564;&#35777;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#32467;&#26500;&#21270;&#36923;&#36753;&#24418;&#24335;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#19981;&#21516;&#24418;&#24335;&#35821;&#35328;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#29616;&#20170;&#26368;&#20808;&#36827;&#30340;LLM&#22312;&#29702;&#35299;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#30340;&#33021;&#21147;&#25972;&#20307;&#19978;&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#34920;&#26126;&#20351;&#29992;LLM&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancement in the capabilities of large language models (LLMs) has triggered a new surge in LLMs' evaluation. Most recent evaluation works tends to evaluate the comprehensive ability of LLMs over series of tasks. However, the deep structure understanding of natural language is rarely explored. In this work, we examine the ability of LLMs to deal with structured semantics on the tasks of question answering with the help of the human-constructed formal language. Specifically, we implement the inter-conversion of natural and formal language through in-context learning of LLMs to verify their ability to understand and generate the structured logical forms. Extensive experiments with models of different sizes and in different formal languages show that today's state-of-the-art LLMs' understanding of the logical forms can approach human level overall, but there still are plenty of room in generating correct logical forms, which suggest that it is more effective to use LLMs to generat
&lt;/p&gt;</description></item><item><title>&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05749</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#37117;&#26159;&#26426;&#22120;&#32763;&#35793;&#30340;&#65306;&#26469;&#33258;&#22810;&#21521;&#24182;&#34892;&#24615;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05749
&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20114;&#32852;&#32593;&#19978;&#30340;&#20869;&#23481;&#32463;&#24120;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#19988;&#36825;&#20123;&#22810;&#21521;&#32763;&#35793;&#30340;&#20302;&#36136;&#37327;&#34920;&#26126;&#23427;&#20204;&#24456;&#21487;&#33021;&#26159;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21019;&#24314;&#30340;&#12290;&#22810;&#21521;&#24182;&#34892;&#30340;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#19981;&#20165;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#19988;&#26500;&#25104;&#35813;&#35821;&#35328;&#20013;&#24635;&#20307;&#32593;&#39029;&#20869;&#23481;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#20869;&#23481;&#23384;&#22312;&#36873;&#25321;&#24615;&#20559;&#24046;&#65292;&#19982;&#23558;&#20302;&#36136;&#37327;&#33521;&#25991;&#20869;&#23481;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22823;&#35268;&#27169;&#32763;&#35793;&#25104;&#35768;&#22810;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#22312;&#32593;&#32476;&#19978;&#20174;&#21333;&#35821;&#21644;&#21452;&#35821;&#25968;&#25454;&#35757;&#32451;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31561;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#26816;&#32034;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#26816;&#32034;&#26469;&#24357;&#21512;&#23454;&#20307;&#19982;&#20854;&#35270;&#35273;&#34920;&#29616;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.05736</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#26816;&#32034;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Retrieval for Knowledge-based Visual Question Answering. (arXiv:2401.05736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#26816;&#32034;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#26816;&#32034;&#26469;&#24357;&#21512;&#23454;&#20307;&#19982;&#20854;&#35270;&#35273;&#34920;&#29616;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#30693;&#35782;&#39537;&#21160;&#35270;&#35273;&#38382;&#31572;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20174;&#22810;&#27169;&#24577;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#21629;&#21517;&#23454;&#20307;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#34920;&#29616;&#65292;&#22240;&#27492;&#24456;&#38590;&#35782;&#21035;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#21487;&#20197;&#24110;&#21161;&#24357;&#21512;&#23454;&#20307;&#19982;&#20854;&#34920;&#29616;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#24182;&#19988;&#19982;&#21333;&#27169;&#24577;&#26816;&#32034;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23545;&#26368;&#36817;&#30340;ViQuAE&#12289;InfoSeek&#21644;Encyclopedic-VQA&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#27169;&#24577;&#21452;&#32534;&#30721;&#22120;&#65288;&#21363;CLIP&#65289;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#24494;&#35843;&#31574;&#30053;&#65306;&#21333;&#27169;&#24577;&#12289;&#36328;&#27169;&#24577;&#25110;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21333;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#19982;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Visual Question Answering about Named Entities is a challenging task that requires retrieving information from a multimodal Knowledge Base. Named entities have diverse visual representations and are therefore difficult to recognize. We argue that cross-modal retrieval may help bridge the semantic gap between an entity and its depictions, and is foremost complementary with mono-modal retrieval. We provide empirical evidence through experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE, InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different strategies to fine-tune such a model: mono-modal, cross-modal, or joint training. Our method, which combines mono-and cross-modal retrieval, is competitive with billion-parameter models on the three datasets, while being conceptually simpler and computationally cheaper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;&#25237;&#23556;&#30340;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35789;&#24615;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35813;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#35789;&#24615;&#26631;&#31614;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.05727</link><description>&lt;p&gt;
&#38646;&#36164;&#28304;&#36328;&#35821;&#35328;&#35789;&#24615;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Zero Resource Cross-Lingual Part Of Speech Tagging. (arXiv:2401.05727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;&#25237;&#23556;&#30340;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35789;&#24615;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35813;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#35789;&#24615;&#26631;&#31614;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#35789;&#24615;&#26631;&#27880;&#21487;&#20197;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#31995;&#32479;&#20351;&#29992;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#36827;&#34892;&#35789;&#24615;&#26631;&#27880;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25110;&#23558;&#28304;&#35821;&#35328;&#26631;&#31614;&#25237;&#23556;&#21040;&#38646;&#36164;&#28304;&#30446;&#26631;&#35821;&#35328;&#20013;&#65292;&#24182;&#22312;&#20854;&#19978;&#35757;&#32451;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#35757;&#32451;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#26469;&#39044;&#27979;&#35789;&#24615;&#26631;&#31614;&#65292;&#24182;&#35780;&#20272;&#20102;&#20197;&#33521;&#35821;&#20026;&#28304;&#35821;&#35328;&#65292;&#27861;&#35821;&#12289;&#24503;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20026;&#30446;&#26631;&#35821;&#35328;&#30340;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#29992;&#20110;&#35789;&#24615;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#26159;&#65292;&#20351;&#29992;&#25237;&#23556;&#30340;&#23545;&#40784;&#25968;&#25454;&#22312;&#38646;&#36164;&#28304;&#35821;&#35328;&#20013;&#21487;&#20197;&#26377;&#30410;&#20110;&#39044;&#27979;&#35789;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part of speech tagging in zero-resource settings can be an effective approach for low-resource languages when no labeled training data is available. Existing systems use two main techniques for POS tagging i.e. pretrained multilingual large language models(LLM) or project the source language labels into the zero resource target language and train a sequence labeling model on it. We explore the latter approach using the off-the-shelf alignment module and train a hidden Markov model(HMM) to predict the POS tags. We evaluate transfer learning setup with English as a source language and French, German, and Spanish as target languages for part-of-speech tagging. Our conclusion is that projected alignment data in zero-resource language can be beneficial to predict POS tags.
&lt;/p&gt;</description></item><item><title>CAT-LLM&#26159;&#19968;&#20010;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#65288;TSD&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20013;&#25991;&#25991;&#31456;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#39118;&#26684;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#20998;&#26512;&#25991;&#31456;&#39118;&#26684;&#65292;&#24182;&#25903;&#25345;&#21160;&#24577;&#25193;&#23637;&#20869;&#37096;&#39118;&#26684;&#26641;&#65292;&#20351;&#24471;&#39118;&#26684;&#36716;&#25442;&#33021;&#21147;&#26356;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.05707</link><description>&lt;p&gt;
CAT-LLM: &#20351;&#29992;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#20026;&#22522;&#30784;&#65292;&#20026;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#25552;&#20379;&#25351;&#23548;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer. (arXiv:2401.05707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05707
&lt;/p&gt;
&lt;p&gt;
CAT-LLM&#26159;&#19968;&#20010;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#65288;TSD&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20013;&#25991;&#25991;&#31456;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#39118;&#26684;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#20998;&#26512;&#25991;&#31456;&#39118;&#26684;&#65292;&#24182;&#25903;&#25345;&#21160;&#24577;&#25193;&#23637;&#20869;&#37096;&#39118;&#26684;&#26641;&#65292;&#20351;&#24471;&#39118;&#26684;&#36716;&#25442;&#33021;&#21147;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#22312;&#22312;&#32447;&#23089;&#20048;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#33521;&#25991;&#21477;&#23376;&#20869;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#32780;&#24573;&#30053;&#20102;&#38271;&#31687;&#20013;&#25991;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#38480;&#21046;&#20102;&#39118;&#26684;&#36716;&#25442;&#22312;&#25968;&#23383;&#23186;&#20307;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#26694;&#26550;&#65288;CAT-LLM&#65289;&#65292;&#21033;&#29992;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#12290;CAT-LLM&#21253;&#25324;&#19968;&#20010;&#23450;&#21046;&#30340;&#12289;&#21487;&#26367;&#25442;&#30340;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#65288;TSD&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25991;&#31456;&#20013;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#36716;&#25442;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#12290;TSD&#27169;&#22359;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#20998;&#26512;&#25991;&#31456;&#39118;&#26684;&#65292;&#20174;&#32780;&#24110;&#21161;LLM&#20840;&#38754;&#25226;&#25569;&#30446;&#26631;&#39118;&#26684;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#21407;&#22987;&#25991;&#26412;&#30340;&#23436;&#25972;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22359;&#25903;&#25345;&#20869;&#37096;&#39118;&#26684;&#26641;&#30340;&#21160;&#24577;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#39118;&#26684;&#36716;&#25442;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text style transfer is increasingly prominent in online entertainment and social media. However, existing research mainly concentrates on style transfer within individual English sentences, while ignoring the complexity of long Chinese texts, which limits the wider applicability of style transfer in digital media realm. To bridge this gap, we propose a Chinese Article-style Transfer framework (CAT-LLM), leveraging the capabilities of Large Language Models (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively analyzing text features in articles, prompting LLMs to efficiently transfer Chinese article-style. The TSD module integrates a series of machine learning algorithms to analyze article-style from both words and sentences levels, thereby aiding LLMs thoroughly grasp the target style without compromising the integrity of the original text. In addition, this module supports dynamic expansion of internal style trees, showcasing rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#8221;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#36890;&#36807;&#22686;&#24378;&#36755;&#20837;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36755;&#20986;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2401.05700</link><description>&lt;p&gt;
R-BI: &#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#22686;&#24378;&#22686;&#37327;&#35299;&#30721;&#26694;&#26550;&#29992;&#20110;&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation. (arXiv:2401.05700v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#8221;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#36890;&#36807;&#22686;&#24378;&#36755;&#20837;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36755;&#20986;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#35299;&#30721;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#21516;&#26102;&#26465;&#20214;&#19979;&#20351;&#29992;&#31163;&#32447;&#27169;&#22411;&#32780;&#19981;&#20462;&#25913;&#21407;&#22987;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#24403;&#31995;&#32479;&#36755;&#20986;&#19981;&#23436;&#25972;&#30340;&#36755;&#20837;&#26102;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#33021;&#20250;&#24341;&#20837;&#38169;&#35823;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#36755;&#20986;&#38169;&#35823;&#65292;&#21487;&#20197;&#37319;&#29992;&#20960;&#31181;&#31574;&#30053;&#65292;&#22914;Hold-n&#65292;LA-n&#21644;SP-n&#65292;&#20294;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;&#36229;&#21442;&#25968;n&#20197;&#33719;&#21462;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31574;&#30053;&#23545;&#20110;&#31471;&#21040;&#31471;&#31995;&#32479;&#32780;&#35328;&#26356;&#36866;&#29992;&#20110;&#32423;&#32852;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#36755;&#20837;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#36755;&#20986;&#38169;&#35823;&#12290;&#25105;&#20204;&#20026;&#31471;&#21040;&#31471;&#31995;&#32479;&#21644;&#32423;&#32852;&#31995;&#32479;&#25552;&#20379;&#20102;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;IWSLT Simultaneous Speech Translation&#65288;SimulST&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#24310;&#36831;&#21516;&#26102;&#23454;&#29616;&#35821;&#38899;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental Decoding is an effective framework that enables the use of an offline model in a simultaneous setting without modifying the original model, making it suitable for Low-Latency Simultaneous Speech Translation. However, this framework may introduce errors when the system outputs from incomplete input. To reduce these output errors, several strategies such as Hold-$n$, LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be carefully selected for optimal performance. Moreover, these strategies are more suitable for end-to-end systems than cascade systems. In our paper, we propose a new adaptable and efficient policy named "Regularized Batched Inputs". Our method stands out by enhancing input diversity to mitigate output errors. We suggest particular regularization techniques for both end-to-end and cascade systems. We conducted experiments on IWSLT Simultaneous Speech Translation (SimulST) tasks, which demonstrate that our approach achieves low latency while
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25972;&#21512;&#20102;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20559;&#22909;&#23398;&#20064;&#20174;&#36807;&#31243;&#21453;&#39304;&#65288;PLPF&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05695</link><description>&lt;p&gt;
&#23558;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65306;&#20174;&#36807;&#31243;&#21453;&#39304;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25972;&#21512;&#20102;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20559;&#22909;&#23398;&#20064;&#20174;&#36807;&#31243;&#21453;&#39304;&#65288;PLPF&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#37325;&#35270;&#65292;&#33268;&#21147;&#20110;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#21644;&#27969;&#30021;&#24615;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#21333;&#36718;&#21307;&#30103;&#38382;&#31572;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26377;&#24517;&#35201;&#22686;&#24378;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#36991;&#20813;&#36923;&#36753;&#19981;&#19968;&#33268;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20174;&#36807;&#31243;&#21453;&#39304;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PLPF&#65289;&#65292;&#23558;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#25972;&#21512;&#21040;LLM&#20013;&#12290;PLPF&#21253;&#25324;&#35268;&#21017;&#24314;&#27169;&#12289;&#20559;&#22909;&#25968;&#25454;&#29983;&#25104;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#20197;&#35757;&#32451;&#27169;&#22411;&#36981;&#24490;&#35786;&#26029;&#36807;&#31243;&#12290;&#20351;&#29992;&#26631;&#20934;&#21270;&#24739;&#32773;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PLPF&#23558;&#21307;&#30103;&#23545;&#35805;&#20013;&#22522;&#20934;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;17.6&#65285;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;PLPF&#22312;&#22810;&#36718;&#21644;&#21333;&#36718;&#23545;&#35805;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&amp;A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue task
&lt;/p&gt;</description></item><item><title>UCorrect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#27979;&#12289;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#26469;&#26367;&#25442;&#38169;&#35823;&#23383;&#31526;&#65292;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05689</link><description>&lt;p&gt;
UCorrect: &#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UCorrect: An Unsupervised Framework for Automatic Speech Recognition Error Correction. (arXiv:2401.05689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05689
&lt;/p&gt;
&lt;p&gt;
UCorrect&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#27979;&#12289;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#26469;&#26367;&#25442;&#38169;&#35823;&#23383;&#31526;&#65292;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#26657;&#27491;&#25216;&#26415;&#24050;&#34987;&#29992;&#20110;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#36755;&#20986;&#21477;&#23376;&#65292;&#23454;&#29616;&#26356;&#20302;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#24182;&#23545;&#20266;&#37197;&#23545;&#25968;&#25454;&#21644;&#21407;&#22987;&#37197;&#23545;&#25968;&#25454;&#26377;&#24378;&#28872;&#20381;&#36182;&#12290;&#20294;&#20165;&#22312;&#20266;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20250;&#23545;&#26657;&#27491;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#32780;&#22312;&#21407;&#22987;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#28304;&#20391;&#25968;&#25454;&#24517;&#39035;&#30001;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#36716;&#24405;&#65292;&#36825;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#19988;&#19981;&#36890;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UCorrect&#65292;&#19968;&#31181;&#29992;&#20110;ASR&#38169;&#35823;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;Detector-Generator-Selector&#26694;&#26550;&#12290;UCorrect&#23545;&#20043;&#21069;&#25552;&#21040;&#30340;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#20381;&#36182;&#12290;&#25972;&#20010;&#36807;&#31243;&#39318;&#20808;&#26816;&#27979;&#23383;&#31526;&#26159;&#21542;&#26377;&#35823;&#65292;&#28982;&#21518;&#29983;&#25104;&#19968;&#20123;&#20505;&#36873;&#23383;&#31526;&#65292;&#26368;&#21518;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#19968;&#20010;&#26469;&#26367;&#25442;&#38169;&#35823;&#23383;&#31526;&#12290;&#22312;&#20844;&#20849;&#30340;AISHELL-1&#25968;&#25454;&#38598;&#21644;WenetSpeech&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error correction techniques have been used to refine the output sentences from automatic speech recognition (ASR) models and achieve a lower word error rate (WER). Previous works usually adopt end-to-end models and has strong dependency on Pseudo Paired Data and Original Paired Data. But when only pre-training on Pseudo Paired Data, previous models have negative effect on correction. While fine-tuning on Original Paired Data, the source side data must be transcribed by a well-trained ASR model, which takes a lot of time and not universal. In this paper, we propose UCorrect, an unsupervised Detector-Generator-Selector framework for ASR Error Correction. UCorrect has no dependency on the training data mentioned before. The whole procedure is first to detect whether the character is erroneous, then to generate some candidate characters and finally to select the most confident one to replace the error character. Experiments on the public AISHELL-1 dataset and WenetSpeech dataset show the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcEPT&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27010;&#24565;&#30693;&#35782;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;ConcEPT&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#26080;&#38656;&#36827;&#34892;&#23454;&#20307;&#38142;&#25509;&#25110;&#27010;&#24565;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2401.05669</link><description>&lt;p&gt;
ConcEPT: &#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#22686;&#24378;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ConcEPT: Concept-Enhanced Pre-Training for Language Models. (arXiv:2401.05669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcEPT&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27010;&#24565;&#30693;&#35782;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;ConcEPT&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#26080;&#38656;&#36827;&#34892;&#23454;&#20307;&#38142;&#25509;&#25110;&#27010;&#24565;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#26041;&#27861;&#20013;&#30427;&#34892;&#65292;&#32780;&#30693;&#35782;&#22686;&#24378;&#30340;PLMs&#36827;&#19968;&#27493;&#25552;&#20986;&#20197;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27010;&#24565;&#30693;&#35782;&#20316;&#20026;&#20154;&#31867;&#35748;&#30693;&#30340;&#19968;&#31181;&#22522;&#26412;&#30693;&#35782;&#65292;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#12290;&#36825;&#38480;&#21046;&#20102;PLMs&#22312;&#38656;&#35201;&#20154;&#31867;&#26679;&#30340;&#35748;&#30693;&#33021;&#21147;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#29702;&#35299;&#20855;&#26377;&#27010;&#24565;&#30340;&#23614;&#37096;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;ConcEPT&#65292;&#21363;&#27010;&#24565;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#27010;&#24565;&#30693;&#35782;&#34701;&#20837;PLMs&#20013;&#12290; ConcEPT&#21033;&#29992;&#22806;&#37096;&#20998;&#31867;&#27861;&#21644;&#23454;&#20307;&#27010;&#24565;&#39044;&#27979;&#65292;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#39044;&#27979;&#39044;&#35757;&#32451;&#19978;&#19979;&#25991;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30340;&#27010;&#24565;&#12290;&#19982;&#20197;&#21069;&#30340;&#27010;&#24565;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#65292;ConcEPT&#21487;&#20197;&#22312;&#27809;&#26377;&#23454;&#20307;&#38142;&#25509;&#25110;&#27010;&#24565;&#26144;&#23556;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ConcEPT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have been prevailing in state-of-the-art methods for natural language processing, and knowledge-enhanced PLMs are further proposed to promote model performance in knowledge-intensive tasks. However, conceptual knowledge, one essential kind of knowledge for human cognition, still remains understudied in this line of research. This limits PLMs' performance in scenarios requiring human-like cognition, such as understanding long-tail entities with concepts. In this paper, we propose ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies with entity concept prediction, a novel pre-training objective to predict the concepts of entities mentioned in the pre-training contexts. Unlike previous concept-enhanced methods, ConcEPT can be readily adapted to various downstream applications without entity linking or concept mapping. Results of extensive experiments sh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#35843;&#26597;&#33258;&#21160;&#21270;&#25991;&#31456;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20197;&#25581;&#31034;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.05655</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25991;&#31456;&#35780;&#20998;&#30340;&#26435;&#23041;&#35843;&#26597;&#65306;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability. (arXiv:2401.05655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05655
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#35843;&#26597;&#33258;&#21160;&#21270;&#25991;&#31456;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20197;&#25581;&#31034;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#31456;&#35780;&#20998;&#65288;AES&#65289;&#26159;&#19968;&#39033;&#25104;&#29087;&#30340;&#25945;&#32946;&#20219;&#21153;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35780;&#20272;&#23398;&#29983;&#25776;&#20889;&#30340;&#25991;&#31456;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#20570;&#20986;&#20102;&#35768;&#22810;&#21162;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#65288;i&#65289;&#25552;&#39640;AES&#27169;&#22411;&#23545;&#29305;&#23450;&#25552;&#31034;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#65288;&#21363;&#24320;&#21457;&#29305;&#23450;&#25552;&#31034;&#30340;&#27169;&#22411;&#65289;&#65292;&#36825;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#26469;&#33258;&#30456;&#21516;&#30446;&#26631;&#25552;&#31034;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#20351;&#29992;&#65307;&#25110;&#32773;&#65288;ii&#65289;&#35780;&#20272;&#38750;&#30446;&#26631;&#25552;&#31034;&#19978;&#24320;&#21457;&#30340;AES&#27169;&#22411;&#22312;&#39044;&#26399;&#30446;&#26631;&#25552;&#31034;&#19978;&#30340;&#36866;&#29992;&#24615;&#26041;&#38754;&#65288;&#21363;&#22312;&#20132;&#21449;&#25552;&#31034;&#29615;&#22659;&#20013;&#24320;&#21457;AES&#27169;&#22411;&#65289;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22266;&#26377;&#20559;&#35265;&#21450;&#20854;&#23545;&#36793;&#32536;&#21270;&#32676;&#20307;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#26377;&#24517;&#35201;&#35843;&#26597;&#24403;&#21069;AES&#26041;&#27861;&#26159;&#21542;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;&#30830;&#23450;&#21518;&#20102;&#35299;&#20854;&#22914;&#20309;&#24178;&#25200;AES&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;AES&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Essay Scoring (AES) is a well-established educational pursuit that employs machine learning to evaluate student-authored essays. While much effort has been made in this area, current research primarily focuses on either (i) boosting the predictive accuracy of an AES model for a specific prompt (i.e., developing prompt-specific models), which often heavily relies on the use of the labeled data from the same target prompt; or (ii) assessing the applicability of AES models developed on non-target prompts to the intended target prompt (i.e., developing the AES models in a cross-prompt setting). Given the inherent bias in machine learning and its potential impact on marginalized groups, it is imperative to investigate whether such bias exists in current AES methods and, if identified, how it intervenes with an AES model's accuracy and generalizability. Thus, our study aimed to uncover the intricate relationship between an AES model's accuracy, fairness, and generalizability, contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;AMIE&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#33258;&#21160;&#21270;&#21453;&#39304;&#26426;&#21046;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#31561;&#32500;&#24230;&#24615;&#33021;&#65292;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.05654</link><description>&lt;p&gt;
&#36808;&#21521;&#23545;&#35805;&#24335;&#35786;&#26029;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;AMIE&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#33258;&#21160;&#21270;&#21453;&#39304;&#26426;&#21046;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#31561;&#32500;&#24230;&#24615;&#33021;&#65292;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30340;&#26680;&#24515;&#22312;&#20110;&#21307;&#29983;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#29087;&#32451;&#30340;&#30149;&#21490;&#37319;&#38598;&#20026;&#20934;&#30830;&#30340;&#35786;&#26029;&#12289;&#26377;&#25928;&#30340;&#27835;&#30103;&#21644;&#25345;&#20037;&#30340;&#20449;&#20219;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#30103;&#30340;&#21487;&#21450;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25509;&#36817;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AMIE&#65288;Articulate Medical Intelligence Explorer&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20248;&#21270;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;AMIE&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#24182;&#24102;&#26377;&#33258;&#21160;&#21270;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#30142;&#30149;&#29366;&#20917;&#12289;&#19987;&#19994;&#39046;&#22495;&#21644;&#24773;&#22659;&#19979;&#23454;&#29616;&#23398;&#20064;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#20020;&#24202;&#26377;&#24847;&#20041;&#32500;&#24230;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#12290;&#25105;&#20204;&#23558;AMIE&#30340;&#24615;&#33021;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#65288;PCPs&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#20102;&#38543;&#26426;&#12289;&#21452;&#30450;&#21313;&#23383;&#35774;&#35745;&#30340;&#35797;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.  AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind cross
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Cherry&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#20986;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#20013;&#32570;&#22833;&#30340;&#37325;&#35201;&#38472;&#36848;&#26469;&#33258;&#21160;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25688;&#36873;&#38472;&#36848;&#12290;Cherry&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#26032;&#38395;&#25253;&#36947;&#20998;&#26512;&#26469;&#35782;&#21035;&#25688;&#36873;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.05650</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#25688;&#36873;
&lt;/p&gt;
&lt;p&gt;
On Detecting Cherry-picking in News Coverage Using Large Language Models. (arXiv:2401.05650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Cherry&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#20986;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#20013;&#32570;&#22833;&#30340;&#37325;&#35201;&#38472;&#36848;&#26469;&#33258;&#21160;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25688;&#36873;&#38472;&#36848;&#12290;Cherry&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#26032;&#38395;&#25253;&#36947;&#20998;&#26512;&#26469;&#35782;&#21035;&#25688;&#36873;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#36873;&#26159;&#25351;&#26377;&#24847;&#36873;&#25321;&#26377;&#21033;&#20110;&#29305;&#23450;&#35266;&#28857;&#30340;&#35777;&#25454;&#25110;&#20107;&#23454;&#65292;&#21516;&#26102;&#24573;&#35270;&#25110;&#25197;&#26354;&#25903;&#25345;&#30456;&#21453;&#35266;&#28857;&#30340;&#35777;&#25454;&#12290;&#22312;&#26032;&#38395;&#25253;&#36947;&#20013;&#25163;&#21160;&#35782;&#21035;&#25688;&#36873;&#38472;&#36848;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#30456;&#21453;&#35266;&#28857;&#30340;&#25253;&#36947;&#32570;&#22833;&#26102;&#12290;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Cherry&#65292;&#29992;&#20110;&#36890;&#36807;&#25214;&#20986;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#20013;&#32570;&#22833;&#30340;&#37325;&#35201;&#38472;&#36848;&#26469;&#33258;&#21160;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25688;&#36873;&#38472;&#36848;&#12290;Cherry&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#26032;&#38395;&#25253;&#36947;&#20998;&#26512;&#26469;&#35782;&#21035;&#25688;&#36873;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32771;&#34385;&#26469;&#33258;&#20854;&#20182;&#26032;&#38395;&#26469;&#28304;&#30340;&#35821;&#22659;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#38472;&#36848;&#23545;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#25152;&#28085;&#30422;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#25688;&#36873;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cherry-picking refers to the deliberate selection of evidence or facts that favor a particular viewpoint while ignoring or distorting evidence that supports an opposing perspective. Manually identifying instances of cherry-picked statements in news stories can be challenging, particularly when the opposing viewpoint's story is absent. This study introduces Cherry, an innovative approach for automatically detecting cherry-picked statements in news articles by finding missing important statements in the target news story. Cherry utilizes the analysis of news coverage from multiple sources to identify instances of cherry-picking. Our approach relies on language models that consider contextual information from other news sources to classify statements based on their importance to the event covered in the target news story. Furthermore, this research introduces a novel dataset specifically designed for cherry-picking detection, which was used to train and evaluate the performance of the mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38024;&#23545;&#26041;&#35328;&#30340;&#26041;&#27861;&#21644;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#26041;&#35328;&#23545;&#20110;NLP&#27169;&#22411;&#24615;&#33021;&#21644;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26041;&#35328;&#30456;&#20851;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.05632</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#26041;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing for Dialects of a Language: A Survey. (arXiv:2401.05632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38024;&#23545;&#26041;&#35328;&#30340;&#26041;&#27861;&#21644;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#26041;&#35328;&#23545;&#20110;NLP&#27169;&#22411;&#24615;&#33021;&#21644;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26041;&#35328;&#30456;&#20851;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#65306;&#35821;&#35328;&#26041;&#35328;&#12290;&#32771;&#34385;&#21040;&#38024;&#23545;&#26041;&#35328;&#25968;&#25454;&#38598;&#30340;NLP&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#21450;&#20854;&#23545;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26377;&#20851;&#26041;&#35328;NLP&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31867;&#21035;&#30340;&#35270;&#35282;&#25551;&#36848;&#20102;&#21508;&#31181;NLP&#20219;&#21153;&#65306;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65288;&#22914;&#26041;&#35328;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#35299;&#26512;&#21644;NLU&#22522;&#20934;&#27979;&#35797;&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65288;&#22914;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#23545;&#35805;&#31995;&#32479;&#65289;&#12290;&#36825;&#39033;&#35843;&#26597;&#36824;&#24191;&#27867;&#28085;&#30422;&#20102;&#33521;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#24503;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26377;&#20851;&#26041;&#35328;&#30340;&#36807;&#21435;NLP&#24037;&#20316;&#19981;&#27490;&#20110;&#26041;&#35328;&#20998;&#31867;&#65292;&#32780;&#26159;...
&lt;/p&gt;
&lt;p&gt;
State-of-the-art natural language processing (NLP) models are trained on massive training corpora, and report a superlative performance on evaluation datasets. This survey delves into an important attribute of these datasets: the dialect of a language. Motivated by the performance degradation of NLP models for dialectic datasets and its implications for the equity of language technologies, we survey past research in NLP for dialects in terms of datasets, and approaches. We describe a wide range of NLP tasks in terms of two categories: natural language understanding (NLU) (for tasks such as dialect classification, sentiment analysis, parsing, and NLU benchmarks) and natural language generation (NLG) (for summarisation, machine translation, and dialogue systems). The survey is also broad in its coverage of languages which include English, Arabic, German among others. We observe that past work in NLP concerning dialects goes deeper than mere dialect classification, and . This includes ear
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.05618</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#27714;&#35299;&#20013;&#65292;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;(CCoT)&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#26631;&#20934;&#30340;CoT&#21644;CCoT&#25552;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#31616;&#27905;&#24615;&#23545;&#22238;&#31572;&#38271;&#24230;&#21644;&#27491;&#30830;&#31572;&#26696;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;(MCQA)&#22522;&#20934;&#30340;&#35780;&#20272;&#12290;CCoT&#23558;GPT-3.5&#21644;GPT-4&#30340;&#24179;&#22343;&#22238;&#31572;&#38271;&#24230;&#20998;&#21035;&#20943;&#23569;&#20102;48.70&#65285;&#65292;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#65292;&#24102;&#26377;CCoT&#30340;GPT-3.5&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;27.69&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;CCoT&#23548;&#33268;&#27599;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#24179;&#22343;&#38477;&#20302;&#20102;22.67&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;CoT&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;LLM&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#30740;&#31350;LLM&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#24418;&#25104;&#34892;&#20026;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#24471;&#20986;&#20102;&#24494;&#35843;&#24615;&#33021;&#19982;&#36951;&#24536;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#21453;&#27604;&#32447;&#24615;&#20851;&#31995;&#30340;&#32467;&#35770;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31243;&#24230;&#38543;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#21644;&#26356;&#26032;&#27493;&#39588;&#25968;&#37327;&#21576;&#24130;&#24459;&#22686;&#38271;&#30340;&#32553;&#25918;&#35268;&#24459;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#25552;&#21069;&#20572;&#27490;&#24494;&#35843;&#25110;&#25913;&#21464;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#37117;&#26080;&#27861;&#36991;&#20813;&#36951;&#24536;&#65292;&#36825;&#20026;&#26410;&#26469;&#20943;&#36731;&#36951;&#24536;&#30340;&#24494;&#35843;&#26041;&#26696;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#20851;&#38190;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.05605</link><description>&lt;p&gt;
&#32553;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#36951;&#24536;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#24471;&#20986;&#20102;&#24494;&#35843;&#24615;&#33021;&#19982;&#36951;&#24536;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#21453;&#27604;&#32447;&#24615;&#20851;&#31995;&#30340;&#32467;&#35770;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31243;&#24230;&#38543;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#21644;&#26356;&#26032;&#27493;&#39588;&#25968;&#37327;&#21576;&#24130;&#24459;&#22686;&#38271;&#30340;&#32553;&#25918;&#35268;&#24459;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#25552;&#21069;&#20572;&#27490;&#24494;&#35843;&#25110;&#25913;&#21464;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#37117;&#26080;&#27861;&#36991;&#20813;&#36951;&#24536;&#65292;&#36825;&#20026;&#26410;&#26469;&#20943;&#36731;&#36951;&#24536;&#30340;&#24494;&#35843;&#26041;&#26696;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#20851;&#38190;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24182;&#37327;&#21270;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#26102;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#65288;&#22914;Low-Rank Adapters&#65289;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#20351;&#29992;Low-Rank Adapters&#36827;&#34892;LLMs&#24494;&#35843;&#26102;&#65292;&#24494;&#35843;&#24615;&#33021;&#19982;&#36951;&#24536;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#21453;&#27604;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#32553;&#25918;&#35268;&#24459;&#65292;&#34920;&#26126;&#36951;&#24536;&#31243;&#24230;&#38543;&#30528;&#24494;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#26356;&#26032;&#27493;&#39588;&#30340;&#25968;&#37327;&#21576;&#29616;&#20986;&#19968;&#31181;&#24179;&#31227;&#30340;&#24130;&#24459;&#22686;&#38271;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36951;&#24536;&#23545;Llama 2 7B&#32842;&#22825;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#38450;&#25252;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26080;&#27861;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#24494;&#35843;&#25110;&#25913;&#21464;&#24494;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20026;&#26410;&#26469;&#35780;&#20272;&#21644;&#24320;&#21457;&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#30340;&#24494;&#35843;&#26041;&#26696;&#24320;&#36767;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#20851;&#38190;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.05604</link><description>&lt;p&gt;
REBUS: &#19968;&#31181;&#23545;&#31526;&#21495;&#29702;&#35299;&#36827;&#34892;&#40065;&#26834;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;333&#20010;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#23383;&#28216;&#25103;&#31034;&#20363;&#65292;&#28085;&#30422;&#20102;&#30005;&#24433;&#12289;&#20316;&#26354;&#23478;&#12289;&#20027;&#35201;&#22478;&#24066;&#21644;&#39135;&#29289;&#31561;13&#20010;&#31867;&#21035;&#12290;&#20026;&#20102;&#22312;&#35782;&#21035;&#25552;&#31034;&#30340;&#35789;&#35821;&#25110;&#30701;&#35821;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#27169;&#22411;&#24517;&#39035;&#32467;&#21512;&#22270;&#20687;&#35782;&#21035;&#21644;&#23383;&#31526;&#20018;&#25805;&#20316;&#65292;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#12289;&#22810;&#27493;&#25512;&#29702;&#21644;&#23545;&#20154;&#31867;&#35748;&#30693;&#30340;&#29702;&#35299;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#33021;&#21147;&#21464;&#24471;&#22797;&#26434;&#32780;&#22810;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#22914;GPT-4V&#21644;Gemini Pro&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#21482;&#26377;24%&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#65292;&#31361;&#26174;&#20986;&#22312;&#25512;&#29702;&#26041;&#38754;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24456;&#23569;&#29702;&#35299;&#35868;&#39064;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20960;&#20046;&#24635;&#26159;&#26080;&#27861;&#20107;&#21518;&#35299;&#37322;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowle
&lt;/p&gt;</description></item><item><title>POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05596</link><description>&lt;p&gt;
POMP:&#29992;&#20110;&#20302;&#36164;&#28304;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;
&lt;/p&gt;
&lt;p&gt;
POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05596
&lt;/p&gt;
&lt;p&gt;
POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#19979;&#38754;&#20020;&#30528;&#22312;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#21253;&#25324;&#21453;&#21521;&#32763;&#35793;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#26530;&#36724;&#30340;&#32763;&#35793;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#21463;&#21040;&#21512;&#25104;&#25968;&#25454;&#22122;&#22768;&#12289;&#35821;&#35328;&#20559;&#24046;&#21644;&#38169;&#35823;&#20256;&#25773;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32531;&#35299;&#12290;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#25913;&#36827;&#20102;NMT&#65292;&#20294;&#26159;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#20351;&#24471;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36741;&#21161;&#35821;&#35328;&#20943;&#23569;&#35821;&#35328;&#22122;&#22768;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POMP&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#21160;&#24577;&#25277;&#26679;&#30340;&#22810;&#20010;&#36741;&#21161;&#35821;&#35328;&#30340;&#22270;&#24418;&#65292;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods. Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs). LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs. We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs. In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs. POMP inv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.05561</link><description>&lt;p&gt;
TrustLLM: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05561
&lt;/p&gt;
&lt;p&gt;
TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#22312;&#21487;&#20449;&#24615;&#26041;&#38754;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;LLMs&#30340;&#21487;&#20449;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustLLM&#65292;&#23427;&#26159;&#23545;LLMs&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;&#24615;&#21407;&#21017;&#12289;&#24314;&#31435;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#20998;&#26512;&#20027;&#27969;LLMs&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#28085;&#30422;&#20843;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;LLMs&#21407;&#21017;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#20845;&#20010;&#32500;&#24230;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#26426;&#22120;&#20262;&#29702;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;TrustLLM&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#35780;&#20272;16&#20010;&#20027;&#27969;LLMs&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#38169;&#35823;&#22914;&#20309;&#24433;&#21709;&#30196;&#21574;&#30151;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#23436;&#32654;&#30340;ASR&#29983;&#25104;&#30340;&#36716;&#24405;&#22312;&#21306;&#20998;AD&#21644;&#20581;&#24247;&#20010;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#25163;&#21160;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2401.05551</link><description>&lt;p&gt;
&#26377;&#29992;&#30340;&#38169;&#35823;&#65306;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#33021;&#25552;&#39640;&#19979;&#28216;&#30196;&#21574;&#30151;&#20998;&#31867;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Useful Blunders: Can Automated Speech Recognition Errors Improve Downstream Dementia Classification?. (arXiv:2401.05551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#38169;&#35823;&#22914;&#20309;&#24433;&#21709;&#30196;&#21574;&#30151;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#23436;&#32654;&#30340;ASR&#29983;&#25104;&#30340;&#36716;&#24405;&#22312;&#21306;&#20998;AD&#21644;&#20581;&#24247;&#20010;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#25163;&#21160;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#26088;&#22312;&#30740;&#31350;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#38169;&#35823;&#22914;&#20309;&#24433;&#21709;&#30196;&#21574;&#30151;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#8220;&#39292;&#24178;&#20599;&#31363;&#8221;&#22270;&#29255;&#25551;&#36848;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#19981;&#23436;&#32654;&#30340;ASR&#29983;&#25104;&#30340;&#36716;&#24405;&#26159;&#21542;&#21487;&#20197;&#20026;&#21306;&#20998;&#35748;&#30693;&#20581;&#24247;&#20010;&#20307;&#21644;&#24739;&#26377;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#35821;&#35328;&#26679;&#26412;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;ASR&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#21518;&#26399;&#32534;&#36753;&#25216;&#26415;&#25913;&#36827;&#23427;&#20204;&#30340;&#36716;&#24405;&#12290;&#36825;&#20123;&#19981;&#23436;&#32654;&#30340;ASR&#36716;&#24405;&#21644;&#25163;&#21160;&#36716;&#24405;&#20316;&#20026;&#19979;&#28216;&#30196;&#21574;&#30151;&#20998;&#31867;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#24182;&#35780;&#20272;ASR&#29983;&#25104;&#30340;&#36716;&#24405;&#22312;&#30196;&#21574;&#30151;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19981;&#23436;&#32654;&#30340;ASR&#29983;&#25104;&#30340;&#36716;&#24405;&#22312;&#21306;&#20998;&#24739;&#26377;AD&#30340;&#20010;&#20307;&#21644;&#20581;&#24247;&#20010;&#20307;&#20043;&#38388;&#30340;&#34920;&#29616;&#20248;&#20110;&#25163;&#21160;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textbf{Objectives}: We aimed to investigate how errors from automatic speech recognition (ASR) systems affect dementia classification accuracy, specifically in the ``Cookie Theft'' picture description task. We aimed to assess whether imperfect ASR-generated transcripts could provide valuable information for distinguishing between language samples from cognitively healthy individuals and those with Alzheimer's disease (AD).  \textbf{Methods}: We conducted experiments using various ASR models, refining their transcripts with post-editing techniques. Both these imperfect ASR transcripts and manually transcribed ones were used as inputs for the downstream dementia classification. We conducted comprehensive error analysis to compare model performance and assess ASR-generated transcript effectiveness in dementia classification.  \textbf{Results}: Imperfect ASR-generated transcripts surprisingly outperformed manual transcription for distinguishing between individuals with AD and those withou
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#24052;&#35199;Rio Grande do Sul&#65288;RS&#65289;&#22320;&#21306;&#30340;&#25991;&#21270;&#36951;&#20135;&#36827;&#34892;&#24494;&#35843;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#21644;&#20445;&#25252;&#22810;&#26679;&#21270;&#30340;&#29420;&#29305;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05520</link><description>&lt;p&gt;
&#20174;&#21335;&#32654;&#22823;&#33609;&#21407;&#21040;&#20687;&#32032;&#65306;&#23545;Ga\'ucho&#25991;&#21270;&#36951;&#20135;&#36827;&#34892;&#24494;&#35843;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\'ucho Heritage. (arXiv:2401.05520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#24052;&#35199;Rio Grande do Sul&#65288;RS&#65289;&#22320;&#21306;&#30340;&#25991;&#21270;&#36951;&#20135;&#36827;&#34892;&#24494;&#35843;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#21644;&#20445;&#25252;&#22810;&#26679;&#21270;&#30340;&#29420;&#29305;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#39046;&#22495;&#65292;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#23637;&#31034;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35270;&#35273;&#20869;&#23481;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LDMs&#22312;&#20195;&#34920;&#26412;&#22303;&#25991;&#21270;&#27010;&#24565;&#12289;&#21382;&#21490;&#20154;&#29289;&#21644;&#28626;&#21361;&#29289;&#31181;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20197;&#24052;&#35199;&#21335;&#37096;&#22320;&#21306;Rio Grande do Sul&#65288;RS&#65289;&#30340;&#25991;&#21270;&#36951;&#20135;&#20026;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#20102;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#25429;&#25417;&#21644;&#20445;&#25252;&#22320;&#21306;&#30340;&#25991;&#21270;&#21644;&#21382;&#21490;&#36523;&#20221;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#20027;&#39064;&#36873;&#25321;&#12289;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#24494;&#35843;&#36807;&#31243;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#27599;&#20010;&#27010;&#24565;&#30340;&#25361;&#25112;&#21644;&#21487;&#34892;&#24615;&#12290;&#24635;&#20043;&#65292;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#21644;&#20445;&#25252;&#22810;&#26679;&#21270;&#30340;&#29420;&#29305;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has become pervasive in society, witnessing significant advancements in various domains. Particularly in the realm of Text-to-Image (TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities in generating visual content based on textual prompts. This paper addresses the potential of LDMs in representing local cultural concepts, historical figures, and endangered species. In this study, we use the cultural heritage of Rio Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to contribute to the broader understanding of how generative models can help to capture and preserve the cultural and historical identity of regions. The paper outlines the methodology, including subject selection, dataset creation, and the fine-tuning process. The results showcase the images generated, alongside the challenges and feasibility of each concept. In conclusion, this work shows the power of these models to represent and preserve unique aspects of diverse
&lt;/p&gt;</description></item><item><title>InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05507</link><description>&lt;p&gt;
InfiAgent-DABench: &#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#35780;&#20272;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05507
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"InfiAgent-DABench"&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;DAEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;55&#20010;CSV&#25991;&#20214;&#34893;&#29983;&#20986;&#30340;311&#20010;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;&#30340;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26684;&#24335;&#25552;&#31034;&#25216;&#26415;&#65292;&#30830;&#20445;&#38382;&#39064;&#26159;&#38381;&#21512;&#24418;&#24335;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#24403;&#21069;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;DAAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19987;&#38376;&#20195;&#29702;&#12290;InfiAgent-DABench&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/InfiAgent/InfiAgent&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
&lt;/p&gt;</description></item><item><title>LLM4PLC&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21644;&#22806;&#37096;&#39564;&#35777;&#24037;&#20855;&#65292;&#22914;&#35821;&#27861;&#26816;&#26597;&#22120;&#12289;&#32534;&#35793;&#22120;&#21644;SMV&#39564;&#35777;&#22120;&#65292;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05443</link><description>&lt;p&gt;
LLM4PLC&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#36827;&#34892;&#21487;&#39564;&#35777;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems. (arXiv:2401.05443v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05443
&lt;/p&gt;
&lt;p&gt;
LLM4PLC&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21644;&#22806;&#37096;&#39564;&#35777;&#24037;&#20855;&#65292;&#22914;&#35821;&#27861;&#26816;&#26597;&#22120;&#12289;&#32534;&#35793;&#22120;&#21644;SMV&#39564;&#35777;&#22120;&#65292;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#26159;&#27809;&#26377;&#32570;&#28857;&#12290;&#30456;&#20851;&#38382;&#39064;&#20027;&#35201;&#19982;&#29983;&#25104;&#30340;&#20195;&#30721;&#32570;&#20047;&#25191;&#34892;&#20445;&#35777;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23545;&#24517;&#35201;&#20294;&#23574;&#31471;&#32534;&#31243;&#35821;&#35328;&#30340;&#25903;&#25345;&#19981;&#36275;&#26377;&#20851;&#12290;&#30446;&#21069;&#30340;LLMs&#22914;GPT-4&#21644;LLaMa2&#26080;&#27861;&#20026;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#25805;&#20316;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#65288;ICS&#65289;&#29983;&#25104;&#26377;&#25928;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4PLC&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#24341;&#23548;&#30340;&#36845;&#20195;&#27969;&#31243;&#65292;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21644;&#22806;&#37096;&#39564;&#35777;&#24037;&#20855;&#65288;&#21253;&#25324;&#35821;&#27861;&#26816;&#26597;&#22120;&#12289;&#32534;&#35793;&#22120;&#21644;SMV&#39564;&#35777;&#22120;&#65289;&#26469;&#25351;&#23548;LLM&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24494;&#35843;&#65288;LoRAs&#30340;&#21019;&#24314;&#21644;&#20351;&#29992;&#65289;&#26469;&#22686;&#24378;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;FischerTechnik&#21046;&#36896;&#27979;&#35797;&#24202;&#65288;MFTB&#65289;&#39564;&#35777;&#20102;&#36825;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#20174;&#29983;&#25104;&#32467;&#26500;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#28436;&#21464;&#20026;&#29983;&#25104;&#26377;&#25928;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools including grammar checkers, compilers and SMV verifiers to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally flawed code to producin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;DeBERTa&#27169;&#22411;&#65292;&#32467;&#21512;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#24230;&#37327;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#33258;&#21160;&#35780;&#20998;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.05433</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#26435;&#37325;&#25200;&#21160;&#21644;&#24230;&#37327;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#22686;&#24378;&#35770;&#25991;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling. (arXiv:2401.05433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;DeBERTa&#27169;&#22411;&#65292;&#32467;&#21512;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#24230;&#37327;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#33258;&#21160;&#35780;&#20998;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#25216;&#26415;&#65288;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25945;&#32946;&#25968;&#25454;&#20998;&#26512;&#65289;&#26469;&#25913;&#36827;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#65288;ELLs&#65289;&#35774;&#35745;&#30340;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#12290;&#33258;&#21160;&#35770;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#30740;&#31350;&#22312;&#35780;&#20272;&#20889;&#20316;&#35770;&#25991;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#35821;&#35328;&#21457;&#23637;&#26041;&#38754;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#19982;BERT&#30456;&#20851;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;AES&#20013;&#23545;ELLs&#20889;&#20316;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#28385;&#36275;ELLs&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;DeBERTa&#65292;&#36825;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#12290;DeBERTa&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#36890;&#29992;&#35821;&#35328;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20960;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs' writing proficiency within AES.  To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#23545;&#20195;&#30721;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#27604;&#36739;&#23398;&#29983;&#30340;&#35299;&#37322;&#21644;&#19987;&#23478;&#35299;&#37322;&#26041;&#38754;&#20855;&#26377;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.05399</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Automated Assessment of Students' Code Comprehension using LLMs. (arXiv:2401.05399v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#23545;&#20195;&#30721;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#27604;&#36739;&#23398;&#29983;&#30340;&#35299;&#37322;&#21644;&#19987;&#23478;&#35299;&#37322;&#26041;&#38754;&#20855;&#26377;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#35780;&#20272;&#23398;&#29983;&#31572;&#26696;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#31572;&#26696;&#65292;&#26159;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;LLMs&#22312;&#33258;&#21160;&#31572;&#26696;&#35780;&#20272;&#39046;&#22495;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#36235;&#21183;&#20013;&#65292;&#23545;LLMs&#30340;&#35780;&#20272;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#26469;&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#31616;&#30701;&#21644;&#24320;&#25918;&#24615;&#22238;&#31572;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#27604;&#36739;&#23398;&#29983;&#23545;&#35745;&#31639;&#26426;&#31243;&#24207;&#36880;&#34892;&#35299;&#37322;&#30340;&#35299;&#37322;&#19982;&#19987;&#23478;&#35299;&#37322;&#12290;&#20026;&#20102;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;&#35780;&#20272;&#23398;&#29983;&#23545;&#35745;&#31639;&#26426;&#20195;&#30721;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;(STS)&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#25552;&#31034;&#23398;&#29983;&#35299;&#37322;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#26102;&#21487;&#20197;&#36215;&#21040;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing student's answers and in particular natural language answers is a crucial challenge in the field of education. Advances in machine learning, including transformer-based models such as Large Language Models(LLMs), have led to significant progress in various natural language tasks. Nevertheless, amidst the growing trend of evaluating LLMs across diverse tasks, evaluating LLMs in the realm of automated answer assesment has not received much attention. To address this gap, we explore the potential of using LLMs for automated assessment of student's short and open-ended answer. Particularly, we use LLMs to compare students' explanations with expert explanations in the context of line-by-line explanations of computer programs.  For comparison purposes, we assess both Large Language Models (LLMs) and encoder-based Semantic Textual Similarity (STS) models in the context of assessing the correctness of students' explanation of computer code. Our findings indicate that LLMs, when promp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05254</link><description>&lt;p&gt;
&#20013;&#32654;&#20004;&#22269;&#20043;&#38388;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#34920;&#36798;&#30340;&#20215;&#20540;&#21644;&#28608;&#21160;&#23545;&#27604;&#65306;&#19968;&#20010;&#36328;&#25991;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20307;&#30340;&#24773;&#24863;&#34920;&#36798;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35199;&#26041;&#29615;&#22659;&#20013;&#12290;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#23384;&#22312;&#30528;&#24341;&#21457;&#24773;&#24863;&#34920;&#36798;&#30340;&#37325;&#35201;&#24046;&#24322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32654;&#22269;Twitter&#21644;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#19978;&#30340;&#20004;&#20010;&#20027;&#35201;&#24773;&#24863;&#32500;&#24230;&#65288;&#20215;&#20540;&#21644;&#28608;&#21160;&#65289;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20010;&#20307;&#20043;&#38388;&#30340;&#28608;&#21160;&#21644;&#20215;&#20540;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#20869;&#23481;&#19978;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24179;&#21488;&#19978;&#30340;&#35789;&#35821;&#20351;&#29992;&#21644;&#35805;&#39064;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#20197;&#35299;&#35835;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;Twitter&#29992;&#25143;&#26469;&#35828;&#65292;&#36127;&#38754;&#24773;&#32490;&#21644;&#27491;&#38754;&#24773;&#32490;&#20043;&#38388;&#30340;&#24773;&#24863;&#24378;&#24230;&#21464;&#21270;&#19981;&#22826;&#26126;&#26174;&#65292;&#32780;&#23545;&#20110;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#26469;&#35828;&#65292;&#20276;&#38543;&#30528;&#24773;&#24863;&#30340;&#19978;&#21319;&#65292;&#28608;&#21160;&#31243;&#24230;&#26377;&#26356;&#26126;&#26174;&#30340;&#21319;&#32423;&#12290;&#20174;&#35821;&#35328;&#29305;&#24449;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressio
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04621</link><description>&lt;p&gt;
DebugBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32534;&#31243;&#33021;&#21147;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20043;&#21069;&#23545;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#35780;&#20272;&#21463;&#21040;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#27979;&#35797;&#28431;&#27934;&#31181;&#31867;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DebugBench&#8221;&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#21253;&#21547;4253&#20010;&#23454;&#20363;&#12290;&#23427;&#28085;&#30422;&#20102;C ++&#65292;Java&#21644;Python&#20013;&#22235;&#20010;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#21035;&#21644;18&#20010;&#27425;&#35201;&#31867;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;DebugBench&#65292;&#25105;&#20204;&#20174;LeetCode&#31038;&#21306;&#25910;&#38598;&#20102;&#20195;&#30721;&#29255;&#27573;&#65292;&#20351;&#29992;GPT-4&#21521;&#28304;&#25968;&#25454;&#20013;&#27880;&#20837;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#20004;&#20010;&#21830;&#19994;&#27169;&#22411;&#21644;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#22914;Code Llama&#26080;&#27861;&#36798;&#21040;&#20219;&#20309;&#21512;&#26684;&#29575;&#65307;&#65288;2&#65289;t
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#36890;&#36807;&#23545;&#36817;&#24180;&#26469;&#21457;&#34920;&#30340;3140&#31687;&#30740;&#31350;&#35770;&#25991;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#35745;&#31639;&#26041;&#27861;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#24341;&#20837;&#23186;&#20307;&#20559;&#35265;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#29366;&#20917;&#30340;&#27010;&#36848;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;transformer&#30340;&#20998;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#33021;&#26816;&#27979;&#26356;&#32454;&#31890;&#24230;&#30340;&#20559;&#35265;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2312.16148</link><description>&lt;p&gt;
&#23186;&#20307;&#20559;&#35265;&#20998;&#31867;&#65306;&#23545;&#23186;&#20307;&#20559;&#35265;&#24418;&#24335;&#21644;&#33258;&#21160;&#26816;&#27979;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias. (arXiv:2312.16148v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#36890;&#36807;&#23545;&#36817;&#24180;&#26469;&#21457;&#34920;&#30340;3140&#31687;&#30740;&#31350;&#35770;&#25991;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#35745;&#31639;&#26041;&#27861;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#24341;&#20837;&#23186;&#20307;&#20559;&#35265;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#29366;&#20917;&#30340;&#27010;&#36848;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;transformer&#30340;&#20998;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#33021;&#26816;&#27979;&#26356;&#32454;&#31890;&#24230;&#30340;&#20559;&#35265;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#21576;&#29616;&#20107;&#20214;&#30340;&#26041;&#24335;&#20250;&#26174;&#33879;&#24433;&#21709;&#20844;&#20247;&#30340;&#30475;&#27861;&#21644;&#20449;&#24565;&#12290;&#23186;&#20307;&#20559;&#35265;&#25551;&#36848;&#20102;&#23545;&#26576;&#19968;&#20027;&#39064;&#30340;&#21333;&#36793;&#25110;&#26497;&#31471;&#31435;&#22330;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#22238;&#39038;2019&#24180;&#33267;2022&#24180;&#38388;&#21457;&#34920;&#30340;3140&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#35745;&#31639;&#26041;&#27861;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26500;&#24314;&#25105;&#20204;&#30340;&#32508;&#36848;&#24182;&#25903;&#25345;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#23545;&#20559;&#35265;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23186;&#20307;&#20559;&#35265;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#29366;&#20917;&#30340;&#19968;&#33268;&#24615;&#27010;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26159;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;transformer&#30340;&#20998;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36825;&#20123;&#25913;&#36827;&#21253;&#25324;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#33021;&#22815;&#26816;&#27979;&#21040;&#26356;&#32454;&#31890;&#24230;&#30340;&#20559;&#35265;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#39033;&#30446;&#32570;&#20047;&#36328;&#23398;&#31185;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#22810;&#30340;
&lt;/p&gt;
&lt;p&gt;
The way the media presents events can significantly affect public perception, which in turn can alter people's beliefs and views. Media bias describes a one-sided or polarizing perspective on a topic. This article summarizes the research on computational methods to detect media bias by systematically reviewing 3140 research papers published between 2019 and 2022. To structure our review and support a mutual understanding of bias across research domains, we introduce the Media Bias Taxonomy, which provides a coherent overview of the current state of research on media bias from different perspectives. We show that media bias detection is a highly active research field, in which transformer-based classification approaches have led to significant improvements in recent years. These improvements include higher classification accuracy and the ability to detect more fine-granular types of bias. However, we have identified a lack of interdisciplinarity in existing projects, and a need for more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.14187</link><description>&lt;p&gt;
WaveCoder: &#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#19982;&#23436;&#21892;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23545;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#21518;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#32463;&#24120;&#20250;&#20135;&#29983;&#37325;&#22797;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#25511;&#21046;&#19981;&#22815;&#28789;&#27963;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#20026;4&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#22120;-&#21028;&#21035;&#22120;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#20174;&#24320;&#28304;&#20195;&#30721;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeOcean&#65292;&#19968;&#20010;&#21253;&#21547;4&#20010;&#36890;&#29992;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#12289;&#20849;&#35745;20,000&#20010;&#25351;&#20196;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#25351;&#20196;&#35843;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#30340;Code LLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#30340;Kaldi-NL&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24037;&#20855;&#36827;&#34892;&#33258;&#21160;&#21270;&#25968;&#23383;&#22312;&#22122;&#22768;&#20013;&#65288;DIN&#65289;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#32773;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#34987;&#27979;&#32773;&#30340;&#21475;&#35821;&#22238;&#31572;&#12290;&#30740;&#31350;&#21457;&#29616;Kaldi-NL&#22312;&#35782;&#21035;&#21644;&#35299;&#30721;&#22238;&#31572;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2312.12269</link><description>&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#21548;&#21147;&#27979;&#35797;&#65306;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#24320;&#28304;&#39044;&#35757;&#32451;&#30340;Kaldi-NL&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#23454;&#29616;&#65311;&#65288;arXiv&#65306;2312.12269v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Automated speech audiometry: Can it work using open-source pre-trained Kaldi-NL automatic speech recognition?. (arXiv:2312.12269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#30340;Kaldi-NL&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24037;&#20855;&#36827;&#34892;&#33258;&#21160;&#21270;&#25968;&#23383;&#22312;&#22122;&#22768;&#20013;&#65288;DIN&#65289;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#32773;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#34987;&#27979;&#32773;&#30340;&#21475;&#35821;&#22238;&#31572;&#12290;&#30740;&#31350;&#21457;&#29616;Kaldi-NL&#22312;&#35782;&#21035;&#21644;&#35299;&#30721;&#22238;&#31572;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22312;&#22122;&#22768;&#20013;&#65288;DIN&#65289;&#27979;&#35797;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#35821;&#38899;&#21548;&#21147;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#31579;&#26597;&#19981;&#21516;&#24180;&#40836;&#21644;&#21548;&#21147;&#29366;&#20917;&#30340;&#20154;&#32676;&#12290;&#36890;&#24120;&#30001;&#20154;&#31867;&#30417;&#30563;&#32773;&#65288;&#20363;&#22914;&#21307;&#25252;&#20154;&#21592;&#65289;&#36827;&#34892;&#27979;&#35797;&#65292;&#35780;&#20998;&#34987;&#27979;&#32773;&#30340;&#22238;&#31572;&#65292;&#25110;&#22312;&#32447;&#27979;&#35797;&#65292;&#22312;&#32447;&#36719;&#20214;&#35780;&#20998;&#34987;&#27979;&#32773;&#36755;&#20837;&#30340;&#22238;&#31572;&#12290;&#27979;&#35797;&#37319;&#29992;24&#20010;&#25968;&#23383;&#19977;&#20803;&#32452;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#19979;&#38477;&#31243;&#24207;&#36827;&#34892;&#21576;&#29616;&#65292;&#24471;&#20986;&#35821;&#38899;&#25509;&#25910;&#38408;&#20540;&#65288;SRT&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#33258;&#21160;&#21270;DIN&#27979;&#35797;&#35774;&#32622;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#32773;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21475;&#35821;&#22238;&#31572;&#65292;&#20351;&#29992;&#24320;&#28304;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24037;&#20855;Kaldi-NL&#12290;30&#20301;&#33258;&#25105;&#25253;&#21578;&#27491;&#24120;&#21548;&#35273;&#30340;&#33655;&#20848;&#25104;&#24180;&#20154;&#65288;19-64&#23681;&#65289;&#23436;&#25104;&#20102;&#19968;&#20010;DIN+Kaldi-NL&#27979;&#35797;&#12290;&#20182;&#20204;&#30340;&#21475;&#35821;&#22238;&#31572;&#34987;&#35760;&#24405;&#19979;&#26469;&#65292;&#24182;&#29992;&#20110;&#35780;&#20272;Kaldi-NL&#35299;&#30721;&#22238;&#31572;&#30340;&#36716;&#24405;&#32467;&#26524;&#12290;&#30740;&#31350;1&#36890;&#36807;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#21644;&#20165;&#32771;&#34385;&#24635;&#21644;&#35299;&#30721;&#38169;&#35823;&#30340;&#30334;&#20998;&#27604;&#26469;&#35780;&#20272;Kaldi-NL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A practical speech audiometry tool is the digits-in-noise (DIN) test for hearing screening of populations of varying ages and hearing status. The test is usually conducted by a human supervisor (e.g., clinician), who scores the responses spoken by the listener, or online, where a software scores the responses entered by the listener. The test has 24 digit-triplets presented in an adaptive staircase procedure, resulting in a speech reception threshold (SRT). We propose an alternative automated DIN test setup that can evaluate spoken responses whilst conducted without a human supervisor, using the open-source automatic speech recognition toolkit, Kaldi-NL. Thirty self-reported normal-hearing Dutch adults (19-64 years) completed one DIN+Kaldi-NL test. Their spoken responses were recorded, and used for evaluating the transcript of decoded responses by Kaldi-NL. Study 1 evaluated the Kaldi-NL performance through its word error rate (WER), percentage of summed decoding errors regarding only 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2312.10813</link><description>&lt;p&gt;
&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65306;&#22312;0.5K&#21442;&#25968;&#20869;&#25512;&#24191;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#35843;&#20248;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#35843;&#25972;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#20923;&#32467;&#39592;&#24178;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#21482;&#35774;&#35745;&#21644;&#35843;&#25972;&#25552;&#31034;&#12290;&#19968;&#26041;&#38754;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#31934;&#24515;&#35774;&#35745;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#26356;&#26032;&#35268;&#21017;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#28436;&#21464;&#27169;&#24335;&#19982;&#36866;&#24212;&#36807;&#31243;&#20013;&#25552;&#31034;&#30697;&#38453;&#31209;&#21464;&#21270;&#36235;&#21183;&#30340;&#35843;&#21644;&#19968;&#33268;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22823;&#22823;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large pre-trained vision-language models, how to effectively transfer the knowledge of such foundational models to downstream tasks becomes a hot topic, especially in a data-deficient scenario. Recently, prompt tuning has become a popular solution. When adapting the vision-language models, researchers freeze the parameters in the backbone and only design and tune the prompts. On the one hand, the delicate design of prompt tuning exhibits strong performance. On the other hand, complicated structures and update rules largely increase the computation and storage cost. Motivated by the observation that the evolution pattern of the generalization capability in visual-language models aligns harmoniously with the trend of rank variations in the prompt matrix during adaptation, we design a new type of prompt, Re-parameterized Low-rank Prompt (RLP), for both efficient and effective adaptation. Our method could largely reduce the number of tunable parameters and storage s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VTG&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#26377;&#36827;&#21270;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#21487;&#39564;&#35777;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#24341;&#20837;&#36827;&#21270;&#22411;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#20004;&#23618;&#39564;&#35777;&#22120;&#65292;VTG&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#20449;&#24687;&#38169;&#35823;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.09075</link><description>&lt;p&gt;
&#23454;&#29616;&#20855;&#26377;&#36827;&#21270;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#21487;&#39564;&#35777;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Verifiable Text Generation with Evolving Memory and Self-Reflection. (arXiv:2312.09075v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VTG&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#26377;&#36827;&#21270;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#21487;&#39564;&#35777;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#24341;&#20837;&#36827;&#21270;&#22411;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#20004;&#23618;&#39564;&#35777;&#22120;&#65292;VTG&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#20449;&#24687;&#38169;&#35823;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#20063;&#34987;&#31216;&#20026;&#24187;&#35273;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21487;&#39564;&#35777;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#23427;&#20419;&#20351;LLMs&#29983;&#25104;&#20855;&#26377;&#24341;&#29992;&#20197;&#36827;&#34892;&#20934;&#30830;&#24615;&#39564;&#35777;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#21487;&#39564;&#35777;&#30340;&#25991;&#26412;&#29983;&#25104;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;&#23384;&#22312;&#28966;&#28857;&#36716;&#31227;&#29616;&#35937;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#26469;&#19982;&#27491;&#30830;&#30340;&#24341;&#25991;&#23545;&#40784;&#65292;&#32780;&#19988;&#22312;&#26816;&#32034;&#25991;&#26723;&#30340;&#31934;&#30830;&#24615;&#21644;&#24191;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#20004;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20855;&#26377;&#36827;&#21270;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#21487;&#39564;&#35777;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;VTG&#12290;VTG&#24341;&#20837;&#20102;&#36827;&#21270;&#22411;&#38271;&#30701;&#26399;&#35760;&#24518;&#20197;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#25991;&#26723;&#21644;&#26368;&#36817;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#22791;&#35777;&#25454;&#21457;&#29616;&#22120;&#30340;&#20004;&#23618;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#37325;&#26032;&#24605;&#32771;&#21644;&#21453;&#24605;&#20027;&#24352;&#19982;&#24341;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#26816;&#32034;&#21644;&#22810;&#26679;&#21270;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#35770;&#35777;&#30340;&#36136;&#37327;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#25193;&#23637;Whisper&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#29305;&#23450;&#35828;&#35805;&#32773;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#21482;&#20351;&#29992;&#32422;1%&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20840;&#35757;&#32451;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08079</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#25193;&#23637;Whisper&#20197;&#38024;&#23545;&#29305;&#23450;&#35828;&#35805;&#32773;&#30340;ASR
&lt;/p&gt;
&lt;p&gt;
Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#25193;&#23637;Whisper&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#29305;&#23450;&#35828;&#35805;&#32773;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#21482;&#20351;&#29992;&#32422;1%&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20840;&#35757;&#32451;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26088;&#22312;&#20174;&#22810;&#20010;&#35828;&#35805;&#32773;&#37325;&#21472;&#30340;&#35805;&#35821;&#20013;&#36716;&#24405;&#29305;&#23450;&#35828;&#35805;&#32773;&#30340;&#25152;&#38656;&#35821;&#38899;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;ASR&#65288;TS-ASR&#65289;&#26041;&#27861;&#35201;&#20040;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#35201;&#20040;&#23436;&#20840;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#65288;prompt tuning&#65289;&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;Whisper&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#21333;&#35828;&#35805;&#32773;ASR&#27169;&#22411;&#65292;&#25193;&#23637;&#21040;TS-ASR&#12290;&#23545;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21450;&#20854;&#37197;&#32622;&#30340;&#21464;&#20307;&#36827;&#34892;&#25506;&#32034;&#21644;&#20248;&#21270;&#20197;&#36866;&#29992;&#20110;TS-ASR&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#35843;&#25972;&#22312;&#21482;&#38656;&#22823;&#32422;1\%&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20840;&#35757;&#32451;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21407;&#26469;Whisper&#30340;&#29305;&#24615;&#65288;&#22914;&#36870;&#25991;&#26412;&#24402;&#19968;&#21270;&#21644;&#26102;&#38388;&#25139;&#26631;&#35760;&#65289;&#22312;&#30446;&#26631;&#35828;&#35805;&#32773;ASR&#20013;&#20445;&#30041;&#19979;&#26469;&#65292;&#20445;&#25345;&#20102;&#29983;&#25104;&#30340;&#36716;&#24405;&#30340;&#33258;&#28982;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target-speaker automatic speech recognition (ASR) aims to transcribe the desired speech of a target speaker from multi-talker overlapped utterances. Most of the existing target-speaker ASR (TS-ASR) methods involve either training from scratch or fully fine-tuning a pre-trained model, leading to significant training costs and becoming inapplicable to large foundation models. This work leverages prompt tuning, a parameter-efficient fine-tuning approach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR. Variants of prompt tuning approaches along with their configurations are explored and optimized for TS-ASR.Experimental results show that prompt tuning can achieve performance comparable to state-of-the-art full training approaches while only requiring about 1\% of task-specific model parameters. Notably, the original Whisper's features, such as inverse text normalization and timestamp tagging, are retained in target-speaker ASR, keeping the generated transcriptions natu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;NLP&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#32423;&#36827;&#34892;&#25805;&#20316;&#65292;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#25442;&#26469;&#28040;&#38500;&#38544;&#21547;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.06499</link><description>&lt;p&gt;
TaCo&#65306;&#36890;&#36807;&#20449;&#24687;&#35770;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;NLP&#20013;&#30340;&#36755;&#20986;&#23884;&#20837;&#20013;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#27010;&#24565;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability. (arXiv:2312.06499v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;NLP&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#32423;&#36827;&#34892;&#25805;&#20316;&#65292;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#25442;&#26469;&#28040;&#38500;&#38544;&#21547;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20449;&#24687;&#35770;&#34920;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#27169;&#22411;&#19981;&#24212;&#33021;&#22815;&#39044;&#27979;&#25935;&#24863;&#21464;&#37327;&#65292;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#20123;&#21464;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#36890;&#24120;&#20197;&#38544;&#24335;&#30340;&#26041;&#24335;&#20986;&#29616;&#22312;&#35821;&#35328;&#20013;&#65292;&#36825;&#32473;&#35782;&#21035;&#21644;&#20943;&#23569;&#20559;&#35265;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;NLP&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#32423;&#19978;&#25805;&#20316;&#65292;&#29420;&#31435;&#20110;&#20855;&#20307;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#36817;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24182;&#37319;&#29992;&#23884;&#20837;&#36716;&#25442;&#26469;&#28040;&#38500;&#36873;&#23450;&#21464;&#37327;&#20013;&#30340;&#38544;&#24335;&#20449;&#24687;&#12290;&#36890;&#36807;&#30452;&#25509;&#25805;&#32437;&#26368;&#21518;&#19968;&#23618;&#30340;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#27169;&#22411;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#37325;&#22823;&#20462;&#25913;&#25110;&#37325;&#35757;&#32451;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#21518;&#22788;&#29702;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness of Natural Language Processing (NLP) models has emerged as a crucial concern. Information theory indicates that to achieve fairness, a model should not be able to predict sensitive variables, such as gender, ethnicity, and age. However, information related to these variables often appears implicitly in language, posing a challenge in identifying and mitigating biases effectively. To tackle this issue, we present a novel approach that operates at the embedding level of an NLP model, independent of the specific architecture. Our method leverages insights from recent advances in XAI techniques and employs an embedding transformation to eliminate implicit information from a selected variable. By directly manipulating the embeddings in the final layer, our approach enables a seamless integration into existing models without requiring significant modifications or retraining. In evaluation, we show that the proposed post-hoc approach significantly reduces gender-related associati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>Fovea Transformer&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#26641;&#21644;&#36880;&#28176;&#31895;&#31890;&#24230;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.07102</link><description>&lt;p&gt;
Fovea Transformer&#65306;&#20855;&#26377;&#32467;&#26500;&#21270;&#31934;&#32454;&#33267;&#31895;&#31890;&#24230;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#38271;&#19978;&#19979;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Fovea Transformer: Efficient Long-Context Modeling with Structured Fine-to-Coarse Attention. (arXiv:2311.07102v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07102
&lt;/p&gt;
&lt;p&gt;
Fovea Transformer&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#26641;&#21644;&#36880;&#28176;&#31895;&#31890;&#24230;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23545;&#38271;&#25991;&#26412;&#30340;&#22788;&#29702;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#31232;&#30095;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20174;&#37051;&#36817;&#21333;&#35789;&#20013;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#32467;&#21512;&#20102;&#23616;&#37096;&#27880;&#24847;&#21147;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#22312;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#36716;&#21464;&#26102;&#24341;&#20837;&#20102;&#32972;&#26223;&#31890;&#24230;&#30340;&#31361;&#21464;&#65292;&#21487;&#33021;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26356;&#24179;&#28369;&#30340;&#36807;&#28193;&#21487;&#33021;&#20250;&#22686;&#24378;&#27169;&#22411;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Fovea Transformer&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#30340;Transformer&#65292;&#26088;&#22312;&#35299;&#20915;&#25429;&#25417;&#20840;&#23616;&#20381;&#36182;&#24615;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#36755;&#20837;&#24207;&#21015;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#26641;&#65292;&#24182;&#22312;&#26641;&#20013;&#20351;&#29992;&#36880;&#28176;&#31895;&#31890;&#24230;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic complexity of self-attention in Transformers has hindered the processing of long text. To alleviate this problem, previous works have proposed to sparsify the attention matrix, taking advantage of the observation that crucial information about a token can be derived from its neighbors. These methods typically combine one or another form of local attention and global attention. Such combinations introduce abrupt changes in contextual granularity when going from local to global, which may be undesirable. We believe that a smoother transition could potentially enhance model's ability to capture long-context dependencies. In this study, we introduce Fovea Transformer, a long-context focused transformer that addresses the challenges of capturing global dependencies while maintaining computational efficiency. To achieve this, we construct a multi-scale tree from the input sequence, and use representations of context tokens with a progressively coarser granularity in the tree, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.03220</link><description>&lt;p&gt;
ALYMPICS&#65306;&#35821;&#35328;&#20195;&#29702;&#20154;&#19982;&#21338;&#24328;&#35770;&#30456;&#36935;&#8212;&#8212;&#29992;AI&#20195;&#29702;&#20154;&#25506;&#32034;&#25112;&#30053;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65288;&#20195;&#29702;&#20154;&#30340;&#22885;&#36816;&#20250;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;Alympics&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#21338;&#24328;&#35770;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#25511;&#21046;&#29615;&#22659;&#26469;&#27169;&#25311;&#19982;LLM&#20195;&#29702;&#20154;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#24357;&#21512;&#20102;&#29702;&#35770;&#21338;&#24328;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#25105;&#20204;&#30340;&#35797;&#28857;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#8220;&#27700;&#36164;&#28304;&#20998;&#37197;&#25361;&#25112;&#8221;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#27880;&#31232;&#32570;&#29983;&#23384;&#36164;&#28304;&#22810;&#36718;&#25293;&#21334;&#30340;&#25361;&#25112;&#24615;&#25112;&#30053;&#28216;&#25103;&#26469;&#25506;&#32034;Alympics&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;LLM&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19981;&#20165;&#25193;&#23637;&#20102;&#23545;LLM&#20195;&#29702;&#20154;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#34892;&#20026;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#36824;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the "Water Allocation Challenge," we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also h
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00541</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#30340;&#35770;&#25991;&#19982;&#19968;&#20010;&#20851;&#20110;&#21476;&#24076;&#33098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#30340;&#24847;&#20041;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#65292;&#35789;&#20041;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20250;&#28436;&#21464;&#12289;&#20986;&#29616;&#25110;&#28040;&#22833;&#12290;&#23545;&#20110;&#21476;&#20195;&#35821;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#36890;&#24120;&#36739;&#23567;&#12289;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#20934;&#30830;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#24847;&#20041;&#21464;&#21270;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#21464;&#24471;&#37325;&#35201;&#12290;GASC&#21644;DiSC&#26159;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#24182;&#27809;&#26377;&#20511;&#21161;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#32473;&#23450;&#30446;&#26631;&#35789;&#27719;&#65288;&#22914;"kosmos"&#65292;&#24847;&#20026;&#35013;&#39280;&#12289;&#31209;&#24207;&#25110;&#19990;&#30028;&#65289;&#30340;&#24847;&#20041;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#35789;&#27719;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#24847;&#20041;&#30340;&#26222;&#36941;&#24615;&#34920;&#31034;&#20026;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#20197;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EDiSC&#65292;&#36825;&#26159;DiSC&#30340;&#23884;&#20837;&#29256;&#26412;&#65292;&#23427;&#23558;&#35789;&#23884;&#20837;&#19982;DiSC&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;EDiSC&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13191</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21098;&#26525;&#65306;&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#30446;&#26631;&#36817;&#26399;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36824;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25345;&#32493;&#22686;&#21152;&#27169;&#22411;&#31232;&#30095;&#24615;&#26102;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#20204;&#27493;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19982;&#20854;&#28085;&#30422;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#20197;&#24544;&#23454;&#22320;&#22797;&#21046;&#23494;&#38598;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27599;&#19968;&#23618;&#30340;&#37325;&#26500;&#35823;&#24046;&#19981;&#20165;&#28304;&#33258;&#33258;&#36523;&#65292;&#36824;&#21253;&#25324;&#21069;&#38754;&#23618;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#28982;&#21518;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#30699;&#27491;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#20462;&#21098;&#30340;&#38543;&#26426;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#38543;&#26426;&#20462;&#21098;&#25513;&#30721;&#65292;&#24182;&#32467;&#21512;&#26377;&#25928;&#30340;&#36873;&#25321;&#35268;&#21017;&#36873;&#21462;&#26368;&#20248;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#20843;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13183</link><description>&lt;p&gt;
&#25171;&#30772;&#30830;&#23450;&#24615;&#38480;&#21046;&#65306;&#38543;&#26426;&#20462;&#21098;&#25513;&#30721;&#30340;&#29983;&#25104;&#21644;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#20462;&#21098;&#30340;&#38543;&#26426;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#38543;&#26426;&#20462;&#21098;&#25513;&#30721;&#65292;&#24182;&#32467;&#21512;&#26377;&#25928;&#30340;&#36873;&#25321;&#35268;&#21017;&#36873;&#21462;&#26368;&#20248;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#20843;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21516;&#27169;&#22411;&#23610;&#23544;&#32422;&#26463;&#19979;&#65292;&#22823;&#19988;&#31232;&#30095;&#30340;&#27169;&#22411;&#24448;&#24448;&#27604;&#23567;&#19988;&#23494;&#38598;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36890;&#36807;&#20462;&#21098;&#26469;&#31227;&#38500;&#20887;&#20313;&#30340;&#31070;&#32463;&#20803;&#25110;&#26435;&#37325;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#20462;&#21098;&#65292;&#20854;&#24615;&#33021;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#20462;&#21098;&#20934;&#21017;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20462;&#21098;&#31574;&#30053;&#65292;&#39318;&#20808;&#20197;&#35774;&#35745;&#22909;&#30340;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#22810;&#20010;&#20462;&#21098;&#25513;&#30721;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25513;&#30721;&#36873;&#25321;&#35268;&#21017;&#65292;&#20174;&#20505;&#36873;&#25513;&#30721;&#27744;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#25513;&#30721;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26089;&#26399;&#25513;&#30721;&#35780;&#20272;&#31574;&#30053;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#22810;&#20010;&#25513;&#30721;&#25152;&#24102;&#26469;&#30340;&#24320;&#38144;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;GLUE&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#22312;&#39640;&#31232;&#30095;&#31243;&#24230;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a deterministic way, the performance of which solely depends on a single pruning criterion and thus lacks variety. Instead, in this paper, we propose a model pruning strategy that first generates several pruning masks in a designed random way. Subsequently, along with an effective mask-selection rule, the optimal mask is chosen from the pool of mask candidates. To further enhance efficiency, we introduce an early mask evaluation strategy, mitigating the overhead associated with training multiple masks. Our extensive experiments demonstrate that this approach achieves state-of-the-art performance across eight datasets from GLUE, particularly excelling at high levels of sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20219;&#21153;&#25928;&#29992;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#26080;&#29992;&#25110;&#26377;&#23475;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;UAR&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.09832</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits. (arXiv:2309.09832v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20219;&#21153;&#25928;&#29992;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#26080;&#29992;&#25110;&#26377;&#23475;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;UAR&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#19982;&#30456;&#20851;&#36741;&#21161;&#20219;&#21153;&#30340;&#32852;&#21512;&#23398;&#20064;&#26469;&#25552;&#39640;&#20027;&#35201;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#36873;&#25321;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21644;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38543;&#26426;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#21487;&#33021;&#23545;&#24615;&#33021;&#27809;&#26377;&#24110;&#21161;&#65292;&#29978;&#33267;&#20250;&#26377;&#23475;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#25506;&#32034;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#30340;&#26032;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#20998;&#37197;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#20855;&#26377;&#19981;&#21516;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20219;&#21153;&#25928;&#29992;&#65292;&#20027;&#21160;&#36991;&#20813;&#26080;&#29992;&#25110;&#26377;&#23475;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#20219;&#21153;&#20998;&#37197;&#12290;&#22312;UAR&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) aims to improve the performance of a primary task by jointly learning with related auxiliary tasks. Traditional MTL methods select tasks randomly during training. However, both previous studies and our results suggest that such a random selection of tasks may not be helpful, and can even be harmful to performance. Therefore, new strategies for task selection and assignment in MTL need to be explored. This paper studies the multi-modal, multi-task dialogue act classification task, and proposes a method for selecting and assigning tasks based on non-stationary multi-armed bandits (MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our experimental results show that in different training stages, different tasks have different utility. Our proposed method can effectively identify the task utility, actively avoid useless or harmful tasks, and realise the task assignment during training. Our proposed method is significantly superior in terms of UAR a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#27880;&#37322;&#26041;&#26696;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#20462;&#36766;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2308.03853</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#23398;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#35814;&#32454;&#30340;&#32959;&#30244;&#30149;&#21490;&#21644;&#27835;&#30103;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. (arXiv:2308.03853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#27880;&#37322;&#26041;&#26696;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#20462;&#36766;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25252;&#29702;&#21644;&#32959;&#30244;&#23398;&#35266;&#23519;&#30740;&#31350;&#37117;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#27835;&#30103;&#21382;&#21490;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#35814;&#32454;&#35760;&#24405;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32959;&#30244;&#23398;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#36825;&#20123;&#35760;&#24405;&#20013;&#35760;&#24405;&#30340;&#22810;&#26679;&#20449;&#24687;&#36827;&#34892;&#23436;&#25972;&#23553;&#35013;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#34920;&#31034;&#21644;&#27880;&#37322;&#26041;&#26696;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#30446;&#21069;&#32570;&#20047;&#20840;&#38754;&#27880;&#37322;&#30340;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;LLM&#22312;&#25552;&#21462;&#21644;&#25512;&#29702;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#30340;&#22797;&#26434;&#20462;&#36766;&#30340;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#27880;&#37322;&#32959;&#30244;&#23398;&#25991;&#26412;&#20449;&#24687;&#65292;&#21253;&#25324;&#24739;&#32773;&#29305;&#24449;&#12289;&#32959;&#30244;&#29305;&#24449;&#12289;&#27979;&#35797;&#12289;&#27835;&#30103;&#21644;&#26102;&#38388;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#22823;&#23398;&#26087;&#37329;&#23665;&#20998;&#26657;&#30340;10&#20010;&#21435;&#26631;&#35782;&#21270;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#35821;&#26009;&#24211;&#65292;&#24212;&#29992;&#20102;&#36825;&#20010;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both medical care and observational studies in oncology require a thorough understanding of a patient's disease progression and treatment history, often elaborately documented in clinical notes. Despite their vital role, no current oncology information representation and annotation schema fully encapsulates the diversity of information recorded within these notes. Although large language models (LLMs) have recently exhibited impressive performance on various medical natural language processing tasks, due to the current lack of comprehensively annotated oncology datasets, an extensive evaluation of LLMs in extracting and reasoning with the complex rhetoric in oncology notes remains understudied. We developed a detailed schema for annotating textual oncology information, encompassing patient characteristics, tumor characteristics, tests, treatments, and temporality. Using a corpus of 10 de-identified breast cancer progress notes at University of California, San Francisco, we applied this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#33258;&#21160;&#29983;&#25104;&#38169;&#35823;&#36873;&#39033;&#21644;&#21453;&#39304;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#20102;&#21453;&#39304;&#20449;&#24687;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.03234</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#38169;&#35823;&#36873;&#39033;&#21644;&#21453;&#39304;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning. (arXiv:2308.03234v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#33258;&#21160;&#29983;&#25104;&#38169;&#35823;&#36873;&#39033;&#21644;&#21453;&#39304;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#20102;&#21453;&#39304;&#20449;&#24687;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#22312;&#20960;&#20046;&#25152;&#26377;&#25945;&#32946;&#23618;&#27425;&#20013;&#37117;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26131;&#20110;&#31649;&#29702;&#12289;&#35780;&#20998;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35780;&#20272;&#24418;&#24335;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#26159;&#38169;&#35823;&#36873;&#39033;&#65292;&#21363;&#34987;&#35774;&#35745;&#26469;&#38024;&#23545;&#23398;&#29983;&#29305;&#23450;&#35823;&#35299;&#25110;&#30693;&#35782;&#19981;&#36275;&#30340;&#19981;&#27491;&#30830;&#36873;&#39033;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#39640;&#36136;&#37327;&#38169;&#35823;&#36873;&#39033;&#30340;&#35774;&#35745;&#19968;&#30452;&#26159;&#25945;&#24072;&#21644;&#23398;&#20064;&#20869;&#23481;&#35774;&#35745;&#32773;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#33258;&#21160;&#29983;&#25104;&#38169;&#35823;&#36873;&#39033;&#21644;&#30456;&#24212;&#21453;&#39304;&#20449;&#24687;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#21453;&#39304;&#20449;&#24687;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30495;&#23454;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#20219;&#21153;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable form of assessment. An important aspect of MCQs is the distractors, i.e., incorrect options that are designed to target specific misconceptions or insufficient knowledge among students. To date, the task of crafting high-quality distractors has largely remained a labor-intensive process for teachers and learning content designers, which has limited scalability. In this work, we explore the task of automated distractor and corresponding feedback message generation in math MCQs using large language models. We establish a formulation of these two tasks and propose a simple, in-context learning-based solution. Moreover, we propose generative AI-based metrics for evaluating the quality of the feedback messages. We conduct extensive experiments on these tasks using a real-world MCQ dataset. Our findings suggest that there is a lot of room for improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17333</link><description>&lt;p&gt;
&#21482;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#30340;&#23384;&#20648;&#31354;&#38388;&#25968;&#37327;&#21464;&#24471;&#36807;&#39640;&#12290;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#29702;&#35770;&#19978;&#20165;&#20351;&#29992;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#23601;&#21487;&#20197;&#20272;&#35745;&#26799;&#24230;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#36895;&#24230;&#38750;&#24120;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65288;MeZO&#65289;&#65292;&#23558;&#32463;&#20856;&#30340;ZO-SGD&#26041;&#27861;&#36866;&#24212;&#20110;&#21407;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#19968;&#24352;A100 80GB GPU&#65292;MeZO&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;300&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#20165;&#35757;&#32451;&#19968;&#20010;27&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#22411;&#65288;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#27169;&#22411;&#35268;&#27169;&#65288;&#39640;&#36798;66B&#65289;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#29983;&#25104;&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;MeZO&#26126;&#26174;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#32447;&#24615;PR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14383</link><description>&lt;p&gt;
&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#32452;&#21512;&#24615;&#19982;&#39044;&#20808;&#23384;&#22312;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#21333;&#35789;&#23884;&#20837;&#30340;&#20195;&#25968;&#36816;&#31639;&#26377;&#20851;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;&#27010;&#24565;&#30340;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20960;&#20309;&#23398;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#29702;&#35299;&#32452;&#21512;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;VLM&#23884;&#20837;&#22312;&#27010;&#29575;&#19978;&#30340;&#36825;&#20123;&#32452;&#21512;&#32467;&#26500;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#30452;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#31867;&#12289;&#21435;&#20559;&#21644;&#26816;&#32034;&#31561;&#19981;&#21516;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
&lt;/p&gt;</description></item></channel></rss>