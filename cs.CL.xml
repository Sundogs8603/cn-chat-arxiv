<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08998</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064; (RLHF)&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#38271;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064; (RL) &#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784; LLM&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22686;&#24378;&#33258;&#23398;&#20064; (ReST)&#12290;&#32473;&#23450;&#21021;&#22987;&#30340;LLM&#31574;&#30053;&#65292;ReST&#36890;&#36807;&#20174;&#31574;&#30053;&#20013;&#29983;&#25104;&#26679;&#26412;&#26469;&#20135;&#29983;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;LLM&#31574;&#30053;&#12290;ReST&#27604;&#20856;&#22411;&#30340;&#22312;&#32447;RLHF&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#31163;&#32447;&#29983;&#25104;&#30340;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;ReST&#26159;&#36866;&#29992;&#20110;&#25152;&#26377;&#29983;&#25104;&#23398;&#20064;&#35774;&#32622;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#21487;&#20197;&#20197;&#35745;&#31639;&#21644;&#37319;&#26679;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#22312;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#19978;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#20856;&#23398;&#20064;&#32773;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;GEC&#31995;&#32479;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#22312;&#20960;&#27425;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;GPT-3&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08982</link><description>&lt;p&gt;
&#23545;&#20248;&#36136;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of really good grammatical error correction. (arXiv:2308.08982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#20856;&#23398;&#20064;&#32773;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;GEC&#31995;&#32479;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#22312;&#20960;&#27425;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;GPT-3&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24456;&#23569;&#25552;&#21450;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#28085;&#30422;&#20102;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#27169;&#22411;&#65292;&#33539;&#22260;&#20174;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21040;&#25913;&#21892;&#27969;&#30021;&#24230;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#31995;&#32479;&#30340;&#20840;&#37096;&#33021;&#21147;&#21644;&#30446;&#26631;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#21463;&#38480;&#20110;&#25429;&#25417;&#21487;&#33021;&#20462;&#27491;&#30340;&#21508;&#31181;&#22810;&#26679;&#21270;&#20197;&#21450;&#21442;&#32771;&#21019;&#24314;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#26131;&#20110;&#20559;&#22909;&#20462;&#22797;&#23616;&#37096;&#38169;&#35823;&#32780;&#24573;&#35270;&#25972;&#20307;&#25991;&#26412;&#25913;&#36827;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#36825;&#20123;&#35780;&#20272;&#31574;&#30053;&#30340;&#32570;&#28857;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#20856;&#23398;&#20064;&#32773;&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;&#21508;&#31181;GEC&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#35780;&#20272;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#22996;&#36827;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;GPT-3&#22312;&#24615;&#33021;&#19978;&#36965;&#36965;&#39046;&#20808;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although rarely stated, in practice, Grammatical Error Correction (GEC) encompasses various models with distinct objectives, ranging from grammatical error detection to improving fluency. Traditional evaluation methods fail to fully capture the full range of system capabilities and objectives. Reference-based evaluations suffer from limitations in capturing the wide variety of possible correction and the biases introduced during reference creation and is prone to favor fixing local errors over overall text improvement. The emergence of large language models (LLMs) has further highlighted the shortcomings of these evaluation strategies, emphasizing the need for a paradigm shift in evaluation methodology. In the current study, we perform a comprehensive evaluation of various GEC systems using a recently published dataset of Swedish learner texts. The evaluation is performed using established evaluation metrics as well as human judges. We find that GPT-3 in a few-shot setting by far outpe
&lt;/p&gt;</description></item><item><title>Beam Retrieval&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#38454;&#27573;&#38382;&#31572;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#30340;&#20551;&#35774;&#21644;&#36890;&#36807;&#26368;&#23567;&#21270;&#32452;&#21512;&#25439;&#22833;&#26469;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#22836;&#65292;&#23454;&#29616;&#20102;&#36817;50%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08973</link><description>&lt;p&gt;
Beam Retrieval: &#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#29992;&#20110;&#22810;&#38454;&#27573;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering. (arXiv:2308.08973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08973
&lt;/p&gt;
&lt;p&gt;
Beam Retrieval&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#38454;&#27573;&#38382;&#31572;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#30340;&#20551;&#35774;&#21644;&#36890;&#36807;&#26368;&#23567;&#21270;&#32452;&#21512;&#25439;&#22833;&#26469;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#22836;&#65292;&#23454;&#29616;&#20102;&#36817;50%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38454;&#27573;&#38382;&#31572;&#28041;&#21450;&#26597;&#25214;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#21644;&#36880;&#27493;&#25512;&#29702;&#20197;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20102;&#29992;&#20110;&#36873;&#25321;&#30456;&#20851;&#25991;&#27573;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36229;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22330;&#26223;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#19968;&#27493;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#20004;&#27493;&#26041;&#27861;&#22312;&#26089;&#26399;&#38454;&#27573;&#36873;&#25321;&#26080;&#20851;&#25991;&#27573;&#26102;&#22833;&#36133;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Beam Retrieval&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#38454;&#27573;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26816;&#32034;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#38454;&#27573;&#20445;&#25345;&#22810;&#20010;&#30456;&#20851;&#25991;&#27573;&#30340;&#20551;&#35774;&#65292;&#25193;&#23637;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#38477;&#20302;&#20102;&#38169;&#36807;&#30456;&#20851;&#25991;&#27573;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;Beam Retrieval&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#26377;&#38454;&#27573;&#30340;&#32452;&#21512;&#25439;&#22833;&#26469;&#32852;&#21512;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#20004;&#20010;&#20998;&#31867;&#22836;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#23436;&#25972;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#38405;&#35835;&#22120;&#25110;&#38646;-shot GPT-3.5&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;Beam Retrieval&#21462;&#24471;&#20102;&#36817;50%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop QA involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete QA system, we incorporate a supervised reader or a zero-shot GPT-3.5. Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with baseli
&lt;/p&gt;</description></item><item><title>CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08833</link><description>&lt;p&gt;
CMB&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMB: A Comprehensive Medical Benchmark in Chinese. (arXiv:2308.08833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08833
&lt;/p&gt;
&lt;p&gt;
CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#21307;&#23398;&#39046;&#22495;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#21307;&#23398;&#22522;&#20934;&#25104;&#20026;&#34913;&#37327;&#36827;&#23637;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#22320;&#21306;&#30340;&#21307;&#23398;&#29615;&#22659;&#20855;&#26377;&#21508;&#33258;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#22312;&#20013;&#22269;&#22659;&#20869;&#20256;&#32479;&#20013;&#21307;&#30340;&#26222;&#36941;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#32763;&#35793;&#22522;&#20110;&#33521;&#35821;&#30340;&#21307;&#23398;&#35780;&#20272;&#21487;&#33021;&#23548;&#33268;&#24403;&#22320;&#29615;&#22659;&#20013;&#30340;&#8220;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMB&#65288;Comprehensive Medical Benchmark in Chinese&#65289;&#30340;&#26412;&#22320;&#21270;&#21307;&#23398;&#22522;&#20934;&#65292;&#23436;&#20840;&#35774;&#35745;&#21644;&#26681;&#26893;&#20110;&#20013;&#22269;&#26412;&#22303;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#12290;&#23613;&#31649;&#20256;&#32479;&#20013;&#21307;&#26159;&#36825;&#20010;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#24182;&#19981;&#26500;&#25104;&#20854;&#20840;&#37096;&#12290;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#30693;&#21517;&#30340;&#22823;&#35268;&#27169;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;GPT-4&#12289;&#19987;&#38376;&#30340;&#20013;&#25991;LLMs&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \textit{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. It is worth not
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#35821;&#25968;&#25454;&#32763;&#35793;&#25104;&#24503;&#35821;&#65292;&#20197;&#35757;&#32451;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#24230;&#26816;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20020;&#24202;&#25968;&#25454;&#38590;&#20197;&#20849;&#20139;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08827</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#30340;&#20107;&#23454;&#24230;&#26816;&#27979; -- &#24503;&#35821;&#20020;&#24202;&#25991;&#26412;&#30340;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Factuality Detection using Machine Translation -- a Use Case for German Clinical Text. (arXiv:2308.08827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#35821;&#25968;&#25454;&#32763;&#35793;&#25104;&#24503;&#35821;&#65292;&#20197;&#35757;&#32451;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#24230;&#26816;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20020;&#24202;&#25968;&#25454;&#38590;&#20197;&#20849;&#20139;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#22788;&#29702;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#20107;&#23454;&#24230;&#21487;&#20197;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#26126;&#30830;&#25351;&#20986;&#26576;&#20123;&#30151;&#29366;&#26159;&#21542;&#23384;&#22312;&#12289;&#21487;&#33021;&#23384;&#22312;&#12289;&#26410;&#25552;&#21450;&#25110;&#32943;&#23450;&#19981;&#23384;&#22312;&#26159;&#26377;&#24046;&#21035;&#30340;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22312;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#22788;&#29702;&#36825;&#31867;&#29616;&#35937;&#38656;&#35201;&#36275;&#22815;&#25968;&#37327;&#30340;&#26679;&#20363;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20020;&#24202;&#25991;&#26412;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#25968;&#25454;&#19981;&#33021;&#36731;&#26131;&#20849;&#20139;&#12290;&#22312;&#20107;&#23454;&#24230;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#35821;&#25968;&#25454;&#32763;&#35793;&#25104;&#24503;&#35821;&#65292;&#20197;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#20107;&#23454;&#24230;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factuality can play an important role when automatically processing clinical text, as it makes a difference if particular symptoms are explicitly not present, possibly present, not mentioned, or affirmed. In most cases, a sufficient number of examples is necessary to handle such phenomena in a supervised machine learning setting. However, as clinical text might contain sensitive information, data cannot be easily shared. In the context of factuality detection, this work presents a simple solution using machine translation to translate English data to German to train a transformer-based factuality detection model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26805;&#35821;&#25163;&#31295;&#24320;&#21457;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#20449;&#24687;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#26805;&#35821;&#30340;&#35789;&#20998;&#21106;&#12289;&#20381;&#36182;&#35299;&#26512;&#12289;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#21644;&#35799;&#27468;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08807</link><description>&lt;p&gt;
&#35821;&#35328;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#26805;&#35821;&#30340;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Linguistically-Informed Neural Architectures for Lexical, Syntactic and Semantic Tasks in Sanskrit. (arXiv:2308.08807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26805;&#35821;&#25163;&#31295;&#24320;&#21457;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#20449;&#24687;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#26805;&#35821;&#30340;&#35789;&#20998;&#21106;&#12289;&#20381;&#36182;&#35299;&#26512;&#12289;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#21644;&#35799;&#27468;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#20351;&#26805;&#35821;&#25163;&#31295;&#26356;&#21152;&#26131;&#20110;&#20351;&#29992;&#12290;&#26805;&#35821;&#30340;&#35789;&#24577;&#20016;&#23500;&#12289;&#22797;&#21512;&#35789;&#12289;&#33258;&#30001;&#30340;&#35789;&#24207;&#20197;&#21450;&#36164;&#28304;&#21294;&#20047;&#30340;&#29305;&#28857;&#32473;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#24320;&#21457;&#26805;&#35821;&#30340;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65306;&#26805;&#35821;&#35789;&#20998;&#21106;&#12289;&#20381;&#36182;&#35299;&#26512;&#12289;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#21644;&#35799;&#27468;&#20998;&#26512;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#65292;&#26805;&#35821;&#35789;&#20998;&#21106;&#65292;&#26159;&#20219;&#20309;&#20854;&#20182;&#19979;&#28216;&#24212;&#29992;&#30340;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35789;&#36793;&#30028;&#22788;&#20462;&#25913;&#23383;&#31526;&#30340;&#26805;&#35821;&#29305;&#27530;&#29616;&#35937;&#65288;&#21152;&#20837;&#25110;&#33073;&#33853;&#65289;&#65292;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#31867;&#20284;&#22320;&#65292;&#29616;&#26377;&#30340;&#20381;&#36182;&#35299;&#26512;&#26041;&#27861;&#22312;&#24418;&#24577;&#20016;&#23500;&#21644;&#36164;&#28304;&#21294;&#20047;&#30340;&#26805;&#35821;&#31561;&#35821;&#35328;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#23545;&#26805;&#35821;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22797;&#21512;&#35789;&#32452;&#20214;&#20043;&#38388;&#23384;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary focus of this thesis is to make Sanskrit manuscripts more accessible to the end-users through natural language technologies. The morphological richness, compounding, free word orderliness, and low-resource nature of Sanskrit pose significant challenges for developing deep learning solutions. We identify four fundamental tasks, which are crucial for developing a robust NLP technology for Sanskrit: word segmentation, dependency parsing, compound type identification, and poetry analysis. The first task, Sanskrit Word Segmentation (SWS), is a fundamental text processing task for any other downstream applications. However, it is challenging due to the sandhi phenomenon that modifies characters at word boundaries. Similarly, the existing dependency parsing approaches struggle with morphologically rich and low-resource languages like Sanskrit. Compound type identification is also challenging for Sanskrit due to the context-sensitive semantic relation between components. All these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#38169;&#35823;&#27169;&#24335;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#26631;&#27880;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08796</link><description>&lt;p&gt;
&#12298;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#20316;&#20026;&#25913;&#20889;&#35821;&#35328;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Correction as Rephrasing Language Model. (arXiv:2308.08796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#38169;&#35823;&#27169;&#24335;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#26631;&#27880;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#65288;CSC&#65289;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#32473;&#23450;&#21477;&#23376;&#20013;&#30340;&#28508;&#22312;&#25340;&#20889;&#38169;&#35823;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;CSC&#35270;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#24182;&#22312;&#21477;&#23376;&#23545;&#19978;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22312;&#23558;&#19968;&#20010;&#23383;&#31526;&#26631;&#35760;&#20026;&#21478;&#19968;&#20010;&#23383;&#31526;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#21363;&#32416;&#27491;&#36807;&#31243;&#36807;&#20110;&#20381;&#36182;&#38169;&#35823;&#12290;&#36825;&#19982;&#20154;&#31867;&#24605;&#32500;&#30456;&#21453;&#65292;&#20154;&#20204;&#26681;&#25454;&#21477;&#23376;&#30340;&#35821;&#20041;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22522;&#20110;&#20043;&#21069;&#35760;&#24518;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;&#36825;&#31181;&#36829;&#21453;&#30452;&#35273;&#30340;&#23398;&#20064;&#36807;&#31243;&#23548;&#33268;&#26426;&#22120;&#25340;&#20889;&#32416;&#38169;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#36801;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#8221;&#65288;ReLM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#22635;&#20805;&#39069;&#22806;&#30340;&#20301;&#32622;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#30340;&#26631;&#27880;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#22312;&#24494;&#35843;&#21518;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Chinese Spelling Correction (CSC), which aims to detect and correct potential spelling errors in a given sentence. Current state-of-the-art methods regard CSC as a sequence tagging task and fine-tune BERT-based models on sentence pairs. However, we note a critical flaw in the process of tagging one character to another, that the correction is excessively conditioned on the error. This is opposite from human mindset, where individuals rephrase the complete sentence based on its semantics, rather than solely on the error patterns memorized before. Such a counter-intuitive learning process results in the bottleneck of generalizability and transferability of machine spelling correction. To address this, we propose $Rephrasing Language Modeling$ (ReLM), where the model is trained to rephrase the entire sentence by infilling additional slots, instead of character-to-character tagging. This novel training paradigm achieves the new state-of-the-art results across fine-tuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#21644;&#21407;&#22411;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22686;&#37327;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#32972;&#26223;&#36716;&#25442;&#38382;&#39064;&#12290;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#38388;&#20851;&#31995;&#33976;&#39311;&#25439;&#22833;&#30830;&#20445;&#20219;&#21153;&#38388;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#20869;&#33258;&#29109;&#25439;&#22833;&#22686;&#24378;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#21407;&#22411;&#20266;&#26631;&#31614;&#31574;&#30053;&#33021;&#22815;&#21306;&#20998;&#26087;&#23454;&#20307;&#31867;&#22411;&#21644;&#24403;&#21069;&#20219;&#21153;&#20013;&#30340;&#38750;&#23454;&#20307;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.08793</link><description>&lt;p&gt;
&#22686;&#37327;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#21644;&#21407;&#22411;&#20266;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Task Relation Distillation and Prototypical Pseudo Label for Incremental Named Entity Recognition. (arXiv:2308.08793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#21644;&#21407;&#22411;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22686;&#37327;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#32972;&#26223;&#36716;&#25442;&#38382;&#39064;&#12290;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#38388;&#20851;&#31995;&#33976;&#39311;&#25439;&#22833;&#30830;&#20445;&#20219;&#21153;&#38388;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#20869;&#33258;&#29109;&#25439;&#22833;&#22686;&#24378;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#21407;&#22411;&#20266;&#26631;&#31614;&#31574;&#30053;&#33021;&#22815;&#21306;&#20998;&#26087;&#23454;&#20307;&#31867;&#22411;&#21644;&#24403;&#21069;&#20219;&#21153;&#20013;&#30340;&#38750;&#23454;&#20307;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Incremental Named Entity Recognition, INER&#65289;&#28041;&#21450;&#22312;&#19981;&#35775;&#38382;&#20808;&#21069;&#23398;&#20064;&#31867;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26032;&#23454;&#20307;&#31867;&#22411;&#36827;&#34892;&#39034;&#24207;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;INER&#38754;&#20020;&#30528;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#25361;&#25112;&#65292;&#24182;&#36827;&#19968;&#27493;&#21463;&#21040;&#32972;&#26223;&#36716;&#25442;&#30340;&#24433;&#21709;&#65288;&#21363;&#65292;&#26087;&#21644;&#26410;&#26469;&#30340;&#23454;&#20307;&#31867;&#22411;&#22312;&#24403;&#21069;&#20219;&#21153;&#20013;&#34987;&#26631;&#35760;&#20026;&#38750;&#23454;&#20307;&#31867;&#22411;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#21644;&#21407;&#22411;&#20266;&#26631;&#31614;&#65288;RDP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;INER&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#21153;&#20851;&#31995;&#33976;&#39311;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20855;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;1&#65289;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#38388;&#20851;&#31995;&#33976;&#39311;&#25439;&#22833;&#26469;&#30830;&#20445;&#19981;&#21516;&#22686;&#37327;&#23398;&#20064;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#38388;&#35821;&#20041;&#19968;&#33268;&#24615;&#65307;2&#65289;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#20869;&#33258;&#29109;&#25439;&#22833;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20943;&#36731;&#32972;&#26223;&#36716;&#25442;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21407;&#22411;&#20266;&#26631;&#31614;&#31574;&#30053;&#65292;&#21487;&#20197;&#21306;&#20998;&#26087;&#23454;&#20307;&#31867;&#22411;&#21644;&#24403;&#21069;&#20219;&#21153;&#20013;&#30340;&#38750;&#23454;&#20307;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental Named Entity Recognition (INER) involves the sequential learning of new entity types without accessing the training data of previously learned types. However, INER faces the challenge of catastrophic forgetting specific for incremental learning, further aggravated by background shift (i.e., old and future entity types are labeled as the non-entity type in the current task). To address these challenges, we propose a method called task Relation Distillation and Prototypical pseudo label (RDP) for INER. Specifically, to tackle catastrophic forgetting, we introduce a task relation distillation scheme that serves two purposes: 1) ensuring inter-task semantic consistency across different incremental learning tasks by minimizing inter-task relation distillation loss, and 2) enhancing the model's prediction confidence by minimizing intra-task self-entropy loss. Simultaneously, to mitigate background shift, we develop a prototypical pseudo label strategy that distinguishes old entit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08780</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#31034;&#20363;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#21363;&#28436;&#31034;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#28436;&#31034;&#19982;&#27979;&#35797;&#36755;&#20837;&#36830;&#25509;&#36215;&#26469;&#25552;&#31034;&#32473;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36830;&#25509;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#27599;&#20010;&#28436;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#24403;&#19968;&#20123;&#28436;&#31034;&#19982;&#27979;&#35797;&#31034;&#20363;&#26080;&#20851;&#26102;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#26576;&#20123;&#21464;&#25442;&#22120;&#27169;&#22411;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#38480;&#21046;&#65292;&#23558;&#35768;&#22810;&#31034;&#20363;&#25918;&#20837;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#20219;&#21153;&#26102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28436;&#31034;&#38598;&#25104;&#65288;DENSE&#65289;&#20316;&#20026;&#31616;&#21333;&#36830;&#25509;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#22411;&#20351;&#29992;&#28436;&#31034;&#30340;&#23376;&#38598;&#65288;&#21363;bucket&#65289;&#26469;&#39044;&#27979;&#36755;&#20986;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#23376;&#38598;&#24471;&#21040;&#30340;&#36755;&#20986;&#27010;&#29575;&#32452;&#21512;&#36215;&#26469;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-j&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for a given task, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are irrelevant to the test example. Second, due to the input length limit of some transformer models, it might be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. \model predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21387;&#32553;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#31561;&#26041;&#38754;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#21046;&#32422;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.08774</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21487;&#33021;&#24615;&#21644;&#21487;&#33021;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models. (arXiv:2308.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21387;&#32553;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#31561;&#26041;&#38754;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#21046;&#32422;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;mBERT&#12289;XLM-R&#21644;BLOOM&#26088;&#22312;&#23454;&#29616;&#22810;&#35821;&#35328;&#27010;&#25324;&#25110;&#21387;&#32553;&#65292;&#20197;&#20415;&#20110;&#36716;&#31227;&#21040;&#22823;&#37327;&#65288;&#21487;&#33021;&#26410;&#30693;&#30340;&#65289;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24212;&#35813;&#20855;&#22791;&#38544;&#31169;&#24615;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#21363;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#21527;&#65311;&#25105;&#20204;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#21387;&#32553;&#21644;&#35821;&#35328;&#20844;&#24179;&#24615;&#19982;&#24046;&#20998;&#38544;&#31169;&#26159;&#20860;&#23481;&#30340;&#65292;&#20294;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#26159;&#30456;&#24726;&#30340;&#65292;&#21518;&#32773;&#26159;&#36879;&#26126;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21387;&#32553;&#21644;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#65292;&#26356;&#35814;&#32454;&#22320;&#25506;&#35752;&#20102;&#36825;&#20123;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#20849;&#21516;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#23454;&#38469;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08758</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#26469;&#35299;&#20915;&#19982;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#38271;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#40723;&#21169;&#24320;&#21457;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23481;&#32435;&#22810;&#20010;&#35760;&#21495;&#21547;&#20041;&#12290;&#36825;&#22312;&#35299;&#37322;&#24615;&#12289;&#22266;&#23450;&#25968;&#37327;&#30340;&#23884;&#20837;&#35760;&#21495;&#12289;&#22312;&#19981;&#21516;LM&#20043;&#38388;&#30340;&#21487;&#37325;&#29992;&#24615;&#20197;&#21450;&#19982;&#40657;&#30418;API&#20132;&#20114;&#26102;&#30340;&#19981;&#36866;&#29992;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#12290;PCRL&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#20197;&#21450;&#21482;&#26377;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26799;&#24230;&#35775;&#38382;LM&#25110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08747</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. (arXiv:2308.08747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65292;&#24182;&#19988;ALPACA&#22312;&#20445;&#30041;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#27169;&#22411;&#23398;&#20064;&#26032;&#20449;&#24687;&#26102;&#65292;&#23427;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25506;&#31350;LLMs&#22312;&#25345;&#32493;&#24494;&#35843;&#20013;&#26159;&#21542;&#23384;&#22312;CF&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#39046;&#22495;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#30340;&#35282;&#24230;&#23545;LLMs&#30340;&#36951;&#24536;&#29616;&#35937;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;1b&#21040;7b&#30340;&#33539;&#22260;&#20869;&#65292;LLMs&#26222;&#36941;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#36951;&#24536;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#21152;&#21095;&#12290;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;mT0&#30456;&#27604;&#65292;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;BLOOMZ&#36951;&#24536;&#36739;&#23569;&#24182;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22312;&#25345;&#32493;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLMs&#21487;&#20197;&#20943;&#36731;&#35821;&#35328;&#20559;&#35265;&#65288;&#22914;&#24615;&#21035;&#20559;&#35265;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;LLAMA&#30456;&#27604;&#65292;ALPACA&#22312;&#20445;&#30041;&#26356;&#22810;&#30693;&#35782;&#21644;&#23481;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. As large language models (LLMs) have shown excellent performance, it is interesting to uncover whether CF exists in the continual fine-tuning of LLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs' knowledge, from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments demonstrate that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b. Furthermore, as the scale increases, the severity of forgetting also intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffers less forgetting and maintains more knowledge. We also observe that LLMs can mitigate language bias (e.g. gender bias) during continual fine-tuning. Moreover, we find that ALPACA can maintain more knowledge and capacity compared with LLAMA du
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#24341;&#23548;&#30340;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#26469;&#22686;&#24378;&#30701;&#35821;&#34920;&#31034;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#35789;&#20449;&#24687;&#65292;&#36890;&#36807;&#20248;&#21270;&#25490;&#21517;&#32593;&#32476;&#21644;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26469;&#25552;&#39640;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08739</link><description>&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#24341;&#23548;&#30340;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#22686;&#24378;&#30701;&#35821;&#34920;&#31034;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Phrase Representation by Information Bottleneck Guided Text Diffusion Process for Keyphrase Extraction. (arXiv:2308.08739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#24341;&#23548;&#30340;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#26469;&#22686;&#24378;&#30701;&#35821;&#34920;&#31034;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#35789;&#20449;&#24687;&#65292;&#36890;&#36807;&#20248;&#21270;&#25490;&#21517;&#32593;&#32476;&#21644;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26469;&#25552;&#39640;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#25552;&#21462;(KPE)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#30417;&#30563;&#26041;&#27861;&#23558;KPE&#35270;&#20026;&#24207;&#21015;&#26631;&#27880;&#12289;&#36328;&#24230;&#32423;&#20998;&#31867;&#25110;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21033;&#29992;&#20851;&#38190;&#35789;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#23548;&#33268;&#32467;&#26524;&#26377;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-KPE&#65292;&#23427;&#21033;&#29992;&#30417;&#30563;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;(VIB)&#26469;&#24341;&#23548;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#65292;&#29983;&#25104;&#22686;&#24378;&#30340;&#20851;&#38190;&#35789;&#34920;&#31034;&#12290;Diff-KPE&#39318;&#20808;&#26681;&#25454;&#25972;&#20010;&#25991;&#26723;&#29983;&#25104;&#25152;&#38656;&#30340;&#20851;&#38190;&#35789;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#23884;&#20837;&#27880;&#20837;&#21040;&#27599;&#20010;&#30701;&#35821;&#34920;&#31034;&#20013;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25490;&#21517;&#32593;&#32476;&#21644;VIB&#21516;&#26102;&#36827;&#34892;&#25490;&#21517;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#30340;&#20248;&#21270;&#12290;Diff-KPE&#30340;&#35774;&#35745;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#20851;&#38190;&#35789;&#21644;&#25991;&#26723;&#30340;&#20449;&#24687;&#23545;&#27599;&#20010;&#20505;&#36873;&#30701;&#35821;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase extraction (KPE) is an important task in Natural Language Processing for many scenarios, which aims to extract keyphrases that are present in a given document. Many existing supervised methods treat KPE as sequential labeling, span-level classification, or generative tasks. However, these methods lack the ability to utilize keyphrase information, which may result in biased results. In this study, we propose Diff-KPE, which leverages the supervised Variational Information Bottleneck (VIB) to guide the text diffusion process for generating enhanced keyphrase representations. Diff-KPE first generates the desired keyphrase embeddings conditioned on the entire document and then injects the generated keyphrase embeddings into each phrase representation. A ranking network and VIB are then optimized together with rank loss and classification loss, respectively. This design of Diff-KPE allows us to rank each candidate phrase by utilizing both the information of keyphrases and the docu
&lt;/p&gt;</description></item><item><title>LLM-FuncMapper&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#23454;&#29616;&#23545;&#24314;&#31569;&#27861;&#35268;&#20013;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21407;&#23376;&#20989;&#25968;&#21644;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#26469;&#35299;&#20915;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08728</link><description>&lt;p&gt;
LLM-FuncMapper:&#36890;&#36807;LLM&#35299;&#37322;&#24314;&#31569;&#27861;&#35268;&#20013;&#30340;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM. (arXiv:2308.08728v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08728
&lt;/p&gt;
&lt;p&gt;
LLM-FuncMapper&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#23454;&#29616;&#23545;&#24314;&#31569;&#27861;&#35268;&#20013;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21407;&#23376;&#20989;&#25968;&#21644;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#26469;&#35299;&#20915;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#21160;&#21270;&#35268;&#21017;&#26816;&#26597;&#65288;ARC&#65289;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23545;&#30417;&#31649;&#24615;&#25991;&#26412;&#30340;&#35268;&#21017;&#35299;&#37322;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#21644;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35299;&#37322;&#20855;&#26377;&#38544;&#24335;&#23646;&#24615;&#25110;&#22797;&#26434;&#35745;&#31639;&#36923;&#36753;&#30340;&#30417;&#31649;&#26465;&#27454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35782;&#21035;&#21508;&#31181;&#30417;&#31649;&#26465;&#27454;&#25152;&#38656;&#30340;&#39044;&#23450;&#20041;&#20989;&#25968;&#30340;&#26041;&#27861;LLM-FuncMapper&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#24314;&#31569;&#27861;&#35268;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#21407;&#23376;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#38544;&#24335;&#23646;&#24615;&#21644;&#22797;&#26434;&#32422;&#26463;&#30340;&#20849;&#20139;&#35745;&#31639;&#36923;&#36753;&#65292;&#21019;&#24314;&#20102;&#24120;&#35265;&#22359;&#30340;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#35299;&#37322;&#30417;&#31649;&#26465;&#27454;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20998;&#31867;&#30340;&#35843;&#20248;&#31574;&#30053;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20989;&#25968;&#35782;&#21035;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a vital stage of automated rule checking (ARC), rule interpretation of regulatory texts requires considerable effort. However, interpreting regulatory clauses with implicit properties or complex computational logic is still challenging due to the lack of domain knowledge and limited expressibility of conventional logic representations. Thus, LLM-FuncMapper, an approach to identifying predefined functions needed to interpret various regulatory clauses based on the large language model (LLM), is proposed. First, by systematically analysis of building codes, a series of atomic functions are defined to capture shared computational logics of implicit properties and complex constraints, creating a database of common blocks for interpreting regulatory clauses. Then, a prompt template with the chain of thought is developed and further enhanced with a classification-based tuning strategy, to enable common LLMs for effective function identification. Finally, the proposed approach is validated
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#22522;&#20934;&#65292;&#35780;&#20272;&#20102;&#20843;&#31181;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#21644;&#20845;&#31181;&#19981;&#21516;&#35821;&#35328;&#12290;&#36890;&#36807;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#26469;&#33258;&#35821;&#38899;&#27169;&#22411;&#21333;&#19968;&#20248;&#21270;&#23618;&#30340;&#29305;&#24449;&#21487;&#20197;&#24179;&#22343;&#38477;&#20302;32%&#30340;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#20013;&#38388;&#23618;&#25429;&#33719;&#20102;&#26368;&#37325;&#35201;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.08713</link><description>&lt;p&gt;
&#35299;&#30721;&#24773;&#32490;&#65306;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#35821;&#35328;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition. (arXiv:2308.08713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#22522;&#20934;&#65292;&#35780;&#20272;&#20102;&#20843;&#31181;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#21644;&#20845;&#31181;&#19981;&#21516;&#35821;&#35328;&#12290;&#36890;&#36807;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#26469;&#33258;&#35821;&#38899;&#27169;&#22411;&#21333;&#19968;&#20248;&#21270;&#23618;&#30340;&#29305;&#24449;&#21487;&#20197;&#24179;&#22343;&#38477;&#20302;32%&#30340;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#20013;&#38388;&#23618;&#25429;&#33719;&#20102;&#26368;&#37325;&#35201;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#22312;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#35780;&#20272;&#20197;&#21450;&#23545;&#20854;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#30740;&#31350;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;SER&#22522;&#20934;&#65292;&#20351;&#29992;&#20843;&#20010;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#21644;&#20845;&#31181;&#19981;&#21516;&#35821;&#35328;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#20123;&#24046;&#36317;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;SER&#20013;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19982;&#20351;&#29992;&#26469;&#33258;&#35821;&#38899;&#27169;&#22411;&#25152;&#26377;&#23618;&#30340;&#29305;&#24449;&#30340;&#31995;&#32479;&#30456;&#27604;&#65292;&#20351;&#29992;&#26469;&#33258;&#35821;&#38899;&#27169;&#22411;&#21333;&#19968;&#20248;&#21270;&#23618;&#30340;&#29305;&#24449;&#21487;&#20197;&#24179;&#22343;&#38477;&#20302;32%&#30340;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#24503;&#35821;&#21644;&#27874;&#26031;&#35821;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#38899;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#25429;&#33719;&#20102;&#26368;&#37325;&#35201;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in transformer-based speech representation models have greatly transformed speech processing. However, there has been limited research conducted on evaluating these models for speech emotion recognition (SER) across multiple languages and examining their internal representations. This article addresses these gaps by presenting a comprehensive benchmark for SER with eight speech representation models and six different languages. We conducted probing experiments to gain insights into inner workings of these models for SER. We find that using features from a single optimal layer of a speech model reduces the error rate by 32\% on average across seven datasets when compared to systems where features from all layers of speech models are used. We also achieve state-of-the-art results for German and Persian languages. Our probing results indicate that the middle layers of speech models capture the most important emotional information for speech emotion recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#65292;&#36890;&#36807;&#29306;&#29298;&#37096;&#20998;&#20934;&#30830;&#24230;&#65292;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;99.8%&#30340;&#21387;&#32553;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08688</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23376;&#31354;&#38388;&#23884;&#20837;&#36827;&#34892;&#36866;&#24212;&#24615;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lightweight Adaptation of Neural Language Models via Subspace Embedding. (arXiv:2308.08688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#65292;&#36890;&#36807;&#29306;&#29298;&#37096;&#20998;&#20934;&#30830;&#24230;&#65292;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;99.8%&#30340;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#35789;&#23884;&#20837;&#36890;&#24120;&#20381;&#36182;&#20110;&#26356;&#20016;&#23500;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36890;&#36807;&#21333;&#35789;&#23884;&#20837;&#21442;&#25968;&#26469;&#35206;&#30422;&#20027;&#35201;&#35789;&#27719;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36890;&#24120;&#35206;&#30422;&#20854;&#25972;&#20307;&#23398;&#20064;&#21442;&#25968;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#65292;&#36890;&#36807;&#29306;&#29298;&#39640;&#36798;4%&#30340;&#32477;&#23545;&#20934;&#30830;&#24230;&#26469;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#24314;&#36981;&#24490;&#19968;&#32452;&#23376;&#31354;&#38388;&#23884;&#20837;&#21644;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#36827;&#34892;&#30340;&#20998;&#37197;&#36807;&#31243;&#12290;&#23376;&#31354;&#38388;&#23884;&#20837;&#32467;&#26500;&#36866;&#24212;&#20102;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#22312;&#30456;&#20284;&#24615;&#21644;&#25991;&#26412;&#34164;&#28085;&#20219;&#21153;&#12289;&#21477;&#23376;&#21644;&#37322;&#20041;&#20219;&#21153;&#20013;&#35780;&#20272;&#25105;&#20204;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;e&#30456;&#27604;&#65292;&#23376;&#31354;&#38388;&#23884;&#20837;&#23454;&#29616;&#20102;&#36229;&#36807;99.8%&#30340;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional neural word embeddings are usually dependent on a richer diversity of vocabulary. However, the language models recline to cover major vocabularies via the word embedding parameters, in particular, for multilingual language models that generally cover a significant part of their overall learning parameters. In this work, we present a new compact embedding structure to reduce the memory footprint of the pre-trained language models with a sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction follows a set of subspace embeddings and an assignment procedure via the contextual relationship among tokens from pre-trained language models. The subspace embedding structure calibrates to masked language models, to evaluate our compact embedding structure on similarity and textual entailment tasks, sentence and paraphrase tasks. Our experimental evaluation shows that the subspace embeddings achieve compression rates beyond 99.8% in comparison with the original e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#30340;&#19968;&#32452;&#26126;&#30830;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#22312;&#22238;&#31572;&#24615;&#33021;&#21644;&#27495;&#20041;&#38382;&#39064;&#28040;&#38500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.08661</link><description>&lt;p&gt;
&#20351;&#29992;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#20462;&#35746;&#30340;&#25968;&#25454;&#24211;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions. (arXiv:2308.08661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#30340;&#19968;&#32452;&#26126;&#30830;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#22312;&#22238;&#31572;&#24615;&#33021;&#21644;&#27495;&#20041;&#38382;&#39064;&#28040;&#38500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#39064;&#37117;&#27809;&#26377;&#26126;&#30830;&#30340;&#35268;&#23450;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22810;&#20010;&#21487;&#33021;&#30340;&#31572;&#26696;&#65292;&#27599;&#20010;&#31572;&#26696;&#22312;&#38382;&#39064;&#30340;&#19981;&#21516;&#35299;&#37322;&#19979;&#37117;&#26159;&#27491;&#30830;&#30340;&#12290;&#22238;&#31572;&#36825;&#26679;&#27169;&#31946;&#30340;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20174;&#22810;&#20010;&#27573;&#33853;&#20013;&#26816;&#32034;&#24182;&#25512;&#29702;&#20986;&#19981;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#21033;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#30340;&#19968;&#32452;&#26126;&#30830;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ASQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21484;&#22238;&#29575;&#27979;&#37327;&#19978;&#25552;&#39640;&#20102;15%&#65288;&#30456;&#23545;&#25913;&#36827;&#65289;&#65292;&#22312;&#35780;&#20272;&#20174;&#39044;&#27979;&#36755;&#20986;&#20013;&#28040;&#38500;&#27495;&#20041;&#38382;&#39064;&#30340;&#24230;&#37327;&#19978;&#25552;&#39640;&#20102;10%&#12290;&#20174;&#29983;&#25104;&#30340;&#38382;&#39064;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#36824;&#22823;&#22823;&#25552;&#39640;&#20102;&#22810;&#26679;&#30340;&#27573;&#33853;&#26816;&#32034;&#65288;&#36890;&#36807;&#38388;&#25509;&#21305;&#37197;&#29992;&#25143;&#38382;&#39064;q&#21040;&#27573;&#33853;p&#65292;&#36890;&#36807;&#20174;p&#29983;&#25104;&#30340;&#38382;&#39064;q'&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many open-domain questions are under-specified and thus have multiple possible answers, each of which is correct under a different interpretation of the question. Answering such ambiguous questions is challenging, as it requires retrieving and then reasoning about diverse information from multiple passages. We present a new state-of-the-art for answering ambiguous questions that exploits a database of unambiguous questions generated from Wikipedia. On the challenging ASQA benchmark, which requires generating long-form answers that summarize the multiple answers to an ambiguous question, our method improves performance by 15% (relative improvement) on recall measures and 10% on measures which evaluate disambiguating questions from predicted outputs. Retrieving from the database of generated questions also gives large improvements in diverse passage retrieval (by matching user questions q to passages p indirectly, via questions q' generated from p).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Barrett&#39135;&#31649;&#28814;&#35786;&#26029;&#20998;&#31867;&#20013;&#23454;&#29616;&#25968;&#25454;&#33258;&#21160;&#21270;&#25552;&#21462;&#65292;&#20855;&#26377;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#31995;&#32479;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08660</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#20998;Barrett&#39135;&#31649;&#28814;&#35786;&#26029;&#20998;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Granularized Barrett's Esophagus Diagnosis Classification. (arXiv:2308.08660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Barrett&#39135;&#31649;&#28814;&#35786;&#26029;&#20998;&#31867;&#20013;&#23454;&#29616;&#25968;&#25454;&#33258;&#21160;&#21270;&#25552;&#21462;&#65292;&#20855;&#26377;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#31995;&#32479;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Barrett&#39135;&#31649;&#28814;&#65288;BE&#65289;&#30340;&#35786;&#26029;&#20195;&#30721;&#22312;&#35768;&#22810;&#30740;&#31350;&#25110;&#20020;&#24202;&#20351;&#29992;&#24773;&#20917;&#19979;&#32570;&#20047;&#32454;&#21270;&#21644;&#31934;&#30830;&#24615;&#12290;&#38656;&#35201;&#36153;&#21147;&#30340;&#25163;&#21160;&#26597;&#38405;&#30149;&#21382;&#20197;&#20174;BE&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#20851;&#38190;&#35786;&#26029;&#34920;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#25968;&#25454;&#25552;&#21462;&#12290;&#20351;&#29992;&#21733;&#20262;&#27604;&#20122;&#22823;&#23398;&#27431;&#25991;&#21307;&#23398;&#20013;&#24515;&#30340;&#30149;&#29702;&#25253;&#21578;&#21644;&#32963;&#32928;&#30149;&#23398;&#23478;&#27880;&#37322;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20108;&#20803;&#24322;&#22411;&#20998;&#32423;&#20197;&#21450;&#32454;&#20998;&#30340;&#22810;&#31867;BE&#30456;&#20851;&#35786;&#26029;&#20998;&#31867;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#20020;&#24202;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26368;&#20339;&#27169;&#22411;&#24615;&#33021;&#19982;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#24320;&#21457;&#30340;&#39640;&#24230;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30456;&#24403;&#12290;&#20108;&#20803;&#24322;&#22411;&#25552;&#21462;&#23454;&#29616;&#20102;0.964&#30340;F1&#20998;&#25968;&#65292;&#32780;&#22810;&#31867;&#27169;&#22411;&#23454;&#29616;&#20102;0.911&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#19988;&#27604;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26356;&#24555;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnostic codes for Barrett's esophagus (BE), a precursor to esophageal cancer, lack granularity and precision for many research or clinical use cases. Laborious manual chart review is required to extract key diagnostic phenotypes from BE pathology reports. We developed a generalizable transformer-based method to automate data extraction. Using pathology reports from Columbia University Irving Medical Center with gastroenterologist-annotated targets, we performed binary dysplasia classification as well as granularized multi-class BE-related diagnosis classification. We utilized two clinically pre-trained large language models, with best model performance comparable to a highly tailored rule-based system developed using the same data. Binary dysplasia extraction achieves 0.964 F1-score, while the multi-class model achieves 0.911 F1-score. Our method is generalizable and faster to implement as compared to a tailored rule-based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#21644;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#20123;&#35789;&#27719;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#36923;&#36753;&#25512;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#20197;&#21450;&#36805;&#36895;&#21457;&#23637;&#20986;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08628</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#20013;&#20197;&#22522;&#20110;&#35821;&#22659;&#35821;&#35328;&#23398;&#20064;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
Learning the meanings of function words from grounded language using a visual question answering model. (arXiv:2308.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#21151;&#33021;&#35789;&#30340;&#24847;&#20041;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#21644;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#20123;&#35789;&#27719;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#36923;&#36753;&#25512;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#20197;&#21450;&#36805;&#36895;&#21457;&#23637;&#20986;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#21151;&#33021;&#35789;&#65292;&#22914;&#8220;&#25110;&#32773;&#8221;&#65292;&#8220;&#22312;......&#21518;&#38754;&#8221;&#65292;&#25110;&#8220;&#26356;&#22810;&#8221;&#21487;&#33021;&#38656;&#35201;&#36923;&#36753;&#12289;&#25968;&#23383;&#21644;&#20851;&#31995;&#25512;&#29702;&#12290;&#20799;&#31461;&#22914;&#20309;&#23398;&#20064;&#36825;&#26679;&#30340;&#35789;&#27719;&#65311;&#26082;&#24448;&#30340;&#20064;&#24471;&#29702;&#35770;&#36890;&#24120;&#20381;&#36182;&#20110;&#35748;&#20026;&#20855;&#26377;&#20808;&#22825;&#30693;&#35782;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26174;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21151;&#33021;&#35789;&#26469;&#22238;&#31572;&#20851;&#20110;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#30340;&#38382;&#39064;&#32780;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#21151;&#33021;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#24076;&#26395;&#26356;&#22909;&#22320;&#20102;&#35299;&#36825;&#20123;&#35789;&#27719;&#30340;&#24847;&#20041;&#22914;&#20309;&#34987;&#27169;&#22411;&#21644;&#20799;&#31461;&#25152;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#36882;&#24402;&#27169;&#22411;&#23398;&#20064;&#20102;&#38656;&#35201;&#31354;&#38388;&#21644;&#25968;&#23383;&#25512;&#29702;&#30340;&#21151;&#33021;&#35789;&#30340;&#26799;&#24230;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#36923;&#36753;&#25512;&#29702;&#30340;&#20808;&#39564;&#30693;&#35782;&#19979;&#23398;&#20064;&#21040;"&#21644;"&#21644;"&#25110;"&#30340;&#24847;&#20041;&#65292;&#24182;&#36805;&#36895;&#21457;&#23637;&#20986;&#36827;&#34892;&#26367;&#25442;&#25512;&#35770;&#30340;&#33021;&#21147;&#30340;&#26089;&#26399;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting a seemingly-simple function word like "or", "behind", or "more" can require logical, numerical, and relational reasoning. How are such words learned by children? Prior acquisition theories have often relied on positing a foundation of innate knowledge. Yet recent neural-network based visual question answering models apparently can learn to use function words as part of answering questions about complex visual scenes. In this paper, we study what these models learn about function words, in the hope of better understanding how the meanings of these words can be learnt by both models and children. We show that recurrent models trained on visually grounded language learn gradient semantics for function words requiring spacial and numerical reasoning. Furthermore, we find that these models can learn the meanings of logical connectives "and" and "or" without any prior knowledge of logical reasoning, as well as early evidence that they can develop the ability to reason about alte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26435;&#37325;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;BERT&#27169;&#22411;&#20013;&#25552;&#21462;&#29616;&#26377;&#26435;&#37325;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#65292;&#20197;&#25506;&#32034;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26356;&#20248;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08625</link><description>&lt;p&gt;
BIOptimus&#65306;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#20026;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39044;&#35757;&#32451;&#20102;&#19968;&#31181;&#26368;&#20248;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition. (arXiv:2308.08625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26435;&#37325;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;BERT&#27169;&#22411;&#20013;&#25552;&#21462;&#29616;&#26377;&#26435;&#37325;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#65292;&#20197;&#25506;&#32034;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26356;&#20248;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24182;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#20013;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#20351;&#29992;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;BioNLP&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#20197;&#25214;&#21040;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26356;&#20248;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22914;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#39044;&#35757;&#32451;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;BERT&#27169;&#22411;&#20013;&#30340;&#29616;&#26377;&#26435;&#37325;&#25552;&#28860;&#21040;&#19978;&#19979;&#25991;&#20013;&#25214;&#21040;&#30340;&#26032;&#26631;&#35760;&#30340;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#21152;&#24555;&#39044;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#8212;&#8212;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65292;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#20013;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;GPT-4&#65292;&#24182;&#19988;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.08614</link><description>&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#22270;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought. (arXiv:2308.08614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#8212;&#8212;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65292;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#20013;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;GPT-4&#65292;&#24182;&#19988;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#26631;&#20934;&#26597;&#35810;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#38656;&#35201;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24613;&#21095;&#19979;&#38477;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#25552;&#31034;&#24037;&#31243;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#8221;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#19978;&#36827;&#34892;&#27979;&#35797;&#65306;24&#28857;&#28216;&#25103;&#65292;&#39640;&#38454;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#35299;&#26512;&#65292;&#20197;&#21450;&#36882;&#24402;&#25968;&#21015;&#30340;&#20844;&#24335;&#25512;&#23548;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;GPT-4&#65292;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;$89.7\%$&#12289;$86\%$&#21644;$56\%$&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#8220;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#8221;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;$23\%$&#12289;$24\%$&#21644;$15\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\%$, $24\%$, and $15\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26368;&#23567;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#22312;&#20110;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#26102;&#38271;&#25110;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;10&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#31616;&#27573;&#33853;&#21644;&#38382;&#31572;&#23545;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#36275;&#29699;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08610</link><description>&lt;p&gt;
FootGPT&#65306;&#22312;&#26368;&#23567;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
FootGPT : A Large Language Model Development Experiment on a Minimal Setting. (arXiv:2308.08610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26368;&#23567;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#22312;&#20110;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#26102;&#38271;&#25110;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;10&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#31616;&#27573;&#33853;&#21644;&#38382;&#31572;&#23545;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#36275;&#29699;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#31070;&#32463;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#26102;&#38271;&#25110;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#24320;&#21457;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#26041;&#38754;&#21487;&#33021;&#26159;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#24615;&#23545;&#19968;&#20010;10&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25968;&#25454;&#38598;&#30001;&#24847;&#22823;&#21033;&#36275;&#29699;&#32852;&#36187;&#21069;&#21313;&#36718;&#30340;&#29699;&#38431;&#32479;&#35745;&#20449;&#24687;&#26500;&#24314;&#65292;&#24182;&#20351;&#29992;&#24378;&#22823;&#30340;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#31616;&#27573;&#33853;&#21644;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#26102;&#38271;&#20445;&#25345;&#30456;&#23545;&#36739;&#30701;&#65292;&#20197;&#25552;&#20379;&#25105;&#20204;&#26368;&#23567;&#35774;&#32622;&#25506;&#32034;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#19982;&#20351;&#29992;&#26377;&#38480;&#36164;&#28304;&#35299;&#37322;&#36275;&#29699;&#25968;&#25454;&#30340;&#29305;&#23450;&#30446;&#30340;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30456;&#20851;&#30340;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent empirical observations, it has been argued that the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters, training duration or dataset size. Following this argument, we opted to fine tune a one billion parameter size trained general purpose causal language model with a dataset curated on team statistics of the Italian football league first ten game weeks, using low rank adaptation. The limited training dataset was compiled based on a framework where a powerful commercial large language model provides distilled paragraphs and question answer pairs as intended. The training duration was kept relatively short to provide a basis for our minimal setting exploration. We share our key observations on the process related to developing a specific purpose language model which is intended to interpret soccer data with constrained resources in this article.
&lt;/p&gt;</description></item><item><title>AffectEcho&#26159;&#19968;&#31181;&#26080;&#20851;&#35828;&#35805;&#20154;&#21644;&#26080;&#20851;&#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#24773;&#24863;&#20256;&#36882;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#30721;&#20070;&#26469;&#24314;&#27169;&#24773;&#24863;&#65292;&#24182;&#22312;&#20116;&#20010;&#32423;&#21035;&#30340;&#24773;&#24863;&#24378;&#24230;&#19978;&#25429;&#25417;&#32454;&#24494;&#24046;&#21035;&#65292;&#20174;&#32780;&#25104;&#21151;&#22320;&#25511;&#21046;&#29983;&#25104;&#30340;&#35821;&#38899;&#30340;&#24773;&#24863;&#65292;&#21516;&#26102;&#20445;&#25345;&#27599;&#20010;&#35828;&#35805;&#20154;&#29420;&#29305;&#30340;&#36523;&#20221;&#12289;&#39118;&#26684;&#21644;&#24773;&#24863;&#33410;&#22863;&#12290;</title><link>http://arxiv.org/abs/2308.08577</link><description>&lt;p&gt;
AffectEcho&#65306;&#38024;&#23545;&#35821;&#38899;&#21512;&#25104;&#30340;&#26080;&#20851;&#35828;&#35805;&#20154;&#21644;&#26080;&#20851;&#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#24773;&#24863;&#20256;&#36882;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis. (arXiv:2308.08577v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08577
&lt;/p&gt;
&lt;p&gt;
AffectEcho&#26159;&#19968;&#31181;&#26080;&#20851;&#35828;&#35805;&#20154;&#21644;&#26080;&#20851;&#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#24773;&#24863;&#20256;&#36882;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#30721;&#20070;&#26469;&#24314;&#27169;&#24773;&#24863;&#65292;&#24182;&#22312;&#20116;&#20010;&#32423;&#21035;&#30340;&#24773;&#24863;&#24378;&#24230;&#19978;&#25429;&#25417;&#32454;&#24494;&#24046;&#21035;&#65292;&#20174;&#32780;&#25104;&#21151;&#22320;&#25511;&#21046;&#29983;&#25104;&#30340;&#35821;&#38899;&#30340;&#24773;&#24863;&#65292;&#21516;&#26102;&#20445;&#25345;&#27599;&#20010;&#35828;&#35805;&#20154;&#29420;&#29305;&#30340;&#36523;&#20221;&#12289;&#39118;&#26684;&#21644;&#24773;&#24863;&#33410;&#22863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26159;&#19968;&#31181;&#21253;&#25324;&#20215;&#20540;&#12289;&#21796;&#37266;&#24230;&#21644;&#24378;&#24230;&#30340;&#24773;&#32490;&#29305;&#24449;&#65292;&#23545;&#20110;&#23454;&#29616;&#30495;&#23454;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#21644;&#35821;&#38899;&#21040;&#35821;&#38899;&#31995;&#32479;&#20381;&#36182;&#20110;&#24378;&#24230;&#23884;&#20837;&#21521;&#37327;&#21644;&#20840;&#23616;&#39118;&#26684;&#26631;&#35760;&#26469;&#25429;&#25417;&#24773;&#24863;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#24773;&#24863;&#34920;&#31034;&#20026;&#39118;&#26684;&#30340;&#32452;&#25104;&#37096;&#20998;&#25110;&#20197;&#31163;&#25955;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#36716;&#25442;&#27169;&#22411;AffectEcho&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#20010;&#21521;&#37327;&#37327;&#21270;&#30721;&#20070;&#26469;&#22312;&#37327;&#21270;&#31354;&#38388;&#20013;&#23545;&#24773;&#24863;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#31354;&#38388;&#20855;&#26377;&#20116;&#20010;&#32423;&#21035;&#30340;&#24773;&#24863;&#24378;&#24230;&#65292;&#20197;&#25429;&#25417;&#21516;&#19968;&#24773;&#24863;&#20013;&#30340;&#22797;&#26434;&#32454;&#24494;&#24046;&#21035;&#12290;&#36825;&#20123;&#37327;&#21270;&#30340;&#24773;&#24863;&#23884;&#20837;&#26159;&#20174;&#21475;&#35821;&#35821;&#38899;&#26679;&#26412;&#20013;&#38544;&#21547;&#22320;&#27966;&#29983;&#20986;&#26469;&#30340;&#65292;&#28040;&#38500;&#20102;&#29420;&#28909;&#21521;&#37327;&#25110;&#26174;&#24335;&#24378;&#24230;&#23884;&#20837;&#30340;&#38656;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#29983;&#25104;&#30340;&#35821;&#38899;&#24773;&#24863;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#27599;&#20010;&#35828;&#35805;&#20154;&#29420;&#29305;&#30340;&#36523;&#20221;&#12289;&#39118;&#26684;&#21644;&#24773;&#24863;&#33410;&#22863;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#19987;&#23478;&#25351;&#23548;&#21644;&#36127;&#38754;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#36830;&#36143;&#21644;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#25805;&#20316;&#65292;&#21033;&#29992;&#23545;&#27604;&#24418;&#24335;&#25351;&#23548;LLMs&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#21644;&#19982;&#20808;&#21069;&#31034;&#20363;&#30340;&#20559;&#31163;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.07645</link><description>&lt;p&gt;
&#24341;&#23548;&#35821;&#35328;&#29983;&#25104;&#65306;&#21033;&#29992;&#23545;&#27604;&#19987;&#23478;&#25351;&#23548;&#21644;&#36127;&#38754;&#25552;&#31034;&#36827;&#34892;&#19968;&#33268;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation. (arXiv:2308.07645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#19987;&#23478;&#25351;&#23548;&#21644;&#36127;&#38754;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#36830;&#36143;&#21644;&#22810;&#26679;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#25805;&#20316;&#65292;&#21033;&#29992;&#23545;&#27604;&#24418;&#24335;&#25351;&#23548;LLMs&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#21644;&#19982;&#20808;&#21069;&#31034;&#20363;&#30340;&#20559;&#31163;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#23454;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#22312;&#20174;&#19979;&#28216;&#27169;&#22411;&#35757;&#32451;&#21040;&#23454;&#38469;&#25968;&#25454;&#21033;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#24456;&#22810;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#20195;&#27169;&#22411;&#30340;&#33021;&#21147;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#21364;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#26082;&#36830;&#36143;&#21448;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#36143;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#24418;&#24335;&#19987;&#23478;&#25351;&#23548;&#65292;&#24378;&#35843;&#20102;&#31934;&#32454;&#35843;&#25972;&#21644;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36923;&#36753;&#20998;&#24067;&#24046;&#24322;&#65292;&#20197;&#30830;&#20445;&#39046;&#22495;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#20445;&#35777;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#20363;&#23376;&#20316;&#20026;&#27169;&#22411;&#30340;&#36127;&#38754;&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#36923;&#36753;&#37325;&#22609;&#30340;&#21452;&#37325;&#26041;&#27861;&#31216;&#20026;STEER&#65306;&#36890;&#36807;&#23884;&#20837;&#37325;&#26032;&#23450;&#20301;&#23454;&#29616;&#30340;&#35821;&#20041;&#25991;&#26412;&#22686;&#24378;&#12290;STEER&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#31995;&#32479;&#22320;&#25351;&#23548;LLMs&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#65288;&#30830;&#20445;&#35821;&#20041;&#20445;&#30495;&#24230;&#65289;&#19982;&#20808;&#21069;&#21512;&#25104;&#31034;&#20363;&#25110;&#29616;&#26377;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;KGSimple&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#25216;&#26415;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#65292;&#23454;&#29616;&#20174;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#29983;&#25104;&#31616;&#26126;&#25991;&#26412;&#65292;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#24182;&#36755;&#20986;&#27969;&#30021;&#19988;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2308.06975</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#33021;&#31616;&#21270;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Knowledge Graphs Simplify Text?. (arXiv:2308.06975v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;KGSimple&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#25216;&#26415;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#65292;&#23454;&#29616;&#20174;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#29983;&#25104;&#31616;&#26126;&#25991;&#26412;&#65292;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#24182;&#36755;&#20986;&#27969;&#30021;&#19988;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#22312;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#21477;&#23376;&#26041;&#38754;&#26377;&#20102;&#26368;&#26032;&#30340;&#25913;&#36827;&#65292;&#36825;&#20123;&#21477;&#23376;&#25551;&#36848;&#20102;&#32473;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#30001;&#20110;&#30693;&#35782;&#22270;&#35889;&#22312;&#22810;&#20010;&#39046;&#22495;&#24191;&#27867;&#23384;&#22312;&#19988;&#21253;&#21547;&#37325;&#35201;&#30340;&#23454;&#20307;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#19988;&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20943;&#23569;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25991;&#26412;&#30340;&#24847;&#24605;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGSimple&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#23427;&#34701;&#20837;&#20102;&#30693;&#35782;&#22270;&#35889;&#25216;&#26415;&#26469;&#26500;&#24314;&#31616;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#36335;&#24452;&#65292;&#24182;&#29983;&#25104;&#20445;&#30041;&#21407;&#22987;&#36755;&#20837;&#24847;&#20041;&#30340;&#31616;&#26126;&#25991;&#26412;&#12290;&#36890;&#36807;&#36845;&#20195;&#21644;&#37319;&#26679;&#30340;&#20197;&#30693;&#35782;&#22270;&#35889;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#31616;&#21270;&#25991;&#26412;&#65292;&#36890;&#36807;&#23398;&#20064;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#24182;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#27969;&#30021;&#19988;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#30446;&#21069;&#21487;&#29992;&#30340;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;KGSimple&#27169;&#22411;&#30340;&#21508;&#31181;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplificat
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WikiTiDe&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#30340;&#26102;&#38388;&#25139;&#23450;&#20041;&#23545;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#21152;&#36895;&#21382;&#26102;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#20197;&#25195;&#25551;&#26680;&#24515;&#26356;&#26032;&#12290;&#36890;&#36807;&#33258;&#20030;&#31181;&#23376;&#29256;&#26412;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.03582</link><description>&lt;p&gt;
WIKITIDE: &#19968;&#20010;&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#26102;&#38388;&#25139;&#23450;&#20041;&#23545;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset. (arXiv:2308.03582v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WikiTiDe&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#30340;&#26102;&#38388;&#25139;&#23450;&#20041;&#23545;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#21152;&#36895;&#21382;&#26102;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#20197;&#25195;&#25551;&#26680;&#24515;&#26356;&#26032;&#12290;&#36890;&#36807;&#33258;&#20030;&#31181;&#23376;&#29256;&#26412;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#65292;&#30001;&#35821;&#35328;&#27169;&#22411;&#20027;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#20307;&#31995;&#26550;&#26500;&#23545;&#20110;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#28789;&#27963;&#24230;&#19981;&#22815;&#12290;&#34429;&#28982;&#23384;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#36830;&#32493;&#23398;&#20064;&#25110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22914;&#20309;&#21487;&#38752;&#22320;&#35782;&#21035;&#35821;&#35328;&#25110;&#19990;&#30028;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;WikiTiDe&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#30340;&#26102;&#38388;&#25139;&#23450;&#20041;&#23545;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#36164;&#28304;&#26377;&#21161;&#20110;&#21152;&#36895;&#21382;&#26102;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#30693;&#35782;&#36164;&#28304;&#20013;&#25195;&#25551;&#19982;&#27010;&#24565;&#12289;&#20107;&#20214;&#25110;&#21629;&#21517;&#23454;&#20307;&#30456;&#20851;&#30340;&#26680;&#24515;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#65292;&#24182;&#21033;&#29992;&#33258;&#20030;&#31639;&#27861;&#36880;&#27493;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#33258;&#20030;WikiTiDe&#30340;&#31181;&#23376;&#29256;&#26412;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#24212;&#29992;&#20102;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental challenge in the current NLP context, dominated by language models, comes from the inflexibility of current architectures to 'learn' new information. While model-centric solutions like continual learning or parameter-efficient fine tuning are available, the question still remains of how to reliably identify changes in language or in the world. In this paper, we propose WikiTiDe, a dataset derived from pairs of timestamped definitions extracted from Wikipedia. We argue that such resource can be helpful for accelerating diachronic NLP, specifically, for training models able to scan knowledge resources for core updates concerning a concept, an event, or a named entity. Our proposed end-to-end method is fully automatic, and leverages a bootstrapping algorithm for gradually creating a high-quality dataset. Our results suggest that bootstrapping the seed version of WikiTiDe leads to better fine-tuned models. We also leverage fine-tuned models in a number of downstream tasks, sh
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.01284</link><description>&lt;p&gt;
&#29992;&#28779;&#25915;&#28779;&#65306;ChatGPT&#33021;&#22815;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01284
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#20363;&#65292;&#21253;&#25324;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#38024;&#23545;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#22312;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#21463;&#21040;&#23558;ChatGPT&#29992;&#20316;&#25968;&#25454;&#26631;&#27880;&#22120;&#25110;&#27880;&#37322;&#22120;&#30340;&#30740;&#31350;&#21551;&#21457;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#25110;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#26159;&#21542;&#20855;&#26377;&#23545;&#31216;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#20851;&#27880;&#38382;&#39064;&#30340;&#29305;&#23450;&#26041;&#38754;&#24182;&#20174;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#25512;&#23548;&#20986;&#20854;&#20313;&#37096;&#20998;&#65292;&#22914;&#20309;&#21033;&#29992;ChatGPT&#21644;&#31867;&#20284;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312; \url{https://github.com/AmritaBh/ChatGPT-as-Detector} &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at \url{https://github.com/AmritaBh/ChatGPT-as-Detector}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GrammarGPT&#65292;&#19968;&#20010;&#24320;&#28304;LLM&#29992;&#20110;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20462;&#27491;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13923</link><description>&lt;p&gt;
GrammarGPT: &#20351;&#29992;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#25506;&#32034;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning. (arXiv:2307.13923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GrammarGPT&#65292;&#19968;&#20010;&#24320;&#28304;LLM&#29992;&#20110;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20462;&#27491;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26088;&#22312;&#33258;&#21160;&#20462;&#27491;&#19981;&#31526;&#21512;&#35821;&#27861;&#30340;&#21477;&#23376;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65292;&#20363;&#22914;ChatGPT&#65289;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;LLM&#30340;&#28508;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;GrammarGPT&#30340;&#24320;&#28304;LLM&#65292;&#26088;&#22312;&#21021;&#27493;&#25506;&#32034;&#20854;&#22312;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;GrammarGPT&#30340;&#26680;&#24515;&#26041;&#27861;&#26159;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#20154;&#24037;&#26631;&#27880;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#24102;&#26377;&#25552;&#31034;&#20449;&#24687;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25351;&#23548;ChatGPT&#36890;&#36807;&#25552;&#20379;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#29983;&#25104;&#19981;&#31526;&#21512;&#35821;&#27861;&#30340;&#21477;&#23376;&#12290;&#23545;&#20110;&#27809;&#26377;&#25552;&#31034;&#20449;&#24687;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#25105;&#20204;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#31449;&#25910;&#38598;&#20102;&#19981;&#31526;&#21512;&#35821;&#27861;&#30340;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25163;&#21160;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#38169;&#35823;&#19981;&#21464;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#35813;&#27169;&#22411;&#32416;&#27491;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction aims to correct ungrammatical sentences automatically. Recently, some work has demonstrated the excellent capabilities of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction. However, the potential of open-source LLMs remains unexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to preliminary explore its potential for native Chinese grammatical error correction. The core recipe of GrammarGPT is to leverage the hybrid dataset of ChatGPT-generated and human-annotated. For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences by providing those clues. For grammatical errors without clues, we collected ungrammatical sentences from publicly available websites and manually corrected them. In addition, we employed an error-invariant augmentation method to enhance the ability of the model to correct native Chinese grammatical errors. We ultimately 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GradObstinate&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24847;&#20041;&#25913;&#21464;&#20294;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#20445;&#25345;&#19981;&#21464;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#26080;&#38656;&#20154;&#24037;&#35774;&#35745;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.12507</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#35789;&#26367;&#25442;&#29992;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models. (arXiv:2307.12507v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GradObstinate&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24847;&#20041;&#25913;&#21464;&#20294;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#20445;&#25345;&#19981;&#21464;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#26080;&#38656;&#20154;&#24037;&#35774;&#35745;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36890;&#36807;&#35789;&#26367;&#25442;&#29983;&#25104;&#39037;&#22266;&#65288;&#36229;&#31283;&#23450;&#24615;&#65289;&#23545;&#25239;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#24847;&#20041;&#21457;&#29983;&#20102;&#25913;&#21464;&#65292;&#20294;&#27169;&#22411;&#30340;&#39044;&#27979;&#21364;&#27809;&#26377;&#21464;&#21270;&#65292;&#23613;&#31649;&#24212;&#35813;&#21457;&#29983;&#21464;&#21270;&#12290;&#20197;&#24448;&#30340;&#35789;&#26367;&#25442;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25163;&#21160;&#35774;&#35745;&#30340;&#21453;&#20041;&#35789;&#31574;&#30053;&#19978;&#65292;&#29992;&#20110;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#65292;&#36825;&#21046;&#32422;&#20102;&#23427;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#31574;&#30053;&#21482;&#33021;&#25214;&#21040;&#37096;&#20998;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35789;&#26367;&#25442;&#26041;&#27861;&#65292;&#21517;&#20026;GradObstinate&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#65292;&#19981;&#21463;&#25628;&#32034;&#31354;&#38388;&#38480;&#21046;&#25110;&#38656;&#27714;&#20154;&#24037;&#35774;&#35745;&#21407;&#21017;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;GradObstinate&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27169;&#22411;&#65288;Electra&#12289;ALBERT&#12289;Roberta&#12289;DistillBERT&#21644;CLIP&#65289;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of generating obstinate (over-stability) adversarial examples by word substitution in NLP, where input text is meaningfully changed but the model's prediction does not, even though it should. Previous word substitution approaches have predominantly focused on manually designed antonym-based strategies for generating obstinate adversarial examples, which hinders its application as these strategies can only find a subset of obstinate adversarial examples and require human efforts. To address this issue, in this paper, we introduce a novel word substitution method named GradObstinate, a gradient-based approach that automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles. To empirically evaluate the efficacy of GradObstinate, we conduct comprehensive experiments on five representative models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP benchmark
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#24378;&#35843;&#20102;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#21547;&#26377;&#24694;&#24847;&#25351;&#20196;&#30340;&#28508;&#22312;&#36234;&#29425;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20998;&#23618;&#27880;&#37322;&#26694;&#26550;&#65292;&#20840;&#38754;&#30740;&#31350;&#20102;&#25991;&#26412;&#23433;&#20840;&#24615;&#21644;&#36755;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08487</link><description>&lt;p&gt;
&#28508;&#22312;&#36234;&#29425;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23433;&#20840;&#24615;&#21644;&#36755;&#20986;&#40065;&#26834;&#24615;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models. (arXiv:2307.08487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08487
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#24378;&#35843;&#20102;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#21547;&#26377;&#24694;&#24847;&#25351;&#20196;&#30340;&#28508;&#22312;&#36234;&#29425;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20998;&#23618;&#27880;&#37322;&#26694;&#26550;&#65292;&#20840;&#38754;&#30740;&#31350;&#20102;&#25991;&#26412;&#23433;&#20840;&#24615;&#21644;&#36755;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#24182;&#29983;&#25104;&#23433;&#20840;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#26576;&#20123;&#20027;&#39064;&#30340;&#36807;&#24230;&#20851;&#27880;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#22312;&#23436;&#25104;&#20219;&#21153;&#26041;&#38754;&#30340;&#25972;&#20307;&#34920;&#29616;&#12290;&#20197;&#24448;&#29992;&#20110;&#36234;&#29425;LLMs&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;LLMs&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20840;&#38754;&#30740;&#31350;&#25991;&#26412;&#23433;&#20840;&#24615;&#21644;&#36755;&#20986;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28508;&#22312;&#36234;&#29425;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#21253;&#21547;&#24694;&#24847;&#25351;&#20196;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#24120;&#35268;&#20219;&#21153;&#65292;&#20363;&#22914;&#32763;&#35793;&#65292;&#20854;&#20013;&#24453;&#32763;&#35793;&#30340;&#25991;&#26412;&#21253;&#21547;&#24694;&#24847;&#25351;&#20196;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#27880;&#37322;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation fram
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13213</link><description>&lt;p&gt;
&#35270;&#35273;&#23545;&#25239;&#26679;&#26412;&#36234;&#29425;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Adversarial Examples Jailbreak Large Language Models. (arXiv:2306.13213v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#39640;&#24230;&#20851;&#27880;&#12290;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#20363;&#22914;Flamingo&#12289;BLIP-2&#21644;GPT-4&#65292;&#26631;&#24535;&#30528;&#35270;&#35273;&#21644;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20808;&#36827;&#21457;&#23637;&#30456;&#20114;&#34701;&#21512;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#28041;&#21450;&#30340;&#39118;&#38505;&#20173;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#22312;&#26412;&#36136;&#19978;&#20351;&#20854;&#25104;&#20026;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#25193;&#22823;&#20102;LLMs&#30340;&#25915;&#20987;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;LLMs&#30340;&#24191;&#27867;&#21151;&#33021;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#65292;&#23558;&#23433;&#20840;&#22833;&#36133;&#30340;&#24433;&#21709;&#25193;&#23637;&#21040;&#20102;&#31616;&#21333;&#30340;&#38169;&#35823;&#20998;&#31867;&#20043;&#22806;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLM&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12725</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26159;&#23558;&#24102;&#26377;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#25552;&#21450;&#26144;&#23556;&#21040;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#30340;&#24341;&#29992;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GEMEL &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#12290;&#25105;&#20204;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#21482;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#23618;&#20197;&#21551;&#29992;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20026;&#20102;&#23558; LLMs &#36866;&#24212; MEL &#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992; LLMs &#30340;&#26032;&#20852;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#23454;&#20363;&#20316;&#20026;&#31034;&#33539;&#26469;&#36827;&#34892;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#35843;&#25972;&#20102;&#22823;&#32422;0.3&#65285;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;GEMEL &#23601;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia). Prior MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking method, which leverages the capabilities of LLMs from large-scale pre-training to directly generate target entity names. We keep the vision and language model frozen and only train a linear layer to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emerging in-context learning (ICL) capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24503;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#24179;&#34892;&#21477;&#23376;&#20013;&#20351;&#29992;mBERT&#21644;XLM-R&#30340;&#35789;&#23884;&#20837;&#65292;&#32467;&#21512;&#30456;&#20284;&#24615;&#23545;&#40784;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;mBERT&#23545;&#32599;&#26364;&#20160;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;mBERT&#30340;&#35789;&#23884;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#32599;&#26364;&#20160;&#35821;&#36827;&#34892;&#35789;&#35821;&#23545;&#40784;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#21644;&#36866;&#29992;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.08702</link><description>&lt;p&gt;
mBERT&#26159;&#21542;&#29702;&#35299;&#32599;&#26364;&#20160;&#35821;&#65311;&#20351;&#29992;&#35789;&#35821;&#23545;&#40784;&#35780;&#20272;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Does mBERT understand Romansh? Evaluating word embeddings using word alignment. (arXiv:2306.08702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24503;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#24179;&#34892;&#21477;&#23376;&#20013;&#20351;&#29992;mBERT&#21644;XLM-R&#30340;&#35789;&#23884;&#20837;&#65292;&#32467;&#21512;&#30456;&#20284;&#24615;&#23545;&#40784;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;mBERT&#23545;&#32599;&#26364;&#20160;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;mBERT&#30340;&#35789;&#23884;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#32599;&#26364;&#20160;&#35821;&#36827;&#34892;&#35789;&#35821;&#23545;&#40784;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#21644;&#36866;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24503;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#20043;&#38388;&#30340;&#24179;&#34892;&#21477;&#23376;&#20013;&#65292;&#20351;&#29992;mBERT&#21644;XLM-R&#30340;&#35789;&#23884;&#20837;&#65292;&#32467;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35789;&#35821;&#23545;&#40784;&#27169;&#22411;&#65288;SimAlign&#21644;awesome-align&#65289;&#36827;&#34892;&#27979;&#35797;&#12290;&#30001;&#20110;&#32599;&#26364;&#20160;&#35821;&#26159;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;&#65292;&#25105;&#20204;&#22788;&#20110;&#38646;&#26679;&#26412;&#30340;&#24773;&#20917;&#12290;&#20351;&#29992;mBERT&#30340;&#35789;&#23884;&#20837;&#65292;&#20004;&#20010;&#27169;&#22411;&#30340;&#23545;&#40784;&#38169;&#35823;&#29575;&#20026;0.22&#65292;&#20248;&#20110;&#32479;&#35745;&#27169;&#22411;fast_align&#65292;&#24182;&#19988;&#19982;&#23545;&#20110;&#24050;&#30693;&#35821;&#35328;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35789;&#35821;&#23545;&#40784;&#30456;&#24403;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#35299;&#37322;&#20026;mBERT&#21253;&#21547;&#20102;&#23545;&#32599;&#26364;&#20160;&#35821;&#26377;&#24847;&#20041;&#21644;&#36866;&#29992;&#30340;&#20449;&#24687;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We test similarity-based word alignment models (SimAlign and awesome-align) in combination with word embeddings from mBERT and XLM-R on parallel sentences in German and Romansh. Since Romansh is an unseen language, we are dealing with a zero-shot setting. Using embeddings from mBERT, both models reach an alignment error rate of 0.22, which outperforms fast_align, a statistical model, and is on par with similarity-based word alignment for seen languages. We interpret these results as evidence that mBERT contains information that can be meaningful and applicable to Romansh.  To evaluate performance, we also present a new trilingual corpus, which we call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton of Grisons in German, Romansh and Italian in the past 25 years. The corpus contains 4 547 parallel documents and approximately 100 000 sentence pairs in each language combination. We additionally present a gold standard for German-Romansh word alignment. The data i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07622</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#25512;&#29702;&#20559;&#24046;&#8212;&#8212;&#20197;&#21450;&#22312;GPT-4&#20013;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#20852;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#65288;&#23588;&#20854;&#26159;GPT-3&#65289;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#65292;&#20197;&#21450;&#36981;&#24490;&#36825;&#31181;&#34892;&#20026;&#32780;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLM&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#23624;&#26381;&#20110;&#36825;&#20123;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Cognitive Reflection Test&#65288;CRT&#65289;&#21450;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#31350;&#20102;&#31867;&#20154;&#30452;&#35273;&#20915;&#31574;&#30340;&#31283;&#23450;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#26041;&#27861;&#35843;&#26597;LLM&#26377;&#28508;&#21147;&#25581;&#31034;&#21542;&#21017;&#26410;&#30693;&#30340;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.13512</link><description>&lt;p&gt;
&#33021;ChatGPT&#26816;&#27979;&#20986;&#24847;&#22270;&#21527;&#65311;&#35780;&#20272;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#20307;&#29616;&#22312;&#36890;&#36807;&#25552;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#23545;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;ChatGPT&#21644;OPT&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#26368;&#22823;&#27169;&#22411;&#29305;&#26377;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#21363;&#22312;&#32473;&#23450;Oracle&#36716;&#24405;&#30340;&#21508;&#31181;&#35821;&#35328;&#19978;&#65292;&#20854;&#21487;&#20197;&#25509;&#36817;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36866;&#21512;&#21333;&#20010;GPU&#30340;&#36739;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#36828;&#36828;&#33853;&#21518;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38169;&#35823;&#26696;&#20363;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26041;&#26696;&#65307;ChatGPT&#30340;&#21709;&#24212;&#20173;&#28982;&#26159;&#21512;&#29702;&#30340;&#12290;&#20294;&#26159;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#23545;ASR&#38169;&#35823;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#34920;&#26126;&#20102;&#23558;&#36825;&#20123;&#25991;&#26412;&#27169;&#22411;&#24212;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#20005;&#23803;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.
&lt;/p&gt;</description></item><item><title>&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12031</link><description>&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#65306;&#19968;&#31181;&#20855;&#26377;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#19987;&#23478;&#32423;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12031
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#25968;&#25454;&#38544;&#31169;&#12289;&#30417;&#31649;&#21512;&#35268;&#24615;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#65288;DBKE&#65289;&#12290;DBKE&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#38544;&#24335;&#30693;&#35782;&#24211;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#24378;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20026;&#21518;&#32493;&#29992;&#20363;&#25552;&#20379;&#20102;&#36719;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Camel&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#26469;&#23637;&#31034;DBKE&#30340;&#26377;&#25928;&#24615;&#12290;Clinical Camel&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#23427;&#36824;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#12289;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present immense potential in the medical field, yet concerns over data privacy, regulatory compliance, and model stability restrict their widespread adoption. Although the distillation of high-performing closed-source LLMs has proven effective for general tasks, their application in healthcare is limited due to reduced domain knowledge and remnants of alignment behavior hindering clinical tasks. To address these challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances models' implicit knowledge base and primes them for conversational recall, augmenting their conversational capabilities and enabling a soft alignment for subsequent use cases. By transforming dense academic source text into synthetic dialogue, DBKE broadens the model's knowledge base and enables a soft alignment that guides downstream behaviours. We present Clinical Camel, an open-source, healthcare-focused conversational model, to showcase the effectiveness of DBKE. Clin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.05189</link><description>&lt;p&gt;
SUR-adapter&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25991;&#26412;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20869;&#23481;&#20016;&#23500;&#24230;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#24403;&#36755;&#20837;&#30340;&#25552;&#31034;&#20026;&#31616;&#30701;&#30340;&#21465;&#36848;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#25552;&#39640;&#21465;&#36848;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Semantic Understanding&#21644;Reasoning adapter&#65288;SUR-adapter&#65289;&#65292;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#27880;&#37322;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;57,000&#20010;&#35821;&#20041;&#20462;&#27491;&#30340;&#22810;&#27169;&#24577;&#26679;&#26412;&#12290;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#31616;&#21333;&#30340;&#21465;&#36848;&#25552;&#31034;&#65292;&#19968;&#20010;&#22797;&#26434;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#21644;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#21465;&#36848;&#25552;&#31034;&#30340;&#35821;&#20041;&#34920;&#31034;&#19982;&#22797;&#26434;&#25552;&#31034;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#23558;&#20854;&#36716;&#31227;&#33267;&#25105;&#20204;&#30340;SUR-adapter&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04087</link><description>&lt;p&gt;
&#33258;&#25105;&#32534;&#36753;&#65306;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20154;&#31867;&#32534;&#31243;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#25552;&#39640;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#38382;&#39064;&#20013;&#25552;&#20379;&#30340;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#34917;&#20805;&#24615;&#27880;&#37322;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#27880;&#37322;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#29992;&#20110;&#32416;&#27491;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#19982;&#30452;&#25509;&#20174;LLMs&#29983;&#25104;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;APPS-dev&#19978;&#23558;pass@1&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;89&#65285;&#65292;&#22312;APPS-test&#19978;&#25552;&#39640;31&#65285;&#65292;&#22312;HumanEval&#19978;&#25552;&#39640;48&#65285;&#65292;&#36229;&#36807;&#20102;&#20061;&#20010;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20026;110M-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30001;&#19981;&#21516;&#35821;&#35328;&#30340;7&#20010;&#35821;&#20041;&#20851;&#31995;&#23450;&#20041;&#30340;&#35821;&#20041;&#32593;&#32476;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#32593;&#32476;&#20855;&#26377;&#26222;&#36941;&#30340;&#22522;&#26412;&#29305;&#24615;&#65306;&#31232;&#30095;&#12289;&#39640;&#24230;&#32858;&#38598;&#21644;&#33258;&#25105;&#32452;&#32455;&#21270;&#65292;&#24182;&#21576;&#29616;&#20986;&#24130;&#24459;&#24230;&#25968;&#20998;&#24067;&#12290;&#19968;&#20123;&#32593;&#32476;&#26174;&#31034;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21463;&#35821;&#27861;&#35268;&#21017;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26469;&#33258;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.12940</link><description>&lt;p&gt;
&#35821;&#20041;&#32593;&#32476;&#30340;&#25299;&#25169;&#24615;&#36136;&#21644;&#32452;&#32455;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Topological properties and organizing principles of semantic networks. (arXiv:2304.12940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30001;&#19981;&#21516;&#35821;&#35328;&#30340;7&#20010;&#35821;&#20041;&#20851;&#31995;&#23450;&#20041;&#30340;&#35821;&#20041;&#32593;&#32476;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#32593;&#32476;&#20855;&#26377;&#26222;&#36941;&#30340;&#22522;&#26412;&#29305;&#24615;&#65306;&#31232;&#30095;&#12289;&#39640;&#24230;&#32858;&#38598;&#21644;&#33258;&#25105;&#32452;&#32455;&#21270;&#65292;&#24182;&#21576;&#29616;&#20986;&#24130;&#24459;&#24230;&#25968;&#20998;&#24067;&#12290;&#19968;&#20123;&#32593;&#32476;&#26174;&#31034;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21463;&#35821;&#27861;&#35268;&#21017;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26469;&#33258;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25104;&#20026;&#35745;&#31639;&#26426;&#31639;&#27861;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#31243;&#24207;&#20381;&#38752;&#35821;&#20041;&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35774;&#35745;NLP&#31639;&#27861;&#26102;&#24517;&#39035;&#32771;&#34385;&#35821;&#20041;&#32593;&#32476;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#32467;&#26500;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;11&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;7&#20010;&#35821;&#20041;&#20851;&#31995;&#23450;&#20041;&#30340;ConceptNet&#35821;&#20041;&#32593;&#32476;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#32593;&#32476;&#20855;&#26377;&#26222;&#36941;&#30340;&#22522;&#26412;&#29305;&#24615;&#65306;&#23427;&#20204;&#26159;&#31232;&#30095;&#30340;&#12289;&#39640;&#24230;&#38598;&#32858;&#30340;&#65292;&#24182;&#21576;&#29616;&#20986;&#24130;&#24459;&#24230;&#25968;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#32593;&#32476;&#37117;&#26159;&#33258;&#25105;&#32452;&#32455;&#30340;&#12290;&#19968;&#20123;&#32593;&#32476;&#26174;&#31034;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21463;&#35821;&#27861;&#35268;&#21017;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;(&#22914;&#25289;&#19969;&#35821;&#12289;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;)&#30340;&#32593;&#32476;&#22312;&#24230;&#25968;&#20998;&#24067;&#26041;&#38754;&#26377;&#23792;&#20540;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting natural language is an increasingly important task in computer algorithms due to the growing availability of unstructured textual data. Natural Language Processing (NLP) applications rely on semantic networks for structured knowledge representation. The fundamental properties of semantic networks must be taken into account when designing NLP algorithms, yet they remain to be structurally investigated. We study the properties of semantic networks from ConceptNet, defined by 7 semantic relations from 11 different languages. We find that semantic networks have universal basic properties: they are sparse, highly clustered, and exhibit power-law degree distributions. Our findings show that the majority of the considered networks are scale-free. Some networks exhibit language-specific properties determined by grammatical rules, for example networks from highly inflected languages, such as e.g. Latin, German, French and Spanish, show peaks in the degree distribution that deviate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.01752</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36719;&#25552;&#31034;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#29992;&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26032;&#39046;&#22495;&#24341;&#21457;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#24615;&#33021;&#39640;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340; V-L &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PromptMize&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#36866;&#37197;&#22120;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#29983;&#25104;&#25552;&#31034;&#20449;&#21495;&#21644;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#25913;&#21892;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04415</link><description>&lt;p&gt;
&#24102;&#26377;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#35760;&#24518;&#30340;&#23569;&#26679;&#26412;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PromptMize&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#36866;&#37197;&#22120;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#29983;&#25104;&#25552;&#31034;&#20449;&#21495;&#21644;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#25913;&#21892;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#34920;&#26684;&#25968;&#25454;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#20351;&#24471;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#25991;&#26412;&#12290;&#22312;&#20302;&#36164;&#28304;&#29983;&#25104;&#20013;&#65292;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#21463;&#21040;&#20154;&#31867;&#22914;&#20309;&#20351;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#25551;&#36848;&#34920;&#26684;&#25968;&#25454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;PromptMize&#65292;&#35813;&#26694;&#26550;&#38024;&#23545;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#35774;&#35745;&#21253;&#21547;&#20004;&#20010;&#26041;&#38754;&#65306;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#36866;&#37197;&#22120;&#12290;&#25552;&#31034;&#31574;&#21010;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#25552;&#31034;&#20449;&#21495;&#65292;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23454;&#20363;&#25351;&#23548;&#65292;&#20197;&#24357;&#21512;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#36866;&#37197;&#22120;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#20013;&#35760;&#24518;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20379;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLM) have achieved remarkable advancement in table-to-text generation tasks. However, the lack of labeled domain-specific knowledge and the topology gap between tabular data and text make it difficult for PLMs to yield faithful text. Low-resource generation likewise faces unique challenges in this domain. Inspired by how humans descript tabular data with prior knowledge, we suggest a new framework: PromptMize, which targets table-to-text generation under few-shot settings. The design of our framework consists of two aspects: a prompt planner and a knowledge adapter. The prompt planner aims to generate a prompt signal that provides instance guidance for PLMs to bridge the topology gap between tabular data and text. Moreover, the knowledge adapter memorizes domain-specific knowledge from the unlabelled corpus to supply essential information during generation. Extensive experiments and analyses are investigated on three open domain few-shot NLG datasets: human
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptCap&#65292;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#35270;&#35273;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.09699</link><description>&lt;p&gt;
PromptCap&#65306;&#20351;&#29992;GPT-3&#30340;&#25552;&#31034;&#24341;&#23548;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3. (arXiv:2211.09699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptCap&#65292;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#35270;&#35273;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#28041;&#21450;&#38656;&#35201;&#36229;&#36234;&#22270;&#29255;&#20197;&#20135;&#29983;&#27491;&#30830;&#31572;&#26696;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#20351;LM&#29702;&#35299;&#22270;&#20687;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#23383;&#24149;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#20010;&#23383;&#24149;&#21477;&#23376;&#20013;&#24635;&#32467;&#22270;&#20687;&#26102;&#65292;&#35201;&#25551;&#36848;&#21738;&#20123;&#35270;&#35273;&#23454;&#20307;&#32463;&#24120;&#19981;&#26126;&#30830;&#12290;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#32463;&#24120;&#38169;&#36807;LM&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#25152;&#24517;&#38656;&#30340;&#35270;&#35273;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptCap&#65288;Prompt-guided image Captioning&#65289;&#65292;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#26088;&#22312;&#25104;&#20026;&#22270;&#20687;&#21644;&#40657;&#30418;LM&#20043;&#38388;&#26356;&#22909;&#30340;&#36830;&#25509;&#22120;&#12290;&#19982;&#36890;&#29992;&#23383;&#24149;&#19981;&#21516;&#65292;PromptCap&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#25511;&#21046;&#29983;&#25104;&#30340;&#23383;&#24149;&#20013;&#35201;&#25551;&#36848;&#30340;&#35270;&#35273;&#23454;&#20307;&#12290;&#25552;&#31034;&#21253;&#21547;&#23383;&#24149;&#24212;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should ai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#26415;&#21644;&#25913;&#36827;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.15445</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Utilization of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#26415;&#21644;&#25913;&#36827;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#24110;&#21161;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#35299;&#20915;&#20102;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#30828;&#20214;&#38480;&#21046;&#21644;&#24212;&#29992;&#31243;&#24207;&#32473;&#20986;&#20102;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#38477;&#20302;&#20854;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36234;&#21335;&#35821;&#21644;&#24503;&#35821;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#30005;&#35805;&#20250;&#35805;&#35821;&#38899;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#26080;&#30417;&#30563;&#25216;&#26415;&#36229;&#36234;&#31616;&#21333;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#23454;&#38469;&#30340;&#30005;&#35805;&#20219;&#21153;&#65292;&#21253;&#25324;&#24102;&#23485;&#20256;&#36755;&#65292;&#24182;&#35843;&#26597;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#25216;&#26415;&#30456;&#23545;&#20110;&#39033;&#30446;&#22522;&#32447;&#25552;&#39640;&#20102;22%&#12290;&#36890;&#36807;&#26550;&#26500;&#21644;&#35757;&#32451;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;29%&#65292;&#36890;&#36807;&#28155;&#21152;0.8&#23567;&#26102;&#30340;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pretraining techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36229;&#22797;&#25968;&#20195;&#25968;&#26469;&#34920;&#31034;&#21333;&#27169;&#24577;&#23884;&#20837;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.02743</link><description>&lt;p&gt;
&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces. (arXiv:2208.02743v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36229;&#22797;&#25968;&#20195;&#25968;&#26469;&#34920;&#31034;&#21333;&#27169;&#24577;&#23884;&#20837;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22914;Wikidata&#22312;&#34920;&#31034;&#30693;&#35782;&#26102;&#21253;&#21547;&#20102;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#12290;&#38024;&#23545;&#36825;&#20004;&#31181;&#27169;&#24577;&#65292;&#19987;&#38376;&#30340;&#22270;&#23884;&#20837;&#21644;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#23398;&#20064;&#20102;&#33021;&#22815;&#39044;&#27979;&#26032;&#30340;&#32467;&#26500;&#24615;&#30693;&#35782;&#30340;&#27169;&#24335;&#12290;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#23558;&#23398;&#20064;&#21644;&#25512;&#29702;&#19982;&#20004;&#31181;&#27169;&#24577;&#25972;&#21512;&#36215;&#26469;&#65292;&#32780;&#19988;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#37096;&#20998;&#22320;&#21033;&#29992;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#24378;&#22823;&#30340;&#21333;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#36229;&#22797;&#25968;&#20195;&#25968;&#34920;&#31034;&#21333;&#27169;&#24577;&#23884;&#20837;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#20197;&#21450;&#23427;&#20204;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#20114;&#34917;&#25163;&#27573;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22235;&#32500;&#36229;&#22797;&#25968;&#30340;&#20108;&#38754;&#20307;&#21644;&#22235;&#20803;&#25968;&#34920;&#31034;&#26469;&#25972;&#21512;&#22235;&#31181;&#27169;&#24577;&#65292;&#21363;&#32467;&#26500;&#24615;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#12289;&#35789;&#32423;&#34920;&#31034;&#65288;&#20363;&#22914;Word2vec&#12289;Fasttext&#65289;&#12289;&#21477;&#32423;&#34920;&#31034;&#65288;Sen
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge in order to represent knowledge. For each of the two modalities dedicated approaches for graph embedding and language models learn patterns that allow for predicting novel structural knowledge. Few approaches have integrated learning and inference with both modalities and these existing ones could only partially exploit the interaction of structural and textual knowledge. In our approach, we build on existing strong representations of single modalities and we use hypercomplex algebra to represent both, (i), single-modality embedding as well as, (ii), the interaction between different modalities and their complementary means of knowledge representation. More specifically, we suggest Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate four modalities namely structural knowledge graph embedding, word-level representations (e.g.\ Word2vec, Fasttext), sentence-level representations (Sen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#24847;&#31532;&#32490;&#35821;&#30340;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#36164;&#28304;&#65292;&#21363;&#21382;&#21490;&#24847;&#31532;&#32490;&#35821;Penn&#35299;&#26512;&#35821;&#26009;&#24211;&#21644;OCR&#24847;&#31532;&#32490;&#35821;&#25991;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#26080;&#38656;&#20808;&#8220;&#26631;&#20934;&#21270;&#8221;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#31616;&#21333;&#30340;&#38750;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#20063;&#33021;&#22815;&#25429;&#25417;&#21040;&#25340;&#20889;&#21464;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23637;&#31034;&#20102;&#26631;&#27880;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.01175</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24847;&#31532;&#32490;&#35821;&#30340;&#35789;&#24615;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Part-of-Speech Tagger for Yiddish. (arXiv:2204.01175v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01175
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#24847;&#31532;&#32490;&#35821;&#30340;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#36164;&#28304;&#65292;&#21363;&#21382;&#21490;&#24847;&#31532;&#32490;&#35821;Penn&#35299;&#26512;&#35821;&#26009;&#24211;&#21644;OCR&#24847;&#31532;&#32490;&#35821;&#25991;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#26080;&#38656;&#20808;&#8220;&#26631;&#20934;&#21270;&#8221;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#31616;&#21333;&#30340;&#38750;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#20063;&#33021;&#22815;&#25429;&#25417;&#21040;&#25340;&#20889;&#21464;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23637;&#31034;&#20102;&#26631;&#27880;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#24847;&#31532;&#32490;&#35821;&#30340;&#35789;&#24615;&#26631;&#27880;&#22120;&#30340;&#26500;&#24314;&#21644;&#35780;&#20272;&#12290;&#36825;&#26159;&#19968;&#20010;&#26356;&#22823;&#39033;&#30446;&#30340;&#31532;&#19968;&#27493;&#65292;&#26088;&#22312;&#33258;&#21160;&#20998;&#37197;&#24847;&#31532;&#32490;&#35821;&#25991;&#26412;&#30340;&#35789;&#24615;&#26631;&#31614;&#21644;&#21477;&#27861;&#32467;&#26500;&#65292;&#20197;&#20379;&#35821;&#35328;&#23398;&#30740;&#31350;&#20043;&#29992;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#36164;&#28304; - &#21382;&#21490;&#24847;&#31532;&#32490;&#35821;Penn&#35299;&#26512;&#35821;&#26009;&#24211;&#65288;PPCHY&#65289;&#30340;&#19968;&#20010;80K&#23383;&#23376;&#38598;&#21644;&#26469;&#33258;&#24847;&#31532;&#32490;&#20070;&#31821;&#20013;&#24515;&#65288;YBC&#65289;&#30340;650&#30334;&#19975;&#23383;&#30340;OCR&#24847;&#31532;&#32490;&#35821;&#25991;&#26412;&#12290;YBC&#35821;&#26009;&#24211;&#20013;&#30340;&#24847;&#31532;&#32490;&#35821;&#27491;&#23383;&#27861;&#23384;&#22312;&#35768;&#22810;&#25340;&#20889;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;YBC&#19978;&#35757;&#32451;&#30340;&#31616;&#21333;&#38750;&#19978;&#19979;&#25991;&#21270;&#30340;&#23884;&#20837;&#20063;&#33021;&#22815;&#25429;&#25417;&#21040;&#25340;&#20889;&#21464;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#20808;&#8220;&#26631;&#20934;&#21270;&#8221;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;YBC&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#23558;&#20854;&#25972;&#21512;&#21040;&#22312;PPCHY&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#26631;&#27880;&#22120;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;10&#25240;&#20132;&#21449;&#39564;&#35777;&#20998;&#26512;&#20102;&#26631;&#27880;&#22120;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;YBC&#25991;&#26412;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the construction and evaluation of a part-of-speech tagger for Yiddish. This is the first step in a larger project of automatically assigning part-of-speech tags and syntactic structure to Yiddish text for purposes of linguistic research. We combine two resources for the current work - an 80K-word subset of the Penn Parsed Corpus of Historical Yiddish (PPCHY) and 650 million words of OCR'd Yiddish text from the Yiddish Book Center (YBC). Yiddish orthography in the YBC corpus has many spelling inconsistencies, and we present some evidence that even simple non-contextualized embeddings trained on YBC are able to capture the relationships among spelling variants without the need to first "standardize" the corpus. We also use YBC for continued pretraining of contexualized embeddings, which are then integrated into a tagger model trained and evaluated on the PPCHY. We evaluate the tagger performance on a 10-fold cross-validation split, showing that the use of the YBC text for th
&lt;/p&gt;</description></item></channel></rss>