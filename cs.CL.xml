<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02456</link><description>&lt;p&gt;
PhonologyBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#38901;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
PhonologyBench: Evaluating Phonological Skills of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02456
&lt;/p&gt;
&lt;p&gt;
PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#38901;&#23398;&#26159;&#30740;&#31350;&#35821;&#38899;&#32467;&#26500;&#21644;&#21457;&#38899;&#35268;&#21017;&#30340;&#23398;&#31185;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30740;&#31350;&#20013;&#19968;&#20010;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LLMs&#22312;&#21508;&#31181;&#21033;&#29992;&#38899;&#38901;&#23398;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#25945;&#32946;&#24037;&#20855;&#21644;&#35799;&#27468;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#23436;&#32654;&#30340;&#27491;&#23383;&#21644;&#38899;&#26631;&#24418;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#30340;&#38899;&#38901;&#25216;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhonologyBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19977;&#20010;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#26126;&#30830;&#27979;&#35797;LLMs&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65306;&#24418;&#38899;&#36716;&#25442;&#12289;&#38899;&#33410;&#35745;&#25968;&#21644;&#25276;&#38901;&#35789;&#29983;&#25104;&#12290;&#23613;&#31649;&#27809;&#26377;&#35775;&#38382;&#35821;&#38899;&#25968;&#25454;&#65292;LLMs&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25276;&#38901;&#35789;&#29983;&#25104;&#21644;&#38899;&#33410;&#35745;&#25968;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;17%&#21644;45%&#30340;&#24046;&#36317;&#65292; respectively, when...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#65292;&#21457;&#29616;&#34429;&#28982;LLMs&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;</title><link>https://arxiv.org/abs/2404.02124</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;
&lt;/p&gt;
&lt;p&gt;
Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#65292;&#21457;&#29616;&#34429;&#28982;LLMs&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#22312;&#20960;&#20046;&#25152;&#26377;&#25945;&#32946;&#23618;&#27425;&#20013;&#37117;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26131;&#20110;&#31649;&#29702;&#12289;&#35780;&#20998;&#65292;&#24182;&#19988;&#26159;&#35780;&#20272;&#21644;&#23454;&#36341;&#20013;&#21487;&#38752;&#30340;&#26684;&#24335;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#24178;&#25200;&#39033;&#65292;&#21363;&#38024;&#23545;&#30495;&#23454;&#23398;&#29983;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#32780;&#35774;&#35745;&#30340;&#19981;&#27491;&#30830;&#36873;&#39033;&#12290;&#30446;&#21069;&#65292;&#21046;&#20316;&#39640;&#36136;&#37327;&#24178;&#25200;&#39033;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#26159;&#25945;&#24072;&#21644;&#23398;&#20064;&#20869;&#23481;&#35774;&#35745;&#32773;&#30340;&#21171;&#21160;&#21644;&#32791;&#26102;&#24037;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#39046;&#22495;&#20013;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#21040;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;&#34429;&#28982;LLM&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#23427;&#20204;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02000</link><description>&lt;p&gt;
&#38750;&#27954;&#20013;&#24515;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#22320;&#21306;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#34920;&#24449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20174;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#35762;&#35805;&#30340;21&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#23398;&#20064;&#20102;&#36817;60,000&#23567;&#26102;&#30340;&#26410;&#26631;&#35760;&#35821;&#38899;&#29255;&#27573;&#12290;&#22312;FLEURS-102&#25968;&#25454;&#38598;&#30340;SSA&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;HuBERT$_{base}$ (0.09B) &#26550;&#26500;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#19982;FLEURS&#22522;&#20934;&#25552;&#20986;&#30340;w2v-bert-51 (0.6B) &#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;ASR&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;7&#20493;&#65292;&#21442;&#25968;&#23569;6&#20493;&#12290;&#27492;&#22806;&#65292;&#22312;LID&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#36229;&#36807;FLEURS&#22522;&#32447;&#36229;&#36807;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01744</link><description>&lt;p&gt;
Octopus v2&#65306;&#29992;&#20110;&#36229;&#32423;&#20195;&#29702;&#30340;&#35774;&#22791;&#19978;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Octopus v2: On-device language model for super agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#19982;&#33258;&#21160;&#24037;&#20316;&#27969;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#35843;&#29992;&#20989;&#25968;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#21019;&#24314;AI&#20195;&#29702;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20113;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#30528;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#29992;&#20110;&#20989;&#25968;&#35843;&#29992;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#38754;&#20020;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;GPT-4&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;&#20102;95%&#12290;&#19982;&#22522;&#20110;RAG&#30340;&#20989;&#25968;&#35843;&#29992;&#26426;&#21046;&#30340;Llama-7B&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;35&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;&#36866;&#21512;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#27700;&#24179;&#19978;&#65292;&#31526;&#21512;&#24615;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#22312;SemEval-2024&#20219;&#21153;8&#19978;&#30340;&#34920;&#29616;&#20998;&#26512;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#37492;&#21035;&#22120;Transformer&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#21477;&#23376;Transformer&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#37096;&#20998;&#24773;&#20917;&#19979;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#38024;&#23545;FLAN-T5&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.14989</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024&#20219;&#21153;8&#19978;&#30340;&#34920;&#29616;&#20998;&#26512;&#65306;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#22312;SemEval-2024&#20219;&#21153;8&#19978;&#30340;&#34920;&#29616;&#20998;&#26512;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#37492;&#21035;&#22120;Transformer&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#21477;&#23376;Transformer&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#37096;&#20998;&#24773;&#20917;&#19979;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#38024;&#23545;FLAN-T5&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#30340;&#24773;&#20917; - &#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#35813;&#20219;&#21153;&#28085;&#30422;&#20102;&#20108;&#20803;&#20154;&#24037;&#25776;&#20889; vs. &#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#65288;Track A&#65289;&#12289;&#22810;&#36335;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#65288;Track B&#65289;&#21644;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65288;Track C&#65289;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#37492;&#21035;&#22120;Transformer&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#20197;&#21450;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#21477;&#23376;Transformer&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;Track A&#21644;B&#65292;&#36824;&#20351;&#29992;&#20102;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#23545;FLAN-T5&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14989v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 8 - Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection. The task encompasses Binary Human-Written vs. Machine-Generated Text Classification (Track A), Multi-Way Machine-Generated Text Classification (Track B), and Human-Machine Mixed Text Detection (Track C). Our best performing approaches utilize mainly the ensemble of discriminator transformer models along with sentence transformer and statistical machine learning approaches in specific cases. Moreover, zero-shot prompting and fine-tuning of FLAN-T5 are used for Track A and B.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14932</link><description>&lt;p&gt;
&#19987;&#27880;&#39537;&#21160;&#30340;&#25512;&#29702;:&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Reasoning: Unlocking the Potential of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#22522;&#30784;&#26426;&#21046;&#20173;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#26469;&#22686;&#24378;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#38750;&#35821;&#20041;&#26631;&#35760;&#23548;&#33268;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#20302;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#37325;&#26032;&#24179;&#34913;&#20559;&#26012;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25277;&#35937;&#26356;&#21152;&#24494;&#22937;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25512;&#29702;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#22312;LLMs&#25512;&#29702;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36825;&#20123;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
&lt;/p&gt;</description></item><item><title>&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.06563</link><description>&lt;p&gt;
&#25581;&#24320;&#32553;&#25918;&#23450;&#24459;&#20043;&#35868;&#65306;&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Mystery of Scaling Laws: Part I
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06563
&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#34920;&#26126;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#24130;&#23450;&#24459;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21407;&#21017;&#22312;&#20248;&#21270;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;Llama&#21644;Gemini&#65289;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;OpenAI&#30340;&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#24182;&#26410;&#25259;&#38706;&#25512;&#23548;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#20844;&#24335;&#25152;&#24517;&#38656;&#30340;&#23436;&#25972;&#32454;&#33410;&#65292;&#20182;&#20204;&#30340;&#32467;&#35770;&#20165;&#22522;&#20110;&#21253;&#21547;&#39640;&#36798;15&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#19968;&#20123;&#21518;&#32493;&#20316;&#21697;&#35797;&#22270;&#25581;&#31034;&#36825;&#20123;&#32454;&#33410;&#24182;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#37325;&#35201;&#22240;&#32032;&#30340;&#35757;&#32451;&#20381;&#36182;&#24615;&#65292;&#22914;&#23398;&#20064;&#36895;&#29575;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#65292;&#23548;&#33268;&#23427;&#20204;&#26410;&#33021;&#24314;&#31435;&#19968;&#20010;&#21487;&#38752;&#30340;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#30340;&#20844;&#24335;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
&lt;/p&gt;</description></item><item><title>Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.04460</link><description>&lt;p&gt;
Pearl: &#19968;&#39033;&#22522;&#20110;&#35780;&#35770;&#39537;&#21160;&#30340;&#35282;&#33394;&#30693;&#35782;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04460
&lt;/p&gt;
&lt;p&gt;
Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#20351;&#24471;&#23545;&#35805;&#36755;&#20837;&#30340;&#22810;&#26679;&#21270;&#25512;&#29702;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#36824;&#26377;&#35768;&#22810;&#26041;&#38754;&#26377;&#24453;&#25506;&#32034;&#12290;&#30446;&#21069;&#21487;&#29992;&#30340;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#32570;&#20047;&#29305;&#23450;&#29992;&#25143;&#20559;&#22909;&#21644;&#23545;&#25512;&#33616;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PEARL&#65292;&#19982;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;LLM&#27169;&#25311;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#35780;&#35770;&#20013;&#33719;&#24471;&#35814;&#32454;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36807;57k&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEARL&#20013;&#30340;&#35805;&#35821;&#21253;&#25324;&#26356;&#20855;&#20307;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#26174;&#31034;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
&lt;/p&gt;</description></item><item><title>Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13917</link><description>&lt;p&gt;
LLM&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#37325;&#35201;&#35821;&#35328;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Linguistic Features and Languages are Important in LLM Translation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13917
&lt;/p&gt;
&lt;p&gt;
Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.13917v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#33021;&#21147;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;Llama2&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#32763;&#35793;&#22914;&#20309;&#21462;&#20915;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;7B Llama2&#27169;&#22411;&#23545;&#20854;&#25152;&#35265;&#30340;&#25152;&#26377;&#35821;&#35328;&#37117;&#21487;&#20197;&#33719;&#24471;&#36229;&#36807;10&#30340;BLEU&#20998;&#25968;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#23545;&#20854;&#26410;&#35265;&#30340;&#35821;&#35328;&#12290;&#23545;&#20110;&#36825;&#20123;&#26410;&#35265;&#35821;&#35328;&#65292;&#19982;&#20351;&#29992;&#32842;&#22825;&#29256;&#26412;&#25110;&#28155;&#21152;&#23569;&#37327;&#25968;&#25454;&#30456;&#27604;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#35266;&#23519;&#21040;&#30340;&#26368;&#22823;&#25910;&#30410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#36317;&#31163;&#20998;&#26512;&#26174;&#31034;&#65292;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#22987;&#32456;&#26159;&#20915;&#23450;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#35821;&#35328;&#22240;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#35821;&#35328;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#26126;&#26174;&#23569;&#20110;&#33521;&#35821;&#65292;&#21364;&#34920;&#29616;&#20986;&#19982;&#33521;&#35821;&#21487;&#27604;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30340;&#21457;&#29616;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13917v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09949</link><description>&lt;p&gt;
&#22810;&#35789;&#26631;&#35760;&#21270;&#29992;&#20110;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Multi-Word Tokenization for Sequence Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24847;&#21619;&#30528;&#35745;&#31639;&#25104;&#26412;&#30340;&#22823;&#24133;&#22686;&#21152;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#12290;MWT&#20135;&#29983;&#20102;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#32467;&#26524;&#65292;&#24102;&#26469;&#20004;&#20010;&#22909;&#22788;&#65306;&#65288;1&#65289;&#22312;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#21644;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35206;&#30422;&#36755;&#20837;&#25968;&#25454;&#65307;&#65288;2&#65289;&#30001;&#20110;&#33021;&#22815;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#32780;&#23545;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#36895;&#21644;&#26356;&#36731;&#37327;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MWT&#22312;&#36739;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#26356;&#20026;&#31283;&#20581;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#26089;&#26399;&#24207;&#21015;&#25130;&#26029;&#23454;&#29616;&#37325;&#22823;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09949v1 Announce Type: new  Abstract: Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01740</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#36127;&#33655;&#19979;&#30340;&#34917;&#20607;&#24615;&#20559;&#35265;&#65306;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36873;&#25321;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;gpt-3.5-turbo&#21644;claude-instant-1.2&#22312;&#35299;&#37322;&#21644;&#25191;&#34892;&#35821;&#20041;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#24433;&#21709;&#26368;&#22823;&#30340;&#26159;&#20174;&#21015;&#34920;&#20013;&#36827;&#34892;&#23545;&#35937;&#36873;&#25321;&#65292;&#36825;&#26159;&#25968;&#23383;&#23548;&#33322;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#26816;&#26597;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20195;&#34920;&#24615;&#21015;&#34920;&#36873;&#25321;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#25511;&#21046;&#23454;&#39564;&#65292;&#25105;&#20204;&#25805;&#32437;&#20102;&#28201;&#24230;&#12289;&#21015;&#34920;&#38271;&#24230;&#12289;&#23545;&#35937;&#36523;&#20221;&#12289;&#23545;&#35937;&#31867;&#22411;&#12289;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23396;&#31435;&#21644;&#27979;&#37327;&#36825;&#20123;&#20559;&#35265;&#23545;&#36873;&#25321;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#35265;&#32467;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#65292;&#32780;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#23384;&#22312;&#36739;&#24378;&#30340;&#21021;&#29616;&#25928;&#24212;&#65292;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#20250;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.00157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematical Reasoning: Progresses and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#35780;&#20272;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30707;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#25968;&#23398;&#38382;&#39064;&#30340;&#31867;&#22411;&#38750;&#24120;&#24191;&#27867;&#65292;LLM&#30456;&#20851;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#24471;&#22914;&#20309;&#21028;&#26029;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#30495;&#27491;&#36827;&#23637;&#21644;&#38556;&#30861;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#21253;&#25324;&#20102;&#20197;&#19979;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;i&#65289;&#20840;&#38754;&#25506;&#32034;&#21508;&#31181;&#24050;&#32463;&#30740;&#31350;&#30340;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#30456;&#24212;&#25968;&#25454;&#38598;&#65307;ii&#65289;&#30740;&#31350;&#25552;&#20986;&#30340;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;LLM&#25216;&#26415;&#30340;&#33539;&#22260;&#65307;iii&#65289;&#27010;&#36848;&#24433;&#21709;LLM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22240;&#32032;&#21644;&#20851;&#27880;&#28857;&#65307;iv&#65289;&#38416;&#26126;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges with
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;REE-HDSC&#39033;&#30446;&#65292;&#26088;&#22312;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#36719;&#20214;&#33258;&#21160;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20845;&#27493;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27969;&#31243;&#22312;&#22788;&#29702;&#24211;&#25289;&#32034;&#27665;&#20107;&#30331;&#35760;&#22788;&#30340;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#27515;&#20129;&#35777;&#20070;&#26102;&#65292;&#26085;&#26399;&#25552;&#21462;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#20154;&#21517;&#25552;&#21462;&#30340;&#31934;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;HTR&#27169;&#22411;&#12289;&#21518;&#22788;&#29702;&#21644;&#35782;&#21035;&#21024;&#38500;&#19981;&#27491;&#30830;&#30340;&#21517;&#23383;&#26469;&#25552;&#39640;&#20154;&#21517;&#25552;&#21462;&#31934;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02972</link><description>&lt;p&gt;
REE-HDSC: &#35782;&#21035;&#21382;&#21490;&#25968;&#25454;&#24211;Suriname Curacao&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
REE-HDSC: Recognizing Extracted Entities for the Historical Database Suriname Curacao. (arXiv:2401.02972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;REE-HDSC&#39033;&#30446;&#65292;&#26088;&#22312;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#36719;&#20214;&#33258;&#21160;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20845;&#27493;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27969;&#31243;&#22312;&#22788;&#29702;&#24211;&#25289;&#32034;&#27665;&#20107;&#30331;&#35760;&#22788;&#30340;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#27515;&#20129;&#35777;&#20070;&#26102;&#65292;&#26085;&#26399;&#25552;&#21462;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#20154;&#21517;&#25552;&#21462;&#30340;&#31934;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;HTR&#27169;&#22411;&#12289;&#21518;&#22788;&#29702;&#21644;&#35782;&#21035;&#21024;&#38500;&#19981;&#27491;&#30830;&#30340;&#21517;&#23383;&#26469;&#25552;&#39640;&#20154;&#21517;&#25552;&#21462;&#31934;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;REE-HDSC&#39033;&#30446;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#21162;&#21147;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#36719;&#20214;&#29983;&#25104;&#30340;&#25991;&#26412;&#33258;&#21160;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#36136;&#37327;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20845;&#27493;&#22788;&#29702;&#27969;&#31243;&#65292;&#24182;&#36890;&#36807;&#22788;&#29702;&#24211;&#25289;&#32034;&#27665;&#20107;&#30331;&#35760;&#22788;&#30340;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#27515;&#20129;&#35777;&#20070;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#27969;&#27700;&#32447;&#25552;&#21462;&#30340;&#26085;&#26399;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#20294;&#20154;&#21517;&#25552;&#21462;&#30340;&#31934;&#24230;&#36739;&#20302;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21547;&#26377;&#21517;&#23383;&#30340;HTR&#27169;&#22411;&#12289;&#21518;&#22788;&#29702;&#20197;&#21450;&#35782;&#21035;&#21644;&#21024;&#38500;&#19981;&#27491;&#30830;&#30340;&#21517;&#23383;&#26469;&#25913;&#21892;&#21517;&#23383;&#25552;&#21462;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the project REE-HDSC and outline our efforts to improve the quality of named entities extracted automatically from texts generated by hand-written text recognition (HTR) software. We describe a six-step processing pipeline and test it by processing 19th and 20th century death certificates from the civil registry of Curacao. We find that the pipeline extracts dates with high precision but that the precision of person name extraction is low. Next we show how name precision extraction can be improved by retraining HTR models with names, post-processing and by identifying and removing incorrect names.
&lt;/p&gt;</description></item><item><title>ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.00176</link><description>&lt;p&gt;
ChipNeMo: &#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;LLMs
&lt;/p&gt;
&lt;p&gt;
ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00176
&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#20351;&#29992;&#21830;&#19994;&#25110;&#24320;&#28304;LLMs&#65292;&#32780;&#26159;&#37319;&#29992;&#20197;&#19979;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65306;&#23450;&#21046;&#20998;&#35789;&#22120;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#24102;&#26377;&#39046;&#22495;&#29305;&#23450;&#25351;&#20196;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33455;&#29255;&#35774;&#35745;&#30340;&#19977;&#20010;&#36873;&#23450;LLM&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65306;&#24037;&#31243;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;EDA&#33050;&#26412;&#29983;&#25104;&#20197;&#21450;&#32570;&#38519;&#25688;&#35201;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20351;LLM&#22312;&#36825;&#19977;&#20010;&#24212;&#29992;&#20013;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#22312;&#21508;&#31181;&#35774;&#35745;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;5&#20493;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32467;&#26524;&#21644;&#29702;&#24819;&#32467;&#26524;&#20043;&#38388;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#30456;&#20449;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
&lt;/p&gt;</description></item><item><title>CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.20550</link><description>&lt;p&gt;
CapsFusion: &#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20550
&lt;/p&gt;
&lt;p&gt;
CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#35268;&#27169;&#22522;&#20110;&#32593;&#32476;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#22312;&#36825;&#19968;&#25104;&#21151;&#20013;&#36215;&#30528;&#26681;&#26412;&#24615;&#30340;&#36129;&#29486;&#65292;&#20294;&#23384;&#22312;&#30528;&#36807;&#22810;&#30340;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#30001;&#29983;&#25104;&#24335;&#23383;&#24149;&#27169;&#22411;&#21512;&#25104;&#30340;&#26367;&#20195;&#23383;&#24149;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#23383;&#24149;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#21644;&#19990;&#30028;&#30693;&#35782;&#20007;&#22833;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#20854;&#21021;&#22987;&#22522;&#20934;&#25104;&#21151;&#20013;&#22823;&#37096;&#20998;&#34987;&#25513;&#30422;&#20102;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#21512;&#25104;&#23383;&#24149;&#20013;&#36807;&#20110;&#31616;&#21270;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#32570;&#20047;&#30693;&#35782;&#32454;&#33410;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CapsFusion&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
&lt;/p&gt;</description></item><item><title>LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;</title><link>http://arxiv.org/abs/2309.17157</link><description>&lt;p&gt;
LatticeGen: &#19968;&#31181;&#22312;&#20113;&#19978;&#36827;&#34892;&#38544;&#31169;&#24863;&#30693;&#29983;&#25104;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#38544;&#34255;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#26684;&#23376;&#20013;
&lt;/p&gt;
&lt;p&gt;
LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17157
&lt;/p&gt;
&lt;p&gt;
LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29992;&#25143;-&#26381;&#21153;&#22120;&#20132;&#20114;&#27169;&#24335;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#26381;&#21153;&#22120;&#23436;&#20840;&#25511;&#21046;&#30528;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20351;&#24471;&#24819;&#35201;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20445;&#30041;&#32473;&#33258;&#24049;&#30340;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LatticeGen&#65292;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#22788;&#29702;&#22823;&#37096;&#20998;&#35745;&#31639;&#20219;&#21153;&#65292;&#32780;&#29992;&#25143;&#25511;&#21046;&#37319;&#26679;&#25805;&#20316;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#25143;&#23558;&#30495;&#23454;&#29983;&#25104;&#24207;&#21015;&#19982;&#22122;&#22768;&#26631;&#35760;&#28151;&#21512;&#65292;&#24182;&#38544;&#34255;&#22312;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20013;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#20551;&#35774;&#24694;&#24847;&#26381;&#21153;&#22120;&#30340;&#28508;&#22312;&#25915;&#20987;&#20197;&#21450;&#29992;&#25143;&#22914;&#20309;&#36827;&#34892;&#38450;&#24481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#22797;&#27874;&#26463;&#25628;&#32034;&#25915;&#20987;&#21644;&#28151;&#21512;&#22122;&#22768;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;LatticeGen&#24212;&#29992;&#20110;&#20445;&#25252;&#25552;&#31034;&#21644;&#29983;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20250;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;LatticeGen&#25104;&#21151;&#22320;&#22312;&#24378;&#25915;&#20987;&#19979;&#26174;&#33879;&#20445;&#25252;&#20102;&#30495;&#23454;&#29983;&#25104;&#65288;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;Struc-Bench&#21644;&#22810;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#24120;&#35265;&#30340;&#26684;&#24335;&#38169;&#35823;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.08963</link><description>&lt;p&gt;
Struc-Bench&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#24471;&#30495;&#30340;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?. (arXiv:2309.08963v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;Struc-Bench&#21644;&#22810;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#24120;&#35265;&#30340;&#26684;&#24335;&#38169;&#35823;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#38656;&#35201;&#22797;&#26434;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#20316;&#20026;&#25913;&#36827;&#36825;&#31181;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Struc-Bench&#65292;&#21253;&#25324;&#20116;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#65288;&#21363;GPT-NeoX 20B&#65292;GPT-3.5&#65292;GPT-4&#21644;Vicuna&#65289;&#65292;&#24182;&#22312;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#30340;&#36328;&#21407;&#22987;&#25991;&#26412;&#12289;HTML&#21644;LaTeX&#34920;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#24403;&#21069;&#27169;&#22411;&#24615;&#33021;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#29305;&#23450;&#30340;&#24120;&#35265;&#26684;&#24335;&#38169;&#35823;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#30340;&#26684;&#24335;&#35201;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;FormatCoT&#65288;&#24605;&#32500;&#38142;&#65289;&#20174;&#30446;&#26631;&#36755;&#20986;&#20013;&#29983;&#25104;&#26684;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23558;&#36825;&#31181;&#32467;&#26500;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#24212;&#29992;&#21040;LLaMA-7B&#19978;&#26102;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the power of Large Language Models (LLMs) like GPT-4, they still struggle with tasks that require generating complex, structured outputs. In this study, we assess the capability of Current LLMs in generating complex structured data and propose a structure-aware fine-tuning approach as a solution to improve this ability. To perform a comprehensive evaluation, we propose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B, GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of current model performance, we identify specific common formatting errors and areas of potential improvement. To address complex formatting requirements, we utilize FormatCoT (Chain-of-Thought) to generate format instructions from target outputs. Our experiments show that our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraint
&lt;/p&gt;</description></item><item><title>PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.07759</link><description>&lt;p&gt;
PROGrasp:&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
PROGrasp: Pragmatic Human-Robot Communication for Object Grasping. (arXiv:2309.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07759
&lt;/p&gt;
&lt;p&gt;
PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#29289;&#20307;&#25235;&#21462;(IOG)&#26159;&#36890;&#36807;&#20154;&#26426;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#24403;&#21069;IOG&#31995;&#32479;&#20551;&#23450;&#20154;&#31867;&#29992;&#25143;&#26368;&#21021;&#25351;&#23450;&#30446;&#26631;&#23545;&#35937;&#30340;&#31867;&#21035;(&#20363;&#22914;&#65292;&#29942;&#23376;)&#12290;&#21463;&#21040;&#35821;&#29992;&#23398;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#24448;&#24448;&#36890;&#36807;&#20381;&#36182;&#19978;&#19979;&#25991;&#26469;&#20256;&#36798;&#24847;&#22270;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;IOG&#20219;&#21153;&#65292;&#21363;&#23454;&#29992;IOG&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;(IM-Dial)&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#38754;&#21521;&#24847;&#22270;&#30340;&#35805;&#35821;(&#20363;&#22914;&#65292;&#8220;&#25105;&#28212;&#20102;&#8221;)&#12290;&#28982;&#21518;&#65292;&#26426;&#22120;&#20154;&#24212;&#36890;&#36807;&#19982;&#20154;&#31867;&#29992;&#25143;&#20114;&#21160;&#26469;&#35782;&#21035;&#30446;&#26631;&#23545;&#35937;&#12290;&#22522;&#20110;&#20219;&#21153;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#37322;&#29992;&#25143;&#30340;&#24847;&#22270;&#24182;&#25441;&#36215;&#30446;&#26631;&#23545;&#35937;&#65292;&#21363;&#23454;&#29992;&#29289;&#20307;&#25235;&#21462;(PROGrasp)&#12290;PROGrasp&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#23450;&#20301;&#12289;&#38382;&#39064;&#25552;&#38382;&#12289;&#29289;&#20307;&#25235;&#21462;&#20197;&#21450;&#26368;&#37325;&#35201;&#30340;&#65292;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#25191;&#34892;&#23454;&#29992;IOG&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#65292;&#21363;&#8220;&#20808;&#35299;&#37322;&#20877;&#27880;&#37322;&#8221;&#65292;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#20026;&#26356;&#22909;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;&#65292;&#39318;&#20808;&#20026;&#27599;&#20010;&#28436;&#31034;&#23454;&#20363;&#21019;&#24314;&#25552;&#31034;&#65292;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#25552;&#31034;&#25552;&#31034;LLM&#25552;&#20379;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.16854</link><description>&lt;p&gt;
AnnoLLM&#65306;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#65292;&#21363;&#8220;&#20808;&#35299;&#37322;&#20877;&#27880;&#37322;&#8221;&#65292;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#20026;&#26356;&#22909;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;&#65292;&#39318;&#20808;&#20026;&#27599;&#20010;&#28436;&#31034;&#23454;&#20363;&#21019;&#24314;&#25552;&#31034;&#65292;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#25552;&#31034;&#25552;&#31034;LLM&#25552;&#20379;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20381;&#36182;&#20110;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#27880;&#37322;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#24403;&#20219;&#21153;&#28041;&#21450;&#22823;&#37327;&#25968;&#25454;&#25110;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#26102;&#12290;&#26368;&#36817;&#65292;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#26412;&#25991;&#39318;&#20808;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23427;&#20204;&#25552;&#20379;&#20805;&#20998;&#30340;&#25351;&#23548;&#21644;&#28436;&#31034;&#31034;&#20363;&#26469;&#20316;&#20026;&#20248;&#31168;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;&#12290;&#20026;&#20102;&#20351;LLMs&#25104;&#20026;&#26356;&#22909;&#30340;&#26631;&#27880;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#8220;&#20808;&#35299;&#37322;&#20877;&#27880;&#37322;&#8221;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#21019;&#24314;&#25552;&#31034;&#65292;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#25552;&#31034;&#25552;&#31034;LLM&#25552;&#20379;&#20851;&#20110;&#20026;&#20160;&#20040;&#23545;&#20110;&#29305;&#23450;&#31034;&#20363;&#36873;&#25321;&#20102;&#29305;&#23450;&#30340;&#22522;&#30784;&#30495;&#30456;&#22238;&#31572;/&#26631;&#31614;&#30340;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;few-shot&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thoug
&lt;/p&gt;</description></item></channel></rss>