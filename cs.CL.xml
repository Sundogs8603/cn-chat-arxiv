<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#26679;&#36866;&#37197;&#22120;&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35821;&#35328;&#29983;&#25104;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#25991;&#26412;&#12290;&#36825;&#31181;&#36716;&#21464;&#21487;&#20197;&#35270;&#20026;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26399;&#26395;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.03749</link><description>&lt;p&gt;
&#37319;&#26679;&#36866;&#37197;&#22120;&#30340;&#25928;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Sampling Adapters. (arXiv:2307.03749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#26679;&#36866;&#37197;&#22120;&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35821;&#35328;&#29983;&#25104;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#25991;&#26412;&#12290;&#36825;&#31181;&#36716;&#21464;&#21487;&#20197;&#35270;&#20026;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26399;&#26395;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#26159;&#20174;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#20294;&#26631;&#20934;&#30340;&#31062;&#20808;&#37319;&#26679;&#24448;&#24448;&#23548;&#33268;&#25991;&#26412;&#19981;&#36830;&#36143;&#25110;&#19981;&#31526;&#21512;&#35821;&#27861;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20462;&#25913;&#27169;&#22411;&#37319;&#26679;&#20998;&#24067;&#30340;&#25216;&#26415;&#65292;&#22914;&#26680;&#24515;&#25110;top-k&#37319;&#26679;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#36825;&#20123;&#25216;&#26415;&#65292;&#31216;&#20043;&#20026;&#37319;&#26679;&#36866;&#37197;&#22120;&#12290;&#37319;&#26679;&#36866;&#37197;&#22120;&#36890;&#24120;&#21487;&#20197;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#25991;&#26412;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#20174;&#24418;&#24335;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20204;&#26159;&#22914;&#20309;&#25913;&#21464;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#65288;&#23376;&#65289;&#35789;&#32423;&#20998;&#24067;&#30340;&#65311;&#20026;&#20160;&#20040;&#36825;&#20123;&#23616;&#37096;&#25913;&#21464;&#20250;&#23548;&#33268;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#23427;&#20204;&#25152;&#24378;&#21046;&#25191;&#34892;&#30340;&#36716;&#21464;&#21487;&#20197;&#34987;&#35270;&#20026;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#34429;&#28982;&#27169;&#22411;&#22833;&#21435;&#20102;&#20135;&#29983;&#26576;&#20123;&#23383;&#31526;&#20018;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26399;&#26395;&#30340;&#25991;&#26412;&#65292;&#20854;&#31934;&#30830;&#29575;&#25552;&#39640;&#20102;&#12290;&#23613;&#31649;&#36825;&#31181;&#26435;&#34913;&#22312;&#26631;&#20934;&#30340;&#36317;&#31163;&#24230;&#37327;&#20013;&#27809;&#26377;&#21453;&#26144;&#20986;&#26469;&#65292;&#20294;&#23427;&#30830;&#23454;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling is a common strategy for generating text from probabilistic models, yet standard ancestral sampling often results in text that is incoherent or ungrammatical. To alleviate this issue, various modifications to a model's sampling distribution, such as nucleus or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the (sub)word-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of dist
&lt;/p&gt;</description></item><item><title>QIGen&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#25512;&#29702;&#30340;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30446;&#26631;&#26550;&#26500;&#21644;&#24615;&#33021;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;LLaMA&#27169;&#22411;&#30340;&#22522;&#20110;CPU&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03738</link><description>&lt;p&gt;
QIGen&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#25512;&#29702;&#30340;&#39640;&#25928;&#20869;&#26680;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models. (arXiv:2307.03738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03738
&lt;/p&gt;
&lt;p&gt;
QIGen&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#25512;&#29702;&#30340;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30446;&#26631;&#26550;&#26500;&#21644;&#24615;&#33021;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;LLaMA&#27169;&#22411;&#30340;&#22522;&#20110;CPU&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;LLMs&#65288;&#22914;LLaMA&#25110;OPT&#65289;&#22312;&#29616;&#25104;&#30340;CPU&#19978;&#36827;&#34892;&#37327;&#21270;&#29983;&#25104;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#30446;&#26631;&#26550;&#26500;&#21644;&#24615;&#33021;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#65292;&#21253;&#25324;&#30828;&#20214;&#29305;&#24615;&#21644;&#26041;&#27861;&#29305;&#23450;&#30340;&#20934;&#30830;&#24615;&#32422;&#26463;&#12290;&#22312;LLaMA&#27169;&#22411;&#30340;&#22522;&#20110;CPU&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#19982;&#29616;&#26377;&#26368;&#20339;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#26356;&#20855;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312;https://github.com/IST-DASLab/QIGen&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ongoing work on a new automatic code generation approach for supporting quantized generative inference on LLMs such as LLaMA or OPT on off-the-shelf CPUs. Our approach is informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints. Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution. A preliminary implementation is available at https://github.com/IST-DASLab/QIGen.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24341;&#29992;&#26631;&#27880;&#35270;&#20026;&#22235;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#23376;&#20219;&#21153;&#65292;&#25913;&#36827;&#20102;&#25991;&#23398;&#23567;&#35828;&#20013;&#30340;&#33258;&#21160;&#24341;&#29992;&#26631;&#27880;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.03734</link><description>&lt;p&gt;
&#22312;&#25991;&#23398;&#23567;&#35828;&#20013;&#25913;&#36827;&#33258;&#21160;&#24341;&#29992;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic Quotation Attribution in Literary Novels. (arXiv:2307.03734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#24341;&#29992;&#26631;&#27880;&#35270;&#20026;&#22235;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#23376;&#20219;&#21153;&#65292;&#25913;&#36827;&#20102;&#25991;&#23398;&#23567;&#35828;&#20013;&#30340;&#33258;&#21160;&#24341;&#29992;&#26631;&#27880;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25991;&#23398;&#23567;&#35828;&#20013;&#30340;&#24341;&#29992;&#26631;&#27880;&#27169;&#22411;&#20551;&#35774;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#21487;&#29992;&#20449;&#24687;&#65292;&#36825;&#32473;&#37326;&#22806;&#25512;&#29702;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#24341;&#29992;&#26631;&#27880;&#35270;&#20026;&#22235;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#23376;&#20219;&#21153;&#65306;&#35282;&#33394;&#35782;&#21035;&#12289;&#20849;&#25351;&#28040;&#35299;&#12289;&#24341;&#35821;&#35782;&#21035;&#21644;&#35828;&#35805;&#32773;&#24402;&#23646;&#12290;&#25105;&#20204;&#20351;&#29992;&#25991;&#23398;&#23567;&#35828;&#20013;&#27880;&#37322;&#30340;&#20849;&#25351;&#21644;&#24341;&#35821;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#39033;&#30446;&#35328;&#23545;&#23567;&#35828;&#35821;&#26009;&#24211;&#65289;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#29305;&#21035;&#35757;&#32451;&#21644;&#35780;&#20272;&#35828;&#35805;&#32773;&#24402;&#23646;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#20998;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current models for quotation attribution in literary novels assume varying levels of available information in their training and test data, which poses a challenge for in-the-wild inference. Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification, coreference resolution, quotation identification, and speaker attribution. We benchmark state-of-the-art models on each of these sub-tasks independently, using a large dataset of annotated coreferences and quotations in literary novels (the Project Dialogism Novel Corpus). We also train and evaluate models for the speaker attribution task in particular, showing that a simple sequential prediction model achieves accuracy scores on par with state-of-the-art models.
&lt;/p&gt;</description></item><item><title>INT-FP-QSim&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#19981;&#21516;&#31934;&#24230;&#21644;&#26684;&#24335;&#19979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#20540;&#26684;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;4&#20301;&#26435;&#37325;&#21644;4&#20301;&#25110;8&#20301;&#28608;&#27963;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03712</link><description>&lt;p&gt;
INT-FP-QSim: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#28151;&#21512;&#31934;&#24230;&#21644;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03712
&lt;/p&gt;
&lt;p&gt;
INT-FP-QSim&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#19981;&#21516;&#31934;&#24230;&#21644;&#26684;&#24335;&#19979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#20540;&#26684;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;4&#20301;&#26435;&#37325;&#21644;4&#20301;&#25110;8&#20301;&#28608;&#27963;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23835;&#36215;&#23548;&#33268;&#20102;&#20943;&#23569;&#31934;&#24230;&#30340;&#36816;&#34892;&#30340;&#22686;&#21152;&#12290;&#38477;&#20302;&#31934;&#24230;&#30340;&#36816;&#34892;&#26041;&#24335;&#25903;&#25345;&#36164;&#28304;&#32422;&#26463;&#65292;&#24182;&#20419;&#36827;&#20854;&#27665;&#20027;&#21270;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#20010;&#20154;&#35774;&#22791;&#19978;&#36816;&#34892;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#34917;&#20805;&#36825;&#19968;&#25345;&#32493;&#21162;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INT-FP-QSim&#65306;&#19968;&#20010;&#24320;&#28304;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#25968;&#20540;&#31934;&#24230;&#21644;&#26684;&#24335;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;INT-FP-QSim&#21033;&#29992;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#65292;&#22914;TensorRT&#12289;QPytorch&#21644;AIMET&#65292;&#23454;&#29616;&#20102;&#25903;&#25345;&#21508;&#31181;&#28014;&#28857;&#21644;&#25972;&#25968;&#26684;&#24335;&#30340;&#32452;&#21512;&#27169;&#25311;&#22120;&#12290;&#20511;&#21161;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25968;&#20540;&#26684;&#24335;&#23545;4&#20301;&#26435;&#37325;&#21644;4&#20301;&#25110;8&#20301;&#28608;&#27963;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#22914;&#33258;&#36866;&#24212;&#22359;&#28014;&#28857;&#12289;SmoothQuant&#12289;GPTQ&#21644;RPTQ&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24076;&#26395;INT-FP-QSim&#33021;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#22312;&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#24615;&#33021;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(ChatGPT)&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#39537;&#21160;&#30340;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#33719;&#21462;&#21644;&#35782;&#21035;&#27450;&#39575;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03699</link><description>&lt;p&gt;
&#25581;&#31034;&#30693;&#35782;&#28608;&#21457;&#30340;ChatGPT&#22312;&#22686;&#24378;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27602;&#21697;&#36137;&#36816;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media. (arXiv:2307.03699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(ChatGPT)&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#39537;&#21160;&#30340;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#33719;&#21462;&#21644;&#35782;&#21035;&#27450;&#39575;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Instagram&#21644;Twitter&#24050;&#32463;&#25104;&#20026;&#27602;&#21697;&#33829;&#38144;&#21644;&#38750;&#27861;&#38144;&#21806;&#30340;&#20851;&#38190;&#28192;&#36947;&#12290;&#26816;&#27979;&#21644;&#26631;&#35760;&#22312;&#32447;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#23545;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#26816;&#27979;&#27602;&#21697;&#36137;&#36816;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33719;&#24471;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#25968;&#25454;&#27880;&#37322;&#26159;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#24403;&#27602;&#36137;&#20351;&#29992;&#27450;&#39575;&#24615;&#35821;&#35328;&#21644;&#22996;&#23113;&#35821;&#36991;&#20813;&#34987;&#26816;&#27979;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38754;&#20020;&#30528;&#20934;&#30830;&#35782;&#21035;&#36137;&#36816;&#27963;&#21160;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#21033;&#29992;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#25104;&#30693;&#35782;&#39537;&#21160;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#20316;&#20026;&#20154;&#31867;&#19982;LLMs&#20132;&#20114;&#30340;&#25509;&#21475;&#26469;&#25191;&#34892;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms such as Instagram and Twitter have emerged as critical channels for drug marketing and illegal sale. Detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. However, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. Furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. To overcome this limitation, we conduct the first systematic study on leveraging large language models (LLMs), such as ChatGPT, to detect illicit drug trafficking activities on social media. We propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use LLMs to perform t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26089;&#20572;&#26631;&#20934;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#33021;&#22815;&#30456;&#23545;&#26089;&#26399;&#22320;&#23398;&#20250;&#36981;&#24490;&#25351;&#20196;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#35821;&#20041;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.03692</link><description>&lt;p&gt;
&#25104;&#20026;&#33258;&#23398;&#32773;&#65306;&#24341;&#20837;&#26368;&#23567;&#25351;&#20196;&#35843;&#25972;&#30340;&#26089;&#20572;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. (arXiv:2307.03692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26089;&#20572;&#26631;&#20934;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#33021;&#22815;&#30456;&#23545;&#26089;&#26399;&#22320;&#23398;&#20250;&#36981;&#24490;&#25351;&#20196;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#35821;&#20041;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#20196;&#36319;&#38543;&#24471;&#20998;&#65288;IFS&#65289;&#65292;&#19968;&#31181;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#20855;&#26377;&#21452;&#37325;&#30446;&#30340;&#12290;&#39318;&#20808;&#65292;IFS&#21487;&#20197;&#29992;&#20110;&#21306;&#20998;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26174;&#31034;&#20986;&#33391;&#22909;&#26684;&#24335;&#21270;&#21709;&#24212;&#19982;&#37096;&#20998;&#21644;&#23436;&#25972;&#21477;&#23376;&#30340;&#27604;&#20363;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#27169;&#22411;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#25928;&#34913;&#37327;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#35813;&#24230;&#37327;&#26631;&#20934;&#21487;&#20197;&#29992;&#20316;&#25351;&#20196;&#35843;&#25972;&#30340;&#26089;&#20572;&#26631;&#20934;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;7B&#21644;13B LLaMA&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;IFS&#65292;&#26174;&#31034;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30456;&#23545;&#26089;&#26399;&#23601;&#23398;&#20250;&#20102;&#36981;&#24490;&#25351;&#20196;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#27169;&#22411;&#35821;&#20041;&#30340;&#21464;&#21270;&#12290;&#20316;&#20026;&#35821;&#20041;&#21464;&#21270;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#36741;&#21161;&#24230;&#37327;&#26631;&#20934;ObjecQA&#23450;&#20041;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#23458;&#35266;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#24403;IFS&#20542;&#21521;&#20110;p&#26102;&#65292;&#35821;&#20041;&#21464;&#21270;&#26368;&#20026;&#21095;&#28872;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models' ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping criteria for instruct tuning. We compute IFS for Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to follow instructions relatively early in the training process, and the further finetuning can result in changes in the underlying base model semantics. As an example of semantics change we show the objectivity of model predictions, as defined by an auxiliary metric ObjecQA. We show that in this particular case, semantic changes are the steepest when the IFS tends to p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03691</link><description>&lt;p&gt;
&#23558;&#33529;&#26524;&#19982;&#33529;&#26524;&#36827;&#34892;&#27604;&#36739;&#65306;&#20174;&#29992;&#25143;&#35780;&#35770;&#29983;&#25104;&#32437;&#21521;&#24863;&#30693;&#30340;&#27604;&#36739;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#30456;&#20284;&#30340;&#36873;&#25321;&#20013;&#25214;&#21040;&#26368;&#20339;&#20135;&#21697;&#26159;&#38750;&#24120;&#32791;&#26102;&#30340;&#12290;&#27604;&#36739;&#21477;&#23376;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20197;&#31361;&#20986;&#30340;&#26041;&#24335;&#23545;&#27604;&#19968;&#20010;&#39033;&#30446;&#19982;&#20854;&#20182;&#39033;&#30446;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#24378;&#35843;&#20986;&#37325;&#35201;&#29305;&#24449;&#12290;&#22522;&#20110;&#29992;&#25143;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#39033;&#30446;&#30340;&#35780;&#35770;&#21450;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#65292;&#25105;&#20204;&#29983;&#25104;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#26469;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#36830;&#32493;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#20010;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#29992;&#20110;&#23545;&#39033;&#30446;&#36827;&#34892;&#32534;&#30721;&#27604;&#36739;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#29983;&#25104;&#27604;&#36739;&#21477;&#23376;&#65292;&#65288;iii&#65289;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#26032;&#22411;&#35299;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#19988;&#22810;&#26679;&#30340;&#27604;&#36739;&#21477;&#23376;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#19988;&#30495;&#23454;&#30340;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is time-consuming to find the best product among many similar alternatives. Comparative sentences can help to contrast one item from others in a way that highlights important features of an item that stand out. Given reviews of one or multiple items and relevant item features, we generate comparative review sentences to aid users to find the best fit. Specifically, our model consists of three successive components in a transformer: (i) an item encoding module to encode an item for comparison, (ii) a comparison generation module that generates comparative sentences in an autoregressive manner, (iii) a novel decoding method for user personalization. We show that our pipeline generates fluent and diverse comparative sentences. We run experiments on the relevance and fidelity of our generated sentences in a human evaluation study and find that our algorithm creates comparative review sentences that are relevant and truthful.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25903;&#25345;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24739;&#32773;&#20449;&#24687;&#65292;&#20026;&#21305;&#37197;&#20998;&#26512;&#24341;&#20837;&#20102;&#25991;&#26412;&#65292;&#25913;&#21892;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25928;&#26524;&#65292;&#24182;&#22686;&#24378;&#20102;&#21305;&#37197;&#36807;&#31243;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03687</link><description>&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging text data for causal inference using electronic health records. (arXiv:2307.03687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03687
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25903;&#25345;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24739;&#32773;&#20449;&#24687;&#65292;&#20026;&#21305;&#37197;&#20998;&#26512;&#24341;&#20837;&#20102;&#25991;&#26412;&#65292;&#25913;&#21892;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25928;&#26524;&#65292;&#24182;&#22686;&#24378;&#20102;&#21305;&#37197;&#36807;&#31243;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26159;&#21307;&#30103;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#21547;&#20102;&#26377;&#20851;&#24739;&#32773;&#29305;&#24449;&#21644;&#27835;&#30103;&#30340;&#23453;&#36149;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#22312;&#32467;&#26500;&#21270;&#22270;&#34920;&#25968;&#25454;&#20013;&#32570;&#22833;&#12290;&#23613;&#31649;&#22914;&#27492;&#20016;&#23500;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#65292;&#23427;&#24456;&#23569;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;&#21033;&#29992;&#22823;&#37327;&#24739;&#32773;&#35760;&#24405;&#21644;&#27835;&#30103;&#21382;&#21490;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#21450;&#38506;&#25252;&#21307;&#29983;&#21644;&#25252;&#22763;&#30340;&#24191;&#27867;&#31508;&#35760;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20013;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#25903;&#25345;&#22240;&#26524;&#25512;&#26029;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#20174;&#26500;&#24605;&#21644;&#35774;&#35745;&#21040;&#20998;&#26512;&#21644;&#35299;&#37322;&#65292;&#20165;&#38656;&#24456;&#23569;&#30340;&#39069;&#22806;&#24037;&#20316;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20351;&#29992;&#37197;&#23545;&#21305;&#37197;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#25991;&#26412;&#32435;&#20837;&#32463;&#20856;&#37197;&#23545;&#20998;&#26512;&#20013;&#65306;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#34917;&#20805;&#22810;&#37325;&#25554;&#34917;&#31243;&#24207;&#65292;&#25913;&#21892;&#20102;&#23545;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#34917;&#20540;&#30340;&#20934;&#30830;&#24615;&#65307;&#36890;&#36807;&#22312;&#21305;&#37197;&#38454;&#27573;&#20013;&#32435;&#20837;&#25991;&#26412;&#65292;&#22686;&#24378;&#20102;&#21305;&#37197;&#36807;&#31243;&#30340;&#21512;&#29702;&#24615;&#65307;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;...
&lt;/p&gt;
&lt;p&gt;
Text is a ubiquitous component of medical data, containing valuable information about patient characteristics and care that are often missing from structured chart data. Despite this richness, it is rarely used in clinical research, owing partly to its complexity. Using a large database of patient records and treatment histories accompanied by extensive notes by attendant physicians and nurses, we show how text data can be used to support causal inference with electronic health data in all stages, from conception and design to analysis and interpretation, with minimal additional effort. We focus on studies using matching for causal inference. We augment a classic matching analysis by incorporating text in three ways: by using text to supplement a multiple imputation procedure, we improve the fidelity of imputed values to handle missing data; by incorporating text in the matching stage, we strengthen the plausibility of the matching procedure; and by conditioning on text, we can estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#19982;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#35821;&#35328;&#23433;&#20840;&#25514;&#26045;&#21644;&#38477;&#22122;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#24182;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#22320;&#29702;&#38142;&#25509;&#65292;&#25104;&#21151;&#25429;&#33719;&#37325;&#35201;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20102;&#31995;&#32479;&#26816;&#27979;&#24322;&#24120;&#21644;&#21306;&#20998;&#21512;&#27861;&#20869;&#23481;&#19982;&#21361;&#38505;&#23041;&#32961;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03679</link><description>&lt;p&gt;
&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#30340;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#23545;&#22810;&#35821;&#35328;&#23433;&#20840;&#25913;&#36827;&#21644;&#38477;&#22122;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages. (arXiv:2307.03679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#19982;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#35821;&#35328;&#23433;&#20840;&#25514;&#26045;&#21644;&#38477;&#22122;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#24182;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#22320;&#29702;&#38142;&#25509;&#65292;&#25104;&#21151;&#25429;&#33719;&#37325;&#35201;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20102;&#31995;&#32479;&#26816;&#27979;&#24322;&#24120;&#21644;&#21306;&#20998;&#21512;&#27861;&#20869;&#23481;&#19982;&#21361;&#38505;&#23041;&#32961;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#19982;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;WESMA&#65289;&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#23433;&#20840;&#25514;&#26045;&#21644;&#38477;&#22122;&#22810;&#31181;&#35821;&#35328;&#30340;&#26032;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#30340;&#25972;&#21512;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#22810;&#35821;&#35328;&#24615;&#38382;&#39064;&#12290;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#34987;&#29992;&#20316;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#20197;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#31361;&#20986;&#30340;&#35821;&#35328;&#27169;&#24335;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#21464;&#25442;&#65292;&#25552;&#35758;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#22320;&#29702;&#38142;&#25509;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25429;&#33719;&#37325;&#35201;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22686;&#21152;&#31995;&#32479;&#26816;&#27979;&#24322;&#24120;&#12289;&#21457;&#29616;&#38544;&#34255;&#27169;&#24335;&#20197;&#21450;&#21306;&#20998;&#21512;&#27861;&#20869;&#23481;&#21644;&#21361;&#38505;&#23041;&#32961;&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#23433;&#20840;&#25514;&#26045;&#12290;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26234;&#33021;&#26694;&#26550;&#26469;&#38477;&#32500;&#21644;&#38477;&#22122;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining the undecimated wavelet transform within a Word Embedded Semantic Marginal Autoencoder (WESMA), this research study provides a novel strategy for improving security measures and denoising multiple languages. The incorporation of these strategies is intended to address the issues of robustness, privacy, and multilingualism in data processing applications. The undecimated wavelet transform is used as a feature extraction tool to identify prominent language patterns and structural qualities in the input data. The proposed system may successfully capture significant information while preserving the temporal and geographical links within the data by employing this transform. This improves security measures by increasing the system's ability to detect abnormalities, discover hidden patterns, and distinguish between legitimate content and dangerous threats. The Word Embedded Semantic Marginal Autoencoder also functions as an intelligent framework for dimensionality and noise redu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03678</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;GPT-2&#21644;BERT&#22312;&#20869;&#30340;LLMs&#23545;&#20960;&#20309;&#30340;&#25991;&#26412;&#26684;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#36755;&#20837;&#20998;&#31867;&#22120;&#21644;&#22238;&#24402;&#22120;&#65292;&#20197;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#23884;&#20837;&#22312;&#20960;&#20309;&#23646;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;LLMs&#29983;&#25104;&#30340;&#23884;&#20837;&#33021;&#22815;&#20445;&#30041;&#20960;&#20309;&#31867;&#22411;&#24182;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65288;&#20934;&#30830;&#24230;&#39640;&#36798;73%&#65289;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#31354;&#38388;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;&#22312;&#25429;&#25417;&#24213;&#23618;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;&#32454;&#24494;&#24046;&#21035;&#21644;&#22797;&#26434;&#24615;&#20197;&#21450;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#20197;&#25903;&#25345;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21508;&#31181;GeoAI&#24212;&#29992;&#26041;&#38754;&#30340;&#25913;&#36827;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.03667</link><description>&lt;p&gt;
&#22312;11&#31181;&#35821;&#35328;&#20013;&#27979;&#35797;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26524;&#26159;&#65292;&#21487;&#39044;&#27979;&#24615;&#36739;&#20302;&#30340;&#35789;&#35821;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#26469;&#22788;&#29702;&#12290;Surprisal&#29702;&#35770;&#65288;Hale, 2001; Levy, 2008&#65289;&#26159;&#23545;&#36825;&#19968;&#21457;&#29616;&#30340;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23427;&#23558;&#19968;&#20010;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#37327;&#21270;&#20026;&#20854;surprisal&#65292;&#21363;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#36127;&#23545;&#25968;&#27010;&#29575;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#25903;&#25345;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#25968;&#25454;&#33539;&#22260;&#20869;&#65292;&#21363;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#20154;&#38405;&#35835;&#33521;&#35821;&#25991;&#26412;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#20116;&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#20998;&#24067;&#30340;&#21313;&#19968;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#22635;&#34917;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20174;&#21333;&#35821;&#21644;&#22810;&#35821;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#20272;&#35745;&#20540;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19982;surprisal&#29702;&#35770;&#30456;&#20851;&#30340;&#19977;&#20010;&#39044;&#27979;&#65306;(i) surprisal&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#38405;&#35835;&#26102;&#38388;&#65307;(ii) &#39044;&#26399;surprisal&#65292;&#21363;&#19978;&#19979;&#25991;&#29109;&#65292;&#26159;&#21542;&#24433;&#21709;&#38405;&#35835;&#26102;&#38388;&#65307;(iii) &#19982;surprisal&#30456;&#20851;&#30340;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#35299;&#37322;&#38405;&#35835;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21457;&#23545;&#35805;&#20013;&#35821;&#31687;&#20851;&#31995;&#30340;&#20998;&#24067;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;&#23545;&#35805;&#29615;&#22659;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#35821;&#31687;&#20851;&#31995;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#27880;&#37322;&#30340;&#35821;&#31687;&#20851;&#31995;&#36136;&#37327;&#36275;&#20197;&#36890;&#36807;&#23884;&#20837;&#23398;&#20064;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.03645</link><description>&lt;p&gt;
&#33258;&#21457;&#23545;&#35805;&#20013;&#35821;&#31687;&#20851;&#31995;&#30340;&#20998;&#24067;&#22312;&#23545;&#35805;&#36716;&#25240;&#20013;&#30340;&#20998;&#24067;&#21450;&#20854;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The distribution of discourse relations within and across turns in spontaneous conversation. (arXiv:2307.03645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21457;&#23545;&#35805;&#20013;&#35821;&#31687;&#20851;&#31995;&#30340;&#20998;&#24067;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;&#23545;&#35805;&#29615;&#22659;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#35821;&#31687;&#20851;&#31995;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#27880;&#37322;&#30340;&#35821;&#31687;&#20851;&#31995;&#36136;&#37327;&#36275;&#20197;&#36890;&#36807;&#23884;&#20837;&#23398;&#20064;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21387;&#21147;&#21644;&#35805;&#39064;&#21327;&#21830;&#21487;&#33021;&#20250;&#23545;&#20154;&#20204;&#22312;&#33258;&#21457;&#23545;&#35805;&#20013;&#21033;&#29992;&#35821;&#31687;&#20851;&#31995;(DRs)&#26045;&#21152;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#25163;&#27880;&#37322;&#32773;&#30340;&#20247;&#21253;&#27880;&#37322;&#65292;&#23558;&#20070;&#38754;&#35821;&#35328;&#30340;DR&#31995;&#32479;&#36866;&#29992;&#20110;&#33258;&#21457;&#23545;&#35805;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#21477;&#35805;&#19978;&#65292;&#35821;&#31687;&#20851;&#31995;&#30340;&#20351;&#29992;&#26159;&#21542;&#19981;&#21516;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#21457;&#35328;&#20154;&#20869;&#37096;&#21644;&#36328;&#21457;&#35328;&#20154;&#20043;&#38388;&#12289;&#22312;&#21457;&#35328;&#36718;&#20869;&#37096;&#21644;&#36328;&#21457;&#35328;&#36718;&#20043;&#38388;&#30340;DR&#27880;&#37322;&#27169;&#24335;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#35821;&#31687;&#29615;&#22659;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#35821;&#31687;&#20851;&#31995;&#20998;&#24067;&#65292;&#20854;&#20013;&#21333;&#19968;&#21457;&#35328;&#36718;&#27880;&#37322;&#23545;&#27880;&#37322;&#32773;&#20135;&#29983;&#20102;&#26368;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#31687;&#20851;&#31995;&#30340;&#27880;&#37322;&#36136;&#37327;&#36275;&#20197;&#20174;&#35821;&#31687;&#21333;&#20803;&#30340;&#23884;&#20837;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time pressure and topic negotiation may impose constraints on how people leverage discourse relations (DRs) in spontaneous conversational contexts. In this work, we adapt a system of DRs for written language to spontaneous dialogue using crowdsourced annotations from novice annotators. We then test whether discourse relations are used differently across several types of multi-utterance contexts. We compare the patterns of DR annotation within and across speakers and within and across turns. Ultimately, we find that different discourse contexts produce distinct distributions of discourse relations, with single-turn annotations creating the most uncertainty for annotators. Additionally, we find that the discourse relation annotations are of sufficient quality to predict from embeddings of discourse units.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#31185;&#23398;&#25991;&#26412;&#31616;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#31185;&#23398;&#25688;&#35201;&#65292;&#20197;&#28385;&#36275;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#38405;&#35835;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#21508;&#31867;&#27169;&#22411;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#21644;&#22797;&#26434;&#30701;&#35821;&#35782;&#21035;&#65292;&#20197;&#25552;&#20379;&#26356;&#26131;&#25026;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03569</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#31185;&#23398;&#25991;&#26412;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text Simplification of Scientific Texts for Non-Expert Readers. (arXiv:2307.03569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#31185;&#23398;&#25991;&#26412;&#31616;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#31185;&#23398;&#25688;&#35201;&#65292;&#20197;&#28385;&#36275;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#38405;&#35835;&#38656;&#27714;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#21508;&#31867;&#27169;&#22411;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#21644;&#22797;&#26434;&#30701;&#35821;&#35782;&#21035;&#65292;&#20197;&#25552;&#20379;&#26356;&#26131;&#25026;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#27700;&#24179;&#22240;&#35821;&#35328;&#12289;&#20010;&#20154;&#35748;&#30693;&#33021;&#21147;&#25110;&#23545;&#26576;&#19968;&#20027;&#39064;&#30340;&#20102;&#35299;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#25991;&#26412;&#31616;&#21270;&#26159;&#23558;&#25991;&#26412;&#37325;&#26032;&#34920;&#36848;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#35835;&#32773;&#32676;&#20307;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#31185;&#23398;&#25688;&#35201;&#30340;&#31616;&#21270;&#26377;&#21161;&#20110;&#38750;&#19987;&#19994;&#20154;&#22763;&#36890;&#36807;&#36339;&#36807;&#38656;&#35201;&#39046;&#22495;&#25110;&#19987;&#23478;&#30693;&#35782;&#30340;&#34920;&#36848;&#26469;&#33719;&#21462;&#26680;&#24515;&#20449;&#24687;&#65292;&#23588;&#20854;&#23545;&#20110;&#35835;&#32773;&#38405;&#35835;&#20851;&#20110;&#26032;&#22411;&#27835;&#30103;&#36873;&#25321;&#30340;&#30284;&#30151;&#24739;&#32773;&#26469;&#35828;&#23588;&#20026;&#37325;&#35201;&#12290;SimpleText&#23454;&#39564;&#23460;&#20027;&#21150;&#20102;&#38024;&#23545;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#31185;&#23398;&#25688;&#35201;&#31616;&#21270;&#27604;&#36187;&#65288;Task 3&#65289;&#20197;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19977;&#20010;&#36816;&#34892;&#23454;&#20363;&#65292;&#20998;&#21035;&#22522;&#20110;T5&#30340;&#20004;&#20010;&#21644;&#22522;&#20110;PEGASUS&#30340;&#19968;&#20010;&#30340;&#20986;&#31665;&#25688;&#35201;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22797;&#26434;&#30701;&#35821;&#35782;&#21035;&#30340;&#19968;&#20010;&#36816;&#34892;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reading levels are highly individual and can depend on a text's language, a person's cognitive abilities, or knowledge on a topic. Text simplification is the task of rephrasing a text to better cater to the abilities of a specific target reader group. Simplification of scientific abstracts helps non-experts to access the core information by bypassing formulations that require domain or expert knowledge. This is especially relevant for, e.g., cancer patients reading about novel treatment options. The SimpleText lab hosts the simplification of scientific abstracts for non-experts (Task 3) to advance this field. We contribute three runs employing out-of-the-box summarization models (two based on T5, one based on PEGASUS) and one run using ChatGPT with complex phrase identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39118;&#26684;&#30340;&#25968;&#25454;&#37319;&#26679;&#26469;&#22686;&#24378;&#20027;&#35266;&#24615;&#26816;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#27169;&#22411;&#26681;&#25454;&#26032;&#38395;&#35760;&#32773;&#35270;&#35282;&#30340;&#20027;&#35266;&#24615;&#26816;&#26597;&#28165;&#21333;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#26448;&#26009;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#38598;&#24494;&#35843;&#35821;&#35328;&#29305;&#23450;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#20027;&#35266;&#39118;&#26684;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22522;&#20110;&#39118;&#26684;&#30340;&#36807;&#37319;&#26679;&#22312;&#22303;&#32819;&#20854;&#35821;&#21644;&#33521;&#35821;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#25913;&#20889;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;GPT-3&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#20047;&#21619;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03550</link><description>&lt;p&gt;
DWReCO&#22312;CheckThat! 2023&#20013;&#36890;&#36807;&#22522;&#20110;&#39118;&#26684;&#30340;&#25968;&#25454;&#37319;&#26679;&#22686;&#24378;&#23458;&#35266;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling. (arXiv:2307.03550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39118;&#26684;&#30340;&#25968;&#25454;&#37319;&#26679;&#26469;&#22686;&#24378;&#20027;&#35266;&#24615;&#26816;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#27169;&#22411;&#26681;&#25454;&#26032;&#38395;&#35760;&#32773;&#35270;&#35282;&#30340;&#20027;&#35266;&#24615;&#26816;&#26597;&#28165;&#21333;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#26448;&#26009;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#38598;&#24494;&#35843;&#35821;&#35328;&#29305;&#23450;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#20027;&#35266;&#39118;&#26684;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22522;&#20110;&#39118;&#26684;&#30340;&#36807;&#37319;&#26679;&#22312;&#22303;&#32819;&#20854;&#35821;&#21644;&#33521;&#35821;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#25913;&#20889;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;GPT-3&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#20047;&#21619;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;CheckThat!&#23454;&#39564;&#23460;&#20027;&#35266;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#25552;&#20132;&#12290;&#20026;&#20102;&#35299;&#20915;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26032;&#38395;&#35760;&#32773;&#35270;&#35282;&#30340;&#20027;&#35266;&#24615;&#26816;&#26597;&#28165;&#21333;&#65292;&#20351;&#29992;&#19981;&#21516;&#39118;&#26684;&#30340;&#25552;&#31034;&#26469;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#26448;&#26009;&#65292;&#20351;&#29992;GPT-3&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#38598;&#26469;&#24494;&#35843;&#35821;&#35328;&#29305;&#23450;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20027;&#35266;&#39118;&#26684;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22303;&#32819;&#20854;&#35821;&#21644;&#33521;&#35821;&#20013;&#65292;&#22522;&#20110;&#39118;&#26684;&#30340;&#36807;&#37319;&#26679;&#27604;&#25913;&#20889;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;GPT-3&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#20047;&#21619;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission for the subjectivity detection task at the CheckThat! Lab. To tackle class imbalances in the task, we have generated additional training materials with GPT-3 models using prompts of different styles from a subjectivity checklist based on journalistic perspective. We used the extended training set to fine-tune language-specific transformer models. Our experiments in English, German and Turkish demonstrate that different subjective styles are effective across all languages. In addition, we observe that the style-based oversampling is better than paraphrasing in Turkish and English. Lastly, the GPT-3 models sometimes produce lacklustre results when generating style-based texts in non-English languages.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25216;&#33021;&#25552;&#21462;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;ESCO&#25216;&#33021;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#20351;&#29992;&#30456;&#20284;&#24615;&#26816;&#32034;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#32844;&#20301;&#25551;&#36848;&#20013;&#25552;&#21462;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.03539</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20307;&#21270;&#38646;-shot ESCO&#25216;&#33021;&#21305;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers. (arXiv:2307.03539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03539
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25216;&#33021;&#25552;&#21462;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;ESCO&#25216;&#33021;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#20351;&#29992;&#30456;&#20284;&#24615;&#26816;&#32034;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#32844;&#20301;&#25551;&#36848;&#20013;&#25552;&#21462;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21171;&#21160;&#21147;&#24066;&#22330;&#21160;&#24577;&#38656;&#35201;&#20934;&#30830;&#22320;&#35782;&#21035;&#21171;&#21160;&#21147;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#34987;&#24320;&#21457;&#20986;&#26469;&#25903;&#25345;&#36825;&#20010;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#25216;&#33021;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#20174;&#32844;&#20301;&#21457;&#24067;&#20013;&#33258;&#21160;&#25552;&#21462;&#25216;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;ESCO&#65288;&#27431;&#27954;&#25216;&#33021;&#12289;&#33021;&#21147;&#12289;&#36164;&#26684;&#21644;&#32844;&#19994;&#65289;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#21442;&#32771;&#65292;&#21015;&#20986;&#20102;&#36229;&#36807;13,000&#20010;&#29420;&#31435;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#25216;&#33021;&#25552;&#21462;&#20173;&#28982;&#22256;&#38590;&#65292;&#24182;&#19988;&#20934;&#30830;&#22320;&#23558;&#24037;&#20316;&#23703;&#20301;&#19982;ESCO&#20998;&#31867;&#36827;&#34892;&#21305;&#37197;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;-shot&#25216;&#33021;&#25552;&#21462;&#31995;&#32479;&#12290;&#25105;&#20204;&#29983;&#25104;ESCO&#25216;&#33021;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#20174;&#32844;&#20301;&#21457;&#24067;&#20013;&#25552;&#21462;&#25216;&#33021;&#25552;&#21450;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30456;&#20284;&#24615;&#26816;&#32034;&#22120;&#29983;&#25104;&#25216;&#33021;&#20505;&#36873;&#39033;&#65292;&#28982;&#21518;&#20351;&#29992;&#31532;&#20108;&#20010;LLM&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#20102;&#25216;&#33021;&#25552;&#21462;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding labour market dynamics requires accurately identifying the skills required for and possessed by the workforce. Automation techniques are increasingly being developed to support this effort. However, automatically extracting skills from job postings is challenging due to the vast number of existing skills. The ESCO (European Skills, Competences, Qualifications and Occupations) framework provides a useful reference, listing over 13,000 individual skills. However, skills extraction remains difficult and accurately matching job posts to the ESCO taxonomy is an open problem. In this work, we propose an end-to-end zero-shot system for skills extraction from job descriptions based on large language models (LLMs). We generate synthetic training data for the entirety of ESCO skills and train a classifier to extract skill mentions from job posts. We also employ a similarity retriever to generate skill candidates which are then re-ranked using a second LLM. Using synthetic data achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#20013;&#35789;&#27719;&#21644;&#38750;&#35789;&#27719;&#20449;&#36947;&#30340;&#30693;&#35273;&#20215;&#20540;&#12290;&#36890;&#36807;&#37327;&#21270;&#38750;&#35789;&#27719;&#20449;&#24687;&#23545;&#23545;&#35805;&#26399;&#26395;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#35789;&#27719;&#20449;&#24687;&#34429;&#28982;&#22312;&#21306;&#20998;&#24615;&#36716;&#21464;&#21028;&#26029;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#35789;&#27719;&#20869;&#23481;&#65292;&#20294;&#33021;&#22815;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#20135;&#29983;&#26356;&#39640;&#30340;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.03534</link><description>&lt;p&gt;
&#37327;&#21270;&#35821;&#38899;&#20013;&#35789;&#27719;&#21644;&#38750;&#35789;&#27719;&#20449;&#36947;&#30340;&#30693;&#35273;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Quantifying the perceptual value of lexical and non-lexical channels in speech. (arXiv:2307.03534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#20013;&#35789;&#27719;&#21644;&#38750;&#35789;&#27719;&#20449;&#36947;&#30340;&#30693;&#35273;&#20215;&#20540;&#12290;&#36890;&#36807;&#37327;&#21270;&#38750;&#35789;&#27719;&#20449;&#24687;&#23545;&#23545;&#35805;&#26399;&#26395;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#35789;&#27719;&#20449;&#24687;&#34429;&#28982;&#22312;&#21306;&#20998;&#24615;&#36716;&#21464;&#21028;&#26029;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#35789;&#27719;&#20869;&#23481;&#65292;&#20294;&#33021;&#22815;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#20135;&#29983;&#26356;&#39640;&#30340;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#27807;&#36890;&#26041;&#24335;&#65292;&#21487;&#20197;&#30475;&#20316;&#20026;&#20256;&#36882;&#20449;&#24687;&#25552;&#20379;&#20102;&#20004;&#31181;&#20449;&#36947;&#65306;&#35789;&#27719;&#20449;&#36947;&#26159;&#25351;&#25152;&#35828;&#30340;&#35789;&#27719;&#65292;&#38750;&#35789;&#27719;&#20449;&#36947;&#26159;&#25351;&#35828;&#35805;&#30340;&#26041;&#24335;&#12290;&#36825;&#20004;&#31181;&#20449;&#36947;&#37117;&#20250;&#22609;&#36896;&#21548;&#20247;&#23545;&#21363;&#23558;&#21040;&#26469;&#30340;&#27807;&#36890;&#30340;&#26399;&#26395;&#65292;&#28982;&#32780;&#30452;&#25509;&#37327;&#21270;&#23427;&#20204;&#23545;&#26399;&#26395;&#30340;&#30456;&#23545;&#24433;&#21709;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20808;&#21069;&#30340;&#23581;&#35797;&#38656;&#35201;&#36890;&#36807;&#35789;&#27719;&#31561;&#25928;&#30340;&#23545;&#35805;&#36716;&#21464;&#25110;&#26126;&#26174;&#30340;&#22768;&#23398;&#22788;&#29702;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#30740;&#31350;&#23545;&#35805;&#20013;&#38750;&#35789;&#27719;&#20449;&#24687;&#30340;&#20215;&#20540;&#65292;&#21253;&#25324;&#19981;&#21463;&#38480;&#21046;&#30340;&#35789;&#27719;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20934;&#30830;&#24230;&#21644;&#29109;&#20943;&#23569;&#37327;&#21270;&#38750;&#35789;&#27719;&#20449;&#36947;&#30340;&#30693;&#35273;&#20215;&#20540;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#35789;&#27719;&#20449;&#24687;&#23545;&#21363;&#23558;&#21040;&#26469;&#30340;&#23545;&#35805;&#20135;&#29983;&#20102;&#19968;&#33268;&#30340;&#24433;&#21709;&#65306;&#21363;&#20351;&#23427;&#23548;&#33268;&#36739;&#35789;&#27719;&#20869;&#23481;&#21333;&#29420;&#26102;&#26356;&#24046;&#30340;&#21306;&#20998;&#24615;&#36716;&#21464;&#21028;&#26029;&#65292;&#20173;&#28982;&#24471;&#21040;&#20102;&#21442;&#19982;&#32773;&#20043;&#38388;&#26356;&#39640;&#30340;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech is a fundamental means of communication that can be seen to provide two channels for transmitting information: the lexical channel of which words are said, and the non-lexical channel of how they are spoken. Both channels shape listener expectations of upcoming communication; however, directly quantifying their relative effect on expectations is challenging. Previous attempts require spoken variations of lexically-equivalent dialogue turns or conspicuous acoustic manipulations. This paper introduces a generalised paradigm to study the value of non-lexical information in dialogue across unconstrained lexical content. By quantifying the perceptual value of the non-lexical channel with both accuracy and entropy reduction, we show that non-lexical information produces a consistent effect on expectations of upcoming dialogue: even when it leads to poorer discriminative turn judgements than lexical content alone, it yields higher consensus among participants.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03506</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Derivative Free Weight-space Ensembling. (arXiv:2307.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#20043;&#38388;&#25554;&#20540;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#25506;&#32034;&#22312;&#20004;&#20010;&#20197;&#19978;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#28304;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#30446;&#26631;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#22312;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#25554;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;FETA-Friends&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work suggests that interpolating between the weights of two specialized language models can transfer knowledge between tasks in a way that multi-task learning cannot. However, very few have explored interpolation between more than two models, where each has a distinct knowledge base. In this paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new few-sample task transfer approach for open-domain dialogue. Our framework creates a set of diverse expert language models trained using a predefined set of source tasks. Next, we finetune each of the expert models on the target task, approaching the target task from several distinct knowledge bases. Finally, we linearly interpolate between the model weights using a gradient-free-optimization algorithm, to efficiently find a good interpolation weighting. We demonstrate the effectiveness of the method on FETA-Friends outperforming the standard pretrain-finetune approach.
&lt;/p&gt;</description></item><item><title>&#20197;&#23398;&#20064;&#19982;&#20998;&#27495;&#30340;&#26426;&#21046;&#20026;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#30340;&#30740;&#31350;&#65292;&#20197;&#25512;&#21160;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#23562;&#37325;&#24615;&#30340;&#22312;&#32447;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.03385</link><description>&lt;p&gt;
AI-UPV&#22312;EXIST 2023&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#8220;&#23398;&#20064;&#19982;&#20998;&#27495;&#8221;&#30340;&#26694;&#26550;&#19979;&#23545;&#24615;&#21035;&#27495;&#35270;&#36827;&#34892;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime. (arXiv:2307.03385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03385
&lt;/p&gt;
&lt;p&gt;
&#20197;&#23398;&#20064;&#19982;&#20998;&#27495;&#30340;&#26426;&#21046;&#20026;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#30340;&#30740;&#31350;&#65292;&#20197;&#25512;&#21160;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#23562;&#37325;&#24615;&#30340;&#22312;&#32447;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#24433;&#21709;&#21147;&#22686;&#21152;&#65292;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#24615;&#21035;&#27495;&#35270;&#21644;&#20854;&#20182;&#19981;&#23562;&#37325;&#21644;&#20167;&#24680;&#34892;&#20026;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20197;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#23562;&#37325;&#24615;&#30340;&#22312;&#32447;&#29615;&#22659;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#30340;&#20167;&#24680;&#31867;&#21035;&#21644;&#20316;&#32773;&#30340;&#24847;&#22270;&#65292;&#23588;&#20854;&#26159;&#22312;&#23398;&#20064;&#19982;&#20998;&#27495;&#30340;&#26426;&#21046;&#19979;&#65292;&#36825;&#20123;&#20219;&#21153;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;AI-UPV&#22242;&#38431;&#22312;CLEF 2023&#30340;EXIST&#65288;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#65289;&#23454;&#39564;&#23460;&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20174;&#20855;&#26377;&#20998;&#27495;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#32858;&#21512;&#26631;&#31614;&#65292;&#26469;&#22788;&#29702;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25253;&#21578;&#20102;&#32771;&#34385;&#36719;&#24615;&#21644;&#30828;&#24615;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;mBERT&#21644;XLM-RoBERTa&#65289;&#21644;&#38598;&#25104;&#31574;&#30053;&#26469;&#36827;&#34892;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing influence of social media platforms, it has become crucial to develop automated systems capable of detecting instances of sexism and other disrespectful and hateful behaviors to promote a more inclusive and respectful online environment. Nevertheless, these tasks are considerably challenging considering different hate categories and the author's intentions, especially under the learning with disagreements regime. This paper describes AI-UPV team's participation in the EXIST (sEXism Identification in Social neTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task of sexism identification and characterization under the learning with disagreements paradigm by training directly from the data with disagreements, without using any aggregated label. Yet, performances considering both soft and hard evaluations are reported. The proposed system uses large language models (i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19971;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#30452;&#25509;&#32454;&#35843;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33521;&#35821;&#38544;&#24335;&#31687;&#31456;&#20851;&#31995;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#20934;&#30830;&#24230;&#12290;&#19982;&#20043;&#21069;&#30340;&#25253;&#36947;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21477;&#23376;&#32423;&#39044;&#35757;&#32451;&#30446;&#26631;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#20102;&#31867;&#20284;&#35268;&#27169;&#30340;PLMs&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;MLM&#21644;&#23436;&#20840;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.03378</link><description>&lt;p&gt;
&#33521;&#35821;&#38544;&#24335;&#31687;&#31456;&#20851;&#31995;&#20998;&#31867;&#30340;Transformer&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification. (arXiv:2307.03378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19971;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#30452;&#25509;&#32454;&#35843;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33521;&#35821;&#38544;&#24335;&#31687;&#31456;&#20851;&#31995;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#20934;&#30830;&#24230;&#12290;&#19982;&#20043;&#21069;&#30340;&#25253;&#36947;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21477;&#23376;&#32423;&#39044;&#35757;&#32451;&#30446;&#26631;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#20102;&#31867;&#20284;&#35268;&#27169;&#30340;PLMs&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;MLM&#21644;&#23436;&#20840;&#27880;&#24847;&#26426;&#21046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31687;&#31456;&#35299;&#26512;&#21487;&#20197;&#24110;&#21161;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20294;&#23545;&#20110;&#38544;&#24335;&#31687;&#31456;&#20851;&#31995;&#20998;&#31867;&#65292;&#23578;&#26410;&#36827;&#34892;&#20840;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#25628;&#32034;&#12290;&#36825;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#36827;&#34892;&#31687;&#31456;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;&#19971;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#32454;&#35843;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;PDTB-3&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#31687;&#31456;&#20851;&#31995;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#27169;&#22411;&#25628;&#32034;&#65292;&#25105;&#20204;&#23558;SOTA&#25552;&#21319;&#21040;&#20102;0.671&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#33719;&#24471;&#20102;&#26032;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#20854;&#20013;&#19968;&#20123;&#19982;&#20043;&#21069;&#30340;&#25253;&#36947;&#30456;&#21453;&#65288;Shi and Demberg, 2019b&#65289;&#65292;&#21363;&#21477;&#23376;&#32423;&#39044;&#35757;&#32451;&#30446;&#26631;&#65288;NSP, SBO, SOP&#65289;&#36890;&#24120;&#26080;&#27861;&#20135;&#29983;&#26368;&#20339;&#30340;&#38544;&#24335;&#31687;&#31456;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;PLMs&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;MLM&#21644;&#23436;&#20840;&#27880;&#24847;&#26426;&#21046;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of seven pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24615;&#21035;&#27495;&#35270;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20943;&#23569;&#36127;&#38754;&#36801;&#31227;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03377</link><description>&lt;p&gt;
&#32531;&#35299;&#20219;&#21153;&#24863;&#30693;&#23545;&#24615;&#21035;&#27495;&#35270;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#30340;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection. (arXiv:2307.03377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24615;&#21035;&#27495;&#35270;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20943;&#23569;&#36127;&#38754;&#36801;&#31227;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#24120;&#30340;&#31574;&#30053;&#26159;&#37319;&#29992;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#35757;&#32451;&#19968;&#20010;&#30417;&#30563;&#27169;&#22411;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#24471;&#22312;&#25968;&#25454;&#19981;&#21487;&#29992;&#25110;&#25910;&#38598;&#25104;&#26412;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#20043;&#38388;&#20449;&#24687;&#20849;&#20139;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65306;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#12290;&#23613;&#31649;&#22312;MTL&#26041;&#38754;&#24050;&#32463;&#26377;&#20102;&#19968;&#20123;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#36127;&#38754;&#36801;&#31227;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#22122;&#22768;&#20449;&#24687;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#26102;&#65292;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#27010;&#24565;&#30340;&#26032;&#26041;&#27861;&#26469;&#32531;&#35299;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#36127;&#38754;&#36801;&#31227;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novelty approach to mitigate the negative transfer problem. In the field of machine learning, the common strategy is to apply the Single-Task Learning approach in order to train a supervised model to solve a specific task. Training a robust model requires a lot of data and a significant amount of computational resources, making this solution unfeasible in cases where data are unavailable or expensive to gather. Therefore another solution, based on the sharing of information between tasks, has been developed: Multi-Task Learning (MTL). Despite the recent developments regarding MTL, the problem of negative transfer has still to be solved. Negative transfer is a phenomenon that occurs when noisy information is shared between tasks, resulting in a drop in performance. This paper proposes a new approach to mitigate the negative transfer problem based on the task awareness concept. The proposed approach results in diminishing the negative transfer together with an impro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20197;&#24050;&#24314;&#31435;&#30340;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03360</link><description>&lt;p&gt;
&#22312;&#20132;&#21449;&#38382;&#31572;&#32972;&#26223;&#19979;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24577;&#24230;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03360
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20197;&#24050;&#24314;&#31435;&#30340;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#23884;&#20837;&#20102;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#35760;&#24405;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65288;&#24841;&#24555;/&#19981;&#24841;&#24555;&#65289;&#20915;&#23450;&#20102;&#31038;&#20250;&#35748;&#30693;&#20013;&#23545;&#32676;&#20307;&#21644;&#27010;&#24565;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20132;&#21449;&#38382;&#31572;&#32972;&#26223;&#30340;&#21477;&#23376;&#27169;&#26495;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24180;&#40836;&#12289;&#25945;&#32946;&#12289;&#24615;&#21035;&#12289;&#36523;&#39640;&#12289;&#26234;&#21147;&#12289;&#25991;&#21270;&#32032;&#20859;&#12289;&#31181;&#26063;&#12289;&#23447;&#25945;&#12289;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#20307;&#37325;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#37319;&#29992;&#27010;&#24565;&#25237;&#24433;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#25429;&#25417;&#24773;&#32490;&#20851;&#32852;&#30340;&#23376;&#31354;&#38388;&#12290;&#23558;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#35843;&#25972;&#20026;&#37327;&#21270;&#20559;&#35265;&#30340;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#21644;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;Transformer-Transducer&#65292;&#21516;&#26102;&#29983;&#25104;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#29616;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36136;&#37327;-&#24310;&#36831;&#24179;&#34913;&#65292;&#24182;&#22312;&#22810;&#35821;&#29615;&#22659;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03354</link><description>&lt;p&gt;
&#22312;&#32852;&#21512;&#27969;&#30021;&#30340;ASR&#21644;ST&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#23545;&#40784;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;Transformer-Transducer&#65292;&#21516;&#26102;&#29983;&#25104;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#29616;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36136;&#37327;-&#24310;&#36831;&#24179;&#34913;&#65292;&#24182;&#22312;&#22810;&#35821;&#29615;&#22659;&#19979;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#21516;&#26102;&#32763;&#35793;&#21644;&#36716;&#24405;&#35821;&#38899;&#20197;&#22686;&#24378;&#20854;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22686;&#37327;&#29983;&#25104;&#30340;&#27969;&#24335;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27969;&#24335;Transformer-Transducer&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#35299;&#30721;&#22120;&#21516;&#26102;&#29983;&#25104;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#36755;&#20986;&#12290;&#20026;&#20102;&#20197;&#26368;&#23567;&#30340;&#24310;&#36831;&#26377;&#25928;&#22320;&#20135;&#29983;ASR&#21644;ST&#20869;&#23481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#30340;&#26631;&#35760;&#32423;&#20018;&#34892;&#36755;&#20986;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#22120;&#20132;&#38169;&#28304;&#35789;&#21644;&#30446;&#26631;&#35789;&#12290;&#22312;&#21333;&#35821;&#65288;it-en&#65289;&#21644;&#22810;&#35821;&#65288;{de,es,it}-en&#65289;&#35774;&#32622;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#36136;&#37327;-&#24310;&#36831;&#24179;&#34913;&#12290;&#22312;&#24179;&#22343;ASR&#24310;&#36831;&#20026;1&#31186;&#21644;ST&#24310;&#36831;&#20026;1.3&#31186;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#21333;&#29420;&#30340;ASR&#21644;ST&#27169;&#22411;&#30456;&#27604;&#65292;&#27809;&#26377;&#38477;&#20302;&#65292;&#29978;&#33267;&#25552;&#39640;&#20102;&#36755;&#20986;&#36136;&#37327;&#65292;&#22312;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#24179;&#22343;WER&#25552;&#39640;&#20102;1.1&#65292;BLEU&#25552;&#39640;&#20102;0.4&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, users often require both translations and transcriptions of speech to enhance their comprehension, particularly in streaming scenarios where incremental generation is necessary. This paper introduces a streaming Transformer-Transducer that jointly generates automatic speech recognition (ASR) and speech translation (ST) outputs using a single decoder. To produce ASR and ST content effectively with minimal latency, we propose a joint token-level serialized output training method that interleaves source and target words by leveraging an off-the-shelf textual aligner. Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settings demonstrate that our approach achieves the best quality-latency balance. With an average ASR latency of 1s and ST latency of 1.3s, our model shows no degradation or even improves output quality compared to separate ASR and ST models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the multilingual case.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36328;&#35821;&#35328;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;L1&#21644;L2&#20043;&#38388;&#30340;&#38899;&#32032;&#27495;&#20041;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#21463;&#24178;&#25200;&#30340;L2&#25991;&#26412;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#21487;&#20449;&#30340;&#21463;&#24178;&#25200;&#30340;L2&#25991;&#26412;&#65292;&#24182;&#23545;&#27969;&#34892;&#30340;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.03322</link><description>&lt;p&gt;
BiPhone:&#27169;&#25311;&#35821;&#38899;&#23398;&#20851;&#31995;&#27169;&#22411; L2 &#25991;&#26412;&#20013;&#30340;&#36328;&#35821;&#35328;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
BiPhone: Modeling Inter Language Phonetic Influences in Text. (arXiv:2307.03322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36328;&#35821;&#35328;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;L1&#21644;L2&#20043;&#38388;&#30340;&#38899;&#32032;&#27495;&#20041;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#21463;&#24178;&#25200;&#30340;L2&#25991;&#26412;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#21487;&#20449;&#30340;&#21463;&#24178;&#25200;&#30340;L2&#25991;&#26412;&#65292;&#24182;&#23545;&#27969;&#34892;&#30340;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25216;&#26415;&#19981;&#23545;&#31216;&#24615;&#65292;&#35768;&#22810;&#20154;&#34987;&#36843;&#22312;&#33258;&#24049;&#19981;&#25797;&#38271;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#32593;&#32476;&#12290;&#36825;&#20123;&#29992;&#25143;&#22312;&#31532;&#20108;&#35821;&#35328;(L2)&#20013;&#30340;&#20070;&#38754;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#21463;&#20854;&#27597;&#35821;(L1)&#24433;&#21709;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25366;&#25496;L1&#21644;L2&#23545;&#20043;&#38388;&#30340;&#38899;&#32032;&#27495;&#20041;(&#21487;&#33021;&#23548;&#33268;L1&#35828;&#35805;&#32773;&#28151;&#28102;&#30340;L2&#35821;&#38899;)&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#27495;&#20041;&#25554;&#20837;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;(Bi-Phone)&#20013;&#65292;&#29992;&#20110;&#21512;&#25104;&#34987;&#30772;&#22351;&#30340;L2&#25991;&#26412;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Bi-Phone&#29983;&#25104;&#30340;&#30772;&#22351;&#26159;&#21487;&#20449;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;L1&#19978;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25216;&#26415;(Phonetically Noised GLUE&#30340;FunGLUE)&#30772;&#22351;&#20102;&#27969;&#34892;&#30340;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;SuperGLUE&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;&#26368;&#20339;&#30340;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#38899;&#32032;&#39044;&#27979;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24110;&#21161;&#23383;&#33410;&#27169;&#22411;&#24674;&#22797;&#25509;&#36817;SuperGLUE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1). We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2. These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web. We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#33268;&#21147;&#20110;&#33258;&#21160;&#29983;&#25104;&#37325;&#28857;&#38388;&#38553;&#38382;&#39064;&#65288;GFQ&#65289;&#65292;&#36890;&#36807;&#23450;&#20041;&#20219;&#21153;&#12289;&#25552;&#20986;&#27169;&#22411;&#24182;&#19982;&#20154;&#24037;&#29983;&#25104;&#38382;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03319</link><description>&lt;p&gt;
&#25513;&#30422;&#32597;&#35265;&#39046;&#22495;&#65306;&#38024;&#23545;&#31572;&#26696;&#35780;&#20272;&#30340;&#37325;&#28857;&#38388;&#38553;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment. (arXiv:2307.03319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#33268;&#21147;&#20110;&#33258;&#21160;&#29983;&#25104;&#37325;&#28857;&#38388;&#38553;&#38382;&#39064;&#65288;GFQ&#65289;&#65292;&#36890;&#36807;&#23450;&#20041;&#20219;&#21153;&#12289;&#25552;&#20986;&#27169;&#22411;&#24182;&#19982;&#20154;&#24037;&#29983;&#25104;&#38382;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20132;&#27969;&#36890;&#24120;&#28041;&#21450;&#23545;&#35805;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#65292;&#23398;&#29983;&#36890;&#24120;&#25552;&#20379;&#19968;&#20010;&#19981;&#23436;&#25972;&#30340;&#31572;&#26696;&#65292;&#19982;&#25945;&#24072;&#26399;&#26395;&#30340;&#23436;&#32654;&#31572;&#26696;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#25104;&#21151;&#30340;&#23545;&#35805;&#20381;&#36182;&#20110;&#32769;&#24072;&#26377;&#25928;&#22320;&#35810;&#38382;&#36825;&#20010;&#24046;&#36317;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#21644;&#20114;&#21160;&#30340;&#25945;&#32946;&#32463;&#39564;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#37325;&#28857;&#38388;&#38553;&#38382;&#39064;&#65288;GFQ&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20219;&#21153;&#65292;&#24378;&#35843;&#20102;&#19968;&#20010;&#22909;&#30340;GFQ&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#21592;&#23545;&#25105;&#20204;&#29983;&#25104;&#30340;&#38382;&#39064;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue, a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the teacher asking about this gap in an effective manner, thus creating a rich and interactive educational experience. We focus on the problem of generating such gap-focused questions (GFQs) automatically. We define the task, highlight key desired aspects of a good GFQ, and propose a model that satisfies these. Finally, we provide an evaluation by human annotators of our generated questions compared against human generated ones, demonstrating competitive performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;InfoSync&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#30340;&#20449;&#24687;&#21516;&#27493;&#12290;&#36890;&#36807;&#20449;&#24687;&#23545;&#40784;&#21644;&#20449;&#24687;&#26356;&#26032;&#65292;&#35813;&#26041;&#27861;&#22312;InfoSync&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03313</link><description>&lt;p&gt;
InfoSync&#65306;&#36328;&#22810;&#35821;&#35328;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#30340;&#20449;&#24687;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
InfoSync: Information Synchronization across Multilingual Semi-structured Tables. (arXiv:2307.03313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;InfoSync&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#30340;&#20449;&#24687;&#21516;&#27493;&#12290;&#36890;&#36807;&#20449;&#24687;&#23545;&#40784;&#21644;&#20449;&#24687;&#26356;&#26032;&#65292;&#35813;&#26041;&#27861;&#22312;InfoSync&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20449;&#24687;&#21516;&#27493;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20363;&#22914;&#65292;&#24212;&#35813;&#36328;&#35821;&#35328;&#21516;&#27493;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;InfoSyncC&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#23454;&#29616;&#34920;&#26684;&#21516;&#27493;&#12290;InfoSync&#21253;&#21547;&#20102;14&#31181;&#35821;&#35328;&#30340;10&#19975;&#20010;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#26684;&#65288;&#32500;&#22522;&#30334;&#31185;Infoboxes&#65289;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#65288;3.5K&#23545;&#65289;&#26159;&#25163;&#21160;&#27880;&#37322;&#30340;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;1&#65289;&#20449;&#24687;&#23545;&#40784;&#26469;&#26144;&#23556;&#34892;&#21644;2&#65289;&#20449;&#24687;&#26356;&#26032;&#26469;&#26356;&#26032;&#36328;&#22810;&#35821;&#35328;&#34920;&#26684;&#20013;&#23545;&#40784;&#34920;&#26684;&#20013;&#30340;&#32570;&#22833;/&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;InfoSync&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20449;&#24687;&#23545;&#40784;&#23454;&#29616;&#20102;87.91&#30340;F1&#24471;&#20998;&#65288;&#33521;&#25991;&lt;-&gt;&#38750;&#33521;&#25991;&#65289;&#12290;&#20026;&#20102;&#35780;&#20272;&#20449;&#24687;&#26356;&#26032;&#65292;&#25105;&#20204;&#23545;603&#20010;&#34920;&#26684;&#23545;&#30340;Infoboxes&#36827;&#34892;&#20102;&#20154;&#24037;&#36741;&#21161;&#30340;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#21462;&#24471;&#20102;77.28%&#30340;&#25509;&#21463;&#29575;&#65292;&#26174;&#31034;&#20986;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Synchronization of semi-structured data across languages is challenging. For instance, Wikipedia tables in one language should be synchronized across languages. To address this problem, we introduce a new dataset InfoSyncC and a two-step method for tabular synchronization. InfoSync contains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages, of which a subset (3.5K pairs) are manually annotated. The proposed method includes 1) Information Alignment to map rows and 2) Information Update for updating missing/outdated information for aligned tables across multilingual tables. When evaluated on InfoSync, information alignment achieves an F1 score of 87.91 (en &lt;-&gt; non-en). To evaluate information updation, we perform human-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach obtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gamma&#38899;&#22270;&#34920;&#31034;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03296</link><description>&lt;p&gt;
&#29992;&#20110;&#31471;&#21040;&#31471;&#21151;&#33021;&#24615;&#35328;&#35821;&#22788;&#29702;&#20219;&#21153;&#30340;Gamma&#38899;&#22270;&#34920;&#31034;&#65306;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Gammatonegram Representation for End-to-End Dysarthric Speech Processing Tasks: Speech Recognition, Speaker Identification, and Intelligibility Assessment. (arXiv:2307.03296v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gamma&#38899;&#22270;&#34920;&#31034;&#35821;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#38899;&#38556;&#30861;&#26159;&#19968;&#31181;&#24433;&#21709;&#20154;&#31867;&#35328;&#35821;&#31995;&#32479;&#24182;&#38477;&#20302;&#20010;&#20154;&#21457;&#38899;&#36136;&#37327;&#21644;&#21487;&#29702;&#35299;&#24615;&#30340;&#27531;&#30142;&#12290;&#30001;&#20110;&#36825;&#31181;&#24433;&#21709;&#65292;&#24120;&#35268;&#30340;&#35328;&#35821;&#22788;&#29702;&#31995;&#32479;&#26080;&#27861;&#22312;&#21463;&#25439;&#30340;&#35328;&#35821;&#19978;&#27491;&#24120;&#24037;&#20316;&#12290;&#36825;&#31181;&#27531;&#30142;&#36890;&#24120;&#19982;&#36523;&#20307;&#27531;&#30142;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#25509;&#25910;&#35821;&#38899;&#21629;&#20196;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#25191;&#34892;&#19968;&#20123;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#23558;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25104;&#23601;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Gamma&#38899;&#22270;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#20855;&#26377;&#21306;&#20998;&#24615;&#32454;&#33410;&#30340;&#38899;&#39057;&#25991;&#20214;&#65292;&#35813;&#26041;&#27861;&#29992;&#20316;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#35821;&#38899;&#25991;&#20214;&#36716;&#25442;&#25104;&#22270;&#20687;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#26469;&#23545;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#36827;&#34892;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;Alexnet&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#31995;&#32479;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#21487;&#29702;&#35299;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dysarthria is a disability that causes a disturbance in the human speech system and reduces the quality and intelligibility of a person's speech. Because of this effect, the normal speech processing systems can not work properly on impaired speech. This disability is usually associated with physical disabilities. Therefore, designing a system that can perform some tasks by receiving voice commands in the smart home can be a significant achievement. In this work, we introduce gammatonegram as an effective method to represent audio files with discriminative details, which is used as input for the convolutional neural network. On the other word, we convert each speech file into an image and propose image recognition system to classify speech in different scenarios. Proposed CNN is based on the transfer learning method on the pre-trained Alexnet. In this research, the efficiency of the proposed system for speech recognition, speaker identification, and intelligibility assessment is evaluat
&lt;/p&gt;</description></item><item><title>&#22312;TikTok&#35270;&#39057;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SexTok&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21306;&#20998;&#24615;&#26263;&#31034;&#20869;&#23481;&#21644;&#34394;&#25311;&#24615;&#25945;&#32946;&#35270;&#39057;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.03274</link><description>&lt;p&gt;
&#19981;&#26159;&#24615;&#26263;&#31034;&#65292;&#26159;&#25945;&#32946;&#12290;&#22312;TikTok&#35270;&#39057;&#20013;&#20998;&#31163;&#24615;&#25945;&#32946;&#21644;&#26263;&#31034;&#24615;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos. (arXiv:2307.03274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03274
&lt;/p&gt;
&lt;p&gt;
&#22312;TikTok&#35270;&#39057;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SexTok&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21306;&#20998;&#24615;&#26263;&#31034;&#20869;&#23481;&#21644;&#34394;&#25311;&#24615;&#25945;&#32946;&#35270;&#39057;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SexTok&#30340;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#26631;&#35760;&#20026;&#24615;&#26263;&#31034;&#65288;&#20174;&#27880;&#37322;&#32773;&#30340;&#35282;&#24230;&#26469;&#30475;&#65289;&#65292;&#24615;&#25945;&#32946;&#20869;&#23481;&#25110;&#20004;&#32773;&#37117;&#19981;&#26159;&#30340;TikTok&#35270;&#39057;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#26159;&#20026;&#20102;&#35299;&#20915;&#22312;TikTok&#19978;&#21306;&#20998;&#24615;&#26263;&#31034;&#20869;&#23481;&#21644;&#34394;&#25311;&#24615;&#25945;&#32946;&#35270;&#39057;&#30340;&#25361;&#25112;&#12290;&#20799;&#31461;&#25509;&#35302;&#24615;&#26263;&#31034;&#30340;&#35270;&#39057;&#24050;&#34987;&#35777;&#26126;&#23545;&#20182;&#20204;&#30340;&#21457;&#23637;&#26377;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#20110;LGBTQIA+&#31038;&#21306;&#26356;&#30456;&#20851;&#30340;&#34394;&#25311;&#24615;&#25945;&#32946;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#24179;&#21488;&#30340;&#24403;&#21069;&#31995;&#32479;&#21024;&#38500;&#25110;&#24809;&#32602;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#19981;&#21516;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#35270;&#39057;URL&#65292;&#24182;&#19988;&#36824;&#26377;&#38899;&#39057;&#36716;&#24405;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#26469;&#23545;&#35270;&#39057;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#21306;&#20998;&#36825;&#20123;&#31867;&#22411;&#30340;&#35270;&#39057;&#26159;&#21487;&#23398;&#20064;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children's exposure to sexually suggestive videos has been shown to have adversarial effects on their development. Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable. The platform's current system removes or penalizes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments sugge
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.03254</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03254
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#22914;&#22238;&#31572;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#25110;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#30340;&#26631;&#39064;&#65292;&#26159;&#35745;&#31639;&#26426;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;transformer&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#39640;&#12290;&#23427;&#20204;&#36890;&#36807;&#22312;&#22823;&#22411;&#36890;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26550;&#26500;&#21644;&#21442;&#25968;&#20540;&#19978;&#36827;&#34892;&#24494;&#23567;&#25913;&#21464;&#21518;&#65292;&#23558;&#23398;&#20064;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#26631;&#20934;&#24314;&#27169;&#23454;&#36341;&#12290;&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#25215;&#35834;&#22312;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#20135;&#29983;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#30446;&#21069;&#21487;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#32508;&#21512;&#65292;&#24182;&#23545;&#20854;&#20248;&#21183;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strength
&lt;/p&gt;</description></item><item><title>PREADD&#26159;&#19968;&#31181;&#21069;&#32512;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;PREADD&#19981;&#38656;&#35201;&#22806;&#37096;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#20219;&#20309;&#23646;&#24615;&#30340;&#27491;&#21521;&#21644;&#36127;&#21521;&#25511;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.03214</link><description>&lt;p&gt;
PREADD: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#30340;&#21069;&#32512;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PREADD: Prefix-Adaptive Decoding for Controlled Text Generation. (arXiv:2307.03214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03214
&lt;/p&gt;
&lt;p&gt;
PREADD&#26159;&#19968;&#31181;&#21069;&#32512;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;PREADD&#19981;&#38656;&#35201;&#22806;&#37096;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#20219;&#20309;&#23646;&#24615;&#30340;&#27491;&#21521;&#21644;&#36127;&#21521;&#25511;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PREADD&#30340;&#21069;&#32512;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#36741;&#21161;&#19987;&#23478;&#27169;&#22411;&#26469;&#25511;&#21046;&#23646;&#24615;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;PREADD&#19981;&#38656;&#35201;&#22806;&#37096;&#27169;&#22411;&#65292;&#32780;&#26159;&#20381;&#38752;&#32447;&#24615;&#32452;&#21512;&#22810;&#20010;&#25552;&#31034;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;PREADD&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#21407;&#22987;&#25552;&#31034;&#29983;&#25104;&#30340;&#36755;&#20986;&#27010;&#29575;&#21644;&#20351;&#29992;&#21069;&#32512;&#21069;&#32622;&#25552;&#31034;&#29983;&#25104;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#23454;&#29616;&#23545;&#20219;&#20309;&#30001;&#21069;&#32512;&#23553;&#35013;&#30340;&#23646;&#24615;&#30340;&#27491;&#21521;&#21644;&#36127;&#21521;&#25511;&#21046;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;PREADD&#65292;&#21253;&#25324;&#27602;&#24615;&#36755;&#20986;&#20943;&#36731;&#12289;&#24615;&#21035;&#20559;&#35265;&#20943;&#23569;&#21644;&#24773;&#24863;&#25511;&#21046;&#65292;&#24182;&#21457;&#29616;PREADD&#22312;&#27599;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25351;&#26631;&#19978;&#30456;&#23545;&#25910;&#30410;&#27604;&#25552;&#31034;&#22522;&#32447;&#21644;&#36741;&#21161;&#19987;&#23478;&#25511;&#21046;&#26041;&#27861;&#39640;&#20986;12%&#25110;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20540;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#21517;&#21644;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#20855;&#26377;&#29305;&#23450;&#20851;&#31995;&#38408;&#20540;&#20197;&#19978;&#30340;&#23545;&#35937;&#21487;&#20197;&#36798;&#21040;49.5%&#30340;F1&#24471;&#20998;&#65292;&#36825;&#23545;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20540;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35813;&#30740;&#31350;&#20026;&#20174;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#20013;&#25552;&#21462;&#20851;&#31995;&#30693;&#35782;&#24320;&#36767;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2307.03122</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20540;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Extracting Multi-valued Relations from Language Models. (arXiv:2307.03122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20540;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#21517;&#21644;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#20855;&#26377;&#29305;&#23450;&#20851;&#31995;&#38408;&#20540;&#20197;&#19978;&#30340;&#23545;&#35937;&#21487;&#20197;&#36798;&#21040;49.5%&#30340;F1&#24471;&#20998;&#65292;&#36825;&#23545;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20540;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35813;&#30740;&#31350;&#20026;&#20174;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#20013;&#25552;&#21462;&#20851;&#31995;&#30693;&#35782;&#24320;&#36767;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#34920;&#26126;&#23427;&#20204;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#27599;&#20010;&#20027;&#39064;-&#20851;&#31995;&#23545;&#20013;&#30340;&#21333;&#20010;&#23545;&#35937;&#65292;&#23613;&#31649;&#36890;&#24120;&#26377;&#22810;&#20010;&#23545;&#35937;&#26159;&#27491;&#30830;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#34920;&#31034;&#20197;&#20102;&#35299;&#23427;&#20204;&#20135;&#29983;&#22810;&#23545;&#35937;&#20851;&#31995;&#30693;&#35782;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#25490;&#21517;-&#36873;&#25321;&#20219;&#21153;&#12290;&#23545;&#20110;&#25490;&#21517;&#20505;&#36873;&#23545;&#35937;&#65292;&#25105;&#20204;&#35780;&#20272;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#26032;&#25216;&#26415;&#12290;&#22312;&#36873;&#25321;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#20855;&#26377;&#39640;&#20110;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#29305;&#23450;&#38408;&#20540;&#30340;&#23545;&#35937;&#21487;&#20197;&#36798;&#21040;49.5%&#30340;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;LMs&#36827;&#34892;&#22810;&#20540;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#30340;&#22256;&#38590;&#65292;&#24182;&#20026;&#20174;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#20013;&#25552;&#21462;&#20851;&#31995;&#30693;&#35782;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread usage of latent language representations via pre-trained language models (LMs) suggests that they are a promising source of structured knowledge. However, existing methods focus only on a single object per subject-relation pair, even though often multiple objects are correct. To overcome this limitation, we analyze these representations for their potential to yield materialized multi-object relational knowledge. We formulate the problem as a rank-then-select task. For ranking candidate objects, we evaluate existing prompting techniques and propose new ones incorporating domain knowledge. Among the selection methods, we find that choosing objects with a likelihood above a learned relation-specific threshold gives a 49.5% F1 score. Our results highlight the difficulty of employing LMs for the multi-valued slot-filling task and pave the way for further research on extracting relational knowledge from latent language representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#25233;&#37057;&#30151;&#20250;&#25913;&#21464;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#36825;&#31181;&#27934;&#23519;&#21147;&#21487;&#20197;&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25233;&#37057;&#30151;&#26816;&#27979;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02892</link><description>&lt;p&gt;
&#25233;&#37057;&#30151;&#23545;&#35821;&#38899;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20135;&#29983;&#24433;&#21709;: &#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#36895;&#24230;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Relationship Between Speech Features Changes When You Get Depressed: Feature Correlations for Improving Speed and Performance of Depression Detection. (arXiv:2307.02892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#25233;&#37057;&#30151;&#20250;&#25913;&#21464;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#36825;&#31181;&#27934;&#23519;&#21147;&#21487;&#20197;&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25233;&#37057;&#30151;&#26816;&#27979;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25233;&#37057;&#30151;&#20250;&#25913;&#21464;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#34920;&#26126;&#21033;&#29992;&#36825;&#26679;&#30340;&#27934;&#23519;&#21147;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;SVM&#21644;LSTMs&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#23454;&#39564;&#26159;&#22312;Androids Corpus&#19978;&#36827;&#34892;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;112&#21517;&#35828;&#35805;&#32773;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;58&#21517;&#30001;&#19987;&#19994;&#31934;&#31070;&#30149;&#23398;&#23478;&#35786;&#26029;&#20026;&#25233;&#37057;&#30151;&#30340;&#20154;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#19982;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#30456;&#27604;&#65292;&#20351;&#29992;&#29305;&#24449;&#30456;&#20851;&#24615;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#38169;&#35823;&#29575;&#30456;&#23545;&#20943;&#23569;&#20102;23.1&#65285;&#21040;26.6&#65285;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#27169;&#22411;&#12290;&#21487;&#33021;&#30340;&#35299;&#37322;&#26159;&#65292;&#22312;&#25233;&#37057;&#30340;&#35828;&#35805;&#32773;&#20013;&#65292;&#29305;&#24449;&#30456;&#20851;&#24615;&#30697;&#38453;&#20284;&#20046;&#26356;&#21152;&#22810;&#21464;&#12290;&#30456;&#24212;&#22320;&#65292;&#36825;&#31181;&#29616;&#35937;&#21487;&#20197;&#34987;&#35270;&#20026;&#25233;&#37057;&#30151;&#30340;&#19968;&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work shows that depression changes the correlation between features extracted from speech. Furthermore, it shows that using such an insight can improve the training speed and performance of depression detectors based on SVMs and LSTMs. The experiments were performed over the Androids Corpus, a publicly available dataset involving 112 speakers, including 58 people diagnosed with depression by professional psychiatrists. The results show that the models used in the experiments improve in terms of training speed and performance when fed with feature correlation matrices rather than with feature vectors. The relative reduction of the error rate ranges between 23.1% and 26.6% depending on the model. The probable explanation is that feature correlation matrices appear to be more variable in the case of depressed speakers. Correspondingly, such a phenomenon can be thought of as a depression marker.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.02792</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#24212;&#35813;&#20570;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02792
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#31561;&#30340;&#24555;&#36895;&#21457;&#23637;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#21644;&#32479;&#35745;&#23398;&#12290;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#21487;&#20197;&#31616;&#21270;&#22797;&#26434;&#30340;&#27969;&#31243;&#65292;&#20174;&#32780;&#37325;&#22609;&#20102;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#35748;&#20026;LLM&#27491;&#22312;&#36716;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#65292;&#23558;&#20182;&#20204;&#30340;&#37325;&#28857;&#20174;&#21160;&#25163;&#32534;&#30721;&#12289;&#25968;&#25454;&#25972;&#29702;&#21644;&#36827;&#34892;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#36825;&#20123;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#35282;&#33394;&#30340;&#28436;&#21464;&#31867;&#20284;&#20110;&#20174;&#36719;&#20214;&#24037;&#31243;&#24072;&#36716;&#21464;&#20026;&#20135;&#21697;&#32463;&#29702;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;LLM&#22312;&#25968;&#25454;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#36716;&#21464;&#12290;&#36825;&#20123;&#21457;&#23637;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#26377;&#24847;&#20041;&#22320;&#21457;&#23637;&#12290;&#25945;&#32946;&#26041;&#27861;&#29616;&#22312;&#24517;&#39035;&#26356;&#21152;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;LLM&#21551;&#21457;&#30340;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;LLM&#36824;&#21487;&#20197;&#22312;&#35838;&#22530;&#19978;&#36215;&#21040;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20316;&#20026;&#20114;&#21160;&#24335;&#25945;&#23398;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard&#36825;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;Bing Chat&#20248;&#20110;ChatGPT&#21644;Bard&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#35821;&#35328;&#25945;&#32946;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20316;&#20026;&#39640;&#20013;&#33521;&#35821;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.02288</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#27604;&#36739;&#65306;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard
&lt;/p&gt;
&lt;p&gt;
Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard. (arXiv:2307.02288v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard&#36825;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;Bing Chat&#20248;&#20110;ChatGPT&#21644;Bard&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#35821;&#35328;&#25945;&#32946;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20316;&#20026;&#39640;&#20013;&#33521;&#35821;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20998;&#21035;&#26159;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard&#65292;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing Chat&#20248;&#20110;ChatGPT&#21644;Bard&#12290;&#22240;&#27492;&#65292;&#22312;ChatGPT&#23578;&#26410;&#22312;&#36234;&#21335;&#27491;&#24335;&#21457;&#24067;&#20043;&#21069;&#65292;Bing Chat&#21644;Bard&#21487;&#20197;&#26367;&#20195;&#23427;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;ChatGPT&#12289;Bing Chat&#21644;Bard&#22312;&#33521;&#35821;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#36229;&#36807;&#20102;&#36234;&#21335;&#23398;&#29983;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;LLMs&#22312;&#33521;&#35821;&#35821;&#35328;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#12290;ChatGPT&#12289;Bing Chat&#21644;Bard&#30340;&#20986;&#33394;&#34920;&#29616;&#35777;&#26126;&#20102;&#23427;&#20204;&#20316;&#20026;&#39640;&#20013;&#33521;&#35821;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a performance comparison of three large language models (LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on the VNHSGE English dataset. The results show that BingChat is better than ChatGPT and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not yet officially available in Vietnam. The results also indicate that ChatGPT, Bing Chat, and Bard outperform Vietnamese students in English language proficiency. The findings of this study contribute to the understanding of the potential of LLMs in English language education. The remarkable performance of ChatGPT, Bing Chat, and Bard demonstrates their potential as effective tools for teaching and learning English at the high school level.
&lt;/p&gt;</description></item><item><title>LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.17103</link><description>&lt;p&gt;
LyricWhiz: &#36890;&#36807;&#21521;ChatGPT&#32819;&#35821;&#36827;&#34892;&#40065;&#26834;&#30340;&#22810;&#35821;&#35328;&#38646;&#23556;&#20987;&#27468;&#35789;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17103
&lt;/p&gt;
&lt;p&gt;
LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LyricWhiz&#30340;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#27966;&#22914;&#25671;&#28378;&#21644;&#37329;&#23646;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#20840;&#26032;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Whisper&#65292;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;GPT-4&#65292;&#24403;&#20170;&#26368;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;Whisper&#20805;&#24403;&#8220;&#32819;&#26421;&#8221;&#65292;&#36127;&#36131;&#36716;&#24405;&#35821;&#38899;&#65292;&#32780;GPT-4&#21017;&#20316;&#20026;&#8220;&#22823;&#33041;&#8221;&#65292;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#19978;&#19979;&#25991;&#36755;&#20986;&#36873;&#25321;&#21644;&#26657;&#27491;&#30340;&#27880;&#37322;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LyricWhiz&#22312;&#33521;&#35821;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#27468;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LyricWhiz&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;CC-BY-NC-SA&#29256;&#26435;&#35768;&#21487;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;MTG-Jamendo&#65292;&#24182;&#25552;&#20379;&#20102;h
&lt;/p&gt;
&lt;p&gt;
We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the "ear" by transcribing the audio, while GPT-4 serves as the "brain," acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#23548;&#21521;&#35780;&#20272;&#22522;&#20934; (KoLA)&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#35748;&#30693;&#26500;&#24314;&#20102;&#22235;&#32423;&#30693;&#35782;&#30456;&#20851;&#33021;&#21147;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#20852;&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20840;&#38754;&#12289;&#20844;&#27491;&#21644;&#23454;&#29992;&#22320;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#26410;&#35265;&#25968;&#25454;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.09296</link><description>&lt;p&gt;
KoLA: &#35748;&#30495;&#22522;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
KoLA: Carefully Benchmarking World Knowledge of Large Language Models. (arXiv:2306.09296v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#23548;&#21521;&#35780;&#20272;&#22522;&#20934; (KoLA)&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#35748;&#30693;&#26500;&#24314;&#20102;&#22235;&#32423;&#30693;&#35782;&#30456;&#20851;&#33021;&#21147;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#20852;&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20840;&#38754;&#12289;&#20844;&#27491;&#21644;&#23454;&#29992;&#22320;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#26410;&#35265;&#25968;&#25454;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#38656;&#35201;&#25913;&#36827;&#35780;&#20272;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#38500;&#20102;&#25506;&#32034;LLM&#33021;&#21147;&#30340;&#24191;&#24230;&#20043;&#22806;&#65292;&#32454;&#33268;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#35774;&#35745;&#23545;&#20110;&#20840;&#38754;&#12289;&#20844;&#27491;&#21644;&#23454;&#29992;&#30340;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#37492;&#20110;&#20840;&#29699;&#30693;&#35782;&#23545;LLM&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;&#30693;&#35782;&#20026;&#23548;&#21521;&#30340;LLM&#35780;&#20272;&#22522;&#20934;(KoLA)&#65292;&#20854;&#20013;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;(1) &#23545;&#20110;&#33021;&#21147;&#24314;&#27169;&#65292;&#25105;&#20204;&#27169;&#20223;&#20154;&#31867;&#35748;&#30693;&#26500;&#24314;&#20102;&#19968;&#20010;&#22235;&#32423;&#30693;&#35782;&#30456;&#20851;&#33021;&#21147;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;19&#20010;&#20219;&#21153;&#12290;(2) &#23545;&#20110;&#25968;&#25454;&#65292;&#20026;&#20102;&#30830;&#20445;&#20844;&#27491;&#27604;&#36739;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;LLM&#26222;&#36941;&#39044;&#35757;&#32451;&#30340;&#35821;&#26009;&#24211;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#25345;&#32493;&#25910;&#38598;&#30340;&#26032;&#20852;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35780;&#20272;&#22788;&#29702;&#26410;&#35265;&#25968;&#25454;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;(3) &#23545;&#20110;&#35780;&#20272;&#26631;&#20934;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#23545;&#27604;&#31995;&#32479;&#65292;&#21253;&#25324;&#25972;&#20307;&#26631;&#20934;&#20998;&#25968;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#21644;&#27169;&#22411;&#20043;&#38388;&#26356;&#22909;&#30340;&#25968;&#20540;&#27604;&#36739;&#24615;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#33258;&#23545;&#29031;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CheXpert&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#26174;&#30528;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#24503;&#35821;&#33016;&#37096;X&#23556;&#32447;&#21307;&#23398;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.05997</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24503;&#35821;&#33016;&#37096;X&#23556;&#32447;&#21307;&#23398;&#25253;&#21578;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning. (arXiv:2306.05997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CheXpert&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#26174;&#30528;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#24503;&#35821;&#33016;&#37096;X&#23556;&#32447;&#21307;&#23398;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#33539;&#22260;&#20869;&#25918;&#23556;&#31185;&#21307;&#29983;&#30701;&#32570;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22521;&#35757;&#36825;&#26679;&#30340;&#27169;&#22411;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#26631;&#35760;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#33258;&#21160;&#25552;&#21462;&#26631;&#31614;&#21487;&#20197;&#20943;&#23569;&#33719;&#24471;&#26631;&#35760;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#20294;&#30001;&#20110;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#35789;&#21644;&#32570;&#23569;&#27880;&#37322;&#25968;&#25454;&#32780;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26631;&#31614;&#22120;&#30340;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CheXpert&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30001;&#22522;&#20110;&#35268;&#21017;&#30340;&#24503;&#35821;CheXpert&#27169;&#22411;&#26631;&#35760;&#30340;&#25253;&#21578;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#23569;&#37327;&#25163;&#21160;&#26631;&#35760;&#30340;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiologists are in short supply globally, and deep learning models offer a promising solution to address this shortage as part of clinical decision-support systems. However, training such models often requires expensive and time-consuming manual labeling of large datasets. Automatic label extraction from radiology reports can reduce the time required to obtain labeled datasets, but this task is challenging due to semantically similar words and missing annotated data. In this work, we explore the potential of weak supervision of a deep learning-based label prediction model, using a rule-based labeler. We propose a deep learning-based CheXpert label prediction model, pre-trained on reports labeled by a rule-based German CheXpert model and fine-tuned on a small dataset of manually labeled reports. Our results demonstrate the effectiveness of our approach, which significantly outperformed the rule-based model on all three tasks. Our findings highlight the benefits of employing deep learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#33521;&#35821;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#21644;&#26032;&#20195;&#35789;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#38750;&#20108;&#20803;&#24615;&#21035;&#36523;&#20221;&#30340;&#37325;&#35201;&#24615;&#65292;&#24341;&#20837;&#20102;MISGENDERED&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#39318;&#36873;&#20195;&#35789;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03950</link><description>&lt;p&gt;
MISGENDERED&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20195;&#35789;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
MISGENDERED: Limits of Large Language Models in Understanding Pronouns. (arXiv:2306.03950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#33521;&#35821;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#21644;&#26032;&#20195;&#35789;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#38750;&#20108;&#20803;&#24615;&#21035;&#36523;&#20221;&#30340;&#37325;&#35201;&#24615;&#65292;&#24341;&#20837;&#20102;MISGENDERED&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#39318;&#36873;&#20195;&#35789;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#35686;&#21578;:&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#20196;&#20154;&#19981;&#24742;&#21644;&#28508;&#22312;&#24341;&#21457;&#24773;&#24863;&#38382;&#39064;&#30340;&#38169;&#35823;&#31216;&#21628;&#21644;&#25273;&#26432;&#30340;&#20363;&#23376;. &#24615;&#21035;&#20559;&#35265;&#22312;&#35821;&#35328;&#25216;&#26415;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#20108;&#20803;&#24615;&#21035;&#33539;&#24335;&#12290;&#32771;&#34385;&#38750;&#20108;&#20803;&#24615;&#21035;&#36523;&#20221;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#25490;&#38500;&#20182;&#20204;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#20260;&#23475;&#36825;&#20010;&#24050;&#32463;&#34987;&#36793;&#32536;&#21270;&#30340;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#33521;&#35821;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#65288;&#20363;&#22914;&#21333;&#25968;they&#12289;them&#65289;&#21644;&#26032;&#20195;&#35789;&#65288;&#20363;&#22914;ze&#12289;xe&#12289;thon&#65289;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20195;&#35789;&#26159;&#30001;&#37027;&#20123;&#24615;&#21035;&#35748;&#21516;&#19981;&#20026;&#20108;&#20803;&#24615;&#21035;&#25152;&#20195;&#34920;&#30340;&#20010;&#20307;&#20351;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MISGENDERED&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#39318;&#36873;&#20195;&#35789;&#30340;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;(i)&#38472;&#36848;&#20010;&#20307;&#20195;&#35789;&#30340;&#23454;&#20363;&#65292;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#32570;&#23569;&#20195;&#35789;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#35780;&#20272;&#25513;&#30422;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#30340;&#23454;&#39564;&#35774;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering.  Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce MISGENDERED, a framework for evaluating large language models' ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual's pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive languag
&lt;/p&gt;</description></item><item><title>BigTranslate&#26159;&#19968;&#20010;&#22522;&#20110;LLaMA&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21407;&#26377;&#30340;&#22522;&#30784;&#19978;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#23454;&#29616;&#20102;&#23545;100&#22810;&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#33021;&#21147;&#65292;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;ChatGPT&#21644;&#35895;&#27468;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2305.18098</link><description>&lt;p&gt;
BigTranslate&#65306;&#36890;&#36807;&#22810;&#35821;&#35328;&#32763;&#35793;&#22686;&#24378;&#36229;&#36807;100&#31181;&#35821;&#35328;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages. (arXiv:2305.18098v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18098
&lt;/p&gt;
&lt;p&gt;
BigTranslate&#26159;&#19968;&#20010;&#22522;&#20110;LLaMA&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21407;&#26377;&#30340;&#22522;&#30784;&#19978;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#23454;&#29616;&#20102;&#23545;100&#22810;&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#33021;&#21147;&#65292;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;ChatGPT&#21644;&#35895;&#27468;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;LLM&#65292;&#29305;&#21035;&#26159;&#24320;&#28304;&#30340;&#65292;&#27604;&#22914;BLOOM&#21644;LLaMA&#65292;&#37117;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#65292;&#24182;&#19988;&#21482;&#25903;&#25345;&#20960;&#21313;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#24471;LLM&#22312;&#35821;&#35328;&#32763;&#35793;&#26041;&#38754;&#30340;&#28508;&#21147;&#19981;&#22826;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BigTranslate&#65292;&#23427;&#37319;&#29992;&#20102;LLaMA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#35206;&#30422;20&#31181;&#35821;&#35328;&#65292;&#24182;&#22312;100&#22810;&#31181;&#35821;&#35328;&#19978;&#22686;&#24378;&#20102;&#20854;&#22810;&#35821;&#35328;&#32763;&#35793;&#33021;&#21147;&#12290;BigTranslate&#26159;&#24314;&#31435;&#22312;LLaMA-13B&#20043;&#19978;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#36827;&#34892;&#20248;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#21333;&#35821;&#25968;&#25454;&#32487;&#32493;&#35757;&#32451;LLaMA&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35206;&#30422;102&#31181;&#33258;&#28982;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;&#25968;&#25454;&#38598;&#32487;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#32763;&#35793;&#25351;&#20196;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#65292;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;BigTranslate&#27169;&#22411;&#12290;&#22810;&#35821;&#35328;&#32763;&#35793;&#30340;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;BigTranslate&#19982;ChatGPT&#21644;&#35895;&#27468;&#32763;&#35793;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with ChatGPT and Google Tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#34987;&#39640;&#20272;&#20102;&#65292;&#22823;&#22810;&#25968;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#21033;&#29992;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.17442</link><description>&lt;p&gt;
&#27604;&#20320;&#24819;&#30340;&#35201;&#24369;&#65306;&#23545;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#25209;&#21028;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Weaker Than You Think: A Critical Look at Weakly Supervised Learning. (arXiv:2305.17442v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17442
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#34987;&#39640;&#20272;&#20102;&#65292;&#22823;&#22810;&#25968;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#21033;&#29992;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#20351;&#29992;&#20174;&#21508;&#31181;&#24369;&#26631;&#27880;&#28304;&#33719;&#24471;&#30340;&#22024;&#26434;&#26631;&#27880;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#35201;&#27714;&#39640;&#36136;&#37327;&#20294;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#31934;&#24039;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#36827;&#34892;&#24378;&#22823;&#30340;&#35757;&#32451;&#65292;&#24182;&#25253;&#21578;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#35774;&#32622;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#34987;&#26174;&#33879;&#39640;&#20272;&#20102;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21487;&#29992;&#30340;&#24178;&#20928;&#39564;&#35777;&#26679;&#26412;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;&#20854;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#24178;&#20928;&#26631;&#31614;&#12290;&#22312;&#20351;&#29992;&#36825;&#20123;&#24178;&#20928;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#31934;&#24039;&#26041;&#27861;&#30340;&#20248;&#21183;&#22823;&#37096;&#20998;&#34987;&#28040;&#38500;&#20102;&#12290;&#21363;&#20351;&#23558;&#21487;&#29992;&#30340;&#24178;&#20928;&#25968;&#25454;&#30340;&#22823;&#23567;&#20943;&#23569;&#21040;&#27599;&#31867;&#21482;&#26377;&#20116;&#20010;&#26679;&#26412;&#65292;&#36825;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16724</link><description>&lt;p&gt;
&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;&#30340;&#30740;&#31350;&#22823;&#22810;&#38656;&#35201;&#22312;&#30446;&#26631;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#32570;&#20047;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GLOSS&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;PMMTM&#65289;&#20043;&#19978;&#65292;&#24182;&#24102;&#26377;&#39069;&#22806;&#30340;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22359;&#65292;&#26080;&#35770;&#26159;&#36866;&#37197;&#22120;&#36824;&#26159;&#39069;&#22806;&#30340;&#21069;&#32512;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20013;&#23398;&#20064;&#20195;&#30721;&#20999;&#25442;&#27169;&#24335;&#65292;&#32780;GLOSS&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;PMMTM&#34987;&#20923;&#32467;&#12290;&#25105;&#20204;&#21482;&#35843;&#25972;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#35774;&#35745;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#35757;&#32451;&#25968;&#25454;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;GLOSS&#34920;&#29616;&#20986;&#20102;&#36328;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#24402;&#32435;&#21644;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#35821;&#35328;&#21333;&#35821;&#25991;&#26412;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GLOSS&#20248;&#20110;&#20854;&#20182;&#20174;&#20855;&#26377;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#35821;&#35328;&#23545;&#20013;&#35843;&#25972;&#30340;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an adapter or extra prefixes, learns code-switching patterns from code-switched data during training, while the primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only adjusting the code-switching module prevents our model from overfitting to the constrained training data for code-switching. Hence, GLOSS exhibits the ability to generalize and synthesize code-switched texts across a broader spectrum of language pairs. Additionally, we develop a self-training algorithm on target langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#35789;&#27719;&#21305;&#37197;&#20316;&#20026;&#35780;&#20272;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#26377;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#38646;-shot&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.06984</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#35789;&#27719;&#21305;&#37197;&#20316;&#20026;&#35780;&#20272;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#26377;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#38646;-shot&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#21305;&#37197;&#20173;&#26159;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#20107;&#23454;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#19968;&#20010;&#21512;&#29702;&#30340;&#20505;&#36873;&#31572;&#26696;&#26410;&#20986;&#29616;&#22312;&#37329;&#26631;&#20934;&#31572;&#26696;&#21015;&#34920;&#20013;&#26102;&#65292;&#35789;&#27719;&#21305;&#37197;&#23436;&#20840;&#22833;&#36133;&#65292;&#38543;&#30528;&#25105;&#20204;&#20174;&#25277;&#21462;&#27169;&#22411;&#36716;&#21521;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#31181;&#24773;&#20917;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;QA&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21152;&#21095;&#20102;&#35789;&#27719;&#21305;&#37197;&#30340;&#22833;&#36133;&#65292;&#22240;&#20026;&#20505;&#36873;&#31572;&#26696;&#21464;&#24471;&#26356;&#38271;&#65292;&#22240;&#27492;&#19982;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#21305;&#37197;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32570;&#20047;&#20934;&#30830;&#30340;&#35780;&#20272;&#65292;&#24320;&#25918;&#39046;&#22495;QA&#30340;&#30495;&#27491;&#36827;&#23637;&#20173;&#28982;&#26410;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;NQ-open&#30340;&#19968;&#20010;&#23376;&#38598;&#19978;&#25163;&#21160;&#35780;&#20272;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;QA&#27169;&#22411;&#65288;&#21253;&#25324;LLMs&#65289;&#30340;&#31572;&#26696;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#65292;&#23613;&#31649;&#25152;&#26377;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#33021;&#34987;&#26174;&#30528;&#20302;&#20272;&#65292;&#20294;InstructGPT&#65288;&#38646;-shot&#65289;LLM&#30340;&#24615;&#33021;&#22686;&#21152;&#20102;&#36817;60&#65285;&#65292;&#20351;&#20854;&#19982;&#29616;&#26377;&#30340;&#39030;&#32423;&#27169;&#22411;&#24182;&#39550;&#40784;&#39537;&#65292;&#32780;I
&lt;/p&gt;
&lt;p&gt;
Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the I
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#35821;&#20041;&#23884;&#20837;API&#22312;&#23454;&#38469;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#20998;&#26512;,&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#24403;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#19978;&#20351;&#29992;API&#37325;&#26032;&#25490;&#21517;BM25&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39044;&#31639;&#21451;&#22909;&#30340;&#26368;&#20248;&#20570;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06300</link><description>&lt;p&gt;
&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#30340;&#23884;&#20837;&#24335;API
&lt;/p&gt;
&lt;p&gt;
Evaluating Embedding APIs for Information Retrieval. (arXiv:2305.06300v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#35821;&#20041;&#23884;&#20837;API&#22312;&#23454;&#38469;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#20998;&#26512;,&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#24403;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#19978;&#20351;&#29992;API&#37325;&#26032;&#25490;&#21517;BM25&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39044;&#31639;&#21451;&#22909;&#30340;&#26368;&#20248;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#22686;&#22823;&#20351;&#24471;&#20854;&#26222;&#21450;&#21270;&#25104;&#20026;&#20102;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#27492;&#35768;&#22810;&#20844;&#21496;&#21644;&#21021;&#21019;&#20225;&#19994;&#36890;&#36807;API&#21521;&#31038;&#21306;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#20854;&#20013;&#19968;&#20010;&#36866;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;&#29305;&#23450;API&#26159;&#35821;&#20041;&#23884;&#20837;&#24335;API&#65292;&#20854;&#21487;&#26500;&#24314;&#32473;&#23450;&#25991;&#26412;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#22312;&#25317;&#26377;&#36234;&#26469;&#36234;&#22810;API&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#22312;&#23454;&#38469;&#26816;&#32034;&#22330;&#26223;&#20013;&#35821;&#20041;&#23884;&#20837;&#24335;API&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#20182;&#20204;&#30340;&#38656;&#27714;&#25214;&#21040;&#36866;&#24403;&#30340;&#26381;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24076;&#26395;&#35843;&#26597;&#29616;&#26377;API&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26631;&#20934;&#22522;&#20934;BEIR&#21644;MIRACL&#19978;&#35780;&#20272;&#20102;&#23884;&#20837;&#24335;API&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;API&#37325;&#26032;&#25490;&#21517;BM25&#32467;&#26524;&#26159;&#19968;&#31181;&#39044;&#31639;&#21451;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#19978;&#26368;&#26377;&#25928;&#65292;&#19982;&#26631;&#20934;&#20570;&#27861;&#21363;&#20316;&#20026;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#22120;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers
&lt;/p&gt;</description></item><item><title>ESPnet-ST-v2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#21151;&#33021;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22810;&#31181;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#26550;&#26500;&#21644;&#25216;&#26415;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04596</link><description>&lt;p&gt;
ESPnet-ST-v2&#65306;&#22810;&#21151;&#33021;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit. (arXiv:2304.04596v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04596
&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#21151;&#33021;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22810;&#31181;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#26550;&#26500;&#21644;&#25216;&#26415;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#31163;&#32447;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;ST&#65289;&#12289;&#21516;&#27493;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;SST&#65289;&#21644;&#31163;&#32447;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#12290;&#19982;&#20854;&#20182;&#24320;&#28304;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#19981;&#21516;&#30340;&#26159;&#65292;ESPnet-ST-v2&#37319;&#29992;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#26550;&#26500;&#65292;&#21253;&#25324;&#36716;&#24405;&#22120;&#12289;&#28151;&#21512;CTC/attention&#12289;&#22810;&#35299;&#30721;&#22120;&#12289;&#26102;&#38388;&#21516;&#27493;&#20998;&#22359;CTC/attention&#12289;Translatotron&#27169;&#22411;&#21644;&#30452;&#25509;&#31163;&#25955;&#21333;&#20803;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#24178;&#20928;&#25110;&#25915;&#20987;&#24773;&#22659;&#19979;&#22343;&#26377;&#25928;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.01005</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24178;&#20928;&#21644;&#25915;&#20987;&#24773;&#26223;&#19979;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#24178;&#20928;&#25110;&#25915;&#20987;&#24773;&#22659;&#19979;&#22343;&#26377;&#25928;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#19968;&#20010;&#26085;&#30410;&#22686;&#38271;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#20854;&#20998;&#25955;&#21644;&#31169;&#23494;&#30340;&#35774;&#35745;&#32780;&#24471;&#21040;&#21457;&#23637;&#12290;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#22823;&#37327;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#28982;&#21518;&#65292;&#26381;&#21153;&#22120;&#32858;&#21512;&#20102;&#22312;&#36825;&#20123;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#35775;&#38382;&#23427;&#20204;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#26159;&#22312;&#20219;&#20309;&#31038;&#20132;&#23186;&#20307;&#26381;&#21153;&#21644;&#21363;&#26102;&#36890;&#35759;&#24179;&#21488;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#34920;&#36798;&#29992;&#25143;&#30340;&#24773;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24178;&#20928;&#21644;&#25915;&#20987;&#24773;&#22659;&#19979;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#12290;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#25968;&#25454;&#20174;Twitter&#21644;SemEval&#34920;&#24773;&#31526;&#21495;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#19981;&#21516;&#22823;&#23567;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#21253;&#25324;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#20013;&#20551;&#23450;&#25968;&#25454;&#24178;&#20928;&#25110;&#22312;&#19968;&#20123;&#23458;&#25143;&#31471;&#20013;&#36827;&#34892;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#31232;&#30095;&#28608;&#27963;&#36716;&#25442;&#22120;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24178;&#20928;&#25110;&#25915;&#20987;&#24773;&#22659;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of client data while maintaining privacy. Then, a server aggregates the training done on these multiple clients without access to their data, which could be emojis widely used in any social media service and instant messaging platforms to express users' sentiments. This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios. Emoji prediction data have been crawled from both Twitter and SemEval emoji datasets. This data is used to train and evaluate different transformer model sizes including a sparsely activated transformer with either the assumption of clean data in all clients or poisoned data via label flipping attack in some clients. Experimental results on these models show that federated learning in either clean or attack
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#25104;&#23450;&#21521;&#21050;&#28608;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.11520</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#21050;&#28608;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#25104;&#23450;&#21521;&#21050;&#28608;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#23450;&#21521;&#21050;&#28608;&#24341;&#23548;&#65292;&#23427;&#20351;&#29992;&#21487;&#35843;&#33410;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#40657;&#30418;&#20923;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25351;&#23548;&#12290;&#19982;&#20197;&#24448;&#25163;&#21160;&#25110;&#33258;&#21160;&#25214;&#21040;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#31163;&#25955;&#30340;token&#20316;&#20026;&#27599;&#20010;&#36755;&#20837;&#30340;&#23450;&#21521;&#21050;&#28608;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#31034;&#25110;&#25552;&#31034;&#65292;&#20363;&#22914;&#25991;&#31456;&#30340;&#20851;&#38190;&#35789;&#29992;&#20110;&#25688;&#35201;&#12290;&#28982;&#21518;&#23558;&#23450;&#21521;&#21050;&#28608;&#19982;&#21407;&#22987;&#36755;&#20837;&#32452;&#21512;&#65292;&#24182;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#25351;&#23548;&#20854;&#21521;&#25152;&#38656;&#30446;&#26631;&#29983;&#25104;&#12290;&#31574;&#30053;LM&#21487;&#20197;&#36890;&#36807;1&#65289;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;2&#65289;&#20174;&#31163;&#32447;&#21644;&#22312;&#32447;&#22870;&#21169;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25506;&#32034;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#23450;&#21521;&#21050;&#28608;&#12290;&#35813;&#26694;&#26550;&#21487;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;LM&#21644;&#20219;&#21153;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#25928;&#26524;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#25688;&#35201;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as directional stimulus of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.09811</link><description>&lt;p&gt;
&#39640;&#25928;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#21024;&#20943;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#21452;&#35821;&#32763;&#35793;&#31995;&#32479;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#20174;&#30693;&#35782;&#36716;&#31227;&#20013;&#33719;&#30410;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#22810;&#35821;&#35328;&#24615;&#30340;&#38480;&#21046;&#65292;&#38500;&#38750;&#36827;&#34892;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#21542;&#21017;&#20250;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22823;&#24133;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;NLLB-200&#26159;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#20363;&#23376;&#12290;&#23427;&#28085;&#30422;&#20102;202&#31181;&#35821;&#35328;&#65292;&#20294;&#20165;&#25512;&#29702;&#23601;&#38656;&#35201;&#33267;&#23569;&#22235;&#20010;32GB&#30340;GPU&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#20801;&#35768;&#21024;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20294;&#32763;&#35793;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#35813;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#21098;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#35782;&#21035;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
&lt;/p&gt;</description></item><item><title>WACO&#26159;&#19968;&#31181;&#29992;&#20110;&#26497;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#35789;&#32423;&#34920;&#31034;&#36830;&#25509;&#36215;&#26469;&#65292;&#23454;&#39564;&#35777;&#26126;WACO&#22312;&#26497;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#27604;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;9+ BLEU&#20998;&#12290;</title><link>http://arxiv.org/abs/2212.09359</link><description>&lt;p&gt;
WACO: &#29992;&#20110;&#35821;&#38899;&#32763;&#35793;&#30340;&#35789;&#23545;&#40784;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09359
&lt;/p&gt;
&lt;p&gt;
WACO&#26159;&#19968;&#31181;&#29992;&#20110;&#26497;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#35789;&#32423;&#34920;&#31034;&#36830;&#25509;&#36215;&#26469;&#65292;&#23454;&#39564;&#35777;&#26126;WACO&#22312;&#26497;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#27604;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;9+ BLEU&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;E2E ST&#65289;&#26088;&#22312;&#30452;&#25509;&#23558;&#28304;&#35821;&#38899;&#36716;&#21270;&#20026;&#30446;&#26631;&#25991;&#26412;&#12290;&#24403;&#20165;&#26377;&#26497;&#23569;&#30340;&#35821;&#38899;&#25991;&#26412;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#29616;&#26377;&#30340;ST&#26041;&#27861;&#30340;&#34920;&#29616;&#24456;&#24046;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ST&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35821;&#38899;&#21644;&#28304;&#25991;&#26412;&#20043;&#38388;&#30340;&#23884;&#20837;&#30456;&#20284;&#24230;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Word-Aligned COntrastive learning&#65288;WACO&#65289;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26497;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#24314;&#31435;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#35789;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;WACO&#21644;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#22522;&#20934;&#65292;&#24182;&#22312;&#20174;IWSLT 2023&#33719;&#21462;&#30340;&#20302;&#36164;&#28304;&#26041;&#21521;&#30340;&#39532;&#32819;&#20182;&#35821;-&#33521;&#35821;&#32763;&#35793;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WACO&#20165;&#20351;&#29992;1&#23567;&#26102;&#30340;&#24182;&#34892;ST&#25968;&#25454;&#65292;&#27604;&#26368;&#22909;&#30340;&#22522;&#20934;&#25552;&#39640;&#20102;9+ BLEU&#20998;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/owaski/WACO&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;SESCORE2&#65292;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#30340;&#27169;&#22411;&#38169;&#35823;&#26469;&#35757;&#32451;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;SESCORE2&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.09305</link><description>&lt;p&gt;
SESCORE2: &#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#38169;&#35823;&#26469;&#23398;&#20064;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes. (arXiv:2212.09305v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;SESCORE2&#65292;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#30340;&#27169;&#22411;&#38169;&#35823;&#26469;&#35757;&#32451;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;SESCORE2&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;&#21542;&#21487;&#33021;&#22312;&#27809;&#26377;&#20154;&#24037;&#35780;&#20998;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#36890;&#29992;&#24230;&#37327;&#26631;&#20934;&#65311;&#29616;&#26377;&#30340;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#29702;&#24819;&#65292;&#25110;&#32773;&#38656;&#35201;&#20154;&#24037;&#35780;&#20998;&#26469;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SESCORE2&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#12290;&#20854;&#20851;&#38190;&#27010;&#24565;&#26159;&#36890;&#36807;&#25200;&#21160;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#21477;&#23376;&#26469;&#21512;&#25104;&#30495;&#23454;&#30340;&#27169;&#22411;&#38169;&#35823;&#12290;SESCORE2&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#20854;&#26131;&#20110;&#25193;&#23637;&#21040;&#35768;&#22810;&#20854;&#20182;&#35821;&#35328;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#20005;&#37325;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#35821;&#35328;&#30340;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;SESCORE2&#21644;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;SESCORE2&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#26080;&#30417;&#30563;&#24230;&#37327;&#26631;&#20934;PRISM&#65292;&#32943;&#24503;&#23572;&#25913;&#36827;&#20026;0.078&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;SESCORE2&#29978;&#33267;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#26377;&#30417;&#30563;&#24230;&#37327;&#26631;&#20934;BLEURT&#21644;COMET&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https:&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is it possible to train a general metric for evaluating text generation quality without human annotated ratings? Existing learned metrics either perform unsatisfactorily across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SESCORE2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. The primary advantage of the SESCORE2 is its ease of extension to many other languages while providing reliable severity estimation. We evaluate SESCORE2 and previous methods on four text generation tasks across three languages. SESCORE2 outperforms unsupervised metric PRISM on four text generation evaluation benchmarks, with a Kendall improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks. The code and data are available at https:
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;ALERT&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#33021;&#21147;&#30340;&#22522;&#20934;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#26356;&#22810;&#25512;&#29702;&#25216;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#26469;&#35780;&#20272;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.08286</link><description>&lt;p&gt;
ALERT&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
ALERT: Adapting Language Models to Reasoning Tasks. (arXiv:2212.08286v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;ALERT&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#33021;&#21147;&#30340;&#22522;&#20934;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#26356;&#22810;&#25512;&#29702;&#25216;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#26469;&#35780;&#20272;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#36880;&#27493;&#25512;&#29702;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#24212;&#29992;&#20102;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#25512;&#29702;&#25216;&#24039;&#24182;&#22312;&#20182;&#20204;&#30340;&#35757;&#32451;&#19978;&#19979;&#25991;&#20043;&#22806;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#26159;&#20165;&#20165;&#22312;&#26356;&#32454;&#31890;&#24230;&#19978;&#35760;&#20303;&#20102;&#20182;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#24182;&#23398;&#20250;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#19978;&#19979;&#25991;&#65311;&#20026;&#20102;&#20998;&#35299;&#36825;&#20123;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ALERT&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#21644;&#19968;&#22871;&#20998;&#26512;&#24037;&#20855;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#22312;&#38656;&#35201;&#25512;&#29702;&#25216;&#33021;&#35299;&#20915;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;ALERT&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#23427;&#28085;&#30422;&#20102;20&#20010;&#25968;&#25454;&#38598;&#21644;10&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;ALERT&#36827;&#19968;&#27493;&#23545;&#24494;&#35843;&#30340;&#20316;&#29992;&#36827;&#34892;&#30740;&#31350;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#26356;&#22810;&#30340;&#25512;&#29702;&#25216;&#33021;&#65292;&#20363;&#22914;&#25991;&#26412;&#34164;&#28085;&#12289;&#28436;&#32462;&#25512;&#29702;&#21644;&#31867;&#27604;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#20316;&#32773;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;</title><link>http://arxiv.org/abs/2211.07443</link><description>&lt;p&gt;
&#26657;&#20934;&#35299;&#37322;&#65306;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#20316;&#32773;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#26469;&#23558;&#35821;&#35328;&#32763;&#35793;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#21363;&#25191;&#34892;&#35821;&#20041;&#35299;&#26512;&#12290;&#35821;&#20041;&#35299;&#26512;&#26088;&#22312;&#25191;&#34892;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#20316;&#65292;&#22240;&#27492;&#24320;&#21457;&#23433;&#20840;&#31995;&#32479;&#26159;&#26377;&#24517;&#35201;&#30340;&#65292;&#32780;&#27979;&#37327;&#26657;&#20934;&#21017;&#26159;&#23433;&#20840;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#23588;&#20854;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#21464;&#21270;&#24040;&#22823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;&#35299;&#26512;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#25286;&#20998;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence generation models are increasingly being used to translate language into executable programs, i.e. to perform executable semantic parsing. The fact that semantic parsing aims to execute actions in the real world motivates developing safe systems, which in turn makes measuring calibration -- a central component to safety -- particularly important. We investigate the calibration of common generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.
&lt;/p&gt;</description></item><item><title>&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;&#32467;&#21512;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#12290;</title><link>http://arxiv.org/abs/2211.05953</link><description>&lt;p&gt;
&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05953
&lt;/p&gt;
&lt;p&gt;
&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;&#32467;&#21512;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#35843;&#24230;&#26041;&#27861;&#8212;&#8212;&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#65292;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#30340;&#32467;&#21512;&#12290;&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#24182;&#32467;&#21512;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;GPU&#21033;&#29992;&#29575;&#12289;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;Megatron-LM&#65292;&#23545;&#20110;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;GPU&#38598;&#32676;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#21644;&#25104;&#26412;&#21516;&#26679;&#38477;&#20302;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Breadth-First Pipeline Parallelism, a novel training schedule which optimizes the combination of pipeline and data parallelism. Breadth-First Pipeline Parallelism lowers training time, cost and memory usage by combining a high GPU utilization with a small batch size per GPU, and by making use of fully sharded data parallelism. Experimentally, we observed an increase of up to 43% in training throughput for a 52 billion-parameter model using a small batch size per GPU compared to Megatron-LM, which would reduce the training time and cost by the same amount on a large GPU cluster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36755;&#20986;&#26469;&#25913;&#36827;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#37319;&#29992;&#35270;&#35273;&#35821;&#20041;&#24230;&#37327;&#26469;&#21305;&#37197;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#19982;&#21512;&#36866;&#30340;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.12817</link><description>&lt;p&gt;
&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#30340;&#35789;&#35821;&#19982;&#21477;&#23376;&#35270;&#35273;&#35821;&#20041;&#30456;&#20284;&#24230;&#65306;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Word to Sentence Visual Semantic Similarity for Caption Generation: Lessons Learned. (arXiv:2209.12817v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36755;&#20986;&#26469;&#25913;&#36827;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#37319;&#29992;&#35270;&#35273;&#35821;&#20041;&#24230;&#37327;&#26469;&#21305;&#37197;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#19982;&#21512;&#36866;&#30340;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22686;&#24378;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#29983;&#25104;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#22270;&#20687;&#26368;&#30456;&#20851;&#30340;&#36755;&#20986;&#32780;&#19981;&#26159;&#27169;&#22411;&#20135;&#29983;&#30340;&#26368;&#21487;&#33021;&#36755;&#20986;&#26469;&#25913;&#36827;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#35270;&#35273;&#32972;&#26223;&#30340;&#35282;&#24230;&#37325;&#26032;&#35843;&#25972;&#20102;&#35821;&#35328;&#29983;&#25104;&#36755;&#20986;&#30340;&#27874;&#26463;&#25628;&#32034;&#12290;&#25105;&#20204;&#20197;&#35789;&#35821;&#21644;&#21477;&#23376;&#32423;&#21035;&#19978;&#30340;&#35270;&#35273;&#35821;&#20041;&#24230;&#37327;&#26469;&#21305;&#37197;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#19982;&#21512;&#36866;&#30340;&#26631;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#26041;&#27861;&#24212;&#29992;&#20110;&#20219;&#20309;&#26631;&#39064;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on enhancing the captions generated by image-caption generation systems. We propose an approach for improving caption generation systems by choosing the most closely related output to the image rather than the most likely output produced by the model. Our model revises the language generation output beam search from a visual context perspective. We employ a visual semantic measure in a word and sentence level manner to match the proper caption to the related information in the image. The proposed approach can be applied to any caption system as a post-processing based method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35782;&#21035;&#21644;&#35299;&#20915;&#20102;&#24403;&#21069;&#21487;&#24494;&#25628;&#32034;&#32034;&#24341;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22312;&#32034;&#24341;&#21644;&#26816;&#32034;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32034;&#24341;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2206.10128</link><description>&lt;p&gt;
&#23558;&#32034;&#24341;&#21644;&#26816;&#32034;&#26725;&#25509;&#36215;&#26469;&#65292;&#20026;&#20855;&#26377;&#26597;&#35810;&#29983;&#25104;&#30340;&#21487;&#24494;&#25628;&#32034;&#32034;&#24341;&#22635;&#34917;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation. (arXiv:2206.10128v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35782;&#21035;&#21644;&#35299;&#20915;&#20102;&#24403;&#21069;&#21487;&#24494;&#25628;&#32034;&#32034;&#24341;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22312;&#32034;&#24341;&#21644;&#26816;&#32034;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32034;&#24341;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#25628;&#32034;&#32034;&#24341;(DSI)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20449;&#24687;&#26816;&#32034;&#33539;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#26816;&#32034;&#26550;&#26500;&#19981;&#21516;&#65292;&#20854;&#20013;&#32034;&#24341;&#21644;&#26816;&#32034;&#26159;&#20004;&#20010;&#19981;&#21516;&#30340;&#32452;&#20214;&#65292;DSI&#20351;&#29992;&#21333;&#20010;transformer&#27169;&#22411;&#26469;&#25191;&#34892;&#32034;&#24341;&#21644;&#26816;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;DSI&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;DSI&#32034;&#24341;&#21644;&#26816;&#32034;&#36807;&#31243;&#20043;&#38388;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#32034;&#24341;&#36807;&#31243;&#20013;&#65292;&#24403;&#21069;DSI&#26041;&#27861;&#23398;&#20064;&#26500;&#24314;&#38271;&#25991;&#26723;&#30340;&#25991;&#26412;&#19982;&#25991;&#26723;&#26631;&#35782;&#31526;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20294;&#26816;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#26597;&#35810;&#36890;&#24120;&#27604;&#32034;&#24341;&#30340;&#25991;&#26723;&#35201;&#30701;&#24471;&#22810;&#12290;&#24403;&#23558;DSI&#29992;&#20110;&#36328;&#35821;&#35328;&#26816;&#32034;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#36827;&#19968;&#27493;&#21152;&#21095;&#65292;&#22240;&#20026;&#25991;&#26723;&#25991;&#26412;&#21644;&#26597;&#35810;&#25991;&#26412;&#22788;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;DSI&#27169;&#22411;&#30340;&#36825;&#20010;&#26681;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;DSI&#32034;&#24341;&#26694;&#26550;&#65292;c
&lt;/p&gt;
&lt;p&gt;
The Differentiable Search Index (DSI) is an emerging paradigm for information retrieval. Unlike traditional retrieval architectures where index and retrieval are two different and separate components, DSI uses a single transformer model to perform both indexing and retrieval.  In this paper, we identify and tackle an important issue of current DSI models: the data distribution mismatch that occurs between the DSI indexing and retrieval processes. Specifically, we argue that, at indexing, current DSI methods learn to build connections between the text of long documents and the identifier of the documents, but then retrieval of document identifiers is based on queries that are commonly much shorter than the indexed documents. This problem is further exacerbated when using DSI for cross-lingual retrieval, where document text and query text are in different languages.  To address this fundamental problem of current DSI models, we propose a simple yet effective indexing framework for DSI, c
&lt;/p&gt;</description></item><item><title>&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26816;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;&#26500;&#24314;&#20102;Mocheg&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;15,601&#20010;&#20027;&#24352;&#65292;33,880&#20010;&#25991;&#26412;&#27573;&#33853;&#21644;12,112&#20010;&#22270;&#20687;&#20316;&#20026;&#35777;&#25454;&#12290;&#36890;&#36807;&#19977;&#20010;&#23376;&#20219;&#21153;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.12487</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26816;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models. (arXiv:2205.12487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12487
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26816;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;&#26500;&#24314;&#20102;Mocheg&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;15,601&#20010;&#20027;&#24352;&#65292;33,880&#20010;&#25991;&#26412;&#27573;&#33853;&#21644;12,112&#20010;&#22270;&#20687;&#20316;&#20026;&#35777;&#25454;&#12290;&#36890;&#36807;&#19977;&#20010;&#23376;&#20219;&#21153;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26816;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#19968;&#20010;&#20027;&#24352;&#21644;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#25512;&#25991;&#30340;&#22823;&#22411;&#32593;&#32476;&#36164;&#28304;&#38598;&#21512;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#24182;&#39044;&#27979;&#19968;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;&#25903;&#25345;&#12289;&#21453;&#39539;&#25110;&#20449;&#24687;&#19981;&#36275;&#65289;&#65292;&#20197;&#21450;&#29983;&#25104;&#19968;&#26465;&#38472;&#36848;&#26469;&#24635;&#32467;&#21644;&#35299;&#37322;&#25512;&#29702;&#21644;&#35009;&#20915;&#36807;&#31243;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Mocheg&#65292;&#19968;&#20010;&#21253;&#21547;15,601&#20010;&#20027;&#24352;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#20027;&#24352;&#37117;&#26631;&#26377;&#30495;&#23454;&#24615;&#26631;&#31614;&#21644;&#35009;&#20915;&#38472;&#36848;&#65292;&#20197;&#21450;&#24635;&#20849;33,880&#20010;&#25991;&#26412;&#27573;&#33853;&#21644;12,112&#20010;&#22270;&#20687;&#20316;&#20026;&#35777;&#25454;&#12290;&#20026;&#20102;&#22312;Mocheg&#19978;&#24314;&#31435;&#22522;&#32447;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#20998;&#27493;&#23376;&#20219;&#21153;&#19978;&#20351;&#29992;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#23454;&#39564;&#65306;&#22810;&#27169;&#24577;&#35777;&#25454;&#26816;&#32034;&#12289;&#20027;&#24352;&#39564;&#35777;&#21644;&#35299;&#37322;&#29983;&#25104;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;&#22810;&#27169;&#24577;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose end-to-end multimodal fact-checking and explanation generation, where the input is a claim and a large collection of web sources, including articles, images, videos, and tweets, and the goal is to assess the truthfulness of the claim by retrieving relevant evidence and predicting a truthfulness label (e.g., support, refute or not enough information), and to generate a statement to summarize and explain the reasoning and ruling process. To support this research, we construct Mocheg, a large-scale dataset consisting of 15,601 claims where each claim is annotated with a truthfulness label and a ruling statement, and 33,880 textual paragraphs and 12,112 images in total as evidence. To establish baseline performances on Mocheg, we experiment with several state-of-the-art neural architectures on the three pipelined subtasks: multimodal evidence retrieval, claim verification, and explanation generation, and demonstrate that the performance of the state-of-the-art end-to-end multimo
&lt;/p&gt;</description></item></channel></rss>