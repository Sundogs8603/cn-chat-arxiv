<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.18313</link><description>&lt;p&gt;
FP8-LM&#65306;&#35757;&#32451;FP8&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;FP8&#20302;&#27604;&#29305;&#25968;&#25454;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#22823;&#22810;&#25968;&#21464;&#37327;&#65288;&#22914;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#65289;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#26684;&#24335;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25913;&#21464;&#36229;&#21442;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;LLMs&#12290;&#35813;&#26694;&#26550;&#20026;LLM&#30340;&#28151;&#21512;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#19977;&#20010;&#32423;&#21035;&#30340;FP8&#21033;&#29992;&#12290;&#23427;&#36880;&#27493;&#24341;&#20837;8&#20301;&#26799;&#24230;&#65292;&#20248;&#21270;&#22120;&#29366;&#24577;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;H100 GPU&#24179;&#21488;&#19978;&#35757;&#32451;GPT-175B&#27169;&#22411;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;FP8&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26694;&#26550;&#19981;&#20165;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;42%&#30340;&#30495;&#23454;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#65292;&#32780;&#19988;&#27604;&#24191;&#27867;&#37319;&#29992;&#30340;BF16&#26694;&#26550;&#65288;&#21363;Megatron-LM&#65289;&#36816;&#34892;&#36895;&#24230;&#24555;64%&#65292;&#27604;Nvidia Transformer Engine&#24555;17%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13023</link><description>&lt;p&gt;
GraphGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36882;&#24402;&#20449;&#24687;&#20132;&#25442;&#21644;&#32858;&#21512;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#25552;&#21319;&#22270;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;LLM&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23558;LLM&#19982;&#22270;&#32467;&#26500;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05161</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#23481;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#20351;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LMs&#24182;&#19981;&#25551;&#36848;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#65292;&#32780;&#26159;&#23450;&#20041;&#20102;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RNN LMs&#21487;&#20197;&#34920;&#31034;&#21738;&#20123;&#31867;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#38472;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;RNN&#31561;&#20215;&#20110;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#21482;&#33021;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;RNNs&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;LMs&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#20026;&#20102;&#34920;&#31034;&#19968;&#20010;&#20219;&#24847;&#30830;&#23450;&#30340;&#26377;&#38480;&#29366;&#24577;LMs&#65292;&#20854;&#20013;&#26377;$N$&#20010;&#29366;&#24577;&#19988;&#23383;&#31526;&#38598;&#20026;$\Sigma$&#30340;RNN requir
&lt;/p&gt;
&lt;p&gt;
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
&lt;/p&gt;</description></item><item><title>GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16583</link><description>&lt;p&gt;
GPT-Fathom&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35299;&#26512;GPT-4&#21450;&#20854;&#21518;&#32493;&#29256;&#26412;&#30340;&#28436;&#21270;&#36335;&#24452;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16583
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#29616;&#26377;&#30340;LLM&#25490;&#34892;&#27036;&#36890;&#24120;&#21442;&#32771;&#20854;&#20182;&#35770;&#25991;&#20013;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#35774;&#32622;&#21644;&#25552;&#31034;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#38388;&#40723;&#21169;&#36873;&#25321;&#26377;&#21033;&#30340;&#35774;&#32622;&#21644;&#25552;&#31034;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GPT-Fathom&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Evals&#26500;&#24314;&#30340;&#24320;&#28304;&#21644;&#21487;&#37325;&#22797;&#30340;LLM&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;&#23545;&#40784;&#30340;&#29615;&#22659;&#35774;&#32622;&#19979;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;LLMs&#20197;&#21450;OpenAI&#30340;&#20256;&#32479;&#27169;&#22411;&#22312;20&#22810;&#20010;&#31934;&#36873;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#28085;&#30422;&#20102;7&#20010;&#33021;&#21147;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;OpenAI&#26089;&#26399;&#27169;&#22411;&#30340;&#22238;&#39038;&#24615;&#30740;&#31350;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#20174;GPT-3&#21040;GPT-4&#30340;&#28436;&#21270;&#36335;&#24452;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#28212;&#26395;&#20102;&#35299;GPT-3&#22914;&#20309;&#36880;&#27493;&#25913;&#36827;&#21040;GPT-4&#65292;&#21253;&#25324;&#20687;&#28155;&#21152;&#20195;&#30721;&#25968;&#25454;&#26159;&#21542;&#25552;&#39640;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;LLM&#33021;&#21147;&#30340;&#21738;&#20123;&#26041;&#38754;&#31561;&#25216;&#26415;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4&#36827;&#34892;&#38382;&#31572;&#30340;&#27861;&#24459;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26469;&#35206;&#30422;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#20027;&#35201;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;GPT-4&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.15016</link><description>&lt;p&gt;
&#29992;&#38382;&#31572;&#26041;&#27861;&#35780;&#20272;&#27861;&#24459;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Question-Answering Approach to Evaluate Legal Summaries. (arXiv:2309.15016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4&#36827;&#34892;&#38382;&#31572;&#30340;&#27861;&#24459;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26469;&#35206;&#30422;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#20027;&#35201;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;GPT-4&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35780;&#20272;&#25351;&#26631;&#22914;ROUGE&#20165;&#27604;&#36739;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#32780;&#19981;&#32771;&#34385;&#35770;&#28857;&#32467;&#26500;&#65292;&#32780;&#36825;&#23545;&#20110;&#27861;&#24459;&#25688;&#35201;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25688;&#35201;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#28085;&#30422;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#20027;&#35201;&#28857;&#21644;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#29983;&#25104;&#25688;&#35201;&#22238;&#31572;&#21442;&#32771;&#25688;&#35201;&#20013;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;GPT-4&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#29983;&#25104;&#25688;&#35201;&#30340;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;GPT-4&#35780;&#20998;&#19982;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;GPT-4&#30340;&#38382;&#31572;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional evaluation metrics like ROUGE compare lexical overlap between the reference and generated summaries without taking argumentative structure into account, which is important for legal summaries. In this paper, we propose a novel legal summarization evaluation framework that utilizes GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary. GPT-4 is then used to generate answers based on the generated summary for the questions from the reference summary. Finally, GPT-4 grades the answers from the reference summary and the generated summary. We examined the correlation between GPT-4 grading with human grading. The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary.
&lt;/p&gt;</description></item><item><title>LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12276</link><description>&lt;p&gt;
LLMR&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#31034;&#20132;&#20114;&#24335;&#19990;&#30028;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12276
&lt;/p&gt;
&lt;p&gt;
LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#29616;&#23454;&#22330;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMR)&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#12290;LLMR&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#35774;&#35745;&#30446;&#26631;&#38656;&#35201;&#21512;&#25104;&#20869;&#37096;&#21160;&#24577;&#12289;&#30452;&#35266;&#20998;&#26512;&#25110;&#39640;&#32423;&#20132;&#20114;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#25991;&#26412;&#20132;&#20114;&#21644;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#36890;&#36807;&#34701;&#21512;&#22330;&#26223;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#33258;&#25105;&#35843;&#35797;&#21644;&#20869;&#23384;&#31649;&#29702;&#25216;&#26415;&#65292;LLMR&#22312;&#24179;&#22343;&#38169;&#35823;&#29575;&#19978;&#27604;&#26631;&#20934;&#30340;GPT-4&#25552;&#39640;&#20102;4&#20493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#19982;&#20960;&#20010;&#31034;&#20363;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#21019;&#24314;&#21644;&#20462;&#25913;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23427;&#33021;&#22815;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26377;&#22810;&#26679;&#24615;&#30340;&#21487;&#29992;&#24615;&#30740;&#31350;&#65288;N=11&#65289;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#23545;&#35813;&#31995;&#32479;&#26377;&#31215;&#26497;&#30340;&#20307;&#39564;&#65292;&#24182;&#24895;&#24847;&#20877;&#27425;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.06453</link><description>&lt;p&gt;
&#32553;&#23567;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24046;&#36317;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65288;CSE&#65289;&#20316;&#20026;&#20027;&#27969;&#25216;&#26415;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;CSE&#20013;&#26377;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30456;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#36317;&#8221;&#21644;&#8220;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24443;&#24213;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;CSE&#22312;&#21508;&#33258;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#26469;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#8221;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#21482;&#35201;RPE&#30340;&#25351;&#25968;&#24207;&#21015;&#25910;&#25947;&#65292;Transformer&#23601;&#20855;&#26377;&#38271;&#24230;&#22806;&#25512;&#30340;&#33021;&#21147;&#12290;&#20174;&#20013;&#23548;&#20986;&#20102;&#20004;&#31181;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24863;&#21463;&#37326;(TRF)&#26469;&#27979;&#37327;RPE&#30340;&#24863;&#21463;&#37326;&#12290;</title><link>http://arxiv.org/abs/2307.10156</link><description>&lt;p&gt;
&#25506;&#32034;Transformer&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Exploring Transformer Extrapolation. (arXiv:2307.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#21482;&#35201;RPE&#30340;&#25351;&#25968;&#24207;&#21015;&#25910;&#25947;&#65292;Transformer&#23601;&#20855;&#26377;&#38271;&#24230;&#22806;&#25512;&#30340;&#33021;&#21147;&#12290;&#20174;&#20013;&#23548;&#20986;&#20102;&#20004;&#31181;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24863;&#21463;&#37326;(TRF)&#26469;&#27979;&#37327;RPE&#30340;&#24863;&#21463;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#22806;&#25512;&#36817;&#26399;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;transformers&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#24207;&#21015;&#38271;&#24230;&#20043;&#22806;&#36827;&#34892;&#27979;&#35797;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;(RPEs)&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#23646;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#38271;&#24230;&#22806;&#25512;&#30340;&#26465;&#20214;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24443;&#24213;&#30340;&#25968;&#23398;&#21644;&#23454;&#35777;&#20998;&#26512;&#30830;&#23450;&#21738;&#31181;&#31867;&#22411;&#30340;RPEs&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#22806;&#25512;&#12290;&#25105;&#20204;&#21457;&#29616;&#21482;&#35201;&#23545;&#24212;&#20110;RPE&#30340;&#25351;&#25968;&#25910;&#25947;&#30340;&#24207;&#21015;&#65292;transformer&#19968;&#23450;&#20855;&#26377;&#36825;&#20010;&#23646;&#24615;&#12290;&#20174;&#36825;&#20123;&#26465;&#20214;&#20013;&#23548;&#20986;&#20102;&#20004;&#31181;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#25991;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#20316;&#20026;&#26465;&#20214;&#34893;&#29983;&#30340;&#39069;&#22806;&#22909;&#22788;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24863;&#21463;&#37326;(TRF)&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#27979;&#37327;RPE&#30340;&#24863;&#21463;&#37326;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training. Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36719;&#20214;&#24320;&#21457;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#35813;&#33539;&#24335;&#20351;&#29992;ChatDev&#20316;&#20026;&#19968;&#20010;&#34394;&#25311;&#32842;&#22825;&#39537;&#21160;&#30340;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#36890;&#36807;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#22235;&#20010;&#38454;&#27573;&#30340;&#20195;&#29702;&#20154;&#22242;&#38431;&#20419;&#36827;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.07924</link><description>&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20132;&#27969;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Communicative Agents for Software Development. (arXiv:2307.07924v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36719;&#20214;&#24320;&#21457;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#35813;&#33539;&#24335;&#20351;&#29992;ChatDev&#20316;&#20026;&#19968;&#20010;&#34394;&#25311;&#32842;&#22825;&#39537;&#21160;&#30340;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#36890;&#36807;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#22235;&#20010;&#38454;&#27573;&#30340;&#20195;&#29702;&#20154;&#22242;&#38431;&#20419;&#36827;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#26159;&#19968;&#20010;&#20197;&#24494;&#22937;&#30340;&#30452;&#35273;&#21644;&#21672;&#35810;&#20026;&#29305;&#24449;&#30340;&#39046;&#22495;&#65292;&#20915;&#31574;&#36807;&#31243;&#22797;&#26434;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24320;&#22987;&#36890;&#36807;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#21508;&#20010;&#38454;&#27573;&#23454;&#26045;&#31934;&#24515;&#35774;&#35745;&#26469;&#38761;&#26032;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20851;&#38190;&#27969;&#31243;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;&#36825;&#20010;&#33539;&#24335;&#30340;&#26680;&#24515;&#26159;ChatDev&#65292;&#19968;&#20010;&#34394;&#25311;&#30340;&#32842;&#22825;&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#23427;&#27169;&#20223;&#20102;&#24050;&#32463;&#24314;&#31435;&#30340;&#28689;&#24067;&#27169;&#22411;&#65292;&#23558;&#24320;&#21457;&#36807;&#31243;&#32454;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#38454;&#27573;&#65306;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#28041;&#21450;&#19968;&#20010;&#22242;&#38431;&#30340;&#20195;&#29702;&#20154;&#65292;&#22914;&#31243;&#24207;&#21592;&#12289;&#20195;&#30721;&#23457;&#26597;&#20154;&#21592;&#21644;&#27979;&#35797;&#24037;&#31243;&#24072;&#65292;&#20419;&#36827;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PoetryDiffusion&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#35799;&#27468;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#35821;&#20041;&#21644;&#38901;&#24459;&#26041;&#38754;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08456</link><description>&lt;p&gt;
PoetryDiffusion: &#23454;&#29616;&#35799;&#27468;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#21644;&#38901;&#24459;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation. (arXiv:2306.08456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PoetryDiffusion&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#35799;&#27468;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#35821;&#20041;&#21644;&#38901;&#24459;&#26041;&#38754;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#24847;&#20041;&#37325;&#22823;&#30340;&#39046;&#22495;&#12290;&#23588;&#20854;&#26159;&#35799;&#27468;&#29983;&#25104;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#65292;&#23545;&#25991;&#26412;&#29983;&#25104;&#26377;&#30528;&#26126;&#30830;&#23450;&#20041;&#21644;&#20005;&#26684;&#30340;&#26465;&#20214;&#65292;&#26159;&#35780;&#20272;&#24403;&#21069;&#26041;&#27861;&#23398;&#30340;&#29702;&#24819;&#23454;&#39564;&#22330;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#25511;&#21046;&#20102;&#35799;&#27468;&#29983;&#25104;&#30340;&#35821;&#20041;&#25110;&#38901;&#24459;&#26041;&#38754;&#65292;&#20294;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#21313;&#22235;&#34892;&#35799;&#21644;&#20013;&#22269;&#23435;&#35789;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23601;&#35821;&#20041;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;PoetryDiffusion&#27169;&#22411;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23436;&#25972;&#30340;&#21477;&#23376;&#25110;&#35799;&#27468;&#65292;&#20840;&#38754;&#32771;&#34385;&#21477;&#23376;&#20449;&#24687;&#30340;&#25972;&#20307;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#35821;&#20041;&#34920;&#36798;&#65292;&#20351;&#20854;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25152;&#21306;&#21035;&#12290;&#23601;&#38901;&#24459;&#25511;&#21046;&#32780;&#35328;&#65292;&#25193;&#25955;&#29983;&#25104;&#21644;&#20854;&#32422;&#26463;&#25511;&#21046;&#27169;&#22359;&#30340;&#20998;&#31163;&#29305;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#28789;&#27963;&#22320;&#25511;&#21046;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a challenging and meaningful field in natural language generation (NLG). Especially, poetry generation is a typical one with well-defined and strict conditions for text generation which is an ideal playground for the assessment of current methodologies. While prior works succeeded in controlling either semantic or metrical aspects of poetry generation, simultaneously addressing both remains a challenge. In this paper, we pioneer the use of the Diffusion model for generating sonnets and Chinese SongCi poetry to tackle such challenges. In terms of semantics, our PoetryDiffusion model, built upon the Diffusion model, generates entire sentences or poetry by comprehensively considering the entirety of sentence information. This approach enhances semantic expression, distinguishing it from autoregressive and large language models (LLMs). For metrical control, the separation feature of diffusion generation and its constraint control module enable us to flexibly
&lt;/p&gt;</description></item><item><title>ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07490</link><description>&lt;p&gt;
ArtGPT-4: &#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#30340;&#33402;&#26415;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07490
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#27604;&#22914;ChatGPT&#21644;GPT-4&#31561;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#23545;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#25214;&#21040;&#19982;&#27169;&#22411;&#35268;&#27169;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20063;&#24456;&#22256;&#38590;&#12290;&#24494;&#35843;&#21644;&#20351;&#29992;&#26032;&#26041;&#27861;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;MiniGPT-4&#27169;&#22411;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#36816;&#29992;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38761;&#26032;&#24615;&#30340;&#22521;&#35757;&#31574;&#30053;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33402;&#26415;&#22270;&#29255;&#26041;&#38754;&#12290;ArtGPT-4&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#12290;ArtGPT-4&#20351;&#29992;Tesla A100&#35774;&#22791;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#29992;&#20102;&#32422;200GB&#30340;&#25968;&#25454;&#65292;&#22312;2&#23567;&#26102;&#20869;&#23601;&#33021;&#23637;&#31034;&#20986;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images wit
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22270;&#24418;&#26368;&#22823;&#21270;&#20989;&#25968;&#65292;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#23616;&#30693;&#35782;&#21644;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#30693;&#35782;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#20256;&#32479;&#30340;softmax&#20989;&#25968;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20849;&#29616;&#20449;&#24687;&#65292;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2101.00153</link><description>&lt;p&gt;
&#22270;&#24418;&#26368;&#22823;&#21270;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graphmax for Text Generation. (arXiv:2101.00153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22270;&#24418;&#26368;&#22823;&#21270;&#20989;&#25968;&#65292;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#23616;&#30693;&#35782;&#21644;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#30693;&#35782;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#20256;&#32479;&#30340;softmax&#20989;&#25968;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20849;&#29616;&#20449;&#24687;&#65292;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20165;&#22522;&#20110;&#19978;&#19979;&#25991;&#20013;&#20808;&#21069;&#36873;&#25321;&#30340;&#20869;&#23481;&#65292;&#20351;&#29992;softmax&#20989;&#25968;&#36873;&#25321;&#27599;&#20010;&#26032;&#35789;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#24182;&#21457;&#35789;&#30340;&#38142;&#25509;&#32479;&#35745;&#20449;&#24687;&#23545;&#36873;&#25321;&#19979;&#19968;&#20010;&#35789;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#21487;&#20197;&#24110;&#21161;&#30830;&#20445;&#29983;&#25104;&#25991;&#26412;&#30340;&#20027;&#39064;&#19982;&#24403;&#21069;&#20219;&#21153;&#30456;&#19968;&#33268;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20849;&#29616;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#24418;&#26368;&#22823;&#21270;&#20989;&#25968;&#12290;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27491;&#21017;&#21270;&#65292;&#22270;&#24418;&#26368;&#22823;&#21270;&#20351;&#26368;&#32456;&#35789;&#30340;&#36873;&#25321;&#30001;LM&#30340;&#20840;&#23616;&#30693;&#35782;&#21644;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#30693;&#35782;&#20849;&#21516;&#30830;&#23450;&#12290;&#20256;&#32479;&#30340;softmax&#20989;&#25968;&#36890;&#36807;&#22270;&#24635;&#21464;&#21270;&#65288;GTV&#65289;&#39033;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23558;&#23616;&#37096;&#30693;&#35782;&#34701;&#20837;&#21040;LM&#20013;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#22270;&#24418;&#26368;&#22823;&#21270;&#21151;&#33021;&#22810;&#26679;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information,we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plu
&lt;/p&gt;</description></item></channel></rss>