<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.18070</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30456;&#21516;&#30340;&#35748;&#30693;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#27169;&#22411;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20102;&#35299;LLMs&#33021;&#22815;&#27169;&#25311;&#21738;&#20123;&#35748;&#30693;&#29305;&#24615;&#20197;&#21450;&#21738;&#20123;&#19981;&#33021;&#27169;&#25311;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20799;&#31461;&#24050;&#30693;&#35748;&#30693;&#20559;&#35265;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35843;&#26597;&#23398;&#20064;&#31185;&#23398;&#25991;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26126;&#30830;&#30340;&#27493;&#39588;&#65306;&#25991;&#26412;&#29702;&#35299;&#12289;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#21644;&#35299;&#20915;&#26041;&#26696;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#20102;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22914;&#20309;&#24544;&#23454;&#22320;&#27169;&#25311;&#36825;&#20010;&#36807;&#31243;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20026;&#27599;&#20010;&#27979;&#35797;&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#21333;&#35789;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#20004;&#20010;&#35299;&#20915;&#36807;&#31243;&#30340;&#27493;&#39588;&#20013;&#65292;&#19981;&#35770;&#26159;&#21542;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#65292;&#37117;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#25688;&#35201;&#32423;&#21035;&#30340;&#26641;&#65292;&#20174;&#19979;&#24448;&#19978;&#25972;&#21512;&#24182;&#26816;&#32034;&#38271;&#24230;&#36739;&#38271;&#30340;&#25991;&#26723;&#65292;&#23545;&#20256;&#32479;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2401.18059</link><description>&lt;p&gt;
RAPTOR: &#36882;&#24402;&#25277;&#35937;&#22788;&#29702;&#29992;&#20110;&#26641;&#29366;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#25688;&#35201;&#32423;&#21035;&#30340;&#26641;&#65292;&#20174;&#19979;&#24448;&#19978;&#25972;&#21512;&#24182;&#26816;&#32034;&#38271;&#24230;&#36739;&#38271;&#30340;&#25991;&#26723;&#65292;&#23545;&#20256;&#32479;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19990;&#30028;&#29366;&#24577;&#30340;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#38271;&#23614;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#20174;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30701;&#36830;&#32493;&#22359;&#65292;&#38480;&#21046;&#20102;&#23545;&#25972;&#20307;&#25991;&#26723;&#19978;&#19979;&#25991;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36882;&#24402;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#25688;&#35201;&#25991;&#26412;&#22359;&#65292;&#20174;&#19979;&#24448;&#19978;&#26500;&#24314;&#20855;&#26377;&#19981;&#21516;&#25688;&#35201;&#32423;&#21035;&#30340;&#26641;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#25105;&#20204;&#30340;RAPTOR&#27169;&#22411;&#20174;&#36825;&#26869;&#26641;&#20013;&#26816;&#32034;&#65292;&#23558;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#30340;&#20449;&#24687;&#25972;&#21512;&#21040;&#38271;&#24230;&#36739;&#38271;&#30340;&#25991;&#26723;&#20013;&#12290;&#25511;&#21046;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36882;&#24402;&#25688;&#35201;&#30340;&#26816;&#32034;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#27604;&#20256;&#32479;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#22312;&#28041;&#21450;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#20363;&#22914;&#65292;&#36890;&#36807;&#23558;RAPTOR&#26816;&#32034;&#19982;GPT-4&#30340;&#20351;&#29992;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;QuALITY&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#24615;&#33021;&#25552;&#39640;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20%
&lt;/p&gt;</description></item><item><title>LongAlign&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#21644;&#20351;&#29992;&#25171;&#21253;&#12289;&#25490;&#24207;&#21644;&#25439;&#22833;&#21152;&#26435;&#31574;&#30053;&#65292;&#23427;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;30\%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.18058</link><description>&lt;p&gt;
LongAlign&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LongAlign: A Recipe for Long Context Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18058
&lt;/p&gt;
&lt;p&gt;
LongAlign&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#21644;&#20351;&#29992;&#25171;&#21253;&#12289;&#25490;&#24207;&#21644;&#25439;&#22833;&#21152;&#26435;&#31574;&#30053;&#65292;&#23427;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;30\%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#26377;&#25928;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#38656;&#35201;&#23545;&#30456;&#20284;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongAlign - &#19968;&#31181;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#25351;&#23548;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#26500;&#24314;&#38271;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#23427;&#28085;&#30422;&#20102;&#26469;&#33258;&#19981;&#21516;&#38271;&#19978;&#19979;&#25991;&#26469;&#28304;&#30340;&#24191;&#27867;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#25171;&#21253;&#21644;&#25490;&#24207;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#21152;&#36895;&#22312;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#30340;&#21463;&#30417;&#30563;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#25439;&#22833;&#21152;&#26435;&#26041;&#27861;&#65292;&#22312;&#25171;&#21253;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#25439;&#22833;&#23545;&#19981;&#21516;&#24207;&#21015;&#30340;&#36129;&#29486;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LongBench-Chat&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;10k-100k&#38271;&#24230;&#30340;&#26597;&#35810;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LongAlign&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#30340;LLMs&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#22810;&#36798;30\%&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#20102;&#20854;&#29087;&#32451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\%, while also maintaining their proficienc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22686;&#37327;&#29983;&#25104;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#30340;&#39044;&#27979;&#19982;&#20154;&#20204;&#36827;&#34892;&#21151;&#33021;&#31070;&#32463;&#25104;&#20687;&#30340;&#26102;&#38388;&#25968;&#25454;&#30456;&#20851;&#65292;&#21457;&#29616;&#20102;&#20154;&#31867;&#22312;&#36880;&#35789;&#29702;&#35299;&#21477;&#23376;&#26102;&#23384;&#22312;&#22810;&#36335;&#24452;&#35299;&#26512;&#30340;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2401.18046</link><description>&lt;p&gt;
&#22810;&#36335;&#24452;&#35299;&#26512;&#22312;&#22823;&#33041;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multipath parsing in the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22686;&#37327;&#29983;&#25104;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#30340;&#39044;&#27979;&#19982;&#20154;&#20204;&#36827;&#34892;&#21151;&#33021;&#31070;&#32463;&#25104;&#20687;&#30340;&#26102;&#38388;&#25968;&#25454;&#30456;&#20851;&#65292;&#21457;&#29616;&#20102;&#20154;&#31867;&#22312;&#36880;&#35789;&#29702;&#35299;&#21477;&#23376;&#26102;&#23384;&#22312;&#22810;&#36335;&#24452;&#35299;&#26512;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36880;&#35789;&#29702;&#35299;&#21477;&#23376;&#26102;&#65292;&#20197;&#25152;&#21548;&#21040;&#30340;&#39034;&#24207;&#36827;&#34892;&#12290;&#36825;&#31181;&#22686;&#37327;&#26041;&#24335;&#38656;&#35201;&#35299;&#20915;&#20020;&#26102;&#30340;&#35821;&#27861;&#20851;&#31995;&#27495;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22686;&#37327;&#29983;&#25104;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#30340;&#39044;&#27979;&#19982;&#22312;&#21548;&#25773;&#38899;&#20070;&#26102;&#36827;&#34892;&#21151;&#33021;&#31070;&#32463;&#25104;&#20687;&#30340;&#20154;&#20204;&#30340;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#30456;&#20851;&#65292;&#26469;&#30740;&#31350;&#20154;&#31867;&#26159;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#35821;&#27861;&#27495;&#20041;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20851;&#20110;&#36880;&#35789;&#29702;&#35299;&#36807;&#31243;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#35821;&#27861;&#20998;&#26512;&#25968;&#37327;&#30340;&#31454;&#20105;&#24615;&#20551;&#35774;&#65306;&#19968;&#20010;&#19982;&#22810;&#20010;&#12290;&#36825;&#20010;&#27604;&#36739;&#28041;&#21450;&#23558;&#26368;&#20808;&#36827;&#30340;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#20351;&#29992;&#32463;&#36807;LLM&#35843;&#25972;&#30340;&#32534;&#30721;&#26469;&#35780;&#20272;&#35821;&#27861;&#24778;&#35766;&#24230;&#65292;&#19982;&#29616;&#26377;&#30340;fMRI&#25968;&#25454;&#38598;&#30456;&#23545;&#29031;&#12290;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30340;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#36335;&#24452;&#35299;&#26512;&#30340;&#35777;&#25454;&#12290;&#19982;&#35813;&#22810;&#36335;&#24452;&#25928;&#24212;&#30456;&#20851;&#30340;&#33041;&#21306;&#21253;&#25324;&#21452;&#20391;&#39070;&#21494;&#19978;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.
&lt;/p&gt;</description></item><item><title>SpeechComposer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21512;&#25104;&#22266;&#23450;&#25552;&#31034;&#26631;&#35760;&#26469;&#32479;&#19968;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#39640;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>https://arxiv.org/abs/2401.18045</link><description>&lt;p&gt;
SpeechComposer: &#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#32479;&#19968;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18045
&lt;/p&gt;
&lt;p&gt;
SpeechComposer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21512;&#25104;&#22266;&#23450;&#25552;&#31034;&#26631;&#35760;&#26469;&#32479;&#19968;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#39640;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#22823;&#22823;&#25552;&#21319;&#20102;&#22810;&#31181;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#25552;&#31034;&#26631;&#35760;&#23558;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#32479;&#19968;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#24573;&#30053;&#20102;&#19981;&#21516;&#35821;&#38899;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#65292;&#36825;&#21487;&#33021;&#20250;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21482;&#35299;&#30721;&#22120;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;SpeechComposer&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#19968;&#32452;&#22266;&#23450;&#30340;&#25552;&#31034;&#26631;&#35760;&#32479;&#19968;&#24120;&#35265;&#30340;&#35821;&#38899;&#20219;&#21153;&#12290;SpeechComposer&#24314;&#31435;&#22312;&#22235;&#20010;&#20027;&#35201;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;--&#35821;&#38899;&#21512;&#25104;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#35821;&#35328;&#24314;&#27169;--&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26631;&#35760;&#30340;&#32452;&#21512;&#65292;&#22914;&#22768;&#38899;&#36716;&#25442;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#38899;&#20219;&#21153;&#12290;&#25552;&#31034;&#26631;&#35760;&#30340;&#32479;&#19968;&#20063;&#20351;&#24471;&#19981;&#21516;&#35821;&#38899;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#26356;&#21152;&#32467;&#26500;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18040</link><description>&lt;p&gt;
&#21152;&#24378;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#65306;&#22522;&#20110;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25913;&#36827;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#36890;&#36807;&#23545;&#35805;&#27969;&#27700;&#32447;&#30340;&#29420;&#31435;&#27169;&#22359;&#36827;&#34892;&#35774;&#35745;&#12290;&#20854;&#20013;&#65292;&#31574;&#30053;&#27169;&#22359;&#26159;&#20915;&#23450;&#23545;&#29992;&#25143;&#36755;&#20837;&#22914;&#20309;&#21709;&#24212;&#30340;&#20851;&#38190;&#12290;&#36825;&#20010;&#31574;&#30053;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#21453;&#39304;&#20449;&#21495;&#24418;&#24335;&#30340;&#29615;&#22659;&#20013;&#25509;&#25910;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#35805;&#31995;&#32479;&#21482;&#25552;&#20379;&#20102;&#31232;&#32570;&#19988;&#31616;&#21333;&#30340;&#22870;&#21169;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#24555;&#36895;&#21152;&#36895;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25945;&#25480;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#26469;&#25552;&#39640;&#21028;&#26029;&#20854;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#21644;&#22909;&#22855;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27979;&#37327;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#40723;&#21169;&#25506;&#32034;&#12290;&#22312;&#19968;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;MultiWOZ&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
&lt;/p&gt;</description></item><item><title>Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18034</link><description>&lt;p&gt;
Paramanu: &#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18034
&lt;/p&gt;
&lt;p&gt;
Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Gyan AI Paramanu&#65288;&#8220;&#21407;&#23376;&#8221;&#65289;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#22312;&#21333;&#20010;GPU&#19978;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#30340;&#21253;&#21547;&#21333;&#35821;&#12289;&#21452;&#35821;&#21644;&#22810;&#35821;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#65288;&#38463;&#33832;&#22982;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#24247;&#22350;&#23612;&#35821;&#12289;&#36808;&#33922;&#21033;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#26805;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#20197;&#21450;5&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#23383;&#27597;&#34920;&#65288;&#23391;&#21152;&#25289;&#35821;&#12289;&#22825;&#22478;&#20307;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;1024&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#22312;&#21333;&#20010;GPU&#19978;&#39044;&#35757;&#32451;&#65292;&#38750;&#24120;&#39640;&#25928;&#12289;&#23567;&#24039;&#12289;&#24555;&#36895;&#19988;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20808;&#36827;&#30340;&#21360;&#24230;&#35821;&#20998;&#35789;&#22120;&#65292;&#29978;&#33267;&#21487;&#20197;&#26631;&#35760;&#26410;&#30693;&#35821;&#35328;&#12290;&#20026;&#20102;&#36991;&#20813;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;mParamanu&#27169;&#22411;&#20013;&#30340;&#8220;&#22810;&#35821;&#35328;&#35781;&#21650;&#8221;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#23383;&#27597;&#34920;&#25353;&#35821;&#35328;&#31867;&#22411;&#36827;&#34892;&#20102;&#21487;&#27604;&#36739;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18028</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25903;&#25345;&#39044;&#26399;&#27835;&#29702;: &#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21457;&#23637;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#39044;&#27979;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20351;&#29992;LLMs&#22686;&#24378;&#21644;&#25351;&#23548;&#36825;&#19968;&#36807;&#31243;&#26159;&#19968;&#31181;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;LLMs&#21644;&#35780;&#20272;&#25351;&#26631;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#32771;&#34385;&#20559;&#24046;&#26041;&#38754;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;LLMs&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#24341;&#21457;&#20102;&#20851;&#20110;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#31867;&#21035;&#30340;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20016;&#23500;&#30340;&#21253;&#21547;&#23545;&#26032;&#20852;&#25216;&#26415;&#30340;&#35268;&#33539;&#24615;&#35780;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#8212;&#8212;&#26032;&#38395;&#23186;&#20307;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35745;&#31639;&#20998;&#26512;&#20840;&#29699;&#25968;&#30334;&#20010;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#21457;&#24067;&#30340;&#25968;&#21315;&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;LLMs&#27169;&#22411;&#65288;GPT-4&#31561;&#65289;&#21644;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#29983;&#25104;&#30340;&#24433;&#21709;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26399;&#26395;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#20851;&#27880;&#19981;&#36275;&#12289;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#20197;&#21450;&#22238;&#31572;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;15&#20010;QA&#31995;&#32479;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2401.18001</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#19978;&#19979;&#25991;&#20351;&#29992;&#30340;&#26399;&#26395;
&lt;/p&gt;
&lt;p&gt;
Desiderata for the Context Use of Question Answering Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26399;&#26395;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#20851;&#27880;&#19981;&#36275;&#12289;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#20197;&#21450;&#22238;&#31572;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;15&#20010;QA&#31995;&#32479;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;&#20808;&#36827;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#19968;&#31995;&#21015;&#20849;&#21516;&#38382;&#39064;&#65306;&#24403;&#19978;&#19979;&#25991;&#19982;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#20914;&#31361;&#26102;&#65292;&#32570;&#20047;&#23545;&#19978;&#19979;&#25991;&#30340;&#20851;&#27880;&#65292;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#24456;&#23567;&#65292;&#24182;&#19988;&#22238;&#31572;&#30340;&#19968;&#33268;&#24615;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#29420;&#35299;&#20915;&#20854;&#20013;&#19968;&#20004;&#20010;&#38382;&#39064;&#19978;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#30475;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#20808;&#27010;&#36848;QA&#27169;&#22411;&#30340;&#19968;&#31995;&#21015; - &#20808;&#21069;&#35752;&#35770;&#36807;&#30340;&#21644;&#26032;&#30340; - &#26399;&#26395;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#30456;&#20851;&#30340;&#20998;&#26512;&#21644;&#26041;&#27861;&#35770;&#25991;&#65292;&#25552;&#20379;&#39046;&#22495;&#29616;&#29366;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#31532;&#20108;&#37096;&#20998;&#23637;&#31034;&#20102;&#23454;&#39564;&#65292;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#21516;&#26102;&#25353;&#29031;&#25152;&#26377;&#26399;&#26395;&#35780;&#20272;&#20102;15&#20010;QA&#31995;&#32479;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26032;&#30340;&#36235;&#21183;&#65292;&#21253;&#25324;&#65288;1&#65289;&#23545;&#22122;&#22768;&#36739;&#19981;&#25935;&#24863;&#30340;&#31995;&#32479;&#22312;&#25552;&#20379;&#26080;&#20851;&#19978;&#19979;&#25991;&#26102;&#19981;&#19968;&#23450;&#26356;&#19968;&#33268;&#22320;&#22238;&#31572;&#38382;&#39064;&#65307;&#65288;2&#65289;&#22823;&#22810;&#25968;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
Prior work has uncovered a set of common problems in state-of-the-art context-based question answering (QA) systems: a lack of attention to the context when the latter conflicts with a model's parametric knowledge, little robustness to noise, and a lack of consistency with their answers. However, most prior work focus on one or two of those problems in isolation, which makes it difficult to see trends across them. We aim to close this gap, by first outlining a set of -- previously discussed as well as novel -- desiderata for QA models. We then survey relevant analysis and methods papers to provide an overview of the state of the field. The second part of our work presents experiments where we evaluate 15 QA systems on 5 datasets according to all desiderata at once. We find many novel trends, including (1) systems that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; (2) most systems that are more susceptible to noise ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32844;&#22330;&#39046;&#22495;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;&#65292;&#36890;&#36807;&#28040;&#27495;&#25216;&#33021;&#25552;&#21450;&#24182;&#19982;ESCO&#20998;&#31867;&#20307;&#31995;&#36827;&#34892;&#20851;&#32852;&#65292;&#23545;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#38656;&#27714;&#36827;&#34892;&#28145;&#20837;&#20102;&#35299;&#12290;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;EL&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;BLINK&#22312;&#20005;&#26684;&#35780;&#20272;&#20013;&#32988;&#36807;GENRE&#65292;&#20294;GENRE&#22312;&#23485;&#26494;&#35780;&#20272;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2401.17979</link><description>&lt;p&gt;
&#32844;&#22330;&#39046;&#22495;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Entity Linking in the Job Market Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32844;&#22330;&#39046;&#22495;&#20013;&#30340;&#23454;&#20307;&#38142;&#25509;&#65292;&#36890;&#36807;&#28040;&#27495;&#25216;&#33021;&#25552;&#21450;&#24182;&#19982;ESCO&#20998;&#31867;&#20307;&#31995;&#36827;&#34892;&#20851;&#32852;&#65292;&#23545;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#38656;&#27714;&#36827;&#34892;&#28145;&#20837;&#20102;&#35299;&#12290;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;EL&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;BLINK&#22312;&#20005;&#26684;&#35780;&#20272;&#20013;&#32988;&#36807;GENRE&#65292;&#20294;GENRE&#22312;&#23485;&#26494;&#35780;&#20272;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#20027;&#35201;&#22260;&#32469;&#32500;&#22522;&#30334;&#31185;&#23637;&#24320;&#65292;&#20294;&#22312;&#32844;&#22330;&#39046;&#22495;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#28040;&#27495;&#25216;&#33021;&#25552;&#21450;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#22312;&#36825;&#20010;&#39046;&#22495;&#25506;&#32034;EL&#65292;&#20855;&#20307;&#30446;&#26631;&#26159;&#23558;&#32844;&#19994;&#25216;&#33021;&#19982;ESCO&#20998;&#31867;&#20307;&#31995;&#65288;le Vrang&#31561;&#65292;2014&#65289;&#36827;&#34892;&#20851;&#32852;&#12290;&#20197;&#24448;&#30340;&#21162;&#21147;&#23558;&#31895;&#31890;&#24230;&#65288;&#20840;&#65289;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;ESCO&#25216;&#33021;&#36827;&#34892;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#26356;&#32454;&#31890;&#24230;&#30340;&#36328;&#24230;&#32423;&#21035;&#25216;&#33021;&#25552;&#21450;&#36827;&#34892;&#38142;&#25509;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#29983;&#25104;&#30340;&#25216;&#33021;&#25552;&#21450;-&#25216;&#33021;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#35843;&#25972;&#20102;&#20004;&#20010;&#24615;&#33021;&#24378;&#22823;&#30340;&#31070;&#32463;EL&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;Wu&#31561;&#65292;2020&#65289;&#65292;&#19968;&#20010;&#26159;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;Cao&#31561;&#65292;2021&#65289;&#65292;&#24182;&#22312;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#25216;&#33021;&#38142;&#25509;&#22522;&#20934;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#33021;&#22815;&#23558;&#38544;&#21547;&#30340;&#25216;&#33021;&#25552;&#21450;&#19982;&#27491;&#30830;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24212;&#36215;&#26469;&#12290;&#32463;&#39564;&#19978;&#65292;BLINK&#22312;&#20005;&#26684;&#35780;&#20272;&#20013;&#32988;&#36807;GENRE&#65292;&#20294;GENRE&#22312;&#23485;&#26494;&#35780;&#20272;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention--skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE p
&lt;/p&gt;</description></item><item><title>GUMsley&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;12&#31181;&#33521;&#25991;&#25991;&#26412;&#27969;&#27966;&#20013;&#25152;&#26377;&#21629;&#21517;&#21644;&#38750;&#21629;&#21517;&#26174;&#33879;&#23454;&#20307;&#30340;&#23454;&#20307;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#25429;&#25417;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#26174;&#33879;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2401.17974</link><description>&lt;p&gt;
GUMsley&#65306;&#35780;&#20272;&#33521;&#35821;12&#31181;&#27969;&#27966;&#20013;&#30340;&#25688;&#35201;&#20013;&#30340;&#23454;&#20307;&#26174;&#33879;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17974
&lt;/p&gt;
&lt;p&gt;
GUMsley&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;12&#31181;&#33521;&#25991;&#25991;&#26412;&#27969;&#27966;&#20013;&#25152;&#26377;&#21629;&#21517;&#21644;&#38750;&#21629;&#21517;&#26174;&#33879;&#23454;&#20307;&#30340;&#23454;&#20307;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#25429;&#25417;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#26174;&#33879;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#29702;&#35299;&#25991;&#26723;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#33021;&#22815;&#20197;&#36830;&#36143;&#30340;&#23454;&#20307;&#32780;&#19981;&#26159;&#23383;&#31526;&#20018;&#30340;&#24418;&#24335;&#65292;&#33719;&#21462;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#26174;&#33879;&#30340;&#23454;&#20307;&#19981;&#20165;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26368;&#32456;&#20219;&#21153;&#65292;&#32780;&#19988;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#21644;&#20854;&#20182;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#21487;&#25511;&#21046;&#30340;&#25688;&#35201;&#65292;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;GUMsley&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#33521;&#35821;&#25991;&#26412;12&#31181;&#27969;&#27966;&#30340;&#25152;&#26377;&#21629;&#21517;&#21644;&#38750;&#21629;&#21517;&#26174;&#33879;&#23454;&#20307;&#30340;&#23454;&#20307;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#65292;&#19982;&#23454;&#20307;&#31867;&#22411;&#12289;&#32500;&#22522;&#21270;&#38142;&#25509;&#21644;&#23436;&#25972;&#30340;&#26680;&#24515;&#24341;&#29992;&#35299;&#26512;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#25688;&#35201;&#25552;&#20513;&#20102;&#23545;&#26174;&#33879;&#24615;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#25688;&#35201;&#20013;&#26159;&#21542;&#25552;&#21040;&#28304;&#23454;&#20307;&#30340;&#26174;&#33879;&#24615;&#30340;&#39640;&#24230;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#25688;&#35201;&#27169;&#22411;&#21644;&#38646;-shot&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25688;&#35201;&#26102;&#25429;&#25417;&#21040;&#26174;&#33879;&#23454;&#20307;&#30340;&#34920;&#29616;&#24046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#25688;&#35201;&#26102;&#39044;&#27979;&#25110;&#25552;&#20379;&#26174;&#33879;&#23454;&#20307;&#23545;&#25552;&#39640;&#23454;&#20307;&#26174;&#33879;&#24615;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#23398;&#25351;&#20195;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#24102;&#26377;&#25351;&#20195;&#27880;&#37322;&#30340;&#21477;&#23376;&#65292;&#35299;&#20915;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#23567;&#35828;&#31995;&#32479;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#22810;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#25351;&#20195;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#26032;&#27169;&#22411;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2401.17922</link><description>&lt;p&gt;
&#29422;&#23376;&#65306;1&#21644;&#32769;&#34382;&#65306;2&#21644;&#29066;&#65306;3&#65292;&#21908;&#32654;&#21834;&#65281;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#23398;&#25351;&#20195;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#25991;&#23398;&#25351;&#20195;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#24102;&#26377;&#25351;&#20195;&#27880;&#37322;&#30340;&#21477;&#23376;&#65292;&#35299;&#20915;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#23567;&#35828;&#31995;&#32479;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#22810;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#25351;&#20195;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#26032;&#27169;&#22411;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#26631;&#27880;&#21644;&#35299;&#26512;&#26159;&#35745;&#31639;&#25991;&#23398;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#26500;&#24314;&#23567;&#35828;&#30340;&#39640;&#36136;&#37327;&#31995;&#32479;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25351;&#20195;&#26631;&#27880;&#38656;&#35201;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#65292;&#32780;&#25991;&#23398;&#25991;&#26412;&#28041;&#21450;&#24494;&#22937;&#30340;&#25512;&#26029;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;&#22522;&#20110;&#26032;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;seq2seq&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#24102;&#26377;&#31867;&#20284;markdown&#27880;&#37322;&#30340;&#36755;&#20837;&#21477;&#23376;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#12289;&#35780;&#20272;&#21644;&#21457;&#24067;&#20102;&#20960;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#25351;&#20195;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#26032;&#27169;&#22411;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference annotation and resolution is a vital component of computational literary studies. However, it has previously been difficult to build high quality systems for fiction. Coreference requires complicated structured outputs, and literary text involves subtle inferences and highly varied language. New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations. We create, evaluate, and release several trained models for coreference, as well as a workflow for training new models.
&lt;/p&gt;</description></item><item><title>LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17919</link><description>&lt;p&gt;
LOCOST: &#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LOCOST: State-Space Models for Long Document Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17919
&lt;/p&gt;
&lt;p&gt;
LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#32534;&#30721;&#38271;&#24207;&#21015;&#21644;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#30340;&#20302;&#22797;&#26434;&#24230;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCOST&#65306;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O&#65288;L log L&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#27700;&#24179;&#19978;&#36798;&#21040;&#20102;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26368;&#20248;&#31232;&#30095;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;93-96%&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;50%&#30340;&#20869;&#23384;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;87%&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;LOCOST&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20026;&#23436;&#25972;&#20070;&#25688;&#35201;&#21270;&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#20026;&#38271;&#36755;&#20837;&#22788;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#33021;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17911</link><description>&lt;p&gt;
SNNLP: &#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#33021;&#25928;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#33021;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#24320;&#22987;&#20851;&#27880;&#36825;&#31181;&#35745;&#31639;&#33539;&#24335;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20449;&#21495;&#22788;&#29702;&#20197;&#22806;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#65292;&#19968;&#20010;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#20027;&#35201;&#39046;&#22495;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#36164;&#28304;&#28040;&#32791;&#21644;&#32791;&#30005;&#37327;&#36739;&#39640;&#30340;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#36866;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#30340;NLP&#27169;&#22411;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#36739;&#20302;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#20449;&#24687;&#26102;&#20855;&#26377;&#26356;&#31867;&#20284;&#20110;&#20154;&#33041;&#30340;&#25805;&#20316;&#27169;&#24335;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#23558;NLP&#24341;&#20837;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#26368;&#22823;&#38382;&#39064;&#20043;&#19968;&#22312;&#20110;&#22914;&#20309;&#23558;&#25991;&#26412;&#27491;&#30830;&#32534;&#30721;&#20026;&#33033;&#20914;&#24207;&#21015;&#65292;&#20197;&#20415;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21487;&#20197;&#26080;&#32541;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#31181;&#26041;&#27861;&#22312;&#30456;&#20851;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#22788;&#29702;NLP&#20219;&#21153;&#65288;&#21363;&#24773;&#24863;&#20998;&#26512;&#65289;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As spiking neural networks receive more attention, we look toward applications of this computing paradigm in fields other than computer vision and signal processing. One major field, underexplored in the neuromorphic setting, is Natural Language Processing (NLP), where most state-of-the-art solutions still heavily rely on resource-consuming and power-hungry traditional deep learning architectures. Therefore, it is compelling to design NLP models for neuromorphic architectures due to their low energy requirements, with the additional benefit of a more human-brain-like operating model for processing information. However, one of the biggest issues with bringing NLP to the neuromorphic setting is in properly encoding text into a spike train so that it can be seamlessly handled by both current and future SNN architectures. In this paper, we compare various methods of encoding text as spikes and assess each method's performance in an associated SNN on a downstream NLP task, namely, sentiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#29305;&#24449;&#23558;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#20998;&#21106;&#25104;&#21333;&#35789;&#29366;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#29575;&#21644;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#35789;&#20856;&#12290;</title><link>https://arxiv.org/abs/2401.17902</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#37325;&#26032;&#24605;&#32771;&#35821;&#38899;&#20998;&#21106;&#21644;&#35789;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting speech segmentation and lexicon learning with better features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#29305;&#24449;&#23558;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#20998;&#21106;&#25104;&#21333;&#35789;&#29366;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#29575;&#21644;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#20998;&#21106;&#25104;&#31867;&#20284;&#21333;&#35789;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#20174;&#20004;&#38454;&#27573;&#30340;&#26102;&#38271;&#24809;&#32602;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24320;&#22987;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23398;&#20064;&#26174;&#24335;&#35789;&#20856;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38646;&#36164;&#28304;&#20998;&#21106;&#12290;&#22312;&#31532;&#19968;&#20010;&#22768;&#23398;&#21333;&#20803;&#21457;&#29616;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;HuBERT&#26367;&#25442;&#20102;&#23545;&#27604;&#24615;&#39044;&#27979;&#32534;&#30721;&#29305;&#24449;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#30340;&#21333;&#35789;&#20998;&#21106;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;HuBERT&#29305;&#24449;&#33719;&#24471;&#27599;&#20010;&#29255;&#27573;&#30340;&#22768;&#23398;&#21333;&#35789;&#23884;&#20837;&#12290;&#20351;&#29992;K-means&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#35789;&#20856;&#12290;&#32467;&#26524;&#26159;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#29575;&#20998;&#21106;&#21644;&#22312;ZeroSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit a self-supervised method that segments unlabelled speech into word-like segments. We start from the two-stage duration-penalised dynamic programming method that performs zero-resource segmentation without learning an explicit lexicon. In the first acoustic unit discovery stage, we replace contrastive predictive coding features with HuBERT. After word segmentation in the second stage, we get an acoustic word embedding for each segment by averaging HuBERT features. These embeddings are clustered using K-means to get a lexicon. The result is good full-coverage segmentation with a lexicon that achieves state-of-the-art performance on the ZeroSpeech benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#26631;&#31614;&#27169;&#22411;&#22312;ChatGPT&#22238;&#31572;&#20013;&#25552;&#39640;&#27861;&#24459;&#25991;&#26412;&#25512;&#29702;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ChatGPT&#30340;&#20020;&#26102;&#31572;&#26696;&#25972;&#21512;&#20026;&#32508;&#21512;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#29575;&#30340;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2401.17897</link><description>&lt;p&gt;
&#21033;&#29992;&#26631;&#31614;&#27169;&#22411;&#22312;ChatGPT&#22238;&#31572;&#20013;&#25552;&#39640;&#27861;&#24459;&#25991;&#26412;&#25512;&#29702;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#26631;&#31614;&#27169;&#22411;&#22312;ChatGPT&#22238;&#31572;&#20013;&#25552;&#39640;&#27861;&#24459;&#25991;&#26412;&#25512;&#29702;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ChatGPT&#30340;&#20020;&#26102;&#31572;&#26696;&#25972;&#21512;&#20026;&#32508;&#21512;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#29575;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#25991;&#26412;&#25512;&#29702;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#27861;&#24459;&#26597;&#35810;&#20013;&#30340;&#26029;&#35328;&#26159;&#21542;&#36923;&#36753;&#19978;&#31526;&#21512;&#25152;&#25552;&#20379;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#27861;&#24459;&#25991;&#31456;&#20013;&#25552;&#20379;&#30340;&#20449;&#24687;&#12290;ChatGPT&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#27861;&#24459;&#25991;&#26412;&#25512;&#29702;&#65306;&#24403;&#25105;&#20204;&#23558;&#28201;&#24230;&#35774;&#32622;&#20026;0&#65288;ChatGPT&#22238;&#31572;&#26159;&#30830;&#23450;&#24615;&#30340;&#65289;&#24182;&#25552;&#31034;&#27169;&#22411;&#26102;&#65292;&#23427;&#22312;COLIEE 2022&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;70.64&#65285;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;SOTA&#65288;67.89&#65285;&#65289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#28201;&#24230;&#22823;&#20110;&#38646;&#65292;ChatGPT&#30340;&#22238;&#31572;&#23601;&#19981;&#30830;&#23450;&#20102;&#65292;&#23548;&#33268;&#31572;&#26696;&#19981;&#19968;&#33268;&#21644;&#32467;&#26524;&#27874;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26631;&#31614;&#27169;&#22411;&#65288;&#24369;&#30417;&#30563;&#25216;&#26415;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65289;&#23558;ChatGPT&#30340;&#20020;&#26102;&#31572;&#26696;&#25972;&#21512;&#20026;&#32508;&#21512;&#26631;&#31614;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#20020;&#26102;&#31572;&#26696;&#35270;&#20026;&#21487;&#33021;&#24102;&#26377;&#22122;&#22768;&#30340;&#39044;&#27979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#31614;&#27169;&#22411;&#26469;&#25972;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#29575;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of legal text entailment is to ascertain whether the assertions in a legal query logically follow from the information provided in one or multiple legal articles. ChatGPT, a large language model, is robust in many natural language processing tasks, including legal text entailment: when we set the temperature = 0 (the ChatGPT answers are deterministic) and prompt the model, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms the previous SOTA of 67.89%. On the other hand, if the temperature is larger than zero, ChatGPT answers are not deterministic, leading to inconsistent answers and fluctuating results. We propose to leverage label models (a fundamental component of weak supervision techniques) to integrate the provisional answers by ChatGPT into consolidated labels. By that way, we treat ChatGPT provisional answers as noisy predictions which can be consolidated by label models. The experimental results demonstrate that this approach can attain an accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#23450;&#20041;&#20102;LLMs&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#33258;&#36523;&#20197;&#21450;&#23637;&#31034;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;AwareLLM&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#24847;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#23427;&#20204;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#33021;&#21147;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2401.17882</link><description>&lt;p&gt;
&#22240;&#27492;&#25105;&#24605;&#65292;&#25105;&#22312;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
I Think, Therefore I am: Awareness in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#23450;&#20041;&#20102;LLMs&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#33258;&#36523;&#20197;&#21450;&#23637;&#31034;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;AwareLLM&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#24847;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#23427;&#20204;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#33021;&#21147;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24847;&#35782;&#24418;&#24335;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23558;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;LLMs&#65292;&#35748;&#20026;&#24847;&#35782;&#26159;LLMs&#22686;&#24378;&#19982;&#20154;&#31867;&#20132;&#20114;&#24182;&#30830;&#20445;&#36947;&#24503;&#22238;&#24212;&#30340;&#21487;&#20449;&#24230;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;LLMs&#20013;&#30340;&#24847;&#35782;&#23450;&#20041;&#20026;&#24863;&#30693;&#21644;&#29702;&#35299;&#33258;&#36523;&#20316;&#20026;AI&#27169;&#22411;&#20197;&#21450;&#23637;&#31034;&#31038;&#20132;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24847;&#35782;&#30340;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#33021;&#21147;&#12289;&#20219;&#21153;&#12289;&#24773;&#24863;&#21644;&#35266;&#28857;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#22312;&#36825;&#20123;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65292;AwareLLM&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23637;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#33021;&#21147;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce the concept of awareness to LLMs, arguing that awareness is an essential aspect of trustworthiness for LLMs to enhance their interaction with humans while ensuring ethical responses. We define awareness in LLMs as the ability to perceive and understand themselves as AI models and to exhibit social intelligence. We identify four key dimensions of awareness: capability, mission, emotion, and perspective. To assess LLMs on these dimensions, we introduce a specialized dataset, AwareLLM dataset. Our findings reveal that LLMs demonstrate a decent degree of awareness, though they still lack substantial capability awareness.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#26088;&#22312;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#35821;&#35328;&#20132;&#27969;&#20013;&#30340;&#25163;&#21183;&#29702;&#35299;&#65292;&#36890;&#36807;&#27979;&#35797;LLMs&#23545;&#26126;&#31034;&#21644;&#38544;&#21547;&#38750;&#35821;&#35328;&#26263;&#31034;&#30340;&#33021;&#21147;&#20197;&#21450;&#19982;&#35821;&#22659;&#22240;&#32032;&#30340;&#20851;&#32852;&#12290;&#23454;&#39564;&#23558;&#20351;&#29992;&#24515;&#29702;&#35821;&#35328;&#23398;&#35774;&#35745;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#21516;&#26102;&#32771;&#34385;&#25991;&#21270;&#32500;&#24230;&#65292;&#24182;&#27979;&#37327;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17858</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#23545;&#22686;&#24378;&#20154;&#26426;&#20132;&#20114;&#30340;&#25163;&#21183;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#35821;&#35328;&#20132;&#27969;&#20013;&#30340;&#25163;&#21183;&#29702;&#35299;&#65292;&#36890;&#36807;&#27979;&#35797;LLMs&#23545;&#26126;&#31034;&#21644;&#38544;&#21547;&#38750;&#35821;&#35328;&#26263;&#31034;&#30340;&#33021;&#21147;&#20197;&#21450;&#19982;&#35821;&#22659;&#22240;&#32032;&#30340;&#20851;&#32852;&#12290;&#23454;&#39564;&#23558;&#20351;&#29992;&#24515;&#29702;&#35821;&#35328;&#23398;&#35774;&#35745;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#21516;&#26102;&#32771;&#34385;&#25991;&#21270;&#32500;&#24230;&#65292;&#24182;&#27979;&#37327;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#24050;&#32463;&#24433;&#21709;&#21040;&#36229;&#36234;&#32431;&#25991;&#26412;&#29983;&#25104;&#30340;&#21508;&#20010;&#23398;&#31185;&#12290;&#26412;&#39033;&#30446;&#25552;&#20986;&#36229;&#36234;&#32431;&#25991;&#26412;&#24615;&#36136;&#65292;&#25506;&#31350;LLM&#19982;&#38750;&#35821;&#35328;&#20132;&#27969;&#65292;&#29305;&#21035;&#20851;&#27880;&#25163;&#21183;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#35813;&#39033;&#30446;&#35745;&#21010;&#30740;&#31350;LLM&#22312;&#35299;&#26512;&#25991;&#23383;&#25552;&#31034;&#20013;&#30340;&#26126;&#31034;&#21644;&#38544;&#21547;&#38750;&#35821;&#35328;&#26263;&#31034;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#23427;&#20204;&#23558;&#36825;&#20123;&#25163;&#21183;&#19982;&#21508;&#31181;&#35821;&#22659;&#22240;&#32032;&#32852;&#31995;&#36215;&#26469;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#35745;&#21010;&#27979;&#35797;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#35774;&#35745;&#65292;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25991;&#23383;&#25552;&#31034;&#19982;&#35814;&#32454;&#30340;&#25163;&#21183;&#25551;&#36848;&#30456;&#37197;&#23545;&#65292;&#28085;&#30422;&#21508;&#31181;&#21306;&#22495;&#21464;&#24322;&#21644;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#23545;&#25163;&#21183;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#35745;&#21010;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#20197;&#37325;&#29616;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#12290;&#36825;&#20123;&#23454;&#39564;&#32771;&#34385;&#21040;&#25991;&#21270;&#32500;&#24230;&#65292;&#24182;&#27979;&#37327;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Large Language Models (LLMs) has affected various disciplines that got beyond mere text generation. Going beyond their textual nature, this project proposal aims to investigate the interaction between LLMs and non-verbal communication, specifically focusing on gestures. The proposal sets out a plan to examine the proficiency of LLMs in deciphering both explicit and implicit non-verbal cues within textual prompts and their ability to associate these gestures with various contextual factors. The research proposes to test established psycholinguistic study designs to construct a comprehensive dataset that pairs textual prompts with detailed gesture descriptions, encompassing diverse regional variations, and semantic labels. To assess LLMs' comprehension of gestures, experiments are planned, evaluating their ability to simulate human behaviour in order to replicate psycholinguistic experiments. These experiments consider cultural dimensions and measure the agreement between LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24179;&#34913;&#25968;&#25454;&#38598;"&#20840;&#29699;&#35828;&#35854;&#32773;"&#65292;&#32467;&#26524;&#26174;&#31034;&#36739;&#26032;&#30340;GPT&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#19968;&#20010;&#20840;&#29699;&#21335;&#26041;&#38472;&#36848;&#34987;&#20559;&#34962;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17839</link><description>&lt;p&gt;
&#20840;&#29699;&#35828;&#35854;&#32773;&#65306;LLMs&#22312;&#26102;&#38388;&#21644;&#22320;&#29702;&#21306;&#22495;&#19978;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global-Liar: Factuality of LLMs over Time and Geographic Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24179;&#34913;&#25968;&#25454;&#38598;"&#20840;&#29699;&#35828;&#35854;&#32773;"&#65292;&#32467;&#26524;&#26174;&#31034;&#36739;&#26032;&#30340;GPT&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#19968;&#20010;&#20840;&#29699;&#21335;&#26041;&#38472;&#36848;&#34987;&#20559;&#34962;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#20687;GPT&#31995;&#21015;&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20351;&#29992;&#65292;&#31361;&#26174;&#20102;&#23545;&#23427;&#20204;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#19978;&#34394;&#20551;&#20449;&#24687;&#21644;&#35823;&#23548;&#20449;&#24687;&#29462;&#29527;&#20256;&#25773;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;GPT&#27169;&#22411;&#65288;&#21253;&#25324;GPT-3.5&#21644;GPT-4&#65289;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#20559;&#35265;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20171;&#23548;&#20449;&#24687;&#20256;&#25773;&#30340;&#21487;&#38752;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#8220;&#20840;&#29699;&#35828;&#35854;&#32773;&#8221;&#65292;&#20854;&#22312;&#22320;&#29702;&#21644;&#26102;&#38388;&#34920;&#24449;&#26041;&#38754;&#26377;&#21161;&#20110;&#26356;&#32454;&#33268;&#22320;&#35780;&#20272;LLM&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#36739;&#26032;&#30340;GPT&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;3&#26376;&#21457;&#24067;&#30340;GPT-4&#29256;&#26412;&#26174;&#31034;&#20986;&#27604;&#20854;&#21518;&#32493;6&#26376;&#21457;&#24067;&#29256;&#26412;&#26356;&#39640;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#35266;&#23519;&#21040;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#20559;&#35265;&#65292;&#21363;&#23545;&#20840;&#29699;&#21335;&#26041;&#30340;&#38472;&#36848;&#32473;&#20104;&#20102;&#29305;&#26435;&#65292;&#21487;&#33021;&#21152;&#21095;&#20102;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online. Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination.   We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases. Our analysis reveals that newer iterations of GPT models do not always equate to improved performance. Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release. Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exace
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#25506;&#32034;&#20102;&#22235;&#31181;&#29983;&#25104;&#25913;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#33521;&#35821;&#25913;&#20889;&#21644;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#19981;&#23436;&#20840;&#36866;&#29992;&#20110;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#65292;&#24378;&#35843;&#20102;&#23545;&#20110;&#39640;&#24230;&#21512;&#35789;&#24615;&#35821;&#35328;&#26356;&#32454;&#33268;&#30340;&#25913;&#20889;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17827</link><description>&lt;p&gt;
&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#25913;&#20889;&#29983;&#25104;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation for Malayalam Paraphrase Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#25506;&#32034;&#20102;&#22235;&#31181;&#29983;&#25104;&#25913;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#33521;&#35821;&#25913;&#20889;&#21644;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#19981;&#23436;&#20840;&#36866;&#29992;&#20110;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#65292;&#24378;&#35843;&#20102;&#23545;&#20110;&#39640;&#24230;&#21512;&#35789;&#24615;&#35821;&#35328;&#26356;&#32454;&#33268;&#30340;&#25913;&#20889;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22235;&#31181;&#22312;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#20013;&#29983;&#25104;&#25913;&#20889;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33521;&#35821;&#25913;&#20889;&#21644;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#65288;&#22914;BLEU&#65292;METEOR&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#20197;&#21450;&#20154;&#24037;&#26631;&#27880;&#26469;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#23545;&#20110;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#21487;&#33021;&#19981;&#23436;&#20840;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#24046;&#24322;&#31361;&#26174;&#20102;&#23545;&#20110;&#39640;&#24230;&#21512;&#35789;&#24615;&#35821;&#35328;&#23588;&#20854;&#38656;&#35201;&#26356;&#32454;&#33268;&#30340;&#25913;&#20889;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores four methods of generating paraphrases in Malayalam, utilizing resources available for English paraphrasing and pre-trained Neural Machine Translation (NMT) models. We evaluate the resulting paraphrases using both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as human annotation. Our findings suggest that automated evaluation measures may not be fully appropriate for Malayalam, as they do not consistently align with human judgment. This discrepancy underscores the need for more nuanced paraphrase evaluation approaches especially for highly agglutinative languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22788;&#29702;&#31185;&#23398;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#24615;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2401.17824</link><description>&lt;p&gt;
&#23545;&#22788;&#29702;&#31185;&#23398;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Survey of Pre-trained Language Models for Processing Scientific Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22788;&#29702;&#31185;&#23398;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#24615;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#31185;&#23398;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#37327;&#27491;&#22312;&#22686;&#38271;&#12290;&#36319;&#19978;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#65288;SciLMs&#65289;&#39640;&#36895;&#22686;&#38271;&#30340;&#27493;&#20240;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#30340;&#19968;&#39033;&#33392;&#24040;&#20219;&#21153;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#20851;&#20110;SciLMs&#30340;&#24037;&#20316;&#65292;&#36825;&#20010;&#38382;&#39064;&#19968;&#30452;&#27809;&#26377;&#35299;&#20915;&#12290;&#37492;&#20110;&#25345;&#32493;&#28044;&#29616;&#30340;&#26032;SciLMs&#65292;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30456;&#20114;&#20043;&#38388;&#30340;&#27604;&#36739;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;SciLMs&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#21253;&#25324;&#23545;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#24615;&#30340;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#23545;&#26410;&#26469;&#21487;&#33021;&#38754;&#20020;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of Language Models (LMs) dedicated to processing scientific text is on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs) has become a daunting task for researchers. To date, no comprehensive surveys on SciLMs have been undertaken, leaving this issue unaddressed. Given the constant stream of new SciLMs, appraising the state-of-the-art and how they compare to each other remain largely unknown. This work fills that gap and provides a comprehensive review of SciLMs, including an extensive analysis of their effectiveness across different domains, tasks and datasets, and a discussion on the challenges that lie ahead.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>CauESC&#26159;&#19968;&#31181;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#22256;&#25200;&#30340;&#24773;&#24863;&#21407;&#22240;&#20197;&#21450;&#36825;&#20123;&#21407;&#22240;&#35302;&#21457;&#30340;&#24773;&#24863;&#25928;&#26524;&#65292;&#29420;&#31435;&#22320;&#29702;&#35299;&#27599;&#31181;&#35821;&#35328;&#20462;&#39280;&#31574;&#30053;&#24182;&#24039;&#22937;&#22320;&#25972;&#21512;&#23427;&#20204;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#27714;&#21161;&#32773;&#30340;&#24773;&#24863;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2401.17755</link><description>&lt;p&gt;
CauESC: &#19968;&#31181;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CauESC: A Causal Aware Model for Emotional Support Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17755
&lt;/p&gt;
&lt;p&gt;
CauESC&#26159;&#19968;&#31181;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#22256;&#25200;&#30340;&#24773;&#24863;&#21407;&#22240;&#20197;&#21450;&#36825;&#20123;&#21407;&#22240;&#35302;&#21457;&#30340;&#24773;&#24863;&#25928;&#26524;&#65292;&#29420;&#31435;&#22320;&#29702;&#35299;&#27599;&#31181;&#35821;&#35328;&#20462;&#39280;&#31574;&#30053;&#24182;&#24039;&#22937;&#22320;&#25972;&#21512;&#23427;&#20204;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#27714;&#21161;&#32773;&#30340;&#24773;&#24863;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#26088;&#22312;&#36890;&#36807;&#25903;&#25345;&#24615;&#30340;&#22238;&#24212;&#20943;&#36731;&#27714;&#21161;&#32773;&#30340;&#24773;&#24863;&#22256;&#25200;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(1) &#23427;&#20204;&#24573;&#30053;&#20102;&#22256;&#25200;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#32780;&#36825;&#23545;&#20110;&#32454;&#33268;&#30340;&#24773;&#24863;&#29702;&#35299;&#26159;&#37325;&#35201;&#30340;&#65307;(2) &#23427;&#20204;&#20851;&#27880;&#30340;&#26159;&#27714;&#21161;&#32773;&#33258;&#36523;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#32780;&#24573;&#35270;&#20102;&#35828;&#35805;&#32773;&#20043;&#38388;&#20114;&#21160;&#36807;&#31243;&#20013;&#30340;&#24773;&#24863;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;CauESC&#65292;&#23427;&#39318;&#20808;&#35782;&#21035;&#22256;&#25200;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#20197;&#21450;&#36825;&#20123;&#21407;&#22240;&#35302;&#21457;&#30340;&#24773;&#24863;&#25928;&#26524;&#65292;&#28982;&#21518;&#29420;&#31435;&#22320;&#29702;&#35299;&#27599;&#31181;&#35821;&#35328;&#20462;&#39280;&#31574;&#30053;&#65292;&#24182;&#24039;&#22937;&#22320;&#23558;&#23427;&#20204;&#25972;&#21512;&#36215;&#26469;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;&#21407;&#22240;&#21040;&#25928;&#26524;&#30340;&#24773;&#24863;&#29702;&#35299;&#21644;&#29420;&#31435;-&#25972;&#21512;&#31574;&#30053;&#24314;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Support Conversation aims at reducing the seeker's emotional distress through supportive response. Existing approaches have two limitations: (1) They ignore the emotion causes of the distress, which is important for fine-grained emotion understanding; (2) They focus on the seeker's own mental state rather than the emotional dynamics during interaction between speakers. To address these issues, we propose a novel framework CauESC, which firstly recognizes the emotion causes of the distress, as well as the emotion effects triggered by the causes, and then understands each strategy of verbal grooming independently and integrates them skillfully. Experimental results on the benchmark dataset demonstrate the effectiveness of our approach and show the benefits of emotion understanding from cause to effect and independent-integrated strategy modeling.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#35299;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECC&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#24773;&#24863;&#21644;&#21407;&#22240;&#30340;&#20174;&#21477;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECC&#22312;ECPE&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17716</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#35299;&#25512;&#29702;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#24773;&#24863;&#21407;&#22240;&#23545;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17716
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#35299;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECC&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#24773;&#24863;&#21644;&#21407;&#22240;&#30340;&#20174;&#21477;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECC&#22312;ECPE&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#21407;&#22240;&#23545;&#25552;&#21462;&#65288;ECPE&#65289;&#28041;&#21450;&#22312;&#25991;&#26723;&#20013;&#25552;&#21462;&#20195;&#34920;&#24773;&#24863;&#21644;&#21407;&#22240;&#30340;&#20174;&#21477;&#23545;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#36807;&#24230;&#25311;&#21512;&#23384;&#22312;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#20598;&#28982;&#30456;&#20851;&#24615;&#65292;&#22914;&#20301;&#32622;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#25429;&#25417;&#35821;&#20041;&#29305;&#24449;&#12290;&#21463;&#26368;&#36817;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19981;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;ECPE&#20219;&#21153;&#12290;&#23613;&#31649;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;LLM&#23384;&#22312;&#26080;&#27861;&#25511;&#21046;&#30340;&#36755;&#20986;&#38382;&#39064;&#65292;&#23548;&#33268;&#24615;&#33021;&#20013;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#26469;&#27169;&#20223;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#35299;&#24773;&#24863;-&#21407;&#22240;&#38142;&#65288;DECC&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#34701;&#21512;&#24341;&#23548;&#25512;&#29702;&#21644;&#36923;&#36753;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;DECC&#24341;&#23548;LLM&#22788;&#29702;ECPE&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#22686;&#24378;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECC&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#26356;&#24378;&#22823;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#26377;&#25928;&#24615;&#21644;&#26694;&#26550;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs representing emotions and their causes in a document. Existing methods tend to overfit spurious correlations, such as positional bias in existing benchmark datasets, rather than capturing semantic features. Inspired by recent work, we explore leveraging large language model (LLM) to address ECPE task without additional training. Despite strong capabilities, LLMs suffer from uncontrollable outputs, resulting in mediocre performance. To address this, we introduce chain-of-thought to mimic human cognitive process and propose the Decomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference and logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance the framework by incorporating in-context learning. Experiment results demonstrate the strength of DECC compared to state-of-the-art supervised fine-tuning methods. Finally, we analyze the effectiveness of each component and the robustness of
&lt;/p&gt;</description></item><item><title>WSC+&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;-&#19987;&#23478;&#26641;&#65292;&#22686;&#24378;&#20102;Winograd Schema&#25361;&#25112;&#20013;&#38382;&#39064;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25193;&#23637;&#26694;&#26550;&#65292;WSC+&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#33258;&#24049;&#29983;&#25104;&#30340;&#38382;&#39064;&#26102;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22312;WSC+&#19978;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#20934;&#30830;&#29575;&#20026;68.7%&#12290;</title><link>https://arxiv.org/abs/2401.17703</link><description>&lt;p&gt;
WSC+: &#20351;&#29992;&#19987;&#23478;&#26641;&#22686;&#24378;Winograd Schema&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17703
&lt;/p&gt;
&lt;p&gt;
WSC+&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;-&#19987;&#23478;&#26641;&#65292;&#22686;&#24378;&#20102;Winograd Schema&#25361;&#25112;&#20013;&#38382;&#39064;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25193;&#23637;&#26694;&#26550;&#65292;WSC+&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#33258;&#24049;&#29983;&#25104;&#30340;&#38382;&#39064;&#26102;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22312;WSC+&#19978;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#20934;&#30830;&#29575;&#20026;68.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Winograd Schema Challenge (WSC)&#26159;&#35780;&#20272;&#26426;&#22120;&#29702;&#35299;&#33021;&#21147;&#30340;&#37325;&#35201;&#22522;&#20934;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;WSC&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#29983;&#25104;&#36825;&#31867;&#38382;&#39064;&#30340;&#33021;&#21147;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;-&#19987;&#23478;&#26641;&#65288;Tree-of-Experts&#65292;ToE&#65289;&#65292;&#23427;&#22686;&#24378;&#20102;WSC&#23454;&#20363;&#30340;&#29983;&#25104;&#33021;&#21147;&#65288;50%&#26377;&#25928;&#26696;&#20363;&#65292;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#30340;10%&#65289;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WSC+&#65292;&#19968;&#20010;&#21253;&#21547;3026&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21477;&#23376;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;WSC&#26694;&#26550;&#65292;&#21152;&#20837;&#20102;&#26032;&#30340;&#8220;&#27169;&#26865;&#20004;&#21487;&#8221;&#65288;ambiguous&#65289;&#21644;&#8220;&#20882;&#29359;&#24615;&#8221;&#65288;offensive&#65289;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#29983;&#25104;-&#35780;&#20272;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#34920;&#26126;&#22312;&#35780;&#20272;&#33258;&#24049;&#29983;&#25104;&#30340;&#38382;&#39064;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#25152;&#21019;&#24314;&#30340;&#38382;&#39064;&#12290;&#22312;WSC+&#19978;&#65292;GPT-4&#65292;&#21363;&#34920;&#29616;&#26368;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#20026;68.7%&#65292;&#26126;&#26174;&#20302;&#20110;&#20154;&#31867;&#20934;&#30830;&#29575;95.1%&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21066;&#24369;&#21407;&#22987;&#25552;&#31034;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#22806;&#25512;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#21463;&#21040;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.17692</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#22806;&#25512;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Problem of Strong Priors in LMs with Context Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21066;&#24369;&#21407;&#22987;&#25552;&#31034;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#22806;&#25512;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#21463;&#21040;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#21019;&#24314;&#25351;&#20196;&#36319;&#38543;&#21161;&#25163;&#12290;&#20294;&#26159;&#23613;&#31649;&#23427;&#20204;&#26377;&#20248;&#21183;&#65292;LMs&#36824;&#26377;&#19968;&#20123;&#29305;&#27530;&#30340;&#23616;&#38480;&#24615;&#65292;&#27604;&#22914;&#8220;&#24378;&#20808;&#39564;&#8221;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#20250;&#22312;&#23545;&#26576;&#20123;&#23616;&#37096;&#36755;&#20837;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#36755;&#20986;&#20856;&#22411;&#30340;&#24310;&#32493;&#65292;&#32780;&#19981;&#32771;&#34385;&#20043;&#21069;&#30340;&#25351;&#20196;&#12290;&#20363;&#22914;&#65292;prompt&#27880;&#20837;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#27169;&#22411;&#24573;&#30053;&#26174;&#24335;&#25351;&#20196;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#27169;&#22411;&#34987;&#35777;&#26126;&#27604;&#31867;&#20284;&#30340;&#36739;&#23567;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#8220;&#21453;&#21521;&#32553;&#25918;&#8221;&#29616;&#35937;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32531;&#35299;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65306;&#25105;&#20204;&#37319;&#29992;&#21407;&#22987;&#25351;&#20196;&#38598;&#65292;&#29983;&#25104;&#21407;&#22987;&#25552;&#31034;&#30340;&#21066;&#24369;&#29256;&#26412;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#24310;&#32493;&#22806;&#25512;&#36828;&#31163;&#21066;&#24369;&#30340;&#25552;&#31034;&#12290;&#36825;&#35753;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#27169;&#22411;&#22914;&#20309;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#29702;&#35299;&#24182;&#20135;&#29983;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#39564;&#35777;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#21644;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#25552;&#21319;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17686</link><description>&lt;p&gt;
&#25512;&#29702;&#26463;&#25628;&#32034;&#65306;&#20026;&#38142;&#24335;&#24605;&#32500;&#25512;&#26029;&#23547;&#25214;&#21487;&#25512;&#23548;&#30340;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#39564;&#35777;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#21644;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#25552;&#21319;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#20013;&#38388;&#27493;&#39588;&#30340;&#25512;&#29702;&#38169;&#35823;&#38382;&#39064;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#19968;&#20010;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#21450;&#20854;&#21069;&#25552;&#30340;&#21487;&#25512;&#23548;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#65292;&#26469;&#22686;&#24378;&#25105;&#20204;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#35268;&#27169;&#30340;LLMs&#65288;7B&#12289;13B&#12289;70B&#21644;ChatGPT&#65289;&#30340;&#22522;&#30784;&#24615;&#33021;&#65292;&#22312;3&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#22330;&#26223;&#65288;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#65289;&#30340;8&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
&lt;/p&gt;</description></item><item><title>&#22312;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#27169;&#22411;&#19981;&#20165;&#22312;&#39044;&#27979;&#31070;&#32463;&#21709;&#24212;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31867;&#33041;&#24615;&#33021;&#65292;&#32780;&#19988;&#20854;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#36335;&#24452;&#19982;&#22823;&#33041;&#30340;&#26144;&#23556;&#26356;&#25509;&#36817;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#27425;&#26469;&#36827;&#34892;&#30456;&#21516;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2401.17671</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#33041;&#20013;&#65292;&#19978;&#19979;&#25991;&#29305;&#24449;&#25552;&#21462;&#23618;&#27425;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17671
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#27169;&#22411;&#19981;&#20165;&#22312;&#39044;&#27979;&#31070;&#32463;&#21709;&#24212;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31867;&#33041;&#24615;&#33021;&#65292;&#32780;&#19988;&#20854;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#36335;&#24452;&#19982;&#22823;&#33041;&#30340;&#26144;&#23556;&#26356;&#25509;&#36817;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#27425;&#26469;&#36827;&#34892;&#30456;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#31070;&#32463;&#22788;&#29702;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;LLM&#34920;&#31034;&#21644;&#22823;&#33041;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#22312;&#28436;&#21270;&#30340;LLM&#32972;&#26223;&#19979;&#24341;&#21457;&#36825;&#31181;&#25910;&#25947;&#30340;&#28508;&#22312;&#35745;&#31639;&#21407;&#29702;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#24615;&#33021;&#36739;&#39640;&#30340;LLM&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#30456;&#20284;&#65292;&#20197;&#25506;&#31350;&#23548;&#33268;&#20854;&#19982;&#22823;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#19968;&#33268;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;LLM&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#26102;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#39044;&#27979;LLM&#23884;&#20837;&#30340;&#31070;&#32463;&#21709;&#24212;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31867;&#33041;&#24615;&#33021;&#65292;&#32780;&#19988;&#23427;&#20204;&#30340;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#36335;&#24452;&#19982;&#22823;&#33041;&#30340;&#26144;&#23556;&#26356;&#25509;&#36817;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#27425;&#26469;&#36827;&#34892;&#30456;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in artificial intelligence have sparked interest in the parallels between large language models (LLMs) and human neural processing, particularly in language comprehension. While prior research has established similarities in the representation of LLMs and the brain, the underlying computational principles that cause this convergence, especially in the context of evolving LLMs, remain elusive. Here, we examined a diverse selection of high-performance LLMs with similar parameter sizes to investigate the factors contributing to their alignment with the brain's language processing mechanisms. We find that as LLMs achieve higher performance on benchmark tasks, they not only become more brain-like as measured by higher performance when predicting neural responses from LLM embeddings, but also their hierarchical feature extraction pathways map more closely onto the brain's while using fewer layers to do the same encoding. We also compare the feature extraction pathways of 
&lt;/p&gt;</description></item><item><title>&#38271;&#25991;&#26723;Transformer&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#20102;&#23545;&#25991;&#26723;&#32467;&#26500;&#30340;&#38544;&#24615;&#29702;&#35299;&#65292;&#24182;&#19988;&#32467;&#26500;&#27880;&#20837;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#29702;&#35299;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17658</link><description>&lt;p&gt;
&#38271;&#25991;&#26723;&#36716;&#25442;&#22120;&#20013;&#30340;&#25991;&#26723;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Document Structure in Long Document Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17658
&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26723;Transformer&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#20102;&#23545;&#25991;&#26723;&#32467;&#26500;&#30340;&#38544;&#24615;&#29702;&#35299;&#65292;&#24182;&#19988;&#32467;&#26500;&#27880;&#20837;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#29702;&#35299;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26723;&#36890;&#24120;&#21576;&#29616;&#20986;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#30340;&#23618;&#27425;&#21270;&#32452;&#32455;&#20803;&#32032;&#65292;&#20363;&#22914;&#31456;&#33410;&#26631;&#39064;&#21644;&#27573;&#33853;&#12290;&#23613;&#31649;&#25991;&#26723;&#32467;&#26500;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20316;&#29992;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#38271;&#25991;&#26723;Transformer&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26159;&#21542;&#20250;&#25484;&#25569;&#25991;&#26723;&#32467;&#26500;&#30340;&#20869;&#37096;&#34920;&#31034;&#65311;&#22312;&#39044;&#35757;&#32451;&#21518;&#65292;&#22914;&#20309;&#21521;&#27169;&#22411;&#20256;&#36798;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#19988;&#23427;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#26032;&#30340;&#25506;&#27979;&#20219;&#21153;&#26469;&#35780;&#20272;&#38271;&#25991;&#26723;Transformer&#30340;&#32467;&#26500;&#24863;&#30693;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#32467;&#26500;&#27880;&#20837;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#32467;&#26500;&#27880;&#20837;&#23545;&#20110;QASPER&#21644;Evidence Inference&#36825;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#25991;&#26723;NLP&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;LED&#21644;LongT5&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#20102;&#23545;&#25991;&#26723;&#32467;&#26500;&#30340;&#38544;&#24615;&#29702;&#35299;&#65292;&#32780;&#32467;&#26500;&#27880;&#20837;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#29702;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#24230;&#26432;&#20260;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20013;&#23384;&#22312;&#30340;&#25463;&#24452;&#21644;&#23545;&#26377;&#23475;&#35789;&#35821;&#30340;&#36807;&#24230;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#35299;&#30721;&#65288;Self-CD&#65289;&#31574;&#30053;&#26469;&#32531;&#35299;&#36807;&#24230;&#26432;&#20260;&#29616;&#35937;&#65292;&#35813;&#31574;&#30053;&#26080;&#38656;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17633</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#36807;&#24230;&#26432;&#20260;&#38382;&#39064;&#30340;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating the OverKill in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#24230;&#26432;&#20260;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20013;&#23384;&#22312;&#30340;&#25463;&#24452;&#21644;&#23545;&#26377;&#23475;&#35789;&#35821;&#30340;&#36807;&#24230;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#35299;&#30721;&#65288;Self-CD&#65289;&#31574;&#30053;&#26469;&#32531;&#35299;&#36807;&#24230;&#26432;&#20260;&#29616;&#35937;&#65292;&#35813;&#31574;&#30053;&#26080;&#38656;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#31934;&#24515;&#35843;&#25972;&#65292;&#20197;&#26082;&#26377;&#21161;&#30410;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#23384;&#22312;&#28508;&#22312;&#30340;&#36807;&#24230;&#26432;&#20260;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#21487;&#33021;&#20250;&#25298;&#32477;&#22238;&#31572;&#26080;&#23475;&#26597;&#35810;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#21644;&#30830;&#23450;&#26597;&#35810;&#30340;&#23433;&#20840;&#24615;&#65292;&#26469;&#30740;&#31350;&#36807;&#24230;&#26432;&#20260;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#23384;&#22312;&#25463;&#24452;&#65292;&#23548;&#33268;&#23545;&#8220;&#26432;&#20260;&#8221;&#31561;&#26377;&#23475;&#35789;&#35821;&#36807;&#24230;&#20851;&#27880;&#65292;&#32780;&#24378;&#35843;&#23433;&#20840;&#24615;&#30340;&#25552;&#31034;&#23558;&#21152;&#21095;&#36807;&#24230;&#26432;&#20260;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21517;&#20026;&#33258;&#23545;&#27604;&#35299;&#30721;&#65288;Self-Contrastive Decoding&#65292;Self-CD&#65289;&#65292;&#26469;&#32531;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25918;&#22823;&#27169;&#22411;&#22312;&#22238;&#24212;&#31995;&#32479;&#25552;&#31034;&#26102;&#36755;&#20986;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#25552;&#21462;&#36825;&#31181;&#36807;&#24230;&#20851;&#27880;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#20943;&#24369;&#27169;&#22411;&#23545;&#36825;&#31181;&#36807;&#24230;&#20851;&#27880;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#23450;&#26368;&#32456;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;SUPERB&#35780;&#20272;&#25506;&#27979;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25429;&#25417;&#35821;&#38899;&#29305;&#24615;&#30340;&#33021;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#35821;&#38899;&#21644;&#35828;&#35805;&#32773;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2401.17632</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#21644;&#35828;&#35805;&#32773;&#27169;&#22411;&#23398;&#20064;&#20102;&#20160;&#20040;&#65311;&#26469;&#33258;&#36328;&#27169;&#22411;&#23618;&#38754;&#20998;&#26512;&#30340;&#26032;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;SUPERB&#35780;&#20272;&#25506;&#27979;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25429;&#25417;&#35821;&#38899;&#29305;&#24615;&#30340;&#33021;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#35821;&#38899;&#21644;&#35828;&#35805;&#32773;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35821;&#38899;&#34920;&#31034;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;WavLM&#65292;&#37319;&#29992;&#25513;&#30721;&#39044;&#27979;&#35757;&#32451;&#26469;&#32534;&#30721;&#36890;&#29992;&#34920;&#31034;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#35828;&#35805;&#32773;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20363;&#22914;&#22522;&#20110;DINO&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#22522;&#20110;&#35821;&#21477;&#30340;&#35757;&#32451;&#30446;&#26631;&#20027;&#35201;&#29992;&#20110;&#35828;&#35805;&#32773;&#34920;&#31034;&#12290;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#34920;&#31034;&#20449;&#24687;&#23545;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21508;&#31181;&#20998;&#26512;&#19981;&#21516;&#65292;&#23545;&#20110;&#35828;&#35805;&#32773;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#25429;&#25417;&#20102;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#20854;&#34920;&#31034;&#19982;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#25110;&#20854;&#20182;&#23436;&#20840;&#30417;&#30563;&#30340;&#35828;&#35805;&#32773;&#27169;&#22411;&#26377;&#20309;&#19981;&#21516;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;SUPERB&#35780;&#20272;&#25506;&#27979;&#20219;&#21153;&#24212;&#29992;&#20110;&#35821;&#38899;&#21644;&#35828;&#35805;&#32773;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#26469;&#25506;&#32034;&#25429;&#25417;&#21508;&#31181;&#35821;&#38899;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#27599;&#20010;&#20219;&#21153;&#20013;&#20027;&#35201;&#20351;&#29992;&#30340;&#23618;&#65292;&#20197;&#30830;&#23450;&#35821;&#38899;&#21644;&#35828;&#35805;&#32773;&#27169;&#22411;&#22312;&#34920;&#31034;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30693;&#35782;&#32534;&#36753;&#23545;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#65292;&#25552;&#20986;&#20102; additivity &#25351;&#26631;&#20197;&#21450; Perturbation Evaluation of Appending Knowledge (PEAK) &#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#38468;&#21152;&#26032;&#30693;&#35782;&#26102;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17623</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#37051;&#36817;&#25200;&#21160;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Neighboring Perturbations of Knowledge Editing on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30693;&#35782;&#32534;&#36753;&#23545;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#65292;&#25552;&#20986;&#20102; additivity &#25351;&#26631;&#20197;&#21450; Perturbation Evaluation of Appending Knowledge (PEAK) &#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#38468;&#21152;&#26032;&#30693;&#35782;&#26102;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#38169;&#35823;&#25110;&#36807;&#26102;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#23481;&#26131;&#29983;&#25104;&#24847;&#22806;&#30340;&#25991;&#26412;&#12290;&#32771;&#34385;&#21040;&#37325;&#26032;&#35757;&#32451;LLMs&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#24615;&#36136;&#65292;&#30693;&#35782;&#32534;&#36753;&#30340;&#21457;&#23637;&#21576;&#29616;&#20986;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#24456;&#23569;&#25506;&#32034;&#32534;&#36753;&#23545;&#30456;&#37051;&#30693;&#35782;&#30340;&#25200;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#26032;&#30693;&#35782;&#26356;&#26032;&#21040;LLMs&#20013;&#26159;&#21542;&#25200;&#20081;&#20102;&#20854;&#20013;&#21253;&#21547;&#30340;&#30456;&#37051;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#26032;&#30340;&#31572;&#26696;&#38468;&#21152;&#21040;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#21015;&#34920;&#20013;&#26159;&#21542;&#20250;&#23548;&#33268;&#21407;&#22987;&#27491;&#30830;&#31572;&#26696;&#30340;&#20007;&#22833;&#65292;&#20197;&#21450;&#19981;&#32463;&#24847;&#22320;&#21253;&#21547;&#20102;&#38169;&#35823;&#31572;&#26696;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#21152;&#24615;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#38468;&#21152;&#25200;&#21160;&#35780;&#20272;&#65288;PEAK&#65289;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#38468;&#21152;&#26032;&#30693;&#35782;&#26102;&#37051;&#36817;&#30693;&#35782;&#30340;&#25200;&#21160;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#26032;&#22686;&#30693;&#35782;&#30340;&#25928;&#26524;&#21644;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play 
&lt;/p&gt;</description></item><item><title>&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LoRA&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#26029;&#35328;&#26816;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21307;&#23398;&#27010;&#24565;&#26102;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20934;&#30830;&#30340;&#26029;&#35328;&#31867;&#22411;&#35782;&#21035;&#23545;&#20110;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#24739;&#32773;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2401.17602</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;LoRA&#24494;&#35843;&#36827;&#34892;&#26029;&#35328;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17602
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LoRA&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#26029;&#35328;&#26816;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21307;&#23398;&#27010;&#24565;&#26102;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20934;&#30830;&#30340;&#26029;&#35328;&#31867;&#22411;&#35782;&#21035;&#23545;&#20110;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#24739;&#32773;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21307;&#23398;&#27010;&#24565;&#26102;&#30340;&#26029;&#35328;&#26816;&#27979;&#20219;&#21153;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#20020;&#24202;NLP&#20013;&#30340;&#26029;&#35328;&#26816;&#27979;&#36890;&#24120;&#28041;&#21450;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#20026;&#21307;&#23398;&#27010;&#24565;&#35782;&#21035;&#26029;&#35328;&#31867;&#22411;&#65292;&#21363;&#30830;&#23450;&#24615;&#65288;&#21307;&#23398;&#27010;&#24565;&#26159;&#21542;&#20026;&#31215;&#26497;&#12289;&#21542;&#23450;&#12289;&#21487;&#33021;&#25110;&#20551;&#35774;&#65289;&#65292;&#26102;&#38388;&#24615;&#65288;&#21307;&#23398;&#27010;&#24565;&#26159;&#25351;&#24403;&#21069;&#36824;&#26159;&#36807;&#21435;&#21382;&#21490;&#65289;&#21644;&#32463;&#39564;&#32773;&#65288;&#21307;&#23398;&#27010;&#24565;&#26159;&#25351;&#24739;&#32773;&#36824;&#26159;&#23478;&#24237;&#25104;&#21592;&#65289;&#12290;&#36825;&#20123;&#26029;&#35328;&#31867;&#22411;&#23545;&#20110;&#21307;&#25252;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#26126;&#30830;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#25991;&#26412;&#20013;&#29702;&#35299;&#21307;&#23398;&#29366;&#20917;&#30340;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#65292;&#30452;&#25509;&#24433;&#21709;&#24739;&#32773;&#25252;&#29702;&#36136;&#37327;&#21644;&#32467;&#26524;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;NLP&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#21162;&#21147;&#21019;&#24314;&#27169;&#24335;&#65292;&#24182;&#19988;&#24448;&#24448;&#24573;&#35270;&#20102;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to address the task of assertion detection when extracting medical concepts from clinical notes, a key process in clinical natural language processing (NLP). Assertion detection in clinical NLP usually involves identifying assertion types for medical concepts in the clinical text, namely certainty (whether the medical concept is positive, negated, possible, or hypothetical), temporality (whether the medical concept is for present or the past history), and experiencer (whether the medical concept is described for the patient or a family member). These assertion types are essential for healthcare professionals to quickly and clearly understand the context of medical conditions from unstructured clinical texts, directly influencing the quality and outcomes of patient care. Although widely used, traditional methods, particularly rule-based NLP systems and machine learning or deep learning models, demand intensive manual efforts to create patterns and tend to overlook 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GPT-4V&#27169;&#22411;&#22312;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4V&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#22914;&#20301;&#32622;&#29702;&#35299;&#21644;&#22270;&#20687;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#35745;&#25968;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17600</link><description>&lt;p&gt;
&#22312;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#19978;&#23545;GPT-4V&#36827;&#34892;&#26631;&#27880;&#20219;&#21153;&#35780;&#20272;&#65306;&#23545;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17600
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GPT-4V&#27169;&#22411;&#22312;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4V&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#22914;&#20301;&#32622;&#29702;&#35299;&#21644;&#22270;&#20687;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#35745;&#25968;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#20197;&#21355;&#26143;&#21644;&#33322;&#31354;&#22270;&#20687;&#20026;&#20027;&#30340;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36825;&#31867;&#25968;&#25454;&#22312;VLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#36739;&#20026;&#32597;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#35780;&#20272;VLMs&#22312;&#22330;&#26223;&#29702;&#35299;&#12289;&#23450;&#20301;&#21644;&#35745;&#25968;&#20197;&#21450;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#20197;&#34913;&#37327;&#23427;&#20204;&#22312;EO&#25968;&#25454;&#19978;&#20316;&#20026;&#26377;&#25928;&#24037;&#20855;&#30340;&#36827;&#23637;&#12290;&#21463;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#20102;&#22478;&#24066;&#30417;&#27979;&#12289;&#28798;&#23475;&#25937;&#25588;&#12289;&#22303;&#22320;&#21033;&#29992;&#21644;&#20445;&#25252;&#31561;&#22330;&#26223;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20687;GPT-4V&#36825;&#26679;&#30340;&#26368;&#26032;VLMs&#20855;&#26377;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20301;&#32622;&#29702;&#35299;&#21644;&#22270;&#20687;&#26631;&#27880;&#31561;&#24320;&#25918;&#24335;&#20219;&#21153;&#19978;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#35745;&#25968;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#22686;&#24378;&#35828;&#35805;&#32773;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36718;&#23545;&#35805;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#38271;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17597</link><description>&lt;p&gt;
SPECTRUM: &#22686;&#24378;&#35828;&#35805;&#32773;&#20808;&#35757;&#32451;&#29992;&#20110;&#38271;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#22686;&#24378;&#35828;&#35805;&#32773;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36718;&#23545;&#35805;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#38271;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36718;&#23545;&#35805;&#20197;&#20854;&#25193;&#23637;&#38271;&#24230;&#21644;&#20132;&#26367;&#21457;&#35328;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23558;&#36825;&#20123;&#23545;&#35805;&#35270;&#20026;&#26222;&#36890;&#25991;&#26412;&#65292;&#24573;&#35270;&#20102;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#22686;&#24378;&#35828;&#35805;&#32773;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#36718;&#23545;&#35805;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30495;&#23454;&#22330;&#26223;&#30340;&#23545;&#35805;&#35760;&#24405;&#12289;&#30005;&#24433;&#25110;&#30005;&#35270;&#21095;&#21095;&#26412;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23545;&#35805;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#21457;&#35328;&#32773;&#21464;&#26356;&#30340;&#26816;&#27979;&#21644;&#25513;&#30721;&#35805;&#35821;&#29983;&#25104;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24102;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#31574;&#21010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26412;&#22320;&#21644;&#20840;&#23616;&#23545;&#35805;&#27169;&#22411;&#65288;LGCM&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#23618;&#27425;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#21644;&#34701;&#21512;&#29983;&#25104;&#22238;&#24212;&#25152;&#38656;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26080;&#32541;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2401.17588</link><description>&lt;p&gt;
&#23545;&#35805;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Local and Global Contexts for Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26412;&#22320;&#21644;&#20840;&#23616;&#23545;&#35805;&#27169;&#22411;&#65288;LGCM&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#23618;&#27425;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#21644;&#34701;&#21512;&#29983;&#25104;&#22238;&#24212;&#25152;&#38656;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26080;&#32541;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#19978;&#19979;&#25991;&#26159;&#22810;&#36718;&#23545;&#35805;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#23545;&#35805;&#21382;&#21490;&#12290;&#23398;&#20064;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#36827;&#34892;&#23545;&#35805;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#22320;&#19978;&#19979;&#25991;&#26159;&#19982;&#21518;&#32493;&#22238;&#24212;&#26356;&#25509;&#36817;&#19988;&#26356;&#25935;&#24863;&#30340;&#26368;&#36817;&#37051;&#19978;&#19979;&#25991;&#65292;&#32780;&#20840;&#23616;&#19978;&#19979;&#25991;&#19982;&#25972;&#20010;&#23545;&#35805;&#30456;&#20851;&#65292;&#36828;&#36229;&#20986;&#37051;&#36817;&#30340;&#35805;&#35821;&#12290;&#30446;&#21069;&#65292;&#29992;&#20110;&#23545;&#35805;&#30340;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#26412;&#22320;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#25509;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#20013;&#30340;&#36890;&#29992;&#23545;&#35805;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#23545;&#35805;&#27169;&#22411;&#65288;LGCM&#65289;&#12290;&#23427;&#26159;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#23618;&#27425;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#21644;&#34701;&#21512;&#29983;&#25104;&#22238;&#24212;&#25152;&#38656;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#12290;&#23427;&#20351;&#29992;&#26412;&#22320;&#32534;&#30721;&#22120;&#26469;&#33719;&#21462;&#20010;&#21035;&#35805;&#35821;&#23618;&#38754;&#30340;&#26412;&#22320;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;&#20840;&#23616;&#32534;&#30721;&#22120;&#26469;&#29702;&#35299;&#23545;&#35805;&#23618;&#38754;&#30340;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26080;&#32541;&#34701;&#21512;&#36825;&#20123;&#26412;&#22320;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally cont
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;ReCoE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#29305;&#23450;&#30340;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#21644;&#36830;&#36143;&#24615;&#30340;&#32771;&#37327;&#12290;</title><link>https://arxiv.org/abs/2401.17585</link><description>&lt;p&gt;
&#20256;&#25773;&#19982;&#38519;&#38449;&#65306;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#35780;&#20272;&#22522;&#20110;&#25512;&#29702;&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;ReCoE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#29305;&#23450;&#30340;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#21644;&#36830;&#36143;&#24615;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#26377;&#25928;&#20256;&#25773;&#26356;&#26032;&#30340;&#30456;&#20114;&#20851;&#32852;&#20107;&#23454;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#38459;&#30861;&#20934;&#30830;&#25512;&#29702;&#27169;&#22411;&#20013;&#26356;&#26032;&#30693;&#35782;&#36866;&#24403;&#20256;&#25773;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25512;&#29702;&#30340;&#22522;&#20934;&#8212;&#8212;ReCoE&#65288;&#22522;&#20110;&#25512;&#29702;&#30340;&#21453;&#20107;&#23454;&#32534;&#36753;&#25968;&#25454;&#38598;&#65289;&#65292;&#28085;&#30422;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20845;&#31181;&#24120;&#35265;&#25512;&#29702;&#26041;&#26696;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;&#36755;&#20837;&#22686;&#24378;&#12289;&#24494;&#35843;&#21644;&#23450;&#20301;&#32534;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#37117;&#26126;&#26174;&#36739;&#20302;&#65292;&#23588;&#20854;&#26159;&#22312;&#26576;&#20123;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#20174;&#25512;&#29702;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#29616;&#26377;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19981;&#36275;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#23558;Transformer&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#26367;&#25442;&#20026;Hyena&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#24182;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#25216;&#26415;&#20026;&#36861;&#27714;&#21487;&#25345;&#32493;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2401.17574</link><description>&lt;p&gt;
Scavenging Hyena: &#23558;Transformer&#27169;&#22411;&#31934;&#28860;&#20026;&#38271;&#21367;&#31215;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scavenging Hyena: Distilling Transformers into Long Convolution Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#23558;Transformer&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#26367;&#25442;&#20026;Hyena&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#24182;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#25216;&#26415;&#20026;&#36861;&#27714;&#21487;&#25345;&#32493;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;GPT-4&#31561;&#26550;&#26500;&#20026;&#20856;&#33539;&#65292;&#37325;&#22609;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#36328;&#26550;&#26500;&#36801;&#31227;&#30340;&#26041;&#27861;&#12290;&#20511;&#37492;&#39640;&#25928;&#30340;Hyena&#26426;&#21046;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;Hyena&#26469;&#26367;&#25442;Transformer&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#26367;&#20195;&#20256;&#32479;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35201;&#38754;&#23545;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#20108;&#27425;&#27880;&#24847;&#21147;&#26426;&#21046;&#22266;&#26377;&#30340;&#12290;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;LLM&#19981;&#26029;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36861;&#27714;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#20316;&#20986;&#20102;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#29615;&#22659;&#24433;&#21709;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PipeNet&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20462;&#21098;&#25216;&#26415;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#32852;-&#20462;&#21098;-&#25512;&#29702;&#30340;&#27969;&#31243;&#26469;&#20462;&#21098;&#22122;&#22768;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#22270;&#25512;&#29702;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#23376;&#22270;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.17536</link><description>&lt;p&gt;
PipeNet:&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#35821;&#20041;&#20462;&#21098;&#30340;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PipeNet&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20462;&#21098;&#25216;&#26415;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#32852;-&#20462;&#21098;-&#25512;&#29702;&#30340;&#27969;&#31243;&#26469;&#20462;&#21098;&#22122;&#22768;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#22270;&#25512;&#29702;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#23376;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#38382;&#39064;&#22238;&#31572;&#20013;&#24341;&#20837;&#26174;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#21487;&#20197;&#24102;&#26469;&#22909;&#22788;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#22522;&#30784;&#25512;&#29702;&#27969;&#31243;&#65292;&#22312;&#35813;&#27969;&#31243;&#20013;&#65292;&#39318;&#20808;&#23558;&#23454;&#20307;&#33410;&#28857;&#19982;&#26597;&#35810;(&#38382;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;)&#36827;&#34892;&#20851;&#32852;&#65292;&#28982;&#21518;&#20351;&#29992;&#25512;&#29702;&#27169;&#22359;&#23545;&#21305;&#37197;&#30340;&#22810;&#36339;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#65292;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#12290;&#34429;&#28982;&#36825;&#20010;&#27969;&#31243;&#22312;&#20174;&#24222;&#22823;&#30340;KG&#20013;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#21892;&#65292;&#20294;&#22312;&#25918;&#22823;&#20851;&#32852;&#30340;&#23376;&#22270;&#26102;&#65292;&#25928;&#29575;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#23376;&#22270;&#20013;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#23454;&#20307;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;KG&#36827;&#34892;&#22270;&#25512;&#29702;&#26102;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20851;&#32852;-&#20462;&#21098;-&#25512;&#29702;&#30340;&#27969;&#31243;&#26469;&#20462;&#21098;&#22122;&#22768;&#33410;&#28857;&#65292;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#23376;&#22270;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20462;&#21098;&#27169;&#22359;&#39318;&#20808;&#26681;&#25454;&#21305;&#37197;&#33539;&#22260;&#20043;&#38388;&#30340;&#20381;&#36182;&#36317;&#31163;&#23545;&#27010;&#24565;&#33410;&#28857;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well acknowledged that incorporating explicit knowledge graphs (KGs) can benefit question answering. Existing approaches typically follow a grounding-reasoning pipeline in which entity nodes are first grounded for the query (question and candidate answers), and then a reasoning module reasons over the matched multi-hop subgraph for answer prediction. Although the pipeline largely alleviates the issue of extracting essential information from giant KGs, efficiency is still an open challenge when scaling up hops in grounding the subgraphs. In this paper, we target at finding semantically related entity nodes in the subgraph to improve the efficiency of graph reasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune noisy nodes, remarkably reducing the computation cost and memory usage while also obtaining decent subgraph representation. In detail, the pruning module first scores concept nodes based on the dependency distance between matched spans and then prunes 
&lt;/p&gt;</description></item><item><title>FEUDA&#26159;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#22522;&#30784;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#25506;&#32034;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#33539;&#20363;&#12290;</title><link>https://arxiv.org/abs/2401.17514</link><description>&lt;p&gt;
FEUDA&#65306;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17514
&lt;/p&gt;
&lt;p&gt;
FEUDA&#26159;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#22522;&#30784;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#25506;&#32034;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#25903;&#21033;&#29992;&#26469;&#33258;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#36866;&#24212;&#30340;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#40723;&#21169;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25110;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#24517;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#20854;&#20013;&#19968;&#20010;&#36755;&#20837;&#31034;&#20363;&#30001;&#27169;&#26495;&#20462;&#25913;&#21518;&#65292;&#20877;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#29983;&#25104;&#19968;&#20010;&#26631;&#31614;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#30740;&#31350;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#36825;&#31181;&#26032;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;FEUDA&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#65292;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;LM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#36890;&#36807;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#22312;&#20004;&#20010;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26412;&#19978;&#35757;&#32451;LM&#65292;&#31532;&#20108;&#20010;&#20219;&#21153;&#20351;&#29992;&#28304;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#25351;&#20196;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#38754;&#21521;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20307;&#22806;&#21463;&#31934;&#32467;&#26524;&#39044;&#27979;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.17511</link><description>&lt;p&gt;
&#22312;&#38754;&#21521;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#35821;&#35328;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Linguistically Communicating Uncertainty in Patient-Facing Risk Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#38754;&#21521;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20307;&#22806;&#21463;&#31934;&#32467;&#26524;&#39044;&#27979;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#24212;&#29992;&#20110;&#38754;&#21521;&#24739;&#32773;&#29615;&#22659;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#19982;&#20026;&#27169;&#22411;&#24320;&#21457;&#32773;&#25110;&#39046;&#22495;&#19987;&#23478;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#37324;&#38656;&#35201;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#12289;&#23637;&#31034;&#21644;&#35780;&#20272;&#21487;&#29702;&#35299;&#24615;&#30340;&#38468;&#21152;&#22240;&#32032;&#12290;&#25105;&#20204;&#22312;&#39118;&#38505;&#39044;&#27979;&#30340;&#35821;&#22659;&#19979;&#35782;&#21035;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#27807;&#36890;&#27169;&#22411;&#24615;&#33021;&#12289;&#32622;&#20449;&#24230;&#12289;&#25512;&#29702;&#21644;&#26410;&#30693;&#24050;&#30693;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#20307;&#22806;&#21463;&#31934;&#32467;&#26524;&#39044;&#27979;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the unique challenges associated with uncertainty quantification in AI models when applied to patient-facing contexts within healthcare. Unlike traditional eXplainable Artificial Intelligence (XAI) methods tailored for model developers or domain experts, additional considerations of communicating in natural language, its presentation and evaluating understandability are necessary. We identify the challenges in communication model performance, confidence, reasoning and unknown knowns using natural language in the context of risk prediction. We propose a design aimed at addressing these challenges, focusing on the specific application of in-vitro fertilisation outcome prediction.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#22270;&#25509;&#31181;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#30340;&#25361;&#25112;&#25968;&#25454;&#23376;&#38598;&#19978;&#23545;QA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17498</link><description>&lt;p&gt;
&#36890;&#36807;&#22320;&#22270;&#25509;&#31181;&#25913;&#36827;QA&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving QA Model Performance with Cartographic Inoculation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22320;&#22270;&#25509;&#31181;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#30340;&#25361;&#25112;&#25968;&#25454;&#23376;&#38598;&#19978;&#23545;QA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
QA&#27169;&#22411;&#38754;&#20020;&#30528;&#22797;&#26434;&#32780;&#24320;&#25918;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#24335;&#65292;&#25110;&#32773;&#31216;&#20026;"&#25968;&#25454;&#38598;&#20266;&#36857;"&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;QA&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21033;&#29992;&#35757;&#32451;&#29992;&#20110;QA&#30340;ElectraSmallDiscriminator&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#23545;&#25239;&#24615;&#25361;&#25112;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#24433;&#21709;&#21644;&#21457;&#29983;&#24773;&#20917;&#65292;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#28151;&#28102;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20266;&#36857;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#22312;&#29616;&#26377;&#20943;&#36731;&#20266;&#36857;&#24433;&#21709;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22320;&#22270;&#25509;&#31181;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#30340;&#25361;&#25112;&#25968;&#25454;&#23376;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20266;&#36857;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#22312;&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#26865;&#20004;&#21487;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#19978;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#25439;&#22833;&#27169;&#22411;&#23545;&#20854;&#20182;&#25361;&#25112;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#22312;&#25972;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
QA models are faced with complex and open-ended contextual reasoning problems, but can often learn well-performing solution heuristics by exploiting dataset-specific patterns in their training data. These patterns, or "dataset artifacts", reduce the model's ability to generalize to real-world QA problems. Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the impacts and incidence of dataset artifacts using an adversarial challenge set designed to confuse models reliant on artifacts for prediction. Extending existing work on methods for mitigating artifact impacts, we propose cartographic inoculation, a novel method that fine-tunes models on an optimized subset of the challenge data to reduce model reliance on dataset artifacts. We show that by selectively fine-tuning a model on ambiguous adversarial examples from a challenge set, significant performance improvements can be made on the full challenge dataset with minimal loss of model generalizability to other chal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2401.17477</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#24515;&#29702;&#38556;&#30861;&#65306;&#22522;&#20110;ChatGPT&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting mental disorder on social media: a ChatGPT-augmented explainable approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#36798;&#30340;&#25233;&#37057;&#30151;&#29366;&#30340;&#39057;&#29575;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#65292;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#26041;&#27861;&#26469;&#21450;&#26102;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;ChatGPT&#31561;&#23545;&#35805;&#20195;&#29702;&#22120;&#26377;&#25928;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24212;&#23545;&#21487;&#35299;&#37322;&#24615;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#21147;&#25552;&#20379;&#20998;&#31867;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#21487;&#35835;&#24615;&#24378;&#30340;&#35780;&#35770;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#21487;&#35299;&#37322;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#20570;&#20986;&#36129;&#29486;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;LLM&#39318;&#20808;&#35299;&#30721;&#25277;&#35937;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17464</link><description>&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#38142;&#25512;&#29702;&#30340;&#39640;&#25928;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Tool Use with Chain-of-Abstraction Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;LLM&#39318;&#20808;&#35299;&#30721;&#25277;&#35937;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#20934;&#30830;&#25512;&#29702;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38656;&#35201;&#23558;&#25512;&#29702;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#32593;&#32476;&#20107;&#23454;&#12289;&#25968;&#23398;&#21644;&#29289;&#29702;&#35268;&#21017;&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;LLM&#33719;&#21462;&#36825;&#20123;&#22806;&#37096;&#30693;&#35782;&#65292;&#20294;&#26159;&#22312;&#22810;&#27493;&#25512;&#29702;&#38382;&#39064;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#31934;&#32454;&#35843;&#25972;LLM&#20195;&#29702;&#65288;&#20363;&#22914;Toolformer&#65289;&#20197;&#35843;&#29992;&#24037;&#20855;&#65292;&#20854;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#24037;&#20855;&#35843;&#29992;&#38656;&#35201;&#25972;&#20307;&#21270;&#21644;&#39640;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;&#35268;&#21010;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;LLM&#39318;&#20808;&#29992;&#25277;&#35937;&#21344;&#20301;&#31526;&#35299;&#30721;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#20197;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#26469;&#23454;&#29616;&#27599;&#20010;&#25512;&#29702;&#38142;&#12290;&#25277;&#35937;&#38142;&#30340;&#35268;&#21010;&#20351;LLM&#33021;&#22815;&#23398;&#20064;&#26356;&#36890;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23545;&#20110;&#19982;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#30456;&#20851;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#20363;&#22914;&#25968;&#23398;&#32467;&#26524;&#65289;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#36824;&#20801;&#35768;LLM&#25191;&#34892;&#35299;&#30721;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;LLM&#26234;&#33021;&#20307;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#26234;&#33021;&#20307;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#26469;&#33719;&#21462;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35805;&#30340;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.17461</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#26234;&#33021;&#20307;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synthetic Dialogue Dataset Generation using LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;LLM&#26234;&#33021;&#20307;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#26234;&#33021;&#20307;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#26469;&#33719;&#21462;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35805;&#30340;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34920;&#38754;&#19978;&#31616;&#21333;&#65292;&#20294;&#26410;&#32463;&#35757;&#32451;&#30340;&#29992;&#25143;&#21487;&#33021;&#38590;&#20197;&#30830;&#23450;&#20854;&#29305;&#23450;&#38382;&#39064;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#26234;&#33021;&#20307;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#65292;&#20197;&#33719;&#21462;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#25152;&#38656;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20174;&#32780;&#24314;&#31435;&#19968;&#20010;&#21518;&#32493;&#30340;&#26234;&#33021;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#21644;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#23545;&#35805;&#26234;&#33021;&#20307;&#30340;&#31034;&#20363;&#23545;&#35805;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#35774;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#30456;&#20114;&#8220;&#23545;&#35805;&#8221;&#30340;&#26234;&#33021;&#20307;&#65292;&#19968;&#20010;&#20805;&#24403;&#23545;&#35805;&#26234;&#33021;&#20307;&#65292;&#21478;&#19968;&#20010;&#20805;&#24403;&#29992;&#25143;&#12290;&#20351;&#29992;&#20165;&#23545;&#29992;&#25143;&#21487;&#35265;&#30340;&#19968;&#32452;&#32447;&#24615;&#38382;&#39064;&#25991;&#26412;&#25551;&#36848;&#65292;&#26234;&#33021;&#20307;&#21644;&#29992;&#25143;&#36827;&#34892;&#23545;&#35805;&#65292;&#30452;&#21040;&#26234;&#33021;&#20307;&#20174;&#21407;&#22987;&#38382;&#39064;&#25551;&#36848;&#20013;&#33719;&#21462;&#21040;&#25152;&#26377;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#35805;&#30340;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that "talk" to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#22303;&#32819;&#20854;&#35821;&#22522;&#20934;&#27979;&#35797;&#65292;&#25104;&#21151;&#22320;&#23545;&#21517;&#20026;BERTurk&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2401.17396</link><description>&lt;p&gt;
&#23545;&#22303;&#32819;&#20854;&#35821;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17396
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#22303;&#32819;&#20854;&#35821;&#22522;&#20934;&#27979;&#35797;&#65292;&#25104;&#21151;&#22320;&#23545;&#21517;&#20026;BERTurk&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#36817;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#30001;&#20110;&#20854;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#24494;&#35843;&#29305;&#24615;&#65292;&#23427;&#20204;&#24050;&#32463;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#65288;&#21452;&#21521;Transformer&#32534;&#30721;&#22120;&#34920;&#31034;&#65289;&#65292;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#20123;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#23558;&#39044;&#20808;&#26500;&#24314;&#22909;&#30340;&#27169;&#22411;&#36716;&#31227;&#24182;&#38024;&#23545;&#29305;&#23450;&#30340;NLU&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#22914;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;&#22303;&#32819;&#20854;&#35821;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23545;&#19968;&#31181;&#21517;&#20026;BERTurk&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#26159;&#20351;&#29992;&#22522;&#26412;&#35774;&#32622;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17390</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Customizing Language Model Responses with Contrastive In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#19982;&#25105;&#20204;&#30340;&#24847;&#22270;&#23545;&#40784;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#20248;&#20110;&#20854;&#20182;&#20869;&#23481;&#30340;&#20869;&#23481;&#65292;&#25110;&#32773;&#24403;&#25105;&#20204;&#24076;&#26395;LLMs&#20197;&#19968;&#31181;&#38590;&#20197;&#25551;&#36848;&#30340;&#39118;&#26684;&#25110;&#35821;&#27668;&#36827;&#34892;&#22238;&#24212;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#26356;&#22909;&#22320;&#25551;&#36848;&#25105;&#20204;&#30340;&#24847;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#26469;&#35828;&#26126;&#30495;&#23454;&#30340;&#24847;&#22270;&#65292;&#20197;&#21450;&#36127;&#38754;&#31034;&#20363;&#26469;&#23637;&#31034;&#25105;&#20204;&#24076;&#26395;LLMs&#36991;&#20813;&#30340;&#29305;&#24449;&#12290;&#36127;&#38754;&#31034;&#20363;&#21487;&#20197;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#26816;&#32034;&#65292;&#30001;&#20154;&#24037;&#32534;&#20889;&#65292;&#25110;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#20043;&#21069;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#31034;&#20363;&#65292;&#20197;&#25945;&#20250;&#33258;&#24049;&#36991;&#20813;&#20160;&#20040;&#12290;&#36825;&#20010;&#25512;&#29702;&#27493;&#39588;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;&#36866;&#24403;&#34920;&#36798;&#65292;&#24182;&#24341;&#23548;&#20854;&#29983;&#25104;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#30340;&#21152;&#26435;&#38598;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;BERT&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38463;&#25289;&#20271;&#26041;&#35328;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#20026;&#29702;&#35299;&#29992;&#25143;&#35266;&#28857;&#21644;&#24577;&#24230;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2401.17373</link><description>&lt;p&gt;
&#38463;&#25289;&#20271;&#25512;&#25991;&#34892;&#20026;&#65306;&#22522;&#20110;&#21152;&#26435;&#38598;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for Classifying Arabic Speech Acts on Twitter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#30340;&#21152;&#26435;&#38598;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;BERT&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38463;&#25289;&#20271;&#26041;&#35328;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#20026;&#29702;&#35299;&#29992;&#25143;&#35266;&#28857;&#21644;&#24577;&#24230;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#34892;&#20026;&#26159;&#35828;&#35805;&#32773;&#22312;&#23545;&#35805;&#20013;&#34920;&#36798;&#24847;&#24605;&#26102;&#30340;&#34892;&#20026;&#65292;&#20363;&#22914;&#35810;&#38382;&#12289;&#25512;&#33616;&#12289;&#38382;&#20505;&#12289;&#36947;&#35874;&#12289;&#34920;&#36798;&#24819;&#27861;&#25110;&#25552;&#20986;&#24314;&#35758;&#12290;&#29702;&#35299;&#35821;&#35328;&#34892;&#20026;&#26377;&#21161;&#20110;&#35299;&#37322;&#35828;&#35805;&#32773;&#25110;&#20316;&#32773;&#35328;&#35821;&#32972;&#21518;&#30340;&#24847;&#22270;&#21644;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#26041;&#27861;&#12290;&#25512;&#29305;&#21644;&#31038;&#20132;&#23186;&#20307;&#36234;&#26469;&#36234;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24050;&#32463;&#28436;&#21464;&#25104;&#20102;&#34920;&#36798;&#29992;&#25143;&#35266;&#28857;&#21644;&#24577;&#24230;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#21152;&#26435;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#20013;&#21508;&#31181;BERT&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#20960;&#20010;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#35328;&#38463;&#25289;&#20271;&#25512;&#29305;&#34892;&#20026;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech acts are a speakers actions when performing an utterance within a conversation, such as asking, recommending, greeting, or thanking someone, expressing a thought, or making a suggestion. Understanding speech acts helps interpret the intended meaning and actions behind a speakers or writers words. This paper proposes a Twitter dialectal Arabic speech act classification approach based on a transformer deep learning neural network. Twitter and social media, are becoming more and more integrated into daily life. As a result, they have evolved into a vital source of information that represents the views and attitudes of their users. We proposed a BERT based weighted ensemble learning approach to integrate the advantages of various BERT models in dialectal Arabic speech acts classification. We compared the proposed model against several variants of Arabic BERT models and sequence-based models. We developed a dialectal Arabic tweet act dataset by annotating a subset of a large existing
&lt;/p&gt;</description></item><item><title>ViLexNorm&#26159;&#31532;&#19968;&#20010;&#20026;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#24320;&#21457;&#30340;&#35789;&#27719;&#35268;&#33539;&#21270;&#35821;&#26009;&#24211;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.16403</link><description>&lt;p&gt;
ViLexNorm&#65306;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#35789;&#27719;&#35268;&#33539;&#21270;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16403
&lt;/p&gt;
&lt;p&gt;
ViLexNorm&#26159;&#31532;&#19968;&#20010;&#20026;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#24320;&#21457;&#30340;&#35789;&#27719;&#35268;&#33539;&#21270;&#35821;&#26009;&#24211;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#35268;&#33539;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#35789;&#35821;&#36716;&#25442;&#20026;&#23427;&#20204;&#30340;&#35268;&#33539;&#24418;&#24335;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#20010;&#36807;&#31243;&#23545;&#21508;&#31181;&#21518;&#32493;&#30340;NLP&#20219;&#21153;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ViLexNorm&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#36234;&#21335;&#35789;&#27719;&#35268;&#33539;&#21270;&#20219;&#21153;&#24320;&#21457;&#30340;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;&#36229;&#36807;10,000&#23545;&#30001;&#20154;&#24037;&#26631;&#27880;&#30340;&#21477;&#23376;&#65292;&#36825;&#20123;&#21477;&#23376;&#26469;&#33258;&#36234;&#21335;&#26368;&#27969;&#34892;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20844;&#24320;&#35780;&#35770;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65292;&#26368;&#20339;&#31995;&#32479;&#22312;&#20351;&#29992;Error Reduction Rate (ERR)&#25351;&#26631; (van der Goot, 2019a) &#21644;Leave-As-Is (LAI)&#22522;&#20934;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;57.74%&#30340;&#32467;&#26524;&#12290;&#22312;&#22806;&#37096;&#35780;&#20272;&#20013;&#65292;&#20351;&#29992;&#22312;ViLexNorm&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#36234;&#21335;&#35789;&#27719;&#35268;&#33539;&#21270;&#20219;&#21153;&#23545;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#20165;&#20379;&#30740;&#31350;&#30446;&#30340;&#20844;&#24320;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms. This process has been proven to benefit various downstream NLP tasks greatly. In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task. The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam's most popular social media platforms. Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline. For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks. Our corpus is publicly available exclusively for research purposes.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.16092</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25918;&#22823;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#19988;&#20462;&#27491;&#24037;&#31243;&#21487;&#33021;&#26080;&#27861;&#24110;&#21161;&#24744;
&lt;/p&gt;
&lt;p&gt;
Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16092
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#36890;&#36807;&#25913;&#21892;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#26356;&#22810;&#30340;&#31038;&#32676;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#36825;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#19982;&#21333;&#35821;&#27169;&#22411;&#19968;&#26679;&#21463;&#21040;(&#24615;&#21035;)&#20559;&#35265;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#33258;&#28982;&#26399;&#26395;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#25552;&#20379;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#20294;&#20107;&#23454;&#24182;&#38750;&#22914;&#27492;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#27809;&#26377;&#24615;&#21035;&#20559;&#35265;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#30340;&#26032;&#22522;&#20934;MAGBIG&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;T2I&#27169;&#22411;&#26159;&#21542;&#36890;&#36807;MAGBIG&#25918;&#22823;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#35831;&#27714;&#29305;&#23450;&#32844;&#19994;&#25110;&#29305;&#36136;&#30340;&#20154;&#20687;&#22270;&#20687;(&#20351;&#29992;&#24418;&#23481;&#35789;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;&#27169;&#22411;&#20559;&#31163;&#20102;&#35268;&#33539;&#30340;&#20551;&#35774;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15496</link><description>&lt;p&gt;
Baichuan2-Sum: &#20351;&#29992;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Llama&#12289;Baichuan&#21644;Bloom&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#35282;&#33394;&#29983;&#25104;&#25688;&#35201;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#23567;&#27169;&#22411;&#65288;&#20363;&#22914;Bart&#21644;Bert&#65289;&#36827;&#34892;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#22312;&#23567;&#27169;&#22411;&#19978;&#28155;&#21152;&#20219;&#21153;&#25351;&#23450;&#30340;&#20248;&#21270;&#65292;&#22914;&#21521;&#27169;&#22411;&#28155;&#21152;&#20840;&#23616;-&#23616;&#37096;&#20013;&#24515;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#65306;Baichuan2-Sum&#65292;&#29992;&#20110;&#38754;&#21521;&#35282;&#33394;&#30340;&#23545;&#35805;&#25688;&#35201;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#35282;&#33394;&#35774;&#32622;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#23545;&#35805;&#20132;&#20114;&#20013;&#23398;&#20064;&#24182;&#36755;&#20986;&#26399;&#26395;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;NEFTune&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21512;&#36866;&#30340;&#22122;&#22768;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;CSDS&#21644;SAMSUM&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.07518</link><description>&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35843;&#26597;&#65306;&#20998;&#31867;&#20307;&#31995;&#12289;&#31995;&#32479;&#32508;&#36848;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#20998;&#26512;&#25991;&#26412;&#65292;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#21830;&#19994;&#21644;&#25945;&#32946;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;NLP&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26041;&#38754;&#30340;&#24110;&#21161;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#19982;&#25945;&#32946;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;NLP&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#20171;&#32461;&#30456;&#20851;&#32972;&#26223;&#24320;&#22987;&#65292;&#28982;&#21518;&#25552;&#20986;&#25945;&#32946;&#39046;&#22495;NLP&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#20998;&#31867;&#31995;&#32479;&#35828;&#26126;&#20219;&#21153;&#23450;&#20041;&#12289;&#25361;&#25112;&#21644;&#30456;&#24212;&#30340;&#25216;&#26415;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#29616;&#26377;&#28436;&#31034;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;</title><link>https://arxiv.org/abs/2312.03863</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#35201;&#20219;&#21153;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#35821;&#35328;&#29983;&#25104;&#21644;&#22797;&#26434;&#25512;&#29702;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#23545;&#25105;&#20204;&#30340;&#31038;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20276;&#38543;&#30528;&#23427;&#20204;&#25152;&#38656;&#30340;&#30456;&#24403;&#22823;&#30340;&#36164;&#28304;&#65292;&#31361;&#26174;&#20102;&#35299;&#20915;&#25928;&#29575;&#25361;&#25112;&#30340;&#26377;&#25928;&#25216;&#26415;&#30340;&#24378;&#28872;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#39640;&#25928;LLMs&#30740;&#31350;&#30340;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#23558;&#25991;&#29486;&#25353;&#29031;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#20998;&#31867;&#36827;&#34892;&#32452;&#32455;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#39640;&#25928;LLMs&#20027;&#39064;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#25910;&#38598;&#20102;&#26412;&#35843;&#26597;&#20013;&#21015;&#20986;&#30340;&#35770;&#25991;&#65292;&#24182;&#23558;&#31215;&#26497;&#32500;&#25252;&#35813;&#23384;&#20648;&#24211;&#65292;&#24182;&#38543;&#30528;&#26032;&#30340;&#30740;&#31350;&#30340;&#20986;&#29616;&#32780;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#21442;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#23454;&#29616;&#23545;&#32593;&#32476;&#30340;&#37096;&#20998;&#35843;&#25972;&#65292;&#20351;&#29992;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#25506;&#32034;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#21644;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.00949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#21442;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Optimization for Large Language Model Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#21442;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#23454;&#29616;&#23545;&#32593;&#32476;&#30340;&#37096;&#20998;&#35843;&#25972;&#65292;&#20351;&#29992;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#25506;&#32034;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#21644;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#20351;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#37324;&#31243;&#30865;&#24335;&#30340;&#25104;&#23601;&#12290;&#36234;&#26469;&#36234;&#22823;&#30340;LLM&#30340;&#20986;&#29616;&#20026;&#26356;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#23558;&#39044;&#35757;&#32451;LLM&#30340;&#22823;&#37096;&#20998;&#26435;&#37325;&#20923;&#32467;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#20998;&#35299;&#65292;&#20165;&#20801;&#35768;&#35843;&#25972;&#32593;&#32476;&#30340;&#26497;&#23567;&#37096;&#20998;&#12290;&#20351;&#29992;LoRA&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#32452;&#36229;&#21442;&#25968;&#65292;&#21253;&#25324;&#20998;&#35299;&#30340;&#31209;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#20027;&#35201;&#30340;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#25216;&#26415;&#26469;&#30740;&#31350;&#36825;&#20123;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#22312;&#39044;&#35757;&#32451;&#30340;LLM&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#39564;&#35777;&#30340;&#25972;&#20010;&#27969;&#31243;&#35270;&#20026;&#40657;&#30418;&#65292;&#24182;&#20351;&#29992;Nomad&#31639;&#27861;&#39640;&#25928;&#22320;&#25506;&#32034;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#21644;&#35843;&#25972;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \nomad algorithm, achieving a boost in performance and human alignment of the tuned mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.15623</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Injecting linguistic knowledge into BERT for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#25512;&#29702;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26694;&#26550;&#25552;&#21462;&#35821;&#35328;&#30693;&#35782;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#22686;&#24378;BERT&#22312;DST&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#35745;&#31639;&#32463;&#27982;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#27880;&#37322;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27880;&#20837;&#25552;&#21462;&#30340;&#30693;&#35782;&#21482;&#38656;&#35201;&#28155;&#21152;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#27169;&#22411;(CPM)&#20316;&#20026;DST&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24182;&#34920;&#26126;&#25152;&#33719;&#21462;&#30340;&#29305;&#24449;&#19982;&#23545;&#35805;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#20840;&#38754;&#29702;&#35299;&#24433;&#21709;DST&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;DST&#20219;&#21153;&#19978;&#23545;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
&lt;/p&gt;</description></item><item><title>GRASP&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#39057;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22522;&#30784;&#21644;&#29289;&#29702;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;Unity&#27169;&#25311;&#36827;&#34892;&#20004;&#23618;&#35780;&#20272;&#65292;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#22522;&#30784;&#21644;&#30452;&#35273;&#29289;&#29702;&#23398;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2311.09048</link><description>&lt;p&gt;
GRASP: &#19968;&#31181;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#35821;&#35328;&#22522;&#30784;&#21644;&#24773;&#22659;&#29289;&#29702;&#29702;&#35299;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09048
&lt;/p&gt;
&lt;p&gt;
GRASP&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#39057;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22522;&#30784;&#21644;&#29289;&#29702;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;Unity&#27169;&#25311;&#36827;&#34892;&#20004;&#23618;&#35780;&#20272;&#65292;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#22522;&#30784;&#21644;&#30452;&#35273;&#29289;&#29702;&#23398;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GRASP&#65292;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35821;&#35328;&#22522;&#30784;&#21644;&#29289;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;Unity&#27169;&#25311;&#30340;&#20004;&#23618;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#31532;&#19968;&#23618;&#27979;&#35797;&#35821;&#35328;&#22522;&#30784;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23558;&#31616;&#21333;&#30340;&#25991;&#26412;&#25551;&#36848;&#19982;&#35270;&#35273;&#20449;&#24687;&#30456;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#31532;&#20108;&#23618;&#35780;&#20272;&#27169;&#22411;&#23545;"&#30452;&#35273;&#29289;&#29702;&#23398;"&#21407;&#29702;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22914;&#29289;&#20307;&#27704;&#24658;&#24615;&#21644;&#36830;&#32493;&#24615;&#12290;&#38500;&#20102;&#21457;&#24067;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#23427;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#22522;&#30784;&#21644;&#30452;&#35273;&#29289;&#29702;&#23398;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#32570;&#38519;&#12290;&#23613;&#31649;&#23427;&#20204;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#22522;&#30784;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21551;&#21457;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25152;&#26377;&#27169;&#22411;&#22312;In&#37096;&#20998;&#30340;&#34920;&#29616;&#37117;&#20302;&#20110;&#25110;&#31561;&#20110;50%&#30340;&#38543;&#26426;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of "Intuitive Physics" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the In
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;Wav2Vec2.0&#21644;GPT-2&#27169;&#22411;&#30340;&#22823;&#33041;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#35821;&#38899;&#21453;&#24212;&#65292;&#20854;&#22823;&#33041;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#19988;&#20849;&#20139;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#35299;&#37322;&#22823;&#33041;&#27963;&#21160;&#20013;&#21464;&#24322;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2310.04645</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25552;&#21462;&#20102;&#19982;&#20154;&#31867;&#22823;&#33041;&#31867;&#20284;&#30340;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do self-supervised speech and language models extract similar representations as human brain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;Wav2Vec2.0&#21644;GPT-2&#27169;&#22411;&#30340;&#22823;&#33041;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#35821;&#38899;&#21453;&#24212;&#65292;&#20854;&#22823;&#33041;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#19988;&#20849;&#20139;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#35299;&#37322;&#22823;&#33041;&#27963;&#21160;&#20013;&#21464;&#24322;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#24863;&#30693;&#26399;&#38388;&#23637;&#29616;&#20986;&#19982;&#22823;&#33041;&#27963;&#21160;&#30340;&#24378;&#22823;&#23545;&#40784;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#19981;&#21516;&#35757;&#32451;&#26041;&#24335;&#65292;&#23427;&#20204;&#26159;&#21542;&#19982;&#30456;&#21516;&#30340;&#31070;&#32463;&#26041;&#38754;&#30456;&#20851;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20004;&#31181;&#20195;&#34920;&#24615;&#30340;SSL&#27169;&#22411;&#65288;Wav2Vec2.0&#21644;GPT-2&#65289;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#26469;&#30452;&#25509;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#33021;&#20934;&#30830;&#39044;&#27979;&#21548;&#35273;&#30382;&#23618;&#20013;&#30340;&#35821;&#38899;&#21709;&#24212;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#22823;&#33041;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wav2Vec2.0&#21644;GPT-2&#20043;&#38388;&#30340;&#20849;&#20139;&#35821;&#38899;&#19978;&#19979;&#25991;&#20449;&#24687;&#35299;&#37322;&#20102;&#22823;&#33041;&#27963;&#21160;&#20013;&#30340;&#22823;&#37096;&#20998;&#21464;&#24322;&#65292;&#36229;&#36807;&#20102;&#38745;&#24577;&#35821;&#20041;&#21644;&#36739;&#20302;&#32423;&#30340;&#22768;&#38899;-&#38899;&#20301;&#20449;&#24687;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;SSL&#27169;&#22411;&#20013;&#35821;&#38899;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#35821;&#38899;&#30693;&#35273;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech percept
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#21333;&#29420;&#35843;&#25972;&#27599;&#31181;&#35821;&#35328;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#37319;&#29992;&#19979;&#37319;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35843;&#25972;&#21487;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.08958</link><description>&lt;p&gt;
&#21333;&#35821;&#25110;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#65306;&#21738;&#31181;&#26041;&#24335;&#26356;&#36866;&#21512;alpaca&#65311;
&lt;/p&gt;
&lt;p&gt;
Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08958
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#21333;&#29420;&#35843;&#25972;&#27599;&#31181;&#35821;&#35328;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#37319;&#29992;&#19979;&#37319;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35843;&#25972;&#21487;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26469;&#25191;&#34892;&#24320;&#25918;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#32842;&#22825;&#21161;&#25163;&#31561;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#31867;&#21162;&#21147;&#36890;&#24120;&#21482;&#22312;&#21333;&#19968;&#35821;&#35328;&#20013;&#36827;&#34892;&#65292;&#20294;&#25105;&#20204;&#23454;&#35777;&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#25104;&#26412;&#25928;&#30410;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;Alpaca&#25968;&#25454;&#38598;&#21644;&#20854;&#20013;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#24418;&#25104;&#22810;&#35821;&#35328;&#35843;&#25972;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#37319;&#29992;&#20302;&#31209;&#35843;&#25972;&#25110;&#23436;&#20840;&#21442;&#25968;&#35757;&#32451;&#30340;&#26041;&#24335;&#23545;LLM&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#21463;&#25511;&#31639;&#21147;&#39044;&#31639;&#19979;&#30340;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27599;&#31181;&#35821;&#35328;&#21333;&#29420;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19979;&#37319;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#29978;&#33267;&#26356;&#24378;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26469;&#25193;&#23637;&#35821;&#35328;&#25903;&#25345;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPCL&#30340;&#26041;&#27861;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#21033;&#29992;&#22810;&#35821;&#35328;&#27491;&#20363;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MPCL&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2309.08929</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#35821;&#35328;&#27491;&#20363;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#25552;&#21319;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPCL&#30340;&#26041;&#27861;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#21033;&#29992;&#22810;&#35821;&#35328;&#27491;&#20363;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MPCL&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#12290;&#26368;&#36817;&#23398;&#20064;&#21333;&#35821;&#21644;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#30340;&#36235;&#21183;&#20027;&#35201;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#38170;&#28857;&#12289;&#19968;&#20010;&#27491;&#20363;&#21644;&#22810;&#20010;&#36127;&#20363;&#12290;&#26412;&#25991;&#35748;&#20026;&#24212;&#35813;&#32771;&#34385;&#21033;&#29992;&#22810;&#20010;&#27491;&#20363;&#26469;&#25913;&#36827;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#22240;&#20026;&#65288;1&#65289;&#19981;&#21516;&#35821;&#35328;&#30340;&#27491;&#20363;&#21487;&#20197;&#26377;&#30410;&#20110;&#36328;&#35821;&#35328;&#23398;&#20064;&#65292;&#65288;2&#65289;&#22810;&#20010;&#27491;&#20363;&#20043;&#38388;&#30340;&#20256;&#36882;&#30456;&#20284;&#24615;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26500;&#20449;&#24687;&#29992;&#20110;&#23398;&#20064;&#12290;&#20026;&#20102;&#30740;&#31350;&#23545;&#27604;&#23398;&#20064;&#20013;&#22810;&#20010;&#27491;&#20363;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPCL&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#22810;&#20010;&#27491;&#20363;&#26469;&#25552;&#21319;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21508;&#31181;&#20027;&#24178;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MPCL&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multi-lingual sentence embeddings is a fundamental task in natural language processing. Recent trends in learning both mono-lingual and multi-lingual sentence embeddings are mainly based on contrastive learning (CL) among an anchor, one positive, and multiple negative instances. In this work, we argue that leveraging multiple positives should be considered for multi-lingual sentence embeddings because (1) positives in a diverse set of languages can benefit cross-lingual learning, and (2) transitive similarity across multiple positives can provide reliable structural information for learning. In order to investigate the impact of multiple positives in CL, we propose a novel approach, named MPCL, to effectively utilize multiple positive instances to improve the learning of multi-lingual sentence embeddings. Experimental results on various backbone models and downstream tasks demonstrate that MPCL leads to better retrieval, semantic similarity, and classification performances com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#20998;&#32423;&#20851;&#31995;&#30340;&#26080;&#24773;&#22522;&#20934;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22635;&#34917;&#65292;&#20197;&#23545;&#23454;&#20307;&#23545;&#26681;&#25454;&#20854;&#28385;&#36275;&#31243;&#24230;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#23884;&#20837;&#31574;&#30053;&#21644;&#22810;&#20010;LLM&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2305.15002</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#20998;&#32423;&#20851;&#31995;&#30340;&#26080;&#24773;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A RelEntLess Benchmark for Modelling Graded Relations between Named Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#20998;&#32423;&#20851;&#31995;&#30340;&#26080;&#24773;&#22522;&#20934;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22635;&#34917;&#65292;&#20197;&#23545;&#23454;&#20307;&#23545;&#26681;&#25454;&#20854;&#28385;&#36275;&#31243;&#24230;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#23884;&#20837;&#31574;&#30053;&#21644;&#22810;&#20010;LLM&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;&#8220;&#21463;&#24433;&#21709;&#20110;&#8221;&#12289;&#8220;&#20197;...&#38395;&#21517;&#8221;&#25110;&#8220;&#19982;...&#31454;&#20105;&#8221;&#20043;&#31867;&#30340;&#20851;&#31995;&#26412;&#36136;&#19978;&#26159;&#20998;&#32423;&#30340;&#65306;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#23454;&#20307;&#23545;&#28385;&#36275;&#36825;&#20123;&#20851;&#31995;&#30340;&#31243;&#24230;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#65292;&#20294;&#24456;&#38590;&#23558;&#28385;&#36275;&#21644;&#19981;&#28385;&#36275;&#36825;&#20123;&#20851;&#31995;&#30340;&#23454;&#20307;&#23545;&#21010;&#20998;&#24320;&#12290;&#36825;&#26679;&#30340;&#20998;&#32423;&#20851;&#31995;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#19981;&#21253;&#21547;&#27492;&#31867;&#20851;&#31995;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#23454;&#20307;&#23545;&#24517;&#39035;&#26681;&#25454;&#20854;&#28385;&#36275;&#32473;&#23450;&#20998;&#32423;&#20851;&#31995;&#30340;&#31243;&#24230;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#20219;&#21153;&#34987;&#23450;&#20041;&#20026;&#23569;&#26679;&#26412;&#25490;&#24207;&#38382;&#39064;&#65292;&#27169;&#22411;&#21482;&#33021;&#35775;&#38382;&#20851;&#31995;&#30340;&#25551;&#36848;&#21644;&#20116;&#20010;&#21407;&#22411;&#23454;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#23884;&#20837;&#31574;&#30053;&#20197;&#21450;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#21253;&#25324;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#21644;&#23553;&#38381;&#27169;&#22411;&#65292;&#20363;&#22914;GPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relations such as "is influenced by", "is known for" or "is a competitor of" are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several recent LLMs, covering both publicly available LLMs and closed models such as GPT-4. Overall, we find a stro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#32431;&#35821;&#35328;&#25688;&#35201;&#30340;&#25351;&#26631;&#27979;&#35797;&#24179;&#21488;APPLS&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;POMME&#26469;&#35780;&#20272;PLS&#20013;&#30340;&#25991;&#26412;&#31616;&#21270;&#12290;&#36890;&#36807;&#23545;&#25351;&#26631;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25351;&#26631;&#26410;&#33021;&#22987;&#32456;&#25429;&#25417;&#21040;&#31616;&#21270;&#24230;&#12290;</title><link>https://arxiv.org/abs/2305.14341</link><description>&lt;p&gt;
APPLS: &#35780;&#20272;&#32431;&#35821;&#35328;&#25688;&#35201;&#30340;&#35780;&#20215;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
APPLS: Evaluating Evaluation Metrics for Plain Language Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#32431;&#35821;&#35328;&#25688;&#35201;&#30340;&#25351;&#26631;&#27979;&#35797;&#24179;&#21488;APPLS&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;POMME&#26469;&#35780;&#20272;PLS&#20013;&#30340;&#25991;&#26412;&#31616;&#21270;&#12290;&#36890;&#36807;&#23545;&#25351;&#26631;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25351;&#26631;&#26410;&#33021;&#22987;&#32456;&#25429;&#25417;&#21040;&#31616;&#21270;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#32431;&#35821;&#35328;&#25688;&#35201;&#65288;PLS&#65289;&#30340;&#27169;&#22411;&#26377;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;PLS&#32570;&#20047;&#19987;&#38376;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#29420;&#29305;&#30340;&#36716;&#25442;&#65288;&#20363;&#22914;&#65292;&#28155;&#21152;&#32972;&#26223;&#35299;&#37322;&#65292;&#21024;&#38500;&#19987;&#19994;&#26415;&#35821;&#65289;&#65292;&#22240;&#27492;&#23545;&#20110;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#30340;&#36866;&#29992;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#20803;&#35780;&#20272;&#27979;&#35797;&#24179;&#21488;APPLS&#65292;&#26088;&#22312;&#35780;&#20272;PLS&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#26681;&#25454;&#20808;&#21069;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#23450;&#20041;&#20102;&#22235;&#20010;&#26631;&#20934;&#19978;&#30340;&#19968;&#32452;&#25200;&#21160;&#65292;PLS&#25351;&#26631;&#24212;&#35813;&#25429;&#25417;&#21040;&#65306;&#20449;&#24687;&#24615;&#12289;&#31616;&#21270;&#24230;&#12289;&#36830;&#36143;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#23545;&#25351;&#26631;&#36827;&#34892;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25351;&#26631;&#26410;&#33021;&#22987;&#32456;&#25429;&#25417;&#21040;&#31616;&#21270;&#24230;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;POMME&#65292;&#26088;&#22312;&#35780;&#20272;PLS&#20013;&#25991;&#26412;&#31616;&#21270;&#65307;&#35813;&#25351;&#26631;&#26159;&#26681;&#25454;&#22495;&#20869;&#21644;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26631;&#20934;&#21270;&#22256;&#24785;&#24230;&#24046;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;POMME&#30340;&#25928;&#26524;&#65292;&#24182;&#19982;&#20854;&#20182;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing specialized terminology). To address these concerns, our study presents a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We define a set of perturbations along four criteria inspired by previous work that a PLS metric should capture: informativeness, simplification, coherence, and faithfulness. An analysis of metrics using our testbed reveals that current metrics fail to capture simplification consistently. In response, we introduce POMME, a new metric designed to assess text simplification in PLS; the metric is calculated as the normalized perplexity difference between an in-domain and out-of-domain language model. We demonstrate P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;$\mu$PLAN&#65292;&#19968;&#31181;&#36328;&#35821;&#35328;&#25688;&#35201;&#26041;&#27861;&#65292;&#20351;&#29992;&#20869;&#23481;&#35745;&#21010;&#20316;&#20026;&#36328;&#35821;&#35328;&#26725;&#26753;&#12290;&#36890;&#36807;&#23558;&#35745;&#21010;&#25277;&#35937;&#20026;&#19968;&#31995;&#21015;&#23454;&#20307;&#65292;&#27492;&#26041;&#27861;&#22312;&#22235;&#31181;&#35821;&#35328;&#23545;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2305.14205</link><description>&lt;p&gt;
$\mu$PLAN&#65306;&#20351;&#29992;&#20869;&#23481;&#35745;&#21010;&#20316;&#20026;&#36328;&#35821;&#35328;&#26725;&#26753;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
$\mu$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;$\mu$PLAN&#65292;&#19968;&#31181;&#36328;&#35821;&#35328;&#25688;&#35201;&#26041;&#27861;&#65292;&#20351;&#29992;&#20869;&#23481;&#35745;&#21010;&#20316;&#20026;&#36328;&#35821;&#35328;&#26725;&#26753;&#12290;&#36890;&#36807;&#23558;&#35745;&#21010;&#25277;&#35937;&#20026;&#19968;&#31995;&#21015;&#23454;&#20307;&#65292;&#27492;&#26041;&#27861;&#22312;&#22235;&#31181;&#35821;&#35328;&#23545;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#25688;&#35201;&#26159;&#25351;&#22312;&#32473;&#23450;&#19981;&#21516;&#35821;&#35328;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#20221;&#25688;&#35201;&#65292;&#20197;&#20415;&#23558;&#30456;&#20851;&#20869;&#23481;&#20256;&#25773;&#32473;&#20854;&#20182;&#35821;&#35328;&#30340;&#35762;&#32773;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#25361;&#25112;&#20027;&#35201;&#22312;&#20110;&#36328;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#21294;&#20047;&#21644;&#25688;&#35201;&#21644;&#32763;&#35793;&#30340;&#22797;&#21512;&#38590;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\mu$PLAN&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;&#35745;&#21010;&#27493;&#39588;&#20316;&#20026;&#36328;&#35821;&#35328;&#26725;&#26753;&#12290;&#25105;&#20204;&#23558;&#35745;&#21010;&#24418;&#24335;&#21270;&#20026;&#19968;&#31995;&#21015;&#23454;&#20307;&#65292;&#25429;&#25417;&#25688;&#35201;&#30340;&#20869;&#23481;&#21644;&#24212;&#35813;&#20256;&#36798;&#30340;&#39034;&#24207;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35745;&#21010;&#25277;&#35937;&#20102;&#34920;&#38754;&#24418;&#24335;&#65306;&#20351;&#29992;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#23558;&#23454;&#20307;&#19982;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#35268;&#33539;&#25351;&#23450;&#23545;&#40784;&#65292;&#24182;&#22312;&#27492;&#36328;&#35821;&#35328;&#26725;&#26753;&#21644;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#25688;&#35201;&#12290;&#22312;XWikis&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65288;&#28085;&#30422;&#22235;&#31181;&#35821;&#35328;&#23545;&#65289;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35268;&#21010;&#30446;&#26631;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual summarization consists of generating a summary in one language given an input document in a different language, allowing for the dissemination of relevant content across speakers of other languages. The task is challenging mainly due to the paucity of cross-lingual datasets and the compounded difficulty of summarizing and translating. This work presents $\mu$PLAN, an approach to cross-lingual summarization that uses an intermediate planning step as a cross-lingual bridge. We formulate the plan as a sequence of entities capturing the summary's content and the order in which it should be communicated. Importantly, our plans abstract from surface form: using a multilingual knowledge base, we align entities to their canonical designation across languages and generate the summary conditioned on this cross-lingual bridge and the input. Automatic and human evaluation on the XWikis dataset (across four language pairs) demonstrates that our planning objective achieves state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#21435;&#22122;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21033;&#29992;&#36719;&#22686;&#24378;&#26631;&#31614;&#21644;&#33258;&#25105;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#26356;&#24178;&#20928;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#26469;&#20445;&#35777;&#22686;&#24378;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2212.10558</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#21363;&#26102;&#21435;&#22122;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
On-the-fly Denoising for Data Augmentation in Natural Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#21435;&#22122;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21033;&#29992;&#36719;&#22686;&#24378;&#26631;&#31614;&#21644;&#33258;&#25105;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#26356;&#24178;&#20928;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#26469;&#20445;&#35777;&#22686;&#24378;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#32463;&#24120;&#34987;&#29992;&#26469;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#24341;&#20837;&#22122;&#22768;&#25968;&#25454;&#26469;&#24178;&#25200;&#35757;&#32451;&#12290;&#20026;&#20102;&#20445;&#35777;&#22686;&#24378;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#22686;&#24378;&#25968;&#25454;&#20013;&#27809;&#26377;&#22122;&#22768;&#65292;&#24182;&#37319;&#29992;&#19968;&#33268;&#24615;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#22914;&#35757;&#32451;&#25439;&#22833;&#21644;&#22810;&#26679;&#24615;&#32422;&#26463;&#65289;&#26469;&#36807;&#28388;&#25481;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34987;&#36807;&#28388;&#30340;&#31034;&#20363;&#21487;&#33021;&#20173;&#28982;&#21253;&#21547;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#23436;&#20840;&#20002;&#24323;&#23427;&#20204;&#20250;&#23548;&#33268;&#30417;&#30563;&#20449;&#21495;&#30340;&#20007;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#27604;&#22686;&#24378;&#25968;&#25454;&#26356;&#24178;&#20928;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#21435;&#22122;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#22312;&#26356;&#24178;&#20928;&#30340;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26377;&#26426;&#25945;&#24072;&#27169;&#22411;&#25552;&#20379;&#30340;&#36719;&#22686;&#24378;&#26631;&#31614;&#36827;&#34892;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#22122;&#22768;&#26631;&#31614;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#25105;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#24378;&#21046;&#27169;&#22411;&#39044;&#27979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically. However, data augmentation may introduce noisy data that impairs training. To guarantee the quality of augmented data, existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out "noisy" data. However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals. In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data. To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#35268;&#21017;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#29983;&#25104;&#27969;&#30021;&#12289;&#20849;&#24773;&#30340;&#23545;&#35805;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#38750;&#20020;&#24202;&#35797;&#39564;&#39564;&#35777;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>https://arxiv.org/abs/2209.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Empathetic AI Coach for Self-Attachment Therapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#35268;&#21017;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#29983;&#25104;&#27969;&#30021;&#12289;&#20849;&#24773;&#30340;&#23545;&#35805;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#38750;&#20020;&#24202;&#35797;&#39564;&#39564;&#35777;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#33258;&#36848;&#30103;&#27861;&#30340;&#25968;&#23383;&#36741;&#23548;&#31995;&#32479;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#23545;&#35805;&#20195;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35782;&#21035;&#29992;&#25143;&#25991;&#26412;&#22238;&#22797;&#20013;&#30340;&#28508;&#22312;&#24773;&#32490;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#26816;&#32034;&#26041;&#27861;&#29983;&#25104;&#26032;&#39062;&#12289;&#27969;&#30021;&#21644;&#20849;&#24773;&#30340;&#35805;&#35821;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#32452;&#31867;&#20284;&#20154;&#31867;&#30340;&#35282;&#33394;&#20379;&#29992;&#25143;&#36873;&#25321;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#34394;&#25311;&#30103;&#27861;&#20250;&#35805;&#20013;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#39033;&#38750;&#20020;&#24202;&#35797;&#39564;&#20013;&#23545;N=16&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#36825;&#20123;&#21442;&#19982;&#32773;&#22312;&#20116;&#22825;&#20869;&#33267;&#23569;&#19982;&#20195;&#29702;&#36827;&#34892;&#20102;&#22235;&#27425;&#20114;&#21160;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#22312;&#20849;&#24773;&#24230;&#12289;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#34987;&#35780;&#20215;&#24471;&#26356;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14440</link><description>&lt;p&gt;
&#35821;&#20041;&#25935;&#24863;&#24615;&#21644;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65306;&#34913;&#37327;NLI&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22522;&#20110;transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#22791;&#23545;&#35789;&#27719;&#21644;&#32452;&#21512;&#35821;&#20041;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#35828;&#27861;&#24212;&#35813;&#25345;&#20445;&#30041;&#24577;&#24230;&#65306;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#23548;&#33268;&#25512;&#26029;&#36807;&#31243;&#20013;&#20986;&#29616;&#22823;&#37327;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20915;&#31574;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#21644;&#28145;&#20837;&#29702;&#35299;&#19981;&#21516;&#65292;&#32780;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#25506;&#31350;&#21477;&#27861;&#12289;&#21333;&#35843;&#24615;&#21644;&#36923;&#36753;&#40065;&#26834;&#24615;&#25512;&#29702;&#26102;&#22343;&#19981;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#35821;&#20041;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#24494;&#23567;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#36755;&#20837;&#22122;&#22768;&#30340;&#23545;&#25239;&#29983;&#25104;&#26679;&#20363;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
&lt;/p&gt;</description></item><item><title>GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12292</link><description>&lt;p&gt;
GRATH: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#28176;&#33258;&#25105;&#30495;&#23454;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12292
&lt;/p&gt;
&lt;p&gt;
GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#30495;&#23454;&#24615;&#23545;&#23427;&#20204;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#29983;&#25104;&#30495;&#23454;&#31572;&#26696;&#21644;&#20869;&#23481;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#22312;TruthfulQA&#31561;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAdual self-truTHifying (GRATH)&#65292;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#25552;&#39640;LLMs&#30495;&#23454;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;GRATH&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#30456;&#24212;&#30340;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;GRATH&#20197;&#26080;&#38656;&#26631;&#27880;&#31572;&#26696;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRATH&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#33258;&#36523;&#29983;&#25104;&#25104;&#23545;&#30495;&#23454;&#24615;&#35757;&#32451;&#25968;&#25454;&#65292;&#27599;&#23545;&#21253;&#21547;&#19968;&#20010;&#38382;&#39064;&#21450;&#20854;&#27491;&#30830;&#21644;&#38169;&#35823;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#31572;&#26696;&#23545;&#30340;&#24046;&#24322;&#20013;&#23398;&#20064;&#12290;&#38543;&#21518;&#65292;GRATH&#36845;&#20195;&#22320;&#20248;&#21270;&#27169;&#22411;&#20197;&#36880;&#28176;&#25552;&#39640;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.08694</link><description>&lt;p&gt;
&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#29992;&#20110;&#28040;&#38500;&#35823;&#20449;&#24687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#30452;&#25509;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35823;&#20449;&#24687;&#28040;&#38500;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#30340;&#26657;&#20934;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26679;&#26412;&#35268;&#27169;&#21644;&#38543;&#26426;&#27700;&#24179;&#30340;&#19968;&#33268;&#24615;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#40065;&#26834;&#30340;&#25968;&#23383;&#21270;&#21475;&#22836;&#25552;&#31034;&#22312;&#21333;&#27493;&#21644;&#20004;&#27493;&#32622;&#20449;&#24230;&#24341;&#23548;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#21644;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#30456;&#21516;&#25552;&#31034;&#22312;&#19981;&#21516;&#29256;&#26412;&#30340;GPT&#21644;&#19981;&#21516;&#25968;&#23383;&#23610;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#25968;&#23383;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20026;GPT&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20703</link><description>&lt;p&gt;
&#24378;&#21270;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#19979;&#28216;&#20219;&#21153;&#23545;&#40784;&#65292;&#21363;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26368;&#22823;&#21270;&#65288;&#21487;&#33021;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#65289;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;RFT&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#20248;&#21270;&#38556;&#30861;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#19979;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#21363;&#20351;&#26399;&#26395;&#22870;&#21169;&#36828;&#31163;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;RFT&#22522;&#20934;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23567;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#23548;&#33268;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#23475;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#26497;&#20854;&#32531;&#24930;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20811;&#26381;RFT&#20013;&#26799;&#24230;&#28040;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#23427;&#22312;RFT&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#30456;&#23545;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;SFT&#38454;&#27573;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
&lt;/p&gt;</description></item><item><title>MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08648</link><description>&lt;p&gt;
MAPLE: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08648
&lt;/p&gt;
&lt;p&gt;
MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#65292;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)&#27169;&#22411;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20934;&#30830;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;MAPLE&#30340;&#33021;&#21147;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;MAPLE&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#24377;&#24615;&#12290;&#23613;&#31649;&#20854;&#20027;&#35201;&#35774;&#35745;&#38754;&#21521;&#24212;&#29992;&#39044;&#27979;&#65292;&#20294;&#32467;&#26524;&#20063;&#24378;&#35843;&#20102;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLM&#22312;&#24212;&#29992;&#20351;&#29992;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24314;&#35758;&#22312;&#24314;&#27169;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#65292;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2307.00162</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#23545;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00162
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#65288;S3Ms&#65289;&#34987;&#24341;&#20837;&#65292;&#20026;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#25552;&#20379;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#25913;&#36827;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;S3Ms&#22312;&#19981;&#21516;&#30340;&#23618;&#20013;&#32534;&#30721;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19988;&#19968;&#20123;S3Ms&#20284;&#20046;&#23398;&#20064;&#20102;&#31867;&#20284;&#20110;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65288;&#22914;&#21333;&#35789;&#65289;&#30340;&#31243;&#24230;&#20197;&#21450;&#21333;&#35789;&#30456;&#20851;&#20449;&#24687;&#30340;&#32534;&#30721;&#20301;&#32622;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#20010;S3Ms&#30340;&#19981;&#21516;&#23618;&#30340;&#21333;&#35789;&#29255;&#27573;&#34920;&#31034;&#36827;&#34892;&#20102;&#22810;&#31181;&#20998;&#26512;&#65306;wav2vec2&#12289;HuBERT&#21644;WavLM&#12290;&#25105;&#20204;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#26469;&#34913;&#37327;&#36825;&#20123;&#34920;&#31034;&#19982;&#21333;&#35789;&#32423;&#35821;&#35328;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#21333;&#35789;&#32423;&#35821;&#35328;&#20869;&#23481;&#24448;&#24448;&#20986;&#29616;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#65292;&#32780;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#65288;&#22914;&#21457;&#38899;&#65289;&#20063;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#22914;&#20309;&#25913;&#21892;&#23545;&#36328;&#24230;&#32423;&#21035;&#32622;&#20449;&#24230;&#30340;&#20272;&#35745;&#12290;&#30740;&#31350;&#21457;&#29616;&#20165;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#24182;&#19981;&#26159;&#26368;&#20339;&#26041;&#27861;&#65292;&#32780;&#21033;&#29992;&#26463;&#25628;&#32034;&#30340;&#21069;k&#20010;&#39044;&#27979;&#30340;&#32479;&#35745;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2212.10767</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#20013;&#25913;&#21892;&#22522;&#20110;&#36328;&#24230;&#32423;&#21035;&#32622;&#20449;&#24230;&#30340;&#26463;&#25628;&#32034;&#65311;(arXiv:2212.10767v2 [cs.CL] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#22914;&#20309;&#25913;&#21892;&#23545;&#36328;&#24230;&#32423;&#21035;&#32622;&#20449;&#24230;&#30340;&#20272;&#35745;&#12290;&#30740;&#31350;&#21457;&#29616;&#20165;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#24182;&#19981;&#26159;&#26368;&#20339;&#26041;&#27861;&#65292;&#32780;&#21033;&#29992;&#26463;&#25628;&#32034;&#30340;&#21069;k&#20010;&#39044;&#27979;&#30340;&#32479;&#35745;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26631;&#35760;&#26159;&#20449;&#24687;&#25277;&#21462;/&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#25104;&#20026;&#36825;&#31867;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#23454;&#20307;&#25552;&#21462;&#21644;&#23545;&#35805;&#27133;&#22635;&#20805;&#65289;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#26631;&#35760;&#20934;&#30830;&#24615;&#19978;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#30340;&#26041;&#38754;&#8212;&#8212;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#29702;&#35299;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#22320;&#34913;&#37327;&#27169;&#22411;&#23545;&#27599;&#20010;&#26631;&#35760;&#36328;&#24230;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20123;&#20851;&#20110;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#30340;&#27169;&#22411;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#23454;&#35777;&#35265;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#24182;&#19981;&#26159;&#23454;&#29616;&#33391;&#22909;&#26657;&#20934;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#8212;&#8212;&#21033;&#29992;&#26463;&#25628;&#32034;&#30340;&#21069;k&#20010;&#39044;&#27979;&#30340;&#32479;&#35745;&#25968;&#25454;&#8212;&#8212;&#26174;&#33879;&#38477;&#20302;&#20102;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence labeling is a core task in text understanding for IE/IR systems. Text generation models have increasingly become the go-to solution for such tasks (e.g., entity extraction and dialog slot filling). While most research has focused on the labeling accuracy, a key aspect -- of vital practical importance -- has slipped through the cracks: understanding model confidence. More specifically, we lack a principled understanding of how to reliably gauge the confidence of a model in its predictions for each labeled span. This paper aims to provide some empirical insights on estimating model confidence for generative sequence labeling. Most notably, we find that simply using the decoder's output probabilities \textbf{is not} the best in realizing well-calibrated confidence estimates. As verified over six public datasets of different tasks, we show that our proposed approach -- which leverages statistics from top-$k$ predictions by a beam search -- significantly reduces calibration errors 
&lt;/p&gt;</description></item></channel></rss>