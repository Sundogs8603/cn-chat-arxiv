<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#38754;&#21521;&#27779;&#27931;&#22827;&#35821;&#65292;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27779;&#27931;&#22827;&#35821;/&#27861;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#23376;&#21333;&#35789;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#23545;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00606</link><description>&lt;p&gt;
&#38754;&#21521;&#22622;&#20869;&#21152;&#23572;&#27779;&#27931;&#22827;&#35821;&#30340;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Low-Resourced Machine Translation for Senegalese Wolof Language. (arXiv:2305.00606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#38754;&#21521;&#27779;&#27931;&#22827;&#35821;&#65292;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27779;&#27931;&#22827;&#35821;/&#27861;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#23376;&#21333;&#35789;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#23545;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30740;&#31350;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#20027;&#35201;&#24800;&#21450;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#21644;&#27861;&#35821;&#12290;&#22823;&#22810;&#25968;&#20854;&#20182;&#35821;&#35328;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#36164;&#28304;&#65292;&#22914;&#25746;&#21704;&#25289;&#20197;&#21335;&#30340;&#38750;&#27954;&#35821;&#35328;&#20043;&#19968;&#30340;&#27779;&#27931;&#22827;&#35821;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21253;&#21547;12.3&#19975;&#20010;&#21477;&#23376;&#30340;&#27779;&#27931;&#22827;&#35821;/&#27861;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#36827;&#34892;&#20102;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#39564;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#37197;&#32622;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#22312;&#23376;&#21333;&#35789;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#23545;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20102;&#24615;&#33021;&#25552;&#21319;&#65292;&#30456;&#27604;&#22312;&#30456;&#21516;&#23454;&#39564;&#26465;&#20214;&#19979;&#20351;&#29992;&#27861;&#35821;-&#27779;&#27931;&#22827;&#35821;&#35328;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) research has made great advancements in recent years with major breakthroughs that have established new benchmarks. However, these advances have mainly benefited a certain group of languages commonly referred to as resource-rich such as English and French. Majority of other languages with weaker resources are then left behind which is the case for most African languages including Wolof. In this work, we present a parallel Wolof/French corpus of 123,000 sentences on which we conducted experiments on machine translation models based on Recurrent Neural Networks (RNN) in different data configurations. We noted performance gains with the models trained on subworded data as well as those trained on the French-English language pair compared to those trained on the French-Wolof pair under the same experimental conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#25361;&#25112;&#24615;&#24773;&#26223;&#65292;&#21363;&#20165;&#20855;&#22791;API&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;API&#36827;&#34892;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#23545;&#25512;&#29702;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.00593</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#26080;&#26799;&#24230;&#21644;&#26080;&#20284;&#28982;&#23545;&#35805;&#24335;&#24314;&#27169;API&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#25361;&#25112;&#24615;&#24773;&#26223;&#65292;&#21363;&#20165;&#20855;&#22791;API&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;API&#36827;&#34892;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#23545;&#25512;&#29702;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#25110;&#21830;&#19994;&#38480;&#21046;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;API&#25552;&#20379;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26082;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#20063;&#26080;&#27861;&#36890;&#36807;&#23427;&#20256;&#25773;&#26799;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#21482;&#26377;API&#35775;&#38382;&#26435;&#38480;&#30340;PLM&#30340;&#33258;&#36866;&#24212;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26368;&#36817;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22312;&#19981;&#38656;&#35201;&#35745;&#31639;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#36719;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#25216;&#26415;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;PLM&#38500;&#20102;&#36755;&#20837;&#23884;&#20837;&#20043;&#22806;&#30340;&#20219;&#20309;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25552;&#31034;&#30340;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#23545;&#25512;&#29702;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#36825;&#26159;&#22312;&#20165;&#20855;&#26377;API&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#32771;&#34385;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20180;&#32454;&#26816;&#26597;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26799;&#24230;&#21644;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#20449;&#24687;&#19982;vanilla self-attention&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.00581</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#22810;&#27169;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#20449;&#24687;&#19982;vanilla self-attention&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#26159;&#26263;&#31034;&#24615;&#22320;&#23398;&#20064;&#24222;&#22823;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#21033;&#29992;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32467;&#26500;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#25972;&#21512;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#20294;&#19982;Transformer&#27169;&#22411;&#20960;&#20046;&#26080;&#27861;&#31454;&#20105;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#20004;&#20010;&#19990;&#30028;&#20013;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#38656;&#35201;&#22312;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28041;&#21450;&#22270;&#24418;&#30340;&#21363;&#25554;&#21363;&#29992;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#23558;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#20013;&#33719;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#20449;&#24687;&#24182;&#20837; vanilla self-attention &#20013;&#20316;&#20026;&#26377;&#25928;&#20808;&#39564;&#30693;&#35782;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#25991;&#26412;&#22270;&#12289;&#23494;&#38598;&#21306;&#22495;&#22270;&#21644;&#35821;&#20041;&#22270;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#36755;&#20837;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29305;&#24449;&#32452;&#21512;&#20197;&#25191;&#34892;&#19979;&#28216;&#25512;&#29702;&#12290;&#20351;&#29992;&#22270;&#24418;&#20449;&#24687;&#26469;&#35268;&#33539;self-attention&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;(VQA&#21644;GQA)&#19978;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26522;&#20030;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#29983;&#25104;&#30340;&#26641;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23545;&#33258;&#28982;&#36923;&#36753;&#35821;&#35328;&#20013;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#32534;&#21495;&#65292;&#24182;&#21487;&#33021;&#25193;&#23637;&#21040;&#20854;&#20182;&#32452;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00522</link><description>&lt;p&gt;
&#22914;&#20309;&#20174;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#20013;&#26522;&#20030;&#26641;
&lt;/p&gt;
&lt;p&gt;
How to enumerate trees from a context-free grammar. (arXiv:2305.00522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26522;&#20030;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#29983;&#25104;&#30340;&#26641;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23545;&#33258;&#28982;&#36923;&#36753;&#35821;&#35328;&#20013;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#32534;&#21495;&#65292;&#24182;&#21487;&#33021;&#25193;&#23637;&#21040;&#20854;&#20182;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26522;&#20030;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#29983;&#25104;&#30340;&#26641;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#37197;&#23545;&#20989;&#25968;&#23558;CFG&#25512;&#23548;&#21644;&#33258;&#28982;&#25968;&#24418;&#25104;&#19968;&#19968;&#26144;&#23556;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#35745;&#25968;&#21807;&#19968;&#35299;&#30721;&#26641;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23545;&#33258;&#28982;&#36923;&#36753;&#35821;&#35328;&#20013;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#32534;&#21495;&#65292;&#24182;&#19988;&#21487;&#33021;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27492;&#31639;&#27861;&#25512;&#24191;&#21040;&#26356;&#19968;&#33324;&#30340;&#27966;&#29983;&#24418;&#24335;&#65292;&#21253;&#25324;&#26641;&#19978;Lempel-Ziv&#32534;&#30721;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
I present a simple algorithm for enumerating the trees generated by a Context Free Grammar (CFG). The algorithm uses a pairing function to form a bijection between CFG derivations and natural numbers, so that trees can be uniquely decoded from counting. This provides a general way to number expressions in natural logical languages, and potentially can be extended to other combinatorial problems. I also show how this algorithm may be generalized to more general forms of derivation, including analogs of Lempel-Ziv coding on trees.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SMILE&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT&#23558;&#20844;&#20849;&#21333;&#36718;&#23545;&#35805;&#25193;&#23637;&#20026;&#22810;&#36718;&#23545;&#35805;&#65292;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#25509;&#36817;&#30495;&#23454;&#29983;&#27963;&#30340;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#19987;&#38376;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.00450</link><description>&lt;p&gt;
SMILE&#65306;&#21033;&#29992;ChatGPT&#23454;&#29616;&#21333;&#36718;&#21040;&#22810;&#36718;&#21253;&#23481;&#24615;&#35821;&#35328;&#25193;&#23637;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support. (arXiv:2305.00450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SMILE&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT&#23558;&#20844;&#20849;&#21333;&#36718;&#23545;&#35805;&#25193;&#23637;&#20026;&#22810;&#36718;&#23545;&#35805;&#65292;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#25509;&#36817;&#30495;&#23454;&#29983;&#27963;&#30340;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#19987;&#38376;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19987;&#38376;&#30340;&#23545;&#35805;&#31995;&#32479;&#20197;&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#24050;&#25104;&#20026;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20154;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SMILE&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;ChatGPT&#23558;&#20844;&#20849;&#21333;&#36718;&#23545;&#35805;&#25193;&#23637;&#20026;&#22810;&#36718;&#23545;&#35805;&#30340;&#21253;&#23481;&#24615;&#35821;&#35328;&#25193;&#23637;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;SMILE&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#21644;&#26410;&#20351;&#29992;SMILE&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#31995;&#32479;&#30340;&#23545;&#27604;&#20998;&#26512;&#65292;&#35777;&#26126;SMILE&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#25509;&#36817;&#30495;&#23454;&#29983;&#27963;&#30340;&#22810;&#36718;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#23545;&#35805;&#20027;&#39064;&#12289;&#35789;&#27719;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#19987;&#38376;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an increasing research interest in developing specialized dialogue systems that can offer mental health support. However, gathering large-scale and real-life multi-turn conversations for mental health support poses challenges due to the sensitivity of personal information, as well as the time and cost involved. To address these issues, we introduce the SMILE approach, an inclusive language expansion technique that employs ChatGPT to extend public single-turn dialogues into multi-turn ones. Our research first presents a preliminary exploratory study that validates the effectiveness of the SMILE approach. Furthermore, we conduct a comprehensive and systematic contrastive analysis of datasets generated with and without the SMILE approach, demonstrating that the SMILE method results in a large-scale, diverse, and close-to-real-life multi-turn mental health support conversation corpus, including dialog topics, lexical and semantic features. Finally, we use the collected corpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#30001;50&#21517;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#25552;&#20379;&#33521;&#35821;&#65288;L2&#65289;&#21465;&#36848;&#65292;&#20026;&#31532;&#20108;&#35821;&#35328;&#25945;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00446</link><description>&lt;p&gt;
&#26500;&#24314;&#19968;&#20221;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65306;&#32534;&#21046;&#21644;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale. (arXiv:2305.00446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#30001;50&#21517;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#25552;&#20379;&#33521;&#35821;&#65288;L2&#65289;&#21465;&#36848;&#65292;&#20026;&#31532;&#20108;&#35821;&#35328;&#25945;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;50&#20301;5-6&#23681;&#30340;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#30340;&#21465;&#36848;&#12290;&#25552;&#20379;&#20102;&#24635;&#35745;6.5&#23567;&#26102;&#30340;&#33521;&#35821;&#65288;L2&#65289;&#21465;&#36848;&#29702;&#35299;&#27979;&#35797;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#20197;&#21450;&#35780;&#20998;&#21644;&#35821;&#27861;&#21450;&#21457;&#38899;&#38169;&#35823;&#30340;&#27880;&#37322;&#12290;&#36825;&#20123;&#23401;&#23376;&#36824;&#23436;&#25104;&#20102;&#20013;&#25991;&#65288;L1&#65289;&#30340;&#24179;&#34892;MAIN&#27979;&#35797;&#20197;&#20415;&#36827;&#34892;&#21442;&#32771;&#12290;&#23545;&#20110;&#25152;&#26377;&#27979;&#35797;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#65292;&#24182;&#37319;&#29992;&#20102;&#33258;&#20027;&#24320;&#21457;&#30340;&#36828;&#31243;&#25910;&#38598;&#26041;&#27861;&#12290;&#35270;&#39057;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#36731;&#22312;&#36716;&#24405;&#36807;&#31243;&#20013;&#30001;&#24180;&#24188;&#20799;&#31461;&#30340;L2&#21465;&#36848;&#20302;&#21487;&#25026;&#24615;&#25152;&#24102;&#26469;&#30340;&#38590;&#24230;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#31532;&#20108;&#35821;&#35328;&#25945;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a non-native speech corpus consisting of narratives from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5 hours of children taking a narrative comprehension test in English (L2) are presented, along with human-rated scores and annotations of grammatical and pronunciation errors. The children also completed the parallel MAIN tests in Chinese (L1) for reference purposes. For all tests we recorded audio and video with our innovative self-developed remote collection methods. The video recordings serve to mitigate the challenge of low intelligibility in L2 narratives produced by young children during the transcription process. This corpus offers valuable resources for second language teaching and has the potential to enhance the overall performance of automatic speech recognition (ASR).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00382</link><description>&lt;p&gt;
&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20363;&#22914;&#28431;&#27934;&#35780;&#20272;&#21644;&#23041;&#32961;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12289;&#20197;&#21450;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#21644;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#23454;&#20307;&#39044;&#27979;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#31185;&#23398;&#34920;&#26684;&#30340; EL &#25968;&#25454;&#38598; S2abEL&#65292;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#30001;&#20110;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#35821;&#22659;&#24433;&#21709;&#65292;&#31185;&#23398;&#34920;&#26684;&#19978;&#30340; EL &#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340; EL&#65292;&#21253;&#21547;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00366</link><description>&lt;p&gt;
S2abEL&#65306;&#19968;&#20221;&#29992;&#20110;&#31185;&#23398;&#34920;&#26684;&#23454;&#20307;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
S2abEL: A Dataset for Entity Linking from Scientific Tables. (arXiv:2305.00366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#31185;&#23398;&#34920;&#26684;&#30340; EL &#25968;&#25454;&#38598; S2abEL&#65292;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#30001;&#20110;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#35821;&#22659;&#24433;&#21709;&#65292;&#31185;&#23398;&#34920;&#26684;&#19978;&#30340; EL &#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340; EL&#65292;&#21253;&#21547;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#23558;&#25991;&#26412;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30456;&#24212;&#26465;&#30446;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#24212;&#29992;&#20110;&#31185;&#23398;&#35770;&#25991;&#20013;&#30340;&#34920;&#26684;&#26102;&#65292;EL&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19968;&#27493;&#65292;&#36825;&#21487;&#20197;&#23454;&#29616;&#20808;&#36827;&#30340;&#31185;&#23398;&#38382;&#31572;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#31185;&#23398;&#34920;&#26684;&#20013;&#30340;EL&#30340;&#25968;&#25454;&#38598;&#12290;&#31185;&#23398;&#34920;&#26684;&#30340;EL&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#31185;&#23398;&#30693;&#35782;&#24211;&#21487;&#33021;&#38750;&#24120;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#29702;&#35299;&#35770;&#25991;&#20013;&#30340;&#25991;&#26412;&#20197;&#21450;&#34920;&#26684;&#30340;&#19978;&#19979;&#25991;&#26469;&#28040;&#38500;&#27495;&#20041;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;S2abEL&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340;EL&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;PaperswithCode&#20998;&#31867;&#27861;&#30340;8,429&#20010;&#21333;&#20803;&#26684;&#30340;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#26469;&#28304;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#34920;&#26684;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#35768;&#22810;&#30693;&#35782;&#24211;&#20043;&#22806;&#25552;&#21450;&#30340;&#23454;&#20307;&#65292;&#24182;&#26174;&#31034;&#23427;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00350</link><description>&lt;p&gt;
POUF: &#38754;&#21521;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models. (arXiv:2305.00350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21464;&#24471;&#26356;&#21152;&#34920;&#29616;&#20986;&#33394;&#21644;&#24378;&#22823;&#12290;&#34429;&#28982;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#38646;-shot &#33021;&#21147;&#65292;&#20294;&#36890;&#24120;&#20173;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#30452;&#25509;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#25110;&#25552;&#31034;&#12290;&#25105;&#20204;&#28436;&#31034;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#35328;&#22686;&#24378;&#30340;&#35270;&#35273;&#21644;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#20174;&#25552;&#31034;&#21644;&#30446;&#26631;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#31163;&#25955;&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312; 13 &#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#21644; 15 &#20010;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22343;&#27604;&#22522;&#32447;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 language-related ones, the proposed approach achieves consistent improvements over the baselines.
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#34920;&#24847;&#30340;&#35748;&#30693;&#26426;&#21046;&#35299;&#37322;&#20026;&#20160;&#20040;&#35821;&#35328;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;</title><link>http://arxiv.org/abs/2305.00296</link><description>&lt;p&gt;
&#31526;&#21495;&#34920;&#24847;&#30340;&#35748;&#30693;&#35299;&#37322;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Account of the Puzzle of Ideography. (arXiv:2305.00296v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00296
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#34920;&#24847;&#30340;&#35748;&#30693;&#26426;&#21046;&#35299;&#37322;&#20026;&#20160;&#20040;&#35821;&#35328;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#23545;Morin&#30340;&#12298;&#31526;&#21495;&#34920;&#24847;&#30340;&#35868;&#39064;&#12299;&#30340;&#35780;&#35770;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#34920;&#24847;&#31526;&#21495;&#35868;&#39064;&#30340;&#35748;&#30693;&#26032;&#35299;&#37322;&#65292;&#23427;&#34917;&#20805;&#20102;Morin&#30340;&#35268;&#33539;&#21270;&#35299;&#37322;&#12290;&#26377;&#25928;&#30340;&#21475;&#35821;&#35821;&#35328;&#35268;&#33539;&#21270;&#34987;&#29616;&#35937;&#23398;&#22320;&#24402;&#22240;&#20026;&#27169;&#24577;&#25928;&#24212;&#19982;&#35748;&#30693;&#34920;&#24449;&#30340;&#22359;&#29366;&#21270;&#65292;&#36827;&#19968;&#27493;&#21463;&#22810;&#24863;&#23448;&#25972;&#21512;&#21644;&#27880;&#24847;&#21147;&#20018;&#34892;&#26412;&#36136;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#35748;&#30693;&#26426;&#21046;&#23545;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#35821;&#35328;&#22312;&#36890;&#29992;&#20154;&#31867;&#20132;&#27969;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this commentary article to 'The Puzzle of Ideography' by Morin, we put forth a new cognitive account of the puzzle of ideography, that complements the standardization account of Morin. Efficient standardization of spoken language is phenomenologically attributed to a modality effect coupled with chunking of cognitive representations, further aided by multi-sensory integration and the serialized nature of attention. These cognitive mechanisms are crucial for explaining why languages dominate graphic codes for general-purpose human communication.
&lt;/p&gt;</description></item><item><title>HiDialog&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#21644;&#25552;&#20986;&#36718;&#27425;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#26469;&#24314;&#27169;&#19981;&#21516;&#36718;&#27425;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#24322;&#26500;&#22270;&#27169;&#22359;&#26469;&#20248;&#21270;&#25152;&#23398;&#30340;&#23884;&#20837;&#12290;&#22312;&#23545;&#35805;&#20851;&#31995;&#25552;&#21462;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;HiDialog&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00262</link><description>&lt;p&gt;
&#24102;&#26377;&#29305;&#27530;&#26631;&#35760;&#21644;&#36718;&#27425;&#32423;&#21035;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention. (arXiv:2305.00262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00262
&lt;/p&gt;
&lt;p&gt;
HiDialog&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#21644;&#25552;&#20986;&#36718;&#27425;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#26469;&#24314;&#27169;&#19981;&#21516;&#36718;&#27425;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#24322;&#26500;&#22270;&#27169;&#22359;&#26469;&#20248;&#21270;&#25152;&#23398;&#30340;&#23884;&#20837;&#12290;&#22312;&#23545;&#35805;&#20851;&#31995;&#25552;&#21462;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;HiDialog&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#26631;&#20934;&#25991;&#26412;&#65292;&#26426;&#22120;&#29702;&#35299;&#23545;&#35805;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#36718;&#27425;&#20013;&#35821;&#20041;&#30340;&#21160;&#24577;&#21644;&#24847;&#22806;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#19981;&#19968;&#33268;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20998;&#23618;&#23545;&#35805;&#29702;&#35299;&#27169;&#22411;HiDialog&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#23545;&#35805;&#20013;&#25554;&#20837;&#22810;&#20010;&#29305;&#27530;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#36718;&#27425;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#26469;&#23618;&#27425;&#23398;&#20064;&#36718;&#27425;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#24322;&#26500;&#22270;&#27169;&#22359;&#26469;&#20248;&#21270;&#25152;&#23398;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#35805;&#20851;&#31995;&#25552;&#21462;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#20197;&#19978;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#28304;&#20195;&#30721;&#37117;&#20844;&#24320;&#22312;https://github.com/ShawX825/HiDialog&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared with standard text, understanding dialogue is more challenging for machines as the dynamic and unexpected semantic changes in each turn. To model such inconsistent semantics, we propose a simple but effective Hierarchical Dialogue Understanding model, HiDialog. Specifically, we first insert multiple special tokens into a dialogue and propose the turn-level attention to learn turn embeddings hierarchically. Then, a heterogeneous graph module is leveraged to polish the learned embeddings. We evaluate our model on various dialogue understanding tasks including dialogue relation extraction, dialogue emotion recognition, and dialogue act classification. Results show that our simple approach achieves state-of-the-art performance on all three tasks above. All our source code is publicly available at https://github.com/ShawX825/HiDialog.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00217</link><description>&lt;p&gt;
&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#24180;&#65289;&#30340;&#22238;&#24212;&#65306;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#35777;&#26126;&#38750;&#27597;&#35821;&#29992;&#25143;&#27604;&#20363;&#23545;&#35821;&#35328;&#22797;&#26434;&#24230;&#26377;&#24433;&#21709;&#65288;arXiv:2305.00217v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus &amp; Walkden (2023). (arXiv:2305.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#12298;&#35821;&#35328;&#36827;&#21270;&#26434;&#24535;&#12299;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;https://doi.org/10.1093/jole/lzad005&#65292;KEW&#65289;&#25361;&#25112;&#20102;&#25105;&#22312;&#19968;&#31687;&#35770;&#25991;&#20013;&#65288;Koplenig&#65292;Royal Society Open Science&#65292;6&#65292;181274&#65288;2019&#65289;&#65292;https://doi.org/10.1098/rsos.181274&#65289;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#35797;&#22270;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#34920;&#26126;&#22823;&#37327;L2&#65288;&#31532;&#20108;&#35821;&#35328;&#65289;&#29992;&#25143;&#20284;&#20046;&#19981;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#65288;&#35821;&#27861;&#25110;&#32479;&#35745;&#65289;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#19987;&#27880;&#20110;Ethnologue&#35780;&#20272;&#35821;&#35328;&#22320;&#20301;&#30340;&#26041;&#24335;&#65306;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#38500;&#20102;&#34987;L1&#65288;&#31532;&#19968;&#35821;&#35328;&#65289;&#20351;&#29992;&#32773;&#20043;&#22806;&#65292;&#36824;&#24212;&#35813;&#26377;&#22823;&#37327;&#30340;L2&#20351;&#29992;&#32773;&#65292;&#37027;&#20040;&#35813;&#35821;&#35328;&#23601;&#34987;&#25551;&#36848;&#20026;&#20256;&#25773;&#24615;&#30340;&#12290;KEW&#25209;&#35780;&#20102;&#23558;&#20256;&#25773;&#24615;&#20316;&#20026;&#35821;&#35328;&#26159;&#21542;&#25317;&#26377;&#22823;&#37327;L2&#20351;&#29992;&#32773;&#65288;&#20108;&#20803;&#65289;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#22312;&#30452;&#25509;&#20272;&#35745;L2&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;L2&#29992;&#25143;&#27604;&#20363;&#24402;&#20026;&#38750;&#20256;&#25773;&#24615;&#35821;&#35328;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#27431;&#27954;&#20256;&#32479;&#26032;&#38395;&#23186;&#20307;&#22312;&#21453;&#23545;Covid-19&#25239;&#30123;&#25298;&#32477;&#30123;&#33495;&#36816;&#21160;&#21644;&#30456;&#20851;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#27954;&#39640;&#36136;&#37327;&#23186;&#20307;&#31215;&#26497;&#21453;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#23545;&#25298;&#32477;&#30123;&#33495;&#30340;&#36235;&#21183;&#25345;&#25209;&#35780;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00182</link><description>&lt;p&gt;
&#30740;&#31350;&#27431;&#27954;&#26032;&#38395;&#22914;&#20309;&#25253;&#36947; Covid-19 &#25239;&#30123;&#25298;&#32477;&#30123;&#33495;&#36816;&#21160;&#65306;&#22522;&#20110; NLP &#30340;&#26694;&#26550;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Examining European Press Coverage of the Covid-19 No-Vax Movement: An NLP Framework. (arXiv:2305.00182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#27431;&#27954;&#20256;&#32479;&#26032;&#38395;&#23186;&#20307;&#22312;&#21453;&#23545;Covid-19&#25239;&#30123;&#25298;&#32477;&#30123;&#33495;&#36816;&#21160;&#21644;&#30456;&#20851;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#27954;&#39640;&#36136;&#37327;&#23186;&#20307;&#31215;&#26497;&#21453;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#23545;&#25298;&#32477;&#30123;&#33495;&#30340;&#36235;&#21183;&#25345;&#25209;&#35780;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#27431;&#27954;&#26032;&#38395;&#23186;&#20307;&#22914;&#20309;&#22788;&#29702;&#25239;&#30123;&#25298;&#32477;&#30123;&#33495;&#36816;&#21160;&#65292;&#20197;&#21450;&#19982;&#35813;&#36816;&#21160;&#30456;&#20851;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#20351;&#29992;22&#20010;&#26376;&#26399;&#38388;&#65288;2020-2021&#24180;&#65289;&#26469;&#33258;19&#23478;&#27431;&#27954;&#25253;&#32440;&#30340;1786&#31687;&#25991;&#31456;&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;&#20027;&#39064;&#24314;&#27169;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#35821;&#20041;&#20851;&#31995;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20197;&#21450;&#35821;&#20041;&#32593;&#32476;&#31561;&#65292;&#20197;&#20102;&#35299;&#27431;&#27954;&#20256;&#32479;&#26032;&#38395;&#23186;&#20307;&#22312;&#34394;&#20551;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#20316;&#29992;&#12290;&#22810;&#35282;&#24230;&#20998;&#26512;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#27954;&#30693;&#21517;&#23186;&#20307;&#31215;&#26497;&#21453;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#27969;&#20256;&#30340;&#21508;&#31181;&#34394;&#20551;&#20449;&#24687;&#65292;&#23545;&#25298;&#32477;&#30123;&#33495;&#30340;&#36235;&#21183;&#25345;&#25209;&#35780;&#24577;&#24230;&#65292;&#26080;&#35770;&#25253;&#32440;&#30340;&#25919;&#27835;&#26041;&#21521;&#22914;&#20309;&#12290;&#36825;&#35777;&#23454;&#20102;&#30740;&#31350;&#39640;&#36136;&#37327;&#26032;&#38395;&#23186;&#20307;&#22312;&#34394;&#20551;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines how the European press dealt with the no-vax reactions against the Covid-19 vaccine and the dis- and misinformation associated with this movement. Using a curated dataset of 1786 articles from 19 European newspapers on the anti-vaccine movement over a period of 22 months in 2020-2021, we used Natural Language Processing techniques including topic modeling, sentiment analysis, semantic relationship with word embeddings, political analysis, named entity recognition, and semantic networks, to understand the specific role of the European traditional press in the disinformation ecosystem. The results of this multi-angle analysis demonstrate that the European well-established press actively opposed a variety of hoaxes mainly spread on social media, and was critical of the anti-vax trend, regardless of the political orientation of the newspaper. This confirms the relevance of studying the role of high-quality press in the disinformation ecosystem.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21517;&#23383;&#22635;&#31354;&#25104;&#21592;&#25512;&#26029;&#26597;&#35810;&#65292;&#35813;&#30740;&#31350;&#32771;&#21476;&#20102;ChatGPT&#21644;GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#35760;&#24518;&#20102;&#22823;&#37327;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#36825;&#25903;&#25345;&#20102;&#19968;&#20010;&#20351;&#29992;&#24050;&#30693;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#25918;&#27169;&#22411;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.00118</link><description>&lt;p&gt;
&#32842;&#22825;GPT/GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#30340;&#32771;&#21476;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. (arXiv:2305.00118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00118
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21517;&#23383;&#22635;&#31354;&#25104;&#21592;&#25512;&#26029;&#26597;&#35810;&#65292;&#35813;&#30740;&#31350;&#32771;&#21476;&#20102;ChatGPT&#21644;GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#35760;&#24518;&#20102;&#22823;&#37327;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#36825;&#25903;&#25345;&#20102;&#19968;&#20010;&#20351;&#29992;&#24050;&#30693;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#25918;&#27169;&#22411;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21517;&#23383;&#22635;&#31354;&#25104;&#21592;&#25512;&#26029;&#26597;&#35810;&#65292;&#23545;ChatGPT&#21644;GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#36827;&#34892;&#25968;&#25454;&#32771;&#21476;&#23398;&#25512;&#26029;&#65292;&#21457;&#29616;OpenAI&#27169;&#22411;&#24050;&#32463;&#35760;&#24518;&#20102;&#22823;&#37327;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#24182;&#19988;&#35760;&#24518;&#30340;&#31243;&#24230;&#19982;&#36825;&#20123;&#20070;&#31821;&#22312;&#32593;&#19978;&#20986;&#29616;&#30340;&#39057;&#29575;&#26377;&#20851;&#12290;&#36825;&#20123;&#27169;&#22411;&#35760;&#24518;&#26410;&#30693;&#30340;&#20070;&#31821;&#30340;&#33021;&#21147;&#20351;&#24471;&#25991;&#21270;&#20998;&#26512;&#30340;&#27979;&#37327;&#26377;&#25928;&#24615;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#20250;&#27745;&#26579;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35760;&#24518;&#30340;&#20070;&#31821;&#19978;&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;&#26410;&#35760;&#24518;&#30340;&#20070;&#31821;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#36825;&#25903;&#25345;&#20102;&#19968;&#20010;&#20351;&#29992;&#24050;&#30693;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#25918;&#27169;&#22411;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#21644;&#28304;&#35821;&#35328;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;SemEval-2023 Task 12&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00090</link><description>&lt;p&gt;
NLNDE&#22312;SemEval-2023&#31532;12&#20219;&#21153;&#20013;&#65306;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#21644;&#26469;&#28304;&#35821;&#35328;&#36873;&#25321;&#29992;&#20110;&#20302;&#36164;&#28304;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis. (arXiv:2305.00090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#21644;&#28304;&#35821;&#35328;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;SemEval-2023 Task 12&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2023&#31532;12&#39033;&#20219;&#21153;&#8220;&#20351;&#29992;Twitter&#25968;&#25454;&#38598;&#36827;&#34892;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30740;&#31350;&#24191;&#27867;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20173;&#28982;&#38598;&#20013;&#22312;&#23569;&#25968;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#12290;&#30001;&#20110;&#27492;&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#24314;&#31435;&#21487;&#38752;&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35821;&#35328;&#33258;&#36866;&#24212;&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#38750;&#27954;&#25991;&#26412;&#65292;&#24182;&#22312;&#38750;&#27954;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#30740;&#31350;&#28304;&#35821;&#35328;&#36873;&#25321;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65306;&#65288;1&#65289;&#20351;&#29992;&#36739;&#23567;&#20294;&#30456;&#20851;&#30340;&#35821;&#26009;&#24211;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30446;&#26631;&#35821;&#35328;&#21644;&#20219;&#21153;&#21487;&#20197;&#20351;F1&#20998;&#25968;&#25552;&#39640;10&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#12290;&#65288;2&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#36873;&#25321;&#20855;&#26377;&#27491;&#36801;&#31227;&#22686;&#30410;&#30340;&#28304;&#35821;&#35328;&#21487;&#20197;&#36991;&#20813;&#26469;&#33258;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#26377;&#23475;&#24178;&#25200;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#65288;3&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Afrikaans&#21644;Zulu&#35821;&#35328;&#30340;SemEval-2023 Task 12&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system developed for the SemEval-2023 Task 12 "Sentiment Analysis for Low-resource African Languages using Twitter Dataset". Sentiment analysis is one of the most widely studied applications in natural language processing. However, most prior work still focuses on a small number of high-resource languages. Building reliable sentiment analysis systems for low-resource languages remains challenging, due to the limited training data in this task. In this work, we propose to leverage language-adaptive and task-adaptive pretraining on African texts and study transfer learning with source language selection on top of an African language-centric pretrained language model. Our key findings are: (1) Adapting the pretrained model to the target language and task using a small yet relevant corpus improves performance remarkably by more than 10 F1 score points. (2) Selecting source languages with positive transfer gains during training can avoid harmful interference from di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#12289;&#21512;&#25104;&#25968;&#25454;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#36827;&#34892;&#22810;&#23618;&#27425;&#24615;&#21035;&#27495;&#35270;&#20998;&#31867;&#65292;&#24182;&#22312;SemEval-2023 Task 10&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00076</link><description>&lt;p&gt;
HausaNLP&#22312;SemEval-2023 Task 10&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#12289;&#21512;&#25104;&#25968;&#25454;&#21644;&#36741;&#21161;&#20449;&#24687;&#30340;&#22810;&#23618;&#27425;&#24615;&#21035;&#27495;&#35270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HausaNLP at SemEval-2023 Task 10: Transfer Learning, Synthetic Data and Side-Information for Multi-Level Sexism Classification. (arXiv:2305.00076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#12289;&#21512;&#25104;&#25968;&#25454;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#36827;&#34892;&#22810;&#23618;&#27425;&#24615;&#21035;&#27495;&#35270;&#20998;&#31867;&#65292;&#24182;&#22312;SemEval-2023 Task 10&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#25105;&#20204;&#21442;&#19982; SemEval-2023 Task 10 &#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#39033;&#38024;&#23545;&#33521;&#25991;Gab&#21644;Reddit&#25968;&#25454;&#38598;&#36827;&#34892;&#24694;&#24847;&#35821;&#35328;&#65288;&#24615;&#21035;&#27495;&#35270;&#65289;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#26524;&#65306;XLM-T&#65288;&#24773;&#24863;&#20998;&#31867;&#65289;&#21644;HateBERT&#65288;&#30456;&#21516;&#39046;&#22495;--Reddit&#65289;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21512;&#25104;&#20998;&#31867;&#21644;&#20013;&#38388;&#31867;&#20449;&#24687;&#26469;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;Task A&#20013;&#25552;&#20132;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#25490;&#21517;&#31532;49&#21517;&#65292;F1&#20998;&#25968;&#20026;0.82&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#23427;&#24456;&#26377;&#31454;&#20105;&#21147;&#65292;&#22240;&#20026;&#23427;&#21482;&#27604;&#26368;&#20339;&#31995;&#32479;&#20302;0.052&#65285;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the findings of our participation in the SemEval-2023 Task 10: Explainable Detection of Online Sexism (EDOS) task, a shared task on offensive language (sexism) detection on English Gab and Reddit dataset. We investigated the effects of transferring two language models: XLM-T (sentiment classification) and HateBERT (same domain -- Reddit) for multi-level classification into Sexist or not Sexist, and other subsequent sub-classifications of the sexist data. We also use synthetic classification of unlabelled dataset and intermediary class information to maximize the performance of our models. We submitted a system in Task A, and it ranked 49th with F1-score of 0.82. This result showed to be competitive as it only under-performed the best system by 0.052% F1-score.
&lt;/p&gt;</description></item><item><title>EVR+&#26159;&#19968;&#31181;&#35821;&#35328;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#21644;&#25191;&#34892;&#31526;&#21495;&#36816;&#31639;&#31526;&#20197;&#21450;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#31616;&#21333;&#20219;&#21153;&#31561;&#26041;&#24335;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#25903;&#25345;&#26356;&#22810;&#31181;&#31867;&#30340;&#25512;&#29702;&#65292;&#20363;&#22914;&#23884;&#22871;&#24490;&#29615;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#36882;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.00061</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#25512;&#29702;&#22686;&#24378;&#22120;&#65306;&#25903;&#25345;&#21508;&#31181;&#32452;&#21512;&#25512;&#29702;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning. (arXiv:2305.00061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00061
&lt;/p&gt;
&lt;p&gt;
EVR+&#26159;&#19968;&#31181;&#35821;&#35328;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#21644;&#25191;&#34892;&#31526;&#21495;&#36816;&#31639;&#31526;&#20197;&#21450;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#31616;&#21333;&#20219;&#21153;&#31561;&#26041;&#24335;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#25903;&#25345;&#26356;&#22810;&#31181;&#31867;&#30340;&#25512;&#29702;&#65292;&#20363;&#22914;&#23884;&#22871;&#24490;&#29615;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#36882;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25512;&#29702;&#20219;&#21153;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#32452;&#21512;&#25512;&#29702;&#27867;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#25512;&#29702;&#22686;&#24378;&#22120;&#65288;EVR+&#65289;&#8221;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65306;&#65288;1&#65289;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#29983;&#25104;&#21644;&#25191;&#34892;&#31526;&#21495;&#36816;&#31639;&#31526;&#65292;&#65288;2&#65289;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#31616;&#21333;&#20219;&#21153;&#12290;&#19982;&#20854;&#21069;&#36523;Explainable Verbal Reasoner (EVR)&#21644;&#37319;&#29992;&#31867;&#20284;&#24605;&#36335;&#30340;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#26356;&#22810;&#31181;&#31867;&#30340;&#25512;&#29702;&#65292;&#20363;&#22914;&#23884;&#22871;&#24490;&#29615;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#36882;&#24402;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#30340;5&#20010;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;5&#20010;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages models have been successfully applied to a variety of reasoning tasks in NLP, yet the language models still suffer from compositional generalization. In this paper we present Explainable Verbal Reasoner Plus (EVR+), a reasoning framework that enhances language models' compositional reasoning ability by (1) allowing the model to explicitly generate and execute symbolic operators, and (2) allowing the model to decompose a complex task into several simpler ones in a flexible manner. Compared with its predecessor Explainable Verbal Reasoner (EVR) and other previous approaches adopting similar ideas, our framework supports more diverse types of reasoning such as nested loops and different types of recursion. To evaluate our reasoning framework, we build a synthetic dataset with five tasks that require compositional reasoning. Results show that our reasoning framework can enhance the language model's compositional generalization performance on the five tasks, using a fine-tuned lan
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;web&#27983;&#35272;&#22120;&#30340;&#20132;&#20114;&#24335;&#24179;&#21488;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#38382;&#39064; - &#22238;&#31572;&#23545;&#20316;&#20026;&#34013;&#22270;&#35745;&#21010;&#65292;&#20197;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#35828;&#26126;&#29992;&#25143;&#22914;&#20309;&#36890;&#36807;&#32534;&#36753;&#21644;&#20462;&#25913;&#34013;&#22270;&#26469;&#25913;&#21892;&#25110;&#25511;&#21046;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.00034</link><description>&lt;p&gt;
&#25991;&#26412;&#34013;&#22270;&#65306;&#22522;&#20110;&#35745;&#21010;&#30340;&#26465;&#20214;&#29983;&#25104;&#30340;&#20132;&#20114;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation. (arXiv:2305.00034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00034
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;web&#27983;&#35272;&#22120;&#30340;&#20132;&#20114;&#24335;&#24179;&#21488;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#38382;&#39064; - &#22238;&#31572;&#23545;&#20316;&#20026;&#34013;&#22270;&#35745;&#21010;&#65292;&#20197;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#35828;&#26126;&#29992;&#25143;&#22914;&#20309;&#36890;&#36807;&#32534;&#36753;&#21644;&#20462;&#25913;&#34013;&#22270;&#26469;&#25913;&#21892;&#25110;&#25511;&#21046;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#29616;&#22312;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20197;&#21019;&#24314;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20173;&#28982;&#22256;&#38590;&#65292;&#23548;&#33268;&#29983;&#25104;&#26080;&#20851;&#65292;&#37325;&#22797;&#21644;&#34394;&#20551;&#30340;&#20869;&#23481;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35268;&#21010;&#21487;&#20197;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#20351;&#26465;&#20214;&#29983;&#25104; less opaque and more grounded. &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;web&#27983;&#35272;&#22120;&#30340;&#28436;&#31034;&#65292;&#29992;&#20110;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#65292;&#35813;&#25688;&#35201;&#20351;&#29992;&#19968;&#31995;&#21015;&#38382;&#39064; - &#22238;&#31572;&#23545;&#20316;&#20026;&#34013;&#22270;&#35745;&#21010;&#65292;&#20197;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#65288;&#21363;&#65292;&#22914;&#20309;&#35828;&#20160;&#20040;&#20197;&#21450;&#20197;&#20160;&#20040;&#39034;&#24207;&#65289;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#29992;&#25143;&#22914;&#20309;&#19982;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#30456;&#20851;&#30340;&#35745;&#21010;&#21487;&#35270;&#21270;&#36827;&#34892;&#20132;&#20114;&#65292;&#20363;&#22914;&#36890;&#36807;&#32534;&#36753;&#21644;&#20462;&#25913;&#34013;&#22270;&#20197;&#25913;&#21892;&#25110;&#25511;&#21046;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30701;&#35270;&#39057;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#21487;&#22312;https://goo.gle/text-blueprint-demo&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
While conditional generation models can now generate natural language well enough to create fluent text, it is still difficult to control the generation process, leading to irrelevant, repetitive, and hallucinated content. Recent work shows that planning can be a useful intermediate step to render conditional generation less opaque and more grounded. We present a web browser-based demonstration for query-focused summarization that uses a sequence of question-answer pairs, as a blueprint plan for guiding text generation (i.e., what to say and in what order). We illustrate how users may interact with the generated text and associated plan visualizations, e.g., by editing and modifying the blueprint in order to improve or control the generated output.  A short video demonstrating our system is available at https://goo.gle/text-blueprint-demo.
&lt;/p&gt;</description></item><item><title>HQP&#26159;&#19968;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#32593;&#32476;&#23459;&#20256;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#30340;&#24369;&#26631;&#31614;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#20351;&#29992;HQP&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;44%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14931</link><description>&lt;p&gt;
HQP&#65306;&#19968;&#20221;&#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HQP: A Human-Annotated Dataset for Detecting Online Propaganda. (arXiv:2304.14931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14931
&lt;/p&gt;
&lt;p&gt;
HQP&#26159;&#19968;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#32593;&#32476;&#23459;&#20256;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#30340;&#24369;&#26631;&#31614;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#20351;&#29992;HQP&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;44%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23459;&#20256;&#23545;&#31038;&#20250;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#23427;&#20204;&#26159;&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#27880;&#37322;&#30340;&#65292;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#29978;&#33267;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#20570;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HQP&#65288;N=30,000&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#32780;&#21019;&#24314;&#30340;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#30340;&#25968;&#25454;&#38598;&#12290;&#65288;2&#65289;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#26041;&#38754;&#22833;&#36133;&#65288;AUC&#65306;64.03&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20351;&#29992;&#25105;&#20204;&#30340;&#39640;&#36136;&#37327;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#65288;AUC&#65306;92.25&#65289;&#65292;&#25552;&#39640;&#20102;&#32422;44%&#12290;&#65288;3&#65289;&#20026;&#20102;&#35299;&#20915;&#26631;&#27880;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#21040;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#25552;&#31034;&#24335;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online propaganda poses a severe threat to the integrity of societies. However, existing datasets for detecting online propaganda have a key limitation: they were annotated using weak labels that can be noisy and even incorrect. To address this limitation, our work makes the following contributions: (1) We present \dataset: a novel dataset (N=30,000) for detecting online propaganda with high-quality labels. To the best of our knowledge, \dataset is the first dataset for detecting online propaganda that was created through human annotation. (2) We show empirically that state-of-the-art language models fail in detecting online propaganda when trained with weak labels (AUC: 64.03). In contrast, state-of-the-art language models can accurately detect online propaganda when trained with our high-quality labels (AUC: 92.25), which is an improvement of ~44%. (3) To address the cost of labeling, we extend our work to few-shot learning. Specifically, we show that prompt-based learning using a sm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14721</link><description>&lt;p&gt;
&#26397;&#33258;&#20027;&#31995;&#32479;&#36808;&#36827;&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22686;&#24378;&#30340;&#28789;&#27963;&#27169;&#22359;&#21270;&#29983;&#20135;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24320;&#21457;&#21253;&#21547;&#29983;&#20135;&#25551;&#36848;&#20449;&#24687;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#65292;&#24182;&#23558;&#33258;&#21160;&#21270;&#31995;&#32479;&#25913;&#36896;&#20026;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#30340;&#32454;&#31890;&#24230;&#21151;&#33021;&#25110;&#27169;&#22359;&#65292;&#20197;&#20379;&#33258;&#21160;&#21270;&#32452;&#20214;&#25110;&#27169;&#22359;&#25191;&#34892;&#12290;&#38543;&#21518;&#65292;&#35774;&#35745;LLM&#20195;&#29702;&#26469;&#35299;&#37322;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25551;&#36848;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;RESTful&#25509;&#21475;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#12290;&#36825;&#20123;LLM&#20195;&#29702;&#20316;&#20026;&#33258;&#21160;&#21270;&#31995;&#32479;&#20869;&#30340;&#26234;&#33021;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25351;&#20196;&#20316;&#20026;&#36755;&#20837;&#65292;LLM&#20195;&#29702;&#21327;&#35843;&#19968;&#31995;&#21015;&#21407;&#23376;&#21151;&#33021;&#21644;&#25216;&#33021;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#21407;&#22411;&#22914;&#20309;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24182;&#35745;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. Our approach involves developing a digital twin system that contains descriptive information about the production and retrofitting the automation system to offer unified interfaces of fine-granular functionalities or skills executable by automation components or modules. Subsequently, LLM-Agents are designed to interpret descriptive information in the digital twins and control the physical system through RESTful interfaces. These LLM-Agents serve as intelligent agents within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13620</link><description>&lt;p&gt;
ChartSumm&#65306;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23558;&#22270;&#34920;&#36716;&#25442;&#20026;&#25991;&#26412;&#25688;&#35201;&#26159;&#35270;&#38556;&#20154;&#22763;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#31934;&#30830;&#27934;&#23519;&#21147;&#12290;&#22823;&#22411;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#22987;&#32456;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20849;84363&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#22270;&#34920;&#31867;&#22411;&#65292;&#21487;&#29983;&#25104;&#38271;&#30701;&#25688;&#35201;&#12290;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#24471;&#20998;&#26469;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35273;&#65292;&#28431;&#25481;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#19981;&#27491;&#30830;&#22320;&#35299;&#37322;&#22270;&#34920;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#25506;&#35752;&#20102;&#23558;ChartSumm&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25104;&#20026;&#19968;&#20010;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#30340;&#22797;&#21046;&#26426;&#21046;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#39046;&#22495;&#22312;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#23558;&#22797;&#21046;&#26426;&#21046;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;NMT&#30340;SPARQL&#29983;&#25104;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28155;&#21152;&#22797;&#21046;&#26426;&#21046;&#25110;&#20351;&#29992;&#38382;&#39064;&#27880;&#37322;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>SemEval-2023&#20030;&#21150;&#20102;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#25361;&#25112;&#36187;&#65288;AfriSenti-SemEval&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25361;&#25112;&#36187;&#21253;&#25324;&#21333;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2304.06845</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;12: &#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65288;AfriSenti-SemEval&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06845
&lt;/p&gt;
&lt;p&gt;
SemEval-2023&#20030;&#21150;&#20102;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#25361;&#25112;&#36187;&#65288;AfriSenti-SemEval&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25361;&#25112;&#36187;&#21253;&#25324;&#21333;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#38750;&#27954;&#35821;&#26448;&#26009;&#30340;SemEval&#25361;&#25112;&#36187;&#8212;&#8212;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65288;AfriSenti-SemEval&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;14&#31181;&#38750;&#27954;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#20234;&#21338;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#33707;&#26705;&#27604;&#20811;&#33889;&#33796;&#29273;&#35821;&#12289;&#23612;&#26085;&#21033;&#20122;&#30382;&#38054;&#35821;&#12289;&#22885;&#32599;&#33707;&#35821;&#12289;&#26031;&#29926;&#24076;&#37324;&#35821;&#12289;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#12289;&#29305;&#23041;&#35821;&#12289;&#20811;&#26862;&#35821;&#21644;&#32422;&#40065;&#24052;&#35821;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#21333;&#35821;&#20998;&#31867;&#65292;&#20849;&#25910;&#21040;44&#20010;&#25552;&#20132;&#32467;&#26524;&#65307;&#65288;2&#65289;&#22810;&#35821;&#35328;&#20998;&#31867;&#65292;&#20849;&#25910;&#21040;32&#20010;&#25552;&#20132;&#32467;&#26524;&#65307;&#65288;3&#65289;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20849;&#25910;&#21040;34&#20010;&#25552;&#20132;&#32467;&#26524;&#12290;&#20854;&#20013;&#65292;NLNDE&#22242;&#38431;&#22312;&#20219;&#21153;A&#21644;B&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;71.31&#21644;75.06&#21152;&#26435;F1&#20998;&#25968;&#30340;&#26368;&#20339;&#31995;&#32479;&#12290;UCAS-IIE-NLP&#22312;&#20219;&#21153;C&#19978;&#24179;&#22343;&#33719;&#24471;&#20102;58.15&#21152;&#26435;F1&#20998;&#25968;&#30340;&#26368;&#20339;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first Africentric SemEval Shared task, Sentiment Analysis for African Languages (AfriSenti-SemEval) - the dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval is a sentiment classification challenge in 14 African languages - Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yor\`ub\'a (Muhammad et al., 2023), using a 3-class labeled data: positive, negative, and neutral. We present three subtasks: (1) Task A: monolingual classification, which received 44 submissions; (2) Task B: multilingual classification, which received 32 submissions; and (3) Task C: zero-shot classification, which received 34 submissions. The best system for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP achieved the best system on average for task C with 58.15 weighted F1. We describe the variou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.07103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26222;&#36941;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24863;&#30693;&#25110;&#24847;&#35782;&#12290;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#36825;&#20010;&#24819;&#27861;&#65311;&#26412;&#25991;&#23558;&#20998;&#26512;&#25903;&#25345;&#21644;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#26681;&#25454;&#24847;&#35782;&#31185;&#23398;&#20013;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20363;&#22914;&#32570;&#20047;&#24490;&#29615;&#22788;&#29702;&#12289;&#20840;&#23616;&#30340;&#24037;&#20316;&#31354;&#38388;&#21644;&#32479;&#19968;&#30340;&#26234;&#33021;&#26426;&#26500;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#38556;&#30861;&#22312;&#26410;&#26469;&#21313;&#24180;&#24038;&#21491;&#37117;&#21487;&#33021;&#34987;&#20811;&#26381;&#12290;&#20316;&#32773;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.04562</link><description>&lt;p&gt;
&#36845;&#20195;&#20462;&#27491;&#30340;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22806;&#25512;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24207;&#21015;&#12290;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#65292;&#36825;&#20010;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#27604;&#29616;&#26377;&#24207;&#21015;&#26356;&#22909;&#65288;&#20363;&#22914;&#26356;&#31283;&#23450;&#65289;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#12290;&#22240;&#27492;&#65292;&#25353;&#29031;&#23450;&#20041;&#65292;&#30446;&#26631;&#24207;&#21015;&#21450;&#20854;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#65292;&#25361;&#25112;&#29616;&#26377;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36845;&#20195;&#25511;&#21046;&#22806;&#25512;&#65288;ICE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#24207;&#21015;&#36827;&#34892;&#23616;&#37096;&#32534;&#36753;&#26469;&#23454;&#29616;&#22806;&#25512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#24207;&#21015;&#23545;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28436;&#31034;&#24494;&#23567;&#30340;&#23646;&#24615;&#20540;&#25913;&#36827;&#12290;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#26512;&#65289;&#21644;&#20004;&#20010;&#34507;&#30333;&#36136;&#24037;&#31243;&#20219;&#21153;&#65288;ACE2&#31283;&#23450;&#24615;&#21644;AAV&#36866;&#24212;&#24615;&#65289;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ICE&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#65306;&#20174;BERT&#21040;ChatGPT&#30340;&#21382;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;(PFMs)&#34987;&#35748;&#20026;&#26159;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;PFM(&#20363;&#22914;BERT&#12289;ChatGPT&#21644;GPT-4)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;BERT&#20174;&#36716;&#25442;&#22120;&#20013;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#31867;&#20284;&#22320;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#26041;&#27861;&#37319;&#29992;&#36716;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#37319;&#29992;&#33258;&#22238;&#24402;&#33539;&#24335;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;ChatGPT&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#21151;&#65292;&#23427;&#37319;&#29992;&#33258;&#22238;&#24402;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#23556;&#20987;&#25110;&#23569;&#23556;&#20987;&#25552;&#31034;&#12290;PFM&#30340;&#21331;&#36234;&#25104;&#23601;&#20026;&#21508;&#31181;AI&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#26356;&#26032;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;PFMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;PFMs&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24418;&#36716;&#25442;&#22120;&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#24086;&#23376;&#21518;&#32493;&#30340;&#35752;&#35770;&#26469;&#25104;&#21151;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19981;&#36275;&#65292;&#20197;&#21450;&#26410;&#26469;&#21487;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#20840;&#38754;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.10871</link><description>&lt;p&gt;
&#22270;&#24418;&#36716;&#25442;&#22120;&#26041;&#27861;&#38024;&#23545;&#21464;&#21270;&#21160;&#24577;&#20869;&#23481;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#30340;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24418;&#36716;&#25442;&#22120;&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#24086;&#23376;&#21518;&#32493;&#30340;&#35752;&#35770;&#26469;&#25104;&#21151;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19981;&#36275;&#65292;&#20197;&#21450;&#26410;&#26469;&#21487;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#20840;&#38754;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24418;&#36716;&#25442;&#22120;&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#24086;&#23376;&#21518;&#32493;&#30340;&#35752;&#35770;&#26469;&#25104;&#21151;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#22312;&#21738;&#20123;&#22330;&#26223;&#19979;&#26377;&#26368;&#20248;&#34920;&#29616;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#24403;&#21069;&#31038;&#20132;&#23186;&#20307;&#20013;&#23384;&#22312;&#30340;&#24694;&#24847;&#22270;&#29255;&#31561;&#21508;&#31181;&#24086;&#23376;&#31867;&#22411;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#27169;&#22411;&#30340;&#24605;&#36335;&#12290;&#20851;&#38190;&#30340;&#27934;&#35265;&#22312;&#20110;&#35813;&#26041;&#27861;&#27880;&#37325;&#23545;&#19978;&#19979;&#25991;&#27010;&#24565;&#30340;&#25512;&#29702;&#65292;&#22240;&#27492;&#35813;&#26041;&#27861;&#20855;&#22791;&#24456;&#22823;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work advances an approach for predicting hate speech in social media, drawing out the critical need to consider the discussions that follow a post to successfully detect when hateful discourse may arise. Using graph transformer networks, coupled with modelling attention and BERT-level natural language processing, our approach can capture context and anticipate upcoming anti-social behaviour. In this paper, we offer a detailed qualitative analysis of this solution for hate speech detection in social networks, leading to insights into where the method has the most impressive outcomes in comparison with competitors and identifying scenarios where there are challenges to achieving ideal performance. Included is an exploration of the kinds of posts that permeate social media today, including the use of hateful images. This suggests avenues for extending our model to be more comprehensive. A key insight is that the focus on reasoning about the concept of context positions us well to be a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#25193;&#23637;&#20102;COCO&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#21521;&#20854;&#20013;&#28155;&#21152;&#20102;&#21253;&#25324;&#22330;&#26223;&#20449;&#24687;&#31561;&#26356;&#22810;&#25991;&#26412;&#20869;&#23481;&#65292;&#21487;&#20197;&#24110;&#21161;&#22270;&#20687;&#23383;&#24149;&#31995;&#32479;&#26356;&#22909;&#22320;&#29702;&#35299;&#22330;&#26223;&#21644;&#29289;&#20307;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.08784</link><description>&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Visual Semantic Relatedness Dataset for Image Captioning. (arXiv:2301.08784v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#25193;&#23637;&#20102;COCO&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#21521;&#20854;&#20013;&#28155;&#21152;&#20102;&#21253;&#25324;&#22330;&#26223;&#20449;&#24687;&#31561;&#26356;&#22810;&#25991;&#26412;&#20869;&#23481;&#65292;&#21487;&#20197;&#24110;&#21161;&#22270;&#20687;&#23383;&#24149;&#31995;&#32479;&#26356;&#22909;&#22320;&#29702;&#35299;&#22330;&#26223;&#21644;&#29289;&#20307;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#20687;&#23383;&#24149;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#30693;&#35782;&#20197;&#25429;&#33719;&#38745;&#24577;&#25925;&#20107;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23383;&#24149;&#30340;&#25991;&#26412;&#35270;&#35273;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;COCO&#23383;&#24149;&#25968;&#25454;&#38598;(Lin&#31561;&#20154;&#65292;2014)&#24050;&#32463;&#25193;&#23637;&#20102;&#22330;&#26223;&#20449;&#24687;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65289;&#12290;&#30001;&#20110;&#36825;&#20123;&#20449;&#24687;&#20855;&#26377;&#25991;&#26412;&#24418;&#24335;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;&#23427;&#20204;&#29992;&#20110;&#23558;&#20219;&#20309;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#30456;&#20284;&#24230;&#25110;&#35821;&#20041;&#20851;&#31995;&#26041;&#27861;&#65289;&#32435;&#20837;&#23383;&#24149;&#31995;&#32479;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#31471;&#21040;&#31471;&#22521;&#35757;&#31574;&#30053;&#36824;&#26159;&#22522;&#20110;&#21518;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern image captioning system relies heavily on extracting knowledge from images to capture the concept of a static story. In this paper, we propose a textual visual context dataset for captioning, in which the publicly available dataset COCO Captions (Lin et al., 2014) has been extended with information about the scene (such as objects in the image). Since this information has a textual form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, either as an end-to-end training strategy or a post-processing based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#36335;&#24452;&#35206;&#30422;&#29575;&#21644;&#20851;&#31995;&#36335;&#24452;&#32622;&#20449;&#24230;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36807;&#28388;&#20986;&#19981;&#21487;&#38752;&#30340;&#36335;&#24452;&#65292;&#25552;&#39640;&#22522;&#20110;&#21477;&#23376;Transformer&#30340;&#30693;&#35782;&#25512;&#29702;&#21477;&#23376;Transformer&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;KG&#20013;&#23454;&#29616;&#20102;&#22810;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24863;&#30693;&#20851;&#31995;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.01664</link><description>&lt;p&gt;
&#22522;&#20110;&#21477;&#23376;Transformer&#36827;&#34892;&#22810;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24863;&#30693;&#20851;&#31995;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#36335;&#24452;&#35206;&#30422;&#29575;&#21644;&#20851;&#31995;&#36335;&#24452;&#32622;&#20449;&#24230;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36807;&#28388;&#20986;&#19981;&#21487;&#38752;&#30340;&#36335;&#24452;&#65292;&#25552;&#39640;&#22522;&#20110;&#21477;&#23376;Transformer&#30340;&#30693;&#35782;&#25512;&#29702;&#21477;&#23376;Transformer&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;KG&#20013;&#23454;&#29616;&#20102;&#22810;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24863;&#30693;&#20851;&#31995;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#24863;&#30693;&#21644;&#21487;&#35299;&#37322;&#20851;&#31995;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#36335;&#24452;&#35206;&#30422;&#29575;&#21644;&#20851;&#31995;&#36335;&#24452;&#32622;&#20449;&#24230;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36807;&#28388;&#20986;&#19981;&#21487;&#38752;&#30340;&#36335;&#24452;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#30693;&#35782;&#25512;&#29702;&#21477;&#23376;Transformer&#8221;&#65288;KRST&#65289;&#26469;&#39044;&#27979;KG&#20013;&#30340;&#24863;&#30693;&#20851;&#31995;&#12290; KRST&#34987;&#35774;&#35745;&#20026;&#22312;KG&#20013;&#32534;&#30721;&#25552;&#21462;&#20986;&#30340;&#21487;&#38752;&#36335;&#24452;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#30830;&#22320;&#32858;&#31867;&#36335;&#24452;&#24182;&#25552;&#20379;&#22810;&#26041;&#38754;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;SOTA&#27169;&#22411;&#30456;&#27604;&#65292;KRST&#22312;&#22823;&#22810;&#25968;&#24863;&#30693;&#21644;&#24863;&#30693;&#27979;&#35797;&#29992;&#20363;&#65288;6&#20010;&#20013;&#30340;4&#20010;&#65289;&#20197;&#21450;12&#20010;few-shot&#27979;&#35797;&#29992;&#20363;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#19978;&#34920;&#29616;&#19981;&#36275;&#20197;&#21450;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SSM&#23618;H3&#65292;&#24182;&#23558;&#20854;&#19982;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#31361;&#20986;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.14052</link><description>&lt;p&gt;
&#39269;&#39295;&#30340;&#27827;&#39532;&#65306;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#19978;&#34920;&#29616;&#19981;&#36275;&#20197;&#21450;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SSM&#23618;H3&#65292;&#24182;&#23558;&#20854;&#19982;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#31361;&#20986;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22312;&#26576;&#20123;&#27169;&#24577;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24207;&#21015;&#24314;&#27169;&#24615;&#33021;&#65292;&#20294;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;SSMs&#30340;&#24207;&#21015;&#38271;&#24230;&#36817;&#20046;&#32447;&#24615;&#22320;&#25193;&#23637;&#32780;&#19981;&#26159;&#20108;&#27425;&#26041;&#65292;&#20294;&#30001;&#20110;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#19979;&#65292;&#23427;&#20204;&#20173;&#28982;&#27604;&#21464;&#21387;&#22120;&#26356;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#29702;&#35299;&#20102;SSMs&#21644;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#36317;&#65292;&#24182;&#38477;&#20302;&#20102;SSMs&#21644;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#20043;&#38388;&#30340;&#30828;&#20214;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#26469;&#29702;&#35299;SSMs&#21644;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;SSMs&#22312;&#20004;&#20010;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65306;&#22238;&#24518;&#20808;&#21069;&#30340;&#26631;&#35760;&#21644;&#36328;&#24207;&#21015;&#27604;&#36739;&#26631;&#35760;&#12290;&#20026;&#20102;&#29702;&#35299;&#23545;&#35821;&#35328;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;SSM&#23618;&#65292;H3&#65292;&#19987;&#38376;&#35774;&#35745;&#36825;&#20123;&#33021;&#21147;&#12290;H3&#22312;&#21512;&#25104;&#35821;&#35328;&#19978;&#19982;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;OpenWebText&#19978;&#27604;&#21464;&#21387;&#22120;&#23569;&#20102;0.4 PPL&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;H3&#21644;&#27880;&#24847;&#21147;&#20197;&#21450;&#30828;&#20214;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#22914;&#20309;&#35774;&#35745;&#26356;&#22909;&#30340;SSMs&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;&#65288;SceneGATE&#65289;&#29992;&#20110;TextVQA&#65292;&#36890;&#36807;&#21457;&#29616;&#22330;&#26223;&#22270;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#25429;&#25417;&#20102;&#22270;&#20687;&#20013;&#29289;&#20307;&#12289;OCR&#26631;&#35760;&#21644;&#38382;&#39064;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22312;Text-VQA&#21644;ST-VQA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08283</link><description>&lt;p&gt;
SceneGATE&#65306;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#25991;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;&#65288;SceneGATE&#65289;&#29992;&#20110;TextVQA&#65292;&#36890;&#36807;&#21457;&#29616;&#22330;&#26223;&#22270;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#25429;&#25417;&#20102;&#22270;&#20687;&#20013;&#29289;&#20307;&#12289;OCR&#26631;&#35760;&#21644;&#38382;&#39064;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22312;Text-VQA&#21644;ST-VQA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;TextVQA&#26041;&#27861;&#37117;&#27880;&#37325;&#36890;&#36807;&#31616;&#21333;&#30340;transformer&#32534;&#30721;&#22120;&#26469;&#25972;&#21512;&#29289;&#20307;&#12289;&#22330;&#26223;&#25991;&#26412;&#21644;&#38382;&#39064;&#35789;&#65292;&#20294;&#36825;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;&#65288;SceneGATE&#65289;&#29992;&#20110;TextVQA&#65292;&#35813;&#32593;&#32476;&#25581;&#31034;&#20102;&#22270;&#20687;&#20013;&#29289;&#20307;&#12289;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#26631;&#35760;&#21644;&#38382;&#39064;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;TextVQA&#30340;&#22330;&#26223;&#22270;&#26469;&#21457;&#29616;&#22270;&#20687;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#25105;&#20204;&#21019;&#36896;&#20102;&#19968;&#20010;&#24341;&#23548;&#20851;&#27880;&#27169;&#22359;&#26469;&#25429;&#33719;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20869;&#37096;&#20132;&#20114;&#20316;&#20026;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#21521;&#23548;&#12290;&#20026;&#20102;&#26126;&#30830;&#25945;&#25480;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#38598;&#25104;&#20102;&#20004;&#20010;&#27880;&#24847;&#27169;&#22359;&#65292;&#21363;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#35821;&#20041;&#20851;&#31995;&#24863;&#30693;&#27880;&#24847;&#21644;&#20301;&#32622;&#20851;&#31995;&#24863;&#30693;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Text-VQA&#21644;ST-VQA&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SceneGATE&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most TextVQA approaches focus on the integration of objects, scene texts and question words by a simple transformer encoder. But this fails to capture the semantic relations between different modalities. The paper proposes a Scene Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We created a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To make explicit teaching of the relations between the two modalities, we proposed and integrated two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#20998;&#21106;&#26041;&#27861;&#23545;Egyptian Arabic-English&#20195;&#30721;&#36716;&#25442;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24418;&#24577;&#23398;&#24863;&#30693;&#20998;&#27573;&#22120;&#22312;&#20998;&#27573;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;MT&#20219;&#21153;&#20013;&#34920;&#29616;&#27424;&#20339;&#12290;&#20294;&#23545;&#20110;&#26497;&#20302;&#36164;&#28304;&#24773;&#24418;&#65292;&#24418;&#24577;&#23398;&#24863;&#30693;&#20998;&#27573;&#22120;&#21644;&#39057;&#29575;&#20998;&#21106;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.06990</link><description>&lt;p&gt;
&#25506;&#32034;&#20998;&#21106;&#26041;&#27861;&#23545;Egyptian Arabic-English&#20195;&#30721;&#36716;&#25442;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text. (arXiv:2210.06990v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#20998;&#21106;&#26041;&#27861;&#23545;Egyptian Arabic-English&#20195;&#30721;&#36716;&#25442;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24418;&#24577;&#23398;&#24863;&#30693;&#20998;&#27573;&#22120;&#22312;&#20998;&#27573;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;MT&#20219;&#21153;&#20013;&#34920;&#29616;&#27424;&#20339;&#12290;&#20294;&#23545;&#20110;&#26497;&#20302;&#36164;&#28304;&#24773;&#24418;&#65292;&#24418;&#24577;&#23398;&#24863;&#30693;&#20998;&#27573;&#22120;&#21644;&#39057;&#29575;&#20998;&#21106;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#26159;&#20195;&#30721;&#36716;&#25442;&#65288;CS&#65289;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#23545;&#20110;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#65292;&#22312;&#21333;&#35821;&#22659;&#20013;&#65292;&#24418;&#24577;&#23398;&#20998;&#21106;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#20294;&#26159;&#23427;&#23578;&#26410;&#34987;&#29992;&#20110;CS&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#19981;&#21516;&#20998;&#21106;&#26041;&#27861;&#23545;MT&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22522;&#20110;&#24418;&#24577;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#20174;&#20195;&#30721;&#36716;&#25442;&#30340;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#21040;&#33521;&#35821;&#30340;MT&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;&#21508;&#31181;&#26465;&#20214;&#65292;&#20363;&#22914;&#25968;&#25454;&#22823;&#23567;&#21644;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;CS&#30340;&#21477;&#23376;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#24418;&#24577;&#23398;&#24863;&#30693;&#20998;&#27573;&#22120;&#22312;&#20998;&#27573;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;MT&#20219;&#21153;&#20013;&#34920;&#29616;&#27424;&#20339;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#29992;&#20110;MT&#30340;&#20998;&#21106;&#35774;&#32622;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#22823;&#23567;&#12290;&#23545;&#20110;&#26497;&#20302;&#36164;&#28304;&#24773;&#24418;&#65292;&#24418;&#24577;&#23398;&#24863;&#30693;&#20998;&#27573;&#22120;&#21644;&#39057;&#29575;&#20998;&#21106;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sparsity is one of the main challenges posed by code-switching (CS), which is further exacerbated in the case of morphologically rich languages. For the task of machine translation (MT), morphological segmentation has proven successful in alleviating data sparsity in monolingual contexts; however, it has not been investigated for CS settings. In this paper, we study the effectiveness of different segmentation approaches on MT performance, covering morphology-based and frequency-based segmentation techniques. We experiment on MT from code-switched Arabic-English to English. We provide detailed analysis, examining a variety of conditions, such as data size and sentences with different degrees of CS. Empirical results show that morphology-aware segmenters perform the best in segmentation tasks but under-perform in MT. Nevertheless, we find that the choice of the segmentation setup to use for MT is highly dependent on the data size. For extreme low-resource scenarios, a combination of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;IterX&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#27169;&#26495;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.06600</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#36845;&#20195;&#24335;&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;IterX&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#27169;&#26495;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#25277;&#21462;&#27169;&#22411;IterX&#65292;&#29992;&#20110;&#22312;&#25991;&#26723;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#20851;&#31995;&#25110;&#27169;&#26495;&#65288;&#21363;&#23558;&#21629;&#21517;&#27133;&#19982;&#25991;&#26412;&#36328;&#24230;&#36827;&#34892;&#26144;&#23556;&#30340;N&#20803;&#32452;&#65289;&#12290;&#27169;&#26495;&#25277;&#21462;&#20219;&#21153;&#21253;&#25324;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#27169;&#26495;&#24182;&#25552;&#21462;&#27599;&#20010;&#27169;&#26495;&#30340;&#27133;&#20540;&#12290;&#25105;&#20204;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#23558;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#28040;&#38500;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#27169;&#26495;&#39034;&#24207;&#26469;&#35757;&#32451;&#25552;&#21462;&#22120;&#30340;&#38656;&#35201;&#12290;&#23427;&#22312;&#20004;&#20010;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;SciREX&#30340;4&#20803;&#20851;&#31995;&#25277;&#21462;&#21644;MUC-4&#30340;&#27169;&#26495;&#25277;&#21462;&#65289;&#20197;&#21450;&#26032;&#30340;BETTER Granular&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel iterative extraction model, IterX, for extracting complex relations, or templates (i.e., N-tuples representing a mapping from named slots to spans of text) within a document. Documents may feature zero or more instances of a template of any given type, and the task of template extraction entails identifying the templates in a document and extracting each template's slot values. Our imitation learning approach casts the problem as a Markov decision process (MDP), and relieves the need to use predefined template orders to train an extractor. It leads to state-of-the-art results on two established benchmarks -- 4-ary relation extraction on SciREX and template extraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular task.
&lt;/p&gt;</description></item><item><title>MiniALBERT &#26159;&#19968;&#31181;&#27169;&#22411;&#33976;&#39311;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#31561;&#31574;&#30053;&#65292;&#23558;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36716;&#25442;&#25104;&#20026;&#32039;&#20945;&#36882;&#24402;&#23398;&#29983;&#27169;&#22411;&#12290;MiniALBERT &#22312;&#22522;&#20934; NLP &#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32039;&#20945;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.06425</link><description>&lt;p&gt;
&#36855;&#20320;ALBERT: &#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#36882;&#24402;&#21464;&#25442;&#30340;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06425
&lt;/p&gt;
&lt;p&gt;
MiniALBERT &#26159;&#19968;&#31181;&#27169;&#22411;&#33976;&#39311;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#31561;&#31574;&#30053;&#65292;&#23558;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36716;&#25442;&#25104;&#20026;&#32039;&#20945;&#36882;&#24402;&#23398;&#29983;&#27169;&#22411;&#12290;MiniALBERT &#22312;&#22522;&#20934; NLP &#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32039;&#20945;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LM) &#30001;&#20110;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#23613;&#31649;&#36825;&#19968;&#36745;&#29004;&#30340;&#25104;&#23601;&#65292;LM &#30340;&#21487;&#29992;&#24615;&#21463;&#38480;&#20110;&#35745;&#31639;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#26085;&#30410;&#22686;&#38271;&#30340;&#27169;&#22411;&#22823;&#23567;&#65307;&#36825;&#26159;&#34987;&#31216;&#20026;&#8220;&#36807;&#21442;&#25968;&#21270;&#8221;&#30340;&#38382;&#39064;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26088;&#22312;&#21019;&#24314;&#26377;&#25928;&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#20854;&#33192;&#32960;&#30340;&#23545;&#24212;&#29289;&#20960;&#20046;&#30456;&#21305;&#37197;&#65292;&#32780;&#20960;&#20046;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#20043;&#19968;&#26159;&#27169;&#22411;&#33976;&#39311;&#12290;&#32780;&#21478;&#19968;&#31181;&#24378;&#22823;&#20294;&#19981;&#24120;&#20351;&#29992;&#30340;&#25216;&#26415;&#26159;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102; MiniALBERT&#65292;&#19968;&#31181;&#23558;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340; LM &#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#32039;&#20945;&#36882;&#24402;&#23398;&#29983;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#21644;&#20854;&#20182;&#25216;&#26415;&#30340;&#25928;&#33021;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102; MiniALBERT &#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934; NLP &#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MiniALBERT &#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32039;&#20945;&#22411; LM&#65292;&#24182;&#20445;&#25345;&#26356;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as `overparameterisation'. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#36825;&#19968;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#65288;UBMEC&#65289;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.06405</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus. (arXiv:2210.06405v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#36825;&#19968;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#65288;UBMEC&#65289;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#22312;&#30740;&#31350;&#20154;&#20204;&#23545;&#21508;&#31181;Web 2.0&#26381;&#21153;&#30340;&#24819;&#27861;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24773;&#24863;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;&#24050;&#32463;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#23391;&#21152;&#25289;&#35821;&#36825;&#26679;&#19990;&#30028;&#19978;&#26368;&#27969;&#34892;&#30340;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#21364;&#27809;&#26377;&#36827;&#34892;&#22826;&#22810;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#23436;&#25972;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#25552;&#21462;&#23391;&#21152;&#25289;&#35821;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20174;&#23391;&#21152;&#25289;&#35821;&#35789;&#27719;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#20845;&#31867;&#65288;&#24868;&#24594;&#12289;&#21388;&#24694;&#12289;&#23475;&#24597;&#12289;&#21916;&#24742;&#12289;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#30340;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#32479;&#19968;&#23391;&#21152;&#25289;&#22810;&#20998;&#31867;&#24773;&#24863;&#35821;&#26009;&#24211;&#65288;UBMEC&#65289;&#8221;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;UBMEC&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#20808;&#21069;&#21457;&#24067;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23391;&#21152;&#25289;&#35821;&#24773;&#24863;&#35821;&#26009;&#24211;&#32780;&#21019;&#24314;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;16,000&#20010;&#23391;&#21152;&#25289;&#25991;&#26412;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of its importance in studying people's thoughts on various Web 2.0 services, emotion classification (EC) is an important undertaking. Existing research, on the other hand, is mostly focused on the English language, with little work on low-resource languages. Though sentiment analysis, particularly the EC in English, has received a lot of attention in recent years, little study has been done in the context of Bangla, one of the world's most widely spoken languages. We propose a complete set of approaches for identifying and extracting emotions from Bangla texts in this research. We provide a Bangla emotion classifier for six classes (anger, disgust, fear, joy, sadness, and surprise) from Bangla words, using transformer-based models which exhibit phenomenal results in recent days, especially for high resource languages. The "Unified Bangla Multi-class Emotion Corpus (UBMEC)" is used to assess the performance of our models. UBMEC was created by combining two previously released ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#27604;&#23398;&#20064;&#34701;&#20837;&#21040;&#22810;&#35821;&#35328;&#34920;&#31034;&#33976;&#39311;&#20013;&#65292;&#29992;&#20110;&#24179;&#34892;&#35821;&#21477;&#30340;&#36136;&#37327;&#20272;&#35745;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#19981;&#21516;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26041;&#27861;&#20248;&#20110;&#20043;&#21069;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#20363;&#22914; LASER&#65292;LASER3 &#21644; LaBSE&#12290;</title><link>http://arxiv.org/abs/2210.05033</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#19979;&#30340;&#22810;&#35821;&#35328;&#34920;&#31034;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multilingual Representation Distillation with Contrastive Learning. (arXiv:2210.05033v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#27604;&#23398;&#20064;&#34701;&#20837;&#21040;&#22810;&#35821;&#35328;&#34920;&#31034;&#33976;&#39311;&#20013;&#65292;&#29992;&#20110;&#24179;&#34892;&#35821;&#21477;&#30340;&#36136;&#37327;&#20272;&#35745;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#19981;&#21516;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26041;&#27861;&#20248;&#20110;&#20043;&#21069;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#20363;&#22914; LASER&#65292;LASER3 &#21644; LaBSE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#20174;&#22810;&#31181;&#35821;&#35328;&#20013;&#32534;&#30721;&#35821;&#20041;&#20449;&#24687;&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#34920;&#31034;&#21487;&#29992;&#20110;&#19981;&#21516;&#30340;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#21644;&#21305;&#37197;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#23545;&#27604;&#23398;&#20064;&#34701;&#20837;&#21040;&#22810;&#35821;&#35328;&#34920;&#31034;&#33976;&#39311;&#20013;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24179;&#34892;&#35821;&#21477;&#30340;&#36136;&#37327;&#20272;&#35745;&#65288;&#21363;&#26597;&#25214;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21477;&#23376;&#65292;&#21487;&#29992;&#20316;&#30456;&#20114;&#20043;&#38388;&#30340;&#32763;&#35793;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#35821;&#35328;&#30456;&#20284;&#24615;&#25628;&#32034;&#21644;&#35821;&#26009;&#24211;&#36807;&#28388;&#20219;&#21153;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#20363;&#22914; LASER&#65292;LASER3 &#21644; LaBSE&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual sentence representations from large models encode semantic information from two or more languages and can be used for different cross-lingual information retrieval and matching tasks. In this paper, we integrate contrastive learning into multilingual representation distillation and use it for quality estimation of parallel sentences (i.e., find semantically similar sentences that can be used as translations of each other). We validate our approach with multilingual similarity search and corpus filtering tasks. Experiments across different low-resource languages show that our method greatly outperforms previous sentence encoders such as LASER, LASER3, and LaBSE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#19968;&#31995;&#21015;&#38382;&#31572;&#23545;&#30340;&#24207;&#21015;&#26469;&#24102;&#26377;&#38382;&#31572;&#34013;&#22270;&#30340;&#26465;&#20214;&#29983;&#25104;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#24544;&#23454;&#24230;&#12289;&#35206;&#30422;&#29575;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2207.00397</link><description>&lt;p&gt;
&#24102;&#26377;&#38382;&#31572;&#34013;&#22270;&#30340;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Generation with a Question-Answering Blueprint. (arXiv:2207.00397v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#19968;&#31995;&#21015;&#38382;&#31572;&#23545;&#30340;&#24207;&#21015;&#26469;&#24102;&#26377;&#38382;&#31572;&#34013;&#22270;&#30340;&#26465;&#20214;&#29983;&#25104;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#24544;&#23454;&#24230;&#12289;&#35206;&#30422;&#29575;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26465;&#20214;&#29983;&#25104;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#20256;&#36798;&#30456;&#20851;&#21644;&#30495;&#23454;&#30340;&#20449;&#24687;&#30340;&#33021;&#21147;&#38750;&#24120;&#20851;&#38190;&#65292;&#28982;&#32780;&#31070;&#32463; seq-to-seq &#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#24187;&#35273;&#32780;&#26410;&#33021;&#27491;&#30830;&#28085;&#30422;&#37325;&#35201;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21010;&#20316;&#20026;&#28210;&#26579;&#26465;&#20214;&#29983;&#25104;&#36739;&#23569;&#27169;&#31946;&#21644;&#26356;&#21487;&#25805;&#20316;&#20013;&#38388;&#34920;&#36798;&#26041;&#24335;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#25991;&#26412;&#35268;&#21010;&#30340;&#26032;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31995;&#21015;&#38382;&#31572;&#23545;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#29983;&#25104;&#25216;&#26415;&#33258;&#21160;&#33719;&#21462;&#34013;&#22270;&#65292;&#24182;&#23558;&#36755;&#20837;-&#36755;&#20986;&#23545;&#36716;&#25442;&#20026;&#36755;&#20837;&#34013;&#22270;-&#36755;&#20986;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#22914;&#20309;&#32467;&#21512;&#34013;&#22270;&#19981;&#21516;&#65288;&#20363;&#22914;&#20316;&#20026;&#20840;&#23616;&#35745;&#21010;&#25110;&#36845;&#20195;&#35745;&#21010;&#65289;&#12290;&#38024;&#23545;&#20004;&#20010;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#35206;&#30422;&#29575;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details. In this work, we advocate planning as a useful intermediate representation for rendering conditional generation less opaque and more grounded. Our work proposes a new conceptualization of text plans as a sequence of question-answer (QA) pairs. We enhance existing datasets (e.g., for summarization) with a QA blueprint operating as a proxy for both content selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain blueprints automatically by exploiting state-of-the-art question generation technology and convert input-output pairs into input-blueprint-output tuples. We develop Transformer-based models, each varying in how they incorporate the blueprint in the generated output (e.g., as a global plan or iteratively). Evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;ILQL&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#37319;&#29992;&#20215;&#20540;&#20445;&#23432;&#24615;&#21644;&#38544;&#24335;&#25968;&#25454;&#38598;&#25903;&#25345;&#32422;&#26463;&#30340;&#32452;&#21512;&#65292;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26368;&#22823;&#21270;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#35821;&#35328;&#36755;&#20986;&#65292;&#20174;&#32780;&#35299;&#20915;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#29992;&#25143;&#25351;&#23450;&#20219;&#21153;&#26102;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11871</link><description>&lt;p&gt;
&#22522;&#20110;&#38544;&#24335;&#35821;&#35328;Q&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;ILQL&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#37319;&#29992;&#20215;&#20540;&#20445;&#23432;&#24615;&#21644;&#38544;&#24335;&#25968;&#25454;&#38598;&#25903;&#25345;&#32422;&#26463;&#30340;&#32452;&#21512;&#65292;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26368;&#22823;&#21270;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#35821;&#35328;&#36755;&#20986;&#65292;&#20174;&#32780;&#35299;&#20915;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#29992;&#25143;&#25351;&#23450;&#20219;&#21153;&#26102;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#25552;&#28860;&#20986;&#24191;&#27867;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#29992;&#25143;&#25351;&#23450;&#30340;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#38544;&#24335;&#35821;&#35328;Q&#23398;&#20064;&#65288;ILQL&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;RL&#31639;&#27861;&#30340;&#28789;&#27963;&#25928;&#29992;&#26368;&#22823;&#21270;&#26694;&#26550;&#19982;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20854;&#31616;&#21333;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#37319;&#29992;&#20215;&#20540;&#20445;&#23432;&#24615;&#21644;&#38544;&#24335;&#25968;&#25454;&#38598;&#25903;&#25345;&#32422;&#26463;&#30340;&#32452;&#21512;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#12290;&#38500;&#20102;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;ILQL&#65292;&#25105;&#20204;&#36824;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#33021;&#22815;&#26377;&#29992;&#30340;&#22330;&#26223;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language gen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20803;&#33258;&#25105;&#23436;&#21892;&#65288;MSR&#65289;&#30340;&#25239;&#22122;&#22768;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#25945;&#24072;&#27169;&#22411;&#26469;&#25552;&#20379;&#39640;&#24230;&#33258;&#20449;&#30340;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#36718;&#27425;&#23398;&#29983;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#26356;&#26032;&#25945;&#24072;&#27169;&#22411;&#65292;&#36739;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2205.07290</link><description>&lt;p&gt;
&#20803;&#33258;&#25105;&#23436;&#21892;&#65306;&#20351;&#29992;&#24369;&#30417;&#30563;&#23454;&#29616;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20803;&#33258;&#25105;&#23436;&#21892;&#65288;MSR&#65289;&#30340;&#25239;&#22122;&#22768;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#25945;&#24072;&#27169;&#22411;&#26469;&#25552;&#20379;&#39640;&#24230;&#33258;&#20449;&#30340;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#36718;&#27425;&#23398;&#29983;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#26356;&#26032;&#25945;&#24072;&#27169;&#22411;&#65292;&#36739;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#19979;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#24369;&#30417;&#30563;&#30340;&#26631;&#31614;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#65292;&#32780;DNN&#30340;&#39640;&#23481;&#37327;&#20351;&#20854;&#24456;&#23481;&#26131;&#36807;&#24230;&#36866;&#24212;&#26631;&#31614;&#22122;&#22768;&#65292;&#23548;&#33268;&#27867;&#21270;&#24615;&#33021;&#24046;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#25105;&#35757;&#32451;&#26469;&#26500;&#24314;&#25239;&#22122;&#22768;&#27169;&#22411;&#65292;&#20854;&#20013;&#20351;&#29992;&#22312;&#24369;&#30417;&#30563;&#19979;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#20026;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#39640;&#24230;&#33258;&#20449;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20174;&#36825;&#31181;&#26694;&#26550;&#20013;&#25512;&#23548;&#20986;&#30340;&#25945;&#24072;&#27169;&#22411;&#21487;&#33021;&#24050;&#32463;&#25311;&#21512;&#20102;&#22823;&#37327;&#22122;&#22768;&#65292;&#24182;&#22240;&#27492;&#20135;&#29983;&#20102;&#39640;&#32622;&#20449;&#24230;&#30340;&#38169;&#35823;&#20266;&#26631;&#31614;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#38169;&#35823;&#20256;&#25773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#33258;&#25105;&#23436;&#21892;&#65288;MSR&#65289;&#30340;&#25239;&#22122;&#22768;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#25269;&#25239;&#26469;&#33258;&#24369;&#30417;&#30563;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#25105;&#20204;&#40723;&#21169;&#25945;&#24072;&#27169;&#22411;&#25913;&#21892;&#20854;&#20266;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#20351;&#29992;&#22122;&#22768;&#26631;&#31614;&#35757;&#32451;&#30340;&#22266;&#23450;&#25945;&#24072;&#27169;&#22411;&#12290;&#22312;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#20013;&#65292;MSR&#21516;&#26102;&#26356;&#26032;&#23398;&#29983;&#21644;&#25945;&#24072;&#27169;&#22411;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#36718;&#27425;&#23398;&#29983;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;MSR&#26041;&#27861;&#22987;&#32456;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noise-resistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudo-labels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudo-labels. At each training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37051;&#36817;&#35821;&#35328;&#25552;&#39640;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21704;&#22827;&#26364;&#26641;&#23454;&#29616;&#22810;&#35821;&#35328;&#30340;&#20998;&#23618;Softmax&#35299;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#31867;&#20284;&#26631;&#35760;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#20849;&#20139;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25552;&#21319;&#25552;&#20379;&#20102;&#26377;&#25928;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2204.03855</link><description>&lt;p&gt;
&#38754;&#21521;&#20302;&#36164;&#28304;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#30340;&#20998;&#23618;Softmax&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition. (arXiv:2204.03855v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37051;&#36817;&#35821;&#35328;&#25552;&#39640;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21704;&#22827;&#26364;&#26641;&#23454;&#29616;&#22810;&#35821;&#35328;&#30340;&#20998;&#23618;Softmax&#35299;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#31867;&#20284;&#26631;&#35760;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#20849;&#20139;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25552;&#21319;&#25552;&#20379;&#20102;&#26377;&#25928;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#22791;&#21463;&#22256;&#25200;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#37051;&#36817;&#35821;&#35328;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#30456;&#37051;&#35821;&#35328;&#20013;&#65292;&#31867;&#20284;&#30340;&#35821;&#35328;&#21333;&#20803;&#21576;&#29616;&#20986;&#21487;&#27604;&#30340;&#35789;&#39033;&#39057;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#21704;&#22827;&#26364;&#26641;&#26469;&#25191;&#34892;&#22810;&#35821;&#35328;&#30340;&#20998;&#23618;Softmax&#35299;&#30721;&#12290;&#36825;&#31181;&#20998;&#23618;&#32467;&#26500;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#26631;&#35760;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#20849;&#20139;&#65292;&#20174;&#32780;&#22686;&#24378;&#20302;&#36164;&#28304;&#35757;&#32451;&#32467;&#26524;&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource speech recognition has been long-suffering from insufficient training data. In this paper, we propose an approach that leverages neighboring languages to improve low-resource scenario performance, founded on the hypothesis that similar linguistic units in neighboring languages exhibit comparable term frequency distributions, which enables us to construct a Huffman tree for performing multilingual hierarchical Softmax decoding. This hierarchical structure enables cross-lingual knowledge sharing among similar tokens, thereby enhancing low-resource training outcomes. Empirical analyses demonstrate that our method is effective in improving the accuracy and efficiency of low-resource speech recognition.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35752;&#35770;&#20102;&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#26680;&#24515;&#27010;&#24565;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30452;&#35273;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#37322;&#21487;&#25552;&#39640;&#20154;&#31867;&#23545;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#21017;&#27809;&#26377;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2202.04092</link><description>&lt;p&gt;
&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Explanations and Human Understanding. (arXiv:2202.04092v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04092
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35752;&#35770;&#20102;&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#26680;&#24515;&#27010;&#24565;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30452;&#35273;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#37322;&#21487;&#25552;&#39640;&#20154;&#31867;&#23545;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#21017;&#27809;&#26377;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#34987;&#20551;&#35774;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#21508;&#31181;&#26377;&#30410;&#30340;&#32467;&#26524;&#65292;&#20174;&#27169;&#22411;&#35843;&#35797;&#21040;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#19981;&#19968;&#33268;&#29978;&#33267;&#36127;&#38754;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#26159;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#30340;&#29702;&#35299;&#65292;&#24182;&#20197;&#20309;&#31181;&#26041;&#24335;&#12290;&#20351;&#29992;&#25913;&#36827;&#30340;&#22240;&#26524;&#22270;&#34920;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27491;&#24335;&#29305;&#24449;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20154;&#31867;&#30452;&#35273;&#22312;&#21551;&#29992;&#20154;&#31867;&#29702;&#35299;&#20013;&#21457;&#25381;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#29616;&#26377;&#37327;&#21270;&#29702;&#35299;&#30340;&#25514;&#26045;&#65292;&#21363;&#22312;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#30340;&#32972;&#26223;&#19979;&#30340;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#12289;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#65292;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30452;&#35273;&#30340;&#20551;&#35774;&#65292;&#35299;&#37322;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#25552;&#39640;&#20154;&#31867;&#23545;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#65292;&#21017;&#27809;&#26377;&#20805;&#20998;&#30340;&#35777;&#25454;&#34920;&#26126;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, bu
&lt;/p&gt;</description></item><item><title>YourTTS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38754;&#21521;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#21512;&#25104;&#19982;&#38646;&#25968;&#25454;&#35821;&#38899;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#22312;&#30456;&#20851;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.02418</link><description>&lt;p&gt;
YourTTS&#65306;&#38754;&#21521;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#21644;&#38646;&#25968;&#25454;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02418
&lt;/p&gt;
&lt;p&gt;
YourTTS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38754;&#21521;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#21512;&#25104;&#19982;&#38646;&#25968;&#25454;&#35821;&#38899;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#22312;&#30456;&#20851;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
YourTTS&#23558;&#22810;&#35821;&#35328;&#26041;&#27861;&#24212;&#29992;&#20110;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;VITS&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;&#21644;&#22810;&#35821;&#35328;&#22521;&#35757;&#12290;&#25105;&#20204;&#22312;VCTK&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;TTS&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#22312;&#38646;&#25968;&#25454;&#35821;&#38899;&#36716;&#25442;&#26041;&#38754;&#24471;&#21040;&#20102;&#19982;&#26368;&#26032;&#32467;&#26524;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#25968;&#25454;&#22810;&#35828;&#35805;&#20154;TTS&#21644;&#38646;&#25968;&#25454;&#35821;&#38899;&#36716;&#25442;&#31995;&#32479;&#24320;&#36767;&#20102;&#21487;&#33021;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23569;&#20110;1&#20998;&#38047;&#30340;&#35821;&#38899;&#24494;&#35843;YourTTS&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#35821;&#38899;&#30456;&#20284;&#24230;&#26041;&#38754;&#30340;&#26368;&#26032;&#32467;&#26524;&#21644;&#21512;&#29702;&#30340;&#36136;&#37327;&#12290;&#36825;&#23545;&#20110;&#20801;&#35768;&#21512;&#25104;&#20855;&#26377;&#19982;&#35757;&#32451;&#26399;&#38388;&#19981;&#21516;&#30340;&#21457;&#35328;&#32773;&#22768;&#38899;&#25110;&#24405;&#21046;&#29305;&#24449;&#30340;&#35762;&#35805;&#20154;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#21644;&#30446;&#26631;&#31572;&#26696;&#20013;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2004.06015</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#24341;&#23548;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#39064;&#29983;&#25104;&#65306;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Toward Subgraph-Guided Knowledge Graph Question Generation with Graph Neural Networks. (arXiv:2004.06015v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.06015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#21644;&#30446;&#26631;&#31572;&#26696;&#20013;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38382;&#39064;&#29983;&#25104;&#26088;&#22312;&#20174;&#30693;&#35782;&#22270;&#35889;&#21644;&#30446;&#26631;&#31572;&#26696;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#20174;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#20013;&#29983;&#25104;&#38382;&#39064;&#30340;&#31616;&#21333;&#35774;&#32622;&#19978;&#12290;&#26412;&#25991;&#38024;&#23545;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#20174;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#21644;&#30446;&#26631;&#31572;&#26696;&#20013;&#29983;&#25104;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#22522;&#20110;RNN&#25110;Transformer&#30340;&#27169;&#22411;&#23545;&#32447;&#24615;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#23436;&#20840;&#20002;&#24323;&#20102;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#30340;&#26174;&#24335;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24212;&#29992;&#21452;&#21521;Graph2Seq&#27169;&#22411;&#23545;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;RNN&#35299;&#30721;&#22120;&#65292;&#21152;&#20837;&#20102;&#33410;&#28857;&#32423;&#21035;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#20801;&#35768;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#20013;&#22797;&#21046;&#33410;&#28857;&#23646;&#24615;&#21040;&#36755;&#20986;&#38382;&#39064;&#20013;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#38382;&#39064;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting which is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most of previous works built on either RNN-based or Transformer based models to encode a linearized KG sugraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with node-level copying mechanism to allow directly copying node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#23454;&#29992;&#32452;&#20214;&#65292;&#21487;&#33258;&#21160;&#26816;&#27979;&#21644;&#20462;&#27491;&#29992;&#25143;&#21457;&#20986;&#30340;&#35831;&#27714;&#20449;&#24687;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#20462;&#27491;&#20449;&#24687;&#36827;&#34892;&#25552;&#21462;&#23545;&#65292;&#20197;&#23454;&#29616;&#23398;&#20064;&#21644;&#36991;&#20813;&#37325;&#22797;&#24320;&#21457;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#24207;&#21015;&#26631;&#31614;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;&#24207;&#21015;&#20998;&#31867;&#31561;&#24773;&#24418;&#12290;</title><link>http://arxiv.org/abs/2004.04243</link><description>&lt;p&gt;
&#35831;&#27714;&#23545;&#35805;&#30340;&#38169;&#35823;&#32416;&#27491;&#21644;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Error correction and extraction in request dialogs. (arXiv:2004.04243v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.04243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#23454;&#29992;&#32452;&#20214;&#65292;&#21487;&#33258;&#21160;&#26816;&#27979;&#21644;&#20462;&#27491;&#29992;&#25143;&#21457;&#20986;&#30340;&#35831;&#27714;&#20449;&#24687;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#20462;&#27491;&#20449;&#24687;&#36827;&#34892;&#25552;&#21462;&#23545;&#65292;&#20197;&#23454;&#29616;&#23398;&#20064;&#21644;&#36991;&#20813;&#37325;&#22797;&#24320;&#21457;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#24207;&#21015;&#26631;&#31614;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;&#24207;&#21015;&#20998;&#31867;&#31561;&#24773;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#23454;&#29992;&#32452;&#20214;&#65292;&#21487;&#20197;&#33719;&#21462;&#29992;&#25143;&#30340;&#26368;&#21518;&#20004;&#20010;&#35805;&#35821;&#65292;&#24182;&#26816;&#27979;&#26368;&#21518;&#19968;&#21477;&#35805;&#26159;&#21542;&#26159;&#23545;&#31532;&#20108;&#21477;&#35805;&#30340;&#38169;&#35823;&#32416;&#27491;&#12290;&#22914;&#26524;&#26159;&#65292;&#21017;&#26681;&#25454;&#26368;&#21518;&#19968;&#21477;&#35805;&#20013;&#30340;&#38169;&#35823;&#32416;&#27491;&#26469;&#32416;&#27491;&#31532;&#20108;&#21477;&#35805;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#32452;&#20214;&#36755;&#20986;&#20102;&#34987;&#20462;&#22797;&#21644;&#20462;&#22797;&#20307;&#30340;&#25552;&#21462;&#23545;&#12290;&#36825;&#20010;&#32452;&#20214;&#25552;&#20379;&#20102;&#20004;&#20010;&#20248;&#28857;&#65292;&#19968;&#26159;&#23398;&#20064;&#32416;&#27491;&#30340;&#27010;&#24565;&#20197;&#36991;&#20813;&#20026;&#27599;&#20010;&#26032;&#22495;&#25910;&#38598;&#32416;&#27491;&#65292;&#20108;&#26159;&#25552;&#21462;&#34987;&#20462;&#22797;&#21644;&#20462;&#22797;&#23545;&#65292;&#20174;&#32780;&#25552;&#20379;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#20110;&#38169;&#35823;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#26631;&#31614;&#21644;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#12290;&#23545;&#20110;&#38169;&#35823;&#32416;&#27491;&#26816;&#27979;&#65292;&#36825;&#19977;&#31181;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#20063;&#21487;&#20197;&#34987;&#29992;&#26469;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#12290;&#19968;&#20010;&#38169;&#35823;&#32416;&#27491;&#26816;&#27979;&#21644;&#19968;&#20010;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#21487;&#20197;&#32452;&#21512;&#25104;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#25110;&#32773;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#21487;&#20197;&#34987;&#20998;&#21035;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a dialog system utility component that gets the two last utterances of a user and can detect whether the last utterance is an error correction of the second last utterance. If yes, it corrects the second last utterance according to the error correction in the last utterance. In addition, the proposed component outputs the extracted pairs of reparandum and repair entity. This component offers two advantages, learning the concept of corrections to avoid collecting corrections for every new domain and extracting reparandum and repair pairs, which offers the possibility to learn out of it.  For the error correction one sequence labeling and two sequence to sequence approaches are presented. For the error correction detection these three error correction approaches can also be used and in addition, we present a sequence classification approach. One error correction detection and one error correction approach can be combined to a pipeline or the error correction approaches can be 
&lt;/p&gt;</description></item></channel></rss>