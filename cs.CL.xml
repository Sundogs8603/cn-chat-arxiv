<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25512;&#26029;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10427</link><description>&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#21152;&#36895;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10427
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25512;&#26029;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#38480;&#21046;&#20102;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#29575;&#12290;&#31038;&#21306;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#24456;&#26114;&#36149;&#24182;&#19988;&#38656;&#35201;&#25913;&#21464;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#25512;&#23548;&#20986;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#20174;&#35299;&#30721;&#31639;&#27861;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#26631;&#20934;&#30340;&#36138;&#24515;&#33258;&#22238;&#24402;&#35299;&#30721;&#36716;&#21270;&#20026;&#24182;&#34892;&#35299;&#30721;&#65292;&#24182;&#21033;&#29992;&#38597;&#20811;&#27604;&#21644;&#39640;&#26031;-&#22622;&#24503;&#23572;&#36845;&#20195;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#24182;&#34892;&#21270;&#35299;&#30721;&#30456;&#23545;&#20110;&#26631;&#20934;&#33258;&#22238;&#24402;&#35299;&#30721;&#21487;&#25552;&#39640;&#36798;38&#65285;&#30340;&#36895;&#24230;&#65292;&#24403;&#25193;&#23637;&#27169;&#22411;&#26102;&#65292;&#36895;&#24230;&#20960;&#20046;&#25552;&#39640;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SLiC-HF&#65292;&#21487;&#20197;&#21033;&#29992;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#30456;&#36739;&#20110;&#36807;&#21435;&#30340;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#39640;&#25928;&#65292;&#24182;&#22312;TL;DR&#33258;&#21160;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#30417;&#30563;&#24494;&#35843;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.10425</link><description>&lt;p&gt;
SLiC-HF&#65306;&#20154;&#31867;&#21453;&#39304;&#30340;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
SLiC-HF: Sequence Likelihood Calibration with Human Feedback. (arXiv:2305.10425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SLiC-HF&#65292;&#21487;&#20197;&#21033;&#29992;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#30456;&#36739;&#20110;&#36807;&#21435;&#30340;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#39640;&#25928;&#65292;&#24182;&#22312;TL;DR&#33258;&#21160;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#30417;&#30563;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20998;&#37197;&#30340;&#22870;&#21169;&#20998;&#25968;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65288;SLiC-HF&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#21487;&#20197;&#20351;&#29992;&#20026;&#19981;&#21516;&#27169;&#22411;&#25910;&#38598;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#26469;&#23436;&#25104;&#65292;&#31867;&#20284;&#20110;&#31163;&#32447;RL&#25968;&#25454;&#30340;&#31163;&#32447;&#23398;&#20064;&#12290;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#65292;SLiC-HF&#26174;&#33879;&#25913;&#36827;&#20102;&#30417;&#30563;&#24494;&#35843;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;SLiC-HF&#26159;&#36807;&#21435;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;PPO RLHF&#23454;&#29616;&#30340;&#31454;&#20105;&#24615;&#26367;&#20195;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26356;&#31616;&#21333;&#12289;&#26356;&#26131;&#20110;&#35843;&#25972;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#21306;&#22359;&#38142;&#39046;&#22495;&#30340;&#20449;&#24687;&#24182;&#32452;&#32455;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#27983;&#35272;&#35813;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.10408</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21306;&#22359;&#38142;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Extracting Blockchain Concepts from Text. (arXiv:2305.10408v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#21306;&#22359;&#38142;&#39046;&#22495;&#30340;&#20449;&#24687;&#24182;&#32452;&#32455;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#27983;&#35272;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#36890;&#36807;&#35813;&#26426;&#21046;&#65292;&#30456;&#20114;&#19981;&#20449;&#20219;&#30340;&#36828;&#31243;&#26041;&#21487;&#20197;&#23601;&#20449;&#24687;&#20998;&#31867;&#36134;&#30340;&#29366;&#24577;&#36798;&#25104;&#20849;&#35782;&#12290;&#38543;&#30528;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#23398;&#20064;&#21306;&#22359;&#38142;&#30340;&#20154;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#25216;&#26415;&#24615;&#30340;&#20027;&#39064;&#65292;&#24320;&#22987;&#23398;&#20064;&#21487;&#33021;&#20250;&#24863;&#21040;&#30456;&#24403;&#19981;&#21487;&#24605;&#35758;&#12290;&#22240;&#27492;&#65292;&#35813;&#39033;&#30446;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#30333;&#30382;&#20070;&#21644;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#20851;&#20110;&#21306;&#22359;&#38142;&#39046;&#22495;&#30340;&#20449;&#24687;&#65292;&#20197;&#32452;&#32455;&#36825;&#20123;&#20449;&#24687;&#24182;&#24110;&#21161;&#29992;&#25143;&#27983;&#35272;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchains provide a mechanism through which mutually distrustful remote parties can reach consensus on the state of a ledger of information. With the great acceleration with which this space is developed, the demand for those seeking to learn about blockchain also grows. Being a technical subject, it can be quite intimidating to start learning. For this reason, the main objective of this project was to apply machine learning models to extract information from whitepapers and academic articles focused on the blockchain area to organize this information and aid users to navigate the space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; BAD &#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#26469;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#19981;&#20844;&#24179;&#21644;&#20559;&#35265;&#65292;&#20197;&#35299;&#20915;&#20154;&#20026;&#24178;&#39044;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10407</link><description>&lt;p&gt;
&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#20506;&#26816;&#27979;&#65306;&#20197; BAD &#27169;&#22411;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
BAD: BiAs Detection for Large Language Models in the context of candidate screening. (arXiv:2305.10407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; BAD &#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#26469;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#19981;&#20844;&#24179;&#21644;&#20559;&#35265;&#65292;&#20197;&#35299;&#20915;&#20154;&#20026;&#24178;&#39044;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#36319;&#36394;&#31995;&#32479;&#65288;ATS&#65289;&#20351;&#24471;&#20154;&#25165;&#32463;&#29702;&#12289;&#25307;&#32856;&#20154;&#21592;&#21644;&#22823;&#23398;&#25307;&#29983;&#22996;&#21592;&#20250;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#37327;&#30340;&#20505;&#36873;&#20154;&#30003;&#35831;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#31579;&#36873;&#36807;&#31243;&#26159;&#25163;&#24037;&#36827;&#34892;&#30340;&#65292;&#30001;&#20110;&#30003;&#35831;&#25968;&#30340;&#25968;&#37327;&#65292;&#23384;&#22312;&#24456;&#22810;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#35768;&#22810;&#30340;&#20154;&#20026;&#20559;&#35265;&#12290;&#38543;&#30528; ChatGPT &#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#20986;&#20197;&#21450;&#23558;&#26041;&#27861;&#24212;&#29992;&#21040;&#24403;&#21069;&#30340;&#33258;&#21160;&#21270;&#24212;&#29992;&#31579;&#36873;&#20013;&#65292;&#36825;&#23548;&#33268;&#20102;&#36827;&#19968;&#27493;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#30340;&#32972;&#26223;&#19979;&#65292;&#35782;&#21035;&#21644;&#37327;&#21270; ChatGPT &#21644;&#20854;&#20182; OpenAI LLMs &#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20197;&#35777;&#26126;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#24310;&#32493;&#29616;&#26377;&#30340;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#22312;&#25307;&#32856;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.
&lt;/p&gt;</description></item><item><title>PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10403</link><description>&lt;p&gt;
PaLM 2 &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10403
&lt;/p&gt;
&lt;p&gt;
PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; PaLM 2&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27604;&#20854;&#21069;&#36523; PaLM &#22312;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26356;&#21152;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;PaLM 2 &#26159;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#20197;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; PaLM 2 &#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#65292;&#21516;&#26102;&#23637;&#29616;&#20102;&#27604; PaLM &#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#25928;&#29575;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#21516;&#26102;&#20063;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#21709;&#24212;&#65292;&#20197;&#33719;&#24471;&#26356;&#33258;&#28982;&#30340;&#20132;&#20114;&#33410;&#22863;&#12290;PaLM 2 &#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312; BIG-Bench &#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110; PaLM &#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;PaLM 2 &#22312;&#19968;&#22871;&#36127;&#36131;&#20154;&#30340; AI &#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38468;&#21152;&#36816;&#34892;&#24320;&#38144;&#25110;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23545;&#27602;&#24615;&#36827;&#34892;&#25512;&#29702;&#26102;&#38388;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.10400</link><description>&lt;p&gt;
&#20320;&#30475;&#21040;&#30340;&#23601;&#26159;&#20320;&#35835;&#21040;&#30340;? &#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30830;&#23450;&#25991;&#26412;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#26159;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;SeeTRUE&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#20154;&#31867;&#30340;&#21028;&#26029;&#65292;&#21028;&#26029;&#32473;&#23450;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#31181;&#33258;&#21160;&#30830;&#23450;&#23545;&#40784;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#38382;&#39064;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#31532;&#20108;&#31181;&#26159;&#36890;&#36807;&#24494;&#35843;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#28041;&#21450;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#28608;&#21169;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35814;&#32454;&#38416;&#36848;&#35270;&#20026;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;ElabQUD&#23545;&#20316;&#32773;&#38416;&#36848;&#20449;&#24687;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.10387</link><description>&lt;p&gt;
&#20316;&#20026;&#38544;&#21547;&#35752;&#35770;&#38382;&#39064;&#30340;&#35814;&#32454;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35814;&#32454;&#38416;&#36848;&#35270;&#20026;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;ElabQUD&#23545;&#20316;&#32773;&#38416;&#36848;&#20449;&#24687;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#19979;&#20174;&#22797;&#26434;&#21477;&#23376;&#21040;&#31616;&#21270;&#21477;&#23376;&#30340;&#21333;&#35821;&#32763;&#35793;&#24037;&#20316;&#65292;&#26377;&#21161;&#20110;&#20351;&#25991;&#26412;&#26356;&#26131;&#20110;&#35753;&#20799;&#31461;&#21644;&#26032;&#20852;&#21452;&#35821;&#32773;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35266;&#28857;&#24573;&#30053;&#20102;&#35814;&#32454;&#31616;&#21270;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#31616;&#21270;&#25991;&#26412;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#35814;&#32454;&#31616;&#21270;&#35270;&#20026;&#35752;&#35770;&#38382;&#39064;&#26694;&#26550;&#65288;QUD&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#35814;&#32454;&#38416;&#36848;&#30340;&#20449;&#24687;&#35270;&#20026;&#23545;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#30740;&#31350;&#20316;&#32773;&#38416;&#36848;&#21738;&#20123;&#20449;&#24687;&#12289;&#22914;&#20309;&#38416;&#36848;&#20197;&#21450;&#38416;&#36848;&#23558;&#22914;&#20309;&#36866;&#24212;&#35805;&#35821;&#32972;&#26223;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ElabQUD&#65292;&#20854;&#20013;&#21253;&#25324;1.3K&#30340;&#35814;&#32454;&#38416;&#36848;&#21644;&#38544;&#21547;&#30340;QUD&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for elaborative simplification, where new information is added into the simplified text. This paper proposes to view elaborative simplification through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context by viewing elaborations as explicit answers to implicit questions. We introduce ElabQUD, consisting of 1.3K elaborations accompanied with implicit QUDs, to study these phenomena. We show that explicitly modeling QUD (via question generation) not only provides essential understanding of elaborative simplification and how the elaborations connect with the re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Logit&#30340;&#38598;&#25104;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#65288;epistemic&#65289;&#21644;&#25968;&#25454;&#65288;aleatoric&#65289;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#33021;&#22815;&#25552;&#39640;student&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10384</link><description>&lt;p&gt;
&#22522;&#20110;Logit&#30340;&#38598;&#25104;&#20998;&#24067;&#33976;&#39311;&#22312;&#33258;&#22238;&#24402;&#24207;&#21015;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Logit&#30340;&#38598;&#25104;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#65288;epistemic&#65289;&#21644;&#25968;&#25454;&#65288;aleatoric&#65289;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#33021;&#22815;&#25552;&#39640;student&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#36890;&#24120;&#38750;&#24120;&#39640;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#30340;&#38598;&#25104;&#20998;&#24067;&#33976;&#39311;&#65288;Ensemble Distribution Distillation&#65292;EDD&#65289;&#26041;&#27861;&#12290;EDD&#26088;&#22312;&#23558;&#26114;&#36149;&#30340;&#65288;teacher&#65289;&#38598;&#25104;&#27169;&#22411;&#30340;&#20248;&#36234;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#21387;&#32553;&#21040;&#26356;&#20415;&#23452;&#30340;&#65288;student&#65289;&#21333;&#19968;&#27169;&#22411;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20445;&#30041;&#20102;&#23558;&#30693;&#35782;&#65288;&#35748;&#30693;&#65289;&#21644;&#25968;&#25454;&#65288;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#27010;&#29575;&#31354;&#38388;&#26041;&#27861;&#23545;&#20110;&#22823;&#35789;&#27719;&#37327;&#30340;&#20219;&#21153;&#26469;&#35828;&#19981;&#26131;&#25193;&#23637;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#32763;&#35793;&#20219;&#21153;&#30340;&#29616;&#20195;Transformers&#27169;&#22411;&#20013;&#65292;&#23545;&#38598;&#25104;&#27169;&#22411;&#30340;logits&#36827;&#34892;&#24314;&#27169;&#27604;&#23545;softmax&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;student&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently and reliably estimating uncertainty is an important objective in deep learning. It is especially pertinent to autoregressive sequence tasks, where training and inference costs are typically very high. However, existing research has predominantly focused on tasks with static data such as image classification. In this work, we investigate Ensemble Distribution Distillation (EDD) applied to large-scale natural language sequence-to-sequence data. EDD aims to compress the superior uncertainty performance of an expensive (teacher) ensemble into a cheaper (student) single model. Importantly, the ability to separate knowledge (epistemic) and data (aleatoric) uncertainty is retained. Existing probability-space approaches to EDD, however, are difficult to scale to large vocabularies. We show, for modern transformer architectures on large-scale translation tasks, that modelling the ensemble logits, instead of softmax probabilities, leads to significantly better students. Moreover, the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10383</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65306;&#22312;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#27010;&#24565;&#32780;&#35328;&#65292;&#20934;&#30830;&#26631;&#35760;&#24120;&#24120;&#24456;&#38590;&#23454;&#29616;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#30340;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;&#32654;&#22269;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;InnovationQ+&#19978;&#25552;&#20132;&#30340;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#32467;&#26524;&#19982;&#26469;&#33258;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#65292;&#24635;&#35745;5.4&#30334;&#19975;&#21477;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#36825;&#20123;AI&#19987;&#21033;&#21477;&#23376;&#20013;&#30340;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;GPT-4&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#30340;&#23450;&#20041;&#12289;&#25351;&#23548;&#26041;&#38024;&#12289;&#31034;&#20363;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;BLEU&#20998;&#25968;&#21644;&#20027;&#39064;&#24314;&#27169;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#21644;&#29702;&#24615;&#21270;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#24433;&#21709;&#24187;&#35273;&#65292;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2305.10355</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#24433;&#21709;&#24187;&#35273;&#65292;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#25496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#33021;&#21147;&#36817;&#26469;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLM)&#65292;&#24182;&#23558;&#24378;&#22823;&#30340;LLM&#38598;&#25104;&#20110;LVLM&#20013;&#65292;&#20197;&#25552;&#39640;LVLM&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;LVLM&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#27493;&#65292;&#20294;&#26159;&#26412;&#30740;&#31350;&#21457;&#29616;LVLM&#23384;&#22312;&#38271;&#24230;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#19982;&#30446;&#26631;&#22270;&#20687;&#19981;&#19968;&#33268;&#30340;&#29289;&#20307;&#25551;&#36848;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24320;&#23637;&#20102;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;LVLM&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;LVLM&#36827;&#34892;&#20102;&#35780;&#20272;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#22823;&#22810;&#25968;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#20250;&#24433;&#21709;&#24187;&#35273;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#25351;&#20196;&#20013;&#32463;&#24120;&#20986;&#29616;&#25110;&#19982;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#20849;&#29616;&#30340;&#29289;&#20307;&#65292;&#26356;&#23481;&#26131;&#34987;LVLM&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#20250;&#21463;&#21040;&#36755;&#20837;&#25351;&#20196;&#30340;&#24433;&#21709;&#65292;&#19981;&#33021;&#36275;&#20197;&#35782;&#21035;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#24187;&#35273;&#38382;&#39064;&#20986;&#29616;&#20301;&#32622;&#21644;&#22914;&#20309;&#32531;&#35299;&#23427;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#21069;&#31471;&#65292;&#20174;&#23545;&#35805;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20132;&#20114;&#24335;&#23545;&#35805;&#36716;&#25442;&#20026;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36882;&#24402;&#22320;&#35201;&#27714;&#26410;&#30693;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#21487;&#20197;&#33719;&#21462;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#24182;&#22312;&#33258;&#28982;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#37325;&#22797;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10349</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#20174;&#23545;&#35805;&#20013;&#20132;&#20114;&#23398;&#20064;&#20998;&#23618;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Interactive Learning of Hierarchical Tasks from Dialog with GPT. (arXiv:2305.10349v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#21069;&#31471;&#65292;&#20174;&#23545;&#35805;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20132;&#20114;&#24335;&#23545;&#35805;&#36716;&#25442;&#20026;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36882;&#24402;&#22320;&#35201;&#27714;&#26410;&#30693;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#21487;&#20197;&#33719;&#21462;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#24182;&#22312;&#33258;&#28982;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#37325;&#22797;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;GPT&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#21069;&#31471;&#65292;&#20174;&#23545;&#35805;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#12290;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#34987;&#34920;&#31034;&#20026;&#35859;&#35789;-&#21442;&#25968;&#32467;&#26500;&#30340;&#20998;&#23618;&#20998;&#35299;&#65292;&#20855;&#26377;&#20316;&#29992;&#22495;&#21464;&#37327;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;GPT&#27169;&#22411;&#23558;&#20132;&#20114;&#24335;&#23545;&#35805;&#36716;&#25442;&#20026;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36882;&#24402;&#22320;&#35201;&#27714;&#26410;&#30693;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#21487;&#20197;&#22312;&#33258;&#28982;&#21644;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#34987;&#33719;&#21462;&#21644;&#37325;&#22797;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479;&#19982;&#20351;&#29992;&#26356;&#20256;&#32479;&#30340;&#35299;&#26512;&#22120;&#30340;&#31867;&#20284;&#26550;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#23481;&#24525;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a system for interpretable, symbolic, interactive task learning from dialog using a GPT model as a conversational front-end. The learned tasks are represented as hierarchical decompositions of predicate-argument structures with scoped variable arguments. By using a GPT model to convert interactive dialog into a semantic representation, and then recursively asking for definitions of unknown steps, we show that hierarchical task knowledge can be acquired and re-used in a natural and unrestrained conversational environment. We compare our system to a similar architecture using a more conventional parser and show that our system tolerates a much wider variety of linguistic variance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;TTS&#35821;&#38899;&#34920;&#29616;&#39118;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#38750;&#34920;&#29616;&#24615;&#35821;&#26009;&#24211;&#19978;&#30340;TTS&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#38901;&#24459;&#24314;&#35758;&#65292;&#20351;&#20854;&#29983;&#25104;&#34920;&#29616;&#21147;&#26356;&#24378;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2305.10321</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#35828;&#35805;&#39118;&#26684;&#20197;&#23454;&#29616;&#34920;&#29616;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Using a Large Language Model to Control Speaking Style for Expressive TTS. (arXiv:2305.10321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10321
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;TTS&#35821;&#38899;&#34920;&#29616;&#39118;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#38750;&#34920;&#29616;&#24615;&#35821;&#26009;&#24211;&#19978;&#30340;TTS&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#38901;&#24459;&#24314;&#35758;&#65292;&#20351;&#20854;&#29983;&#25104;&#34920;&#29616;&#21147;&#26356;&#24378;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24688;&#24403;&#30340;&#38901;&#24459;&#23545;&#20110;&#25104;&#21151;&#30340;&#21475;&#22836;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#27979;&#38901;&#24459;&#26041;&#38754;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#19981;&#20801;&#35768;&#22312;&#21487;&#33021;&#30340;&#38901;&#24459;&#28436;&#32462;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#22522;&#20110;&#21442;&#32771;&#35821;&#38899;&#30340;TTS&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#22312;&#21442;&#32771;&#35821;&#38899;&#26679;&#26412;&#22522;&#30784;&#19978;&#29983;&#25104;&#35821;&#38899;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#65292;&#20294;&#38656;&#35201;&#25214;&#21040;&#36866;&#24403;&#30340;&#21442;&#32771;&#26679;&#26412;&#12290;&#24050;&#32463;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#21508;&#31181;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#24314;&#35758;&#36866;&#24403;&#30340;&#38901;&#24459;&#20197;&#23454;&#29616;&#34920;&#29616;&#24615;TTS&#12290;&#25105;&#20204;&#22312;&#38750;&#34920;&#29616;&#24615;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;TTS&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#26356;&#25913;&#38899;&#35843;&#12289;&#33021;&#37327;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;&#25552;&#31034;&#21487;&#20197;&#20026;&#20219;&#20309;&#20219;&#21153;&#35774;&#35745;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#35828;&#35805;&#39118;&#26684;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#25552;&#31034;&#27169;&#22411;&#36827;&#34892;&#24314;&#35758;&#12290;&#19982;&#22522;&#32447;&#27169;&#22411;&#30340;31.0&#65285;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;49.9&#65285;&#30340;&#24773;&#20917;&#19979;&#34987;&#35780;&#20026;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriate prosody is critical for successful spoken communication. Contextual word embeddings are proven to be helpful in predicting prosody but do not allow for choosing between plausible prosodic renditions. Reference-based TTS models attempt to address this by conditioning speech generation on a reference speech sample. These models can generate expressive speech but this requires finding an appropriate reference.  Sufficiently large generative language models have been used to solve various language-related tasks. We explore whether such models can be used to suggest appropriate prosody for expressive TTS. We train a TTS model on a non-expressive corpus and then prompt the language model to suggest changes to pitch, energy and duration. The prompt can be designed for any task and we prompt the model to make suggestions based on target speaking style and dialogue context. The proposed method is rated most appropriate in 49.9\% of cases compared to 31.0\% for a baseline model.
&lt;/p&gt;</description></item><item><title>LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.10314</link><description>&lt;p&gt;
LeTI&#65306;&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10314
&lt;/p&gt;
&lt;p&gt;
LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LM)&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#25216;&#26415;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#23545;&#65288;&#20363;&#22914;&#25351;&#20196;&#24494;&#35843;&#65289;&#25110;&#29992;&#35780;&#20272;&#36755;&#20986;&#36136;&#37327;&#30340;&#25968;&#23383;&#22870;&#21169;&#65288;&#20363;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;LM&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#28508;&#21147;(LeTI)&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20108;&#36827;&#21046;&#26631;&#31614;&#26816;&#26597;&#20854;&#27491;&#30830;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21453;&#39304;&#25351;&#20986;&#21644;&#35299;&#37322;&#20854;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#12290;&#36825;&#31181;&#35774;&#32622;&#21487;&#20197;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#22320;&#33719;&#21462;&#25991;&#26412;&#21453;&#39304;&#65306;&#20351;&#29992;Python&#35299;&#37322;&#22120;&#36827;&#34892;&#20195;&#30721;&#25191;&#34892;&#26102;&#30340;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#12290; LeTI&#20351;&#29992;LM&#30446;&#26631;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#25991;&#26412;&#21453;&#39304;&#36827;&#34892;&#20018;&#32852;&#30340;&#36845;&#20195;&#24494;&#35843;&#65292;&#21482;&#26377;&#22312;&#29983;&#25104;&#20195;&#30721;&#26080;&#27861;&#25191;&#34892;&#26102;&#25165;&#25552;&#20379;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;58k&#20010;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LeTI&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the gen
&lt;/p&gt;</description></item><item><title>FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.10307</link><description>&lt;p&gt;
FACE: &#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10307
&lt;/p&gt;
&lt;p&gt;
FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#35821;&#35328;&#23398;&#24515;&#29702;&#23398;&#20851;&#20110;&#35821;&#35328;&#29109;&#21608;&#26399;&#24615;&#23454;&#35777;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FACE&#8212;&#8212;&#19968;&#32452;&#22522;&#20110;&#35821;&#35328;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#20070;&#20889;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#24320;&#25918;&#24335;&#30340;&#29983;&#25104;&#20219;&#21153;&#21644;&#20197;&#21069;&#30740;&#31350;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;FACE&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#27169;&#22411;&#24046;&#36317;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26377;&#25152;&#32553;&#25918;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#19982;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#21028;&#26029;&#20998;&#25968;&#30456;&#20851;&#33391;&#22909;&#12290;FACE&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#24182;&#25552;&#20379;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the distance between machine-produced and human language is acritical open problem. Inspired by empirical findings from psycholinguistics on theperiodicity of entropy in language, we propose FACE, a set of metrics based onFourier Analysis of the estimated Cross-Entropy of language, for measuring thesimilarity between model-generated and human-written languages. Based on anopen-ended generation task and the experimental data from previous studies, weind that FACE can effectively identify the human-model gap, scales with modelsize, reflects the outcomes of different sampling methods for decoding, correlateswell with other evaluation metrics and with human judgment scores. FACE iscomputationally efficient and provides intuitive interpretations.
&lt;/p&gt;</description></item><item><title>UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10306</link><description>&lt;p&gt;
UniEX&#65306;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25552;&#21462;&#30340;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#30340;&#26377;&#25928;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10306
&lt;/p&gt;
&lt;p&gt;
UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#33539;&#24335;&#65292;&#23427;&#19982;&#20219;&#20309;&#27169;&#24335;&#26684;&#24335;&#20860;&#23481;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026; token-pair &#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#25552;&#21462;&#26694;&#26550; UniEX&#65292;&#23558;&#25152;&#26377;&#25552;&#21462;&#30446;&#26631;&#37117;&#32479;&#19968;&#20998;&#35299;&#20026;&#32852;&#21512;&#36328;&#24230;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#20851;&#32852;&#38382;&#39064;&#12290;UniEX &#21487;&#20197;&#21516;&#26102;&#32534;&#30721;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#23398;&#20064;&#39044;&#23450;&#20041;&#20449;&#24687;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102; traffine &#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#21253;&#25324;&#20219;&#21153;&#12289;&#26631;&#31614;&#21644;&#20869;&#37096; token &#22312;&#20869;&#30340;&#24322;&#26500;&#22240;&#32032;&#38598;&#25104;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#30697;&#38453;&#33719;&#24471;&#25552;&#21462;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniEX &#22312; $14$&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#37117;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#20013;&#26576;&#20123;&#31995;&#32479;&#30340;&#24471;&#20998;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.10284</link><description>&lt;p&gt;
&#26356;&#40065;&#26834;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#65306;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32570;&#22833;&#24471;&#20998;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#20013;&#26576;&#20123;&#31995;&#32479;&#30340;&#24471;&#20998;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#35780;&#20272;&#23545;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#25152;&#26377;&#31995;&#32479;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26377;&#21487;&#29992;&#30340;&#24471;&#20998;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#30340;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#33509;&#24178;&#22240;&#32032;&#65288;&#20363;&#22914;&#36816;&#34892;&#22522;&#32447;&#65292;&#31169;&#26377;&#31995;&#32479;&#65292;&#35745;&#31639;&#38480;&#21046;&#25110;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#65289;&#21487;&#33021;&#20250;&#38459;&#27490;&#26576;&#20123;&#31995;&#32479;&#22312;&#25972;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#27491;&#24335;&#38416;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#29616;&#26377;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#19968;&#20123;&#31995;&#32479;&#30340;&#20219;&#21153;&#24471;&#20998;&#32570;&#22833;&#26102;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20860;&#23481;&#30340;&#37096;&#20998;&#25490;&#21517;&#26041;&#27861;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;Borda&#35745;&#25968;&#26041;&#27861;&#36827;&#34892;&#32858;&#21512;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#29305;&#23450;&#20110;&#20219;&#21153;&#32423;&#24471;&#20998;&#25110;&#23454;&#20363;&#32423;&#24471;&#20998;&#21487;&#29992;&#30340;&#22330;&#26223;&#30340;&#32454;&#21270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;1.31&#20159;&#20010;&#24471;&#20998;&#65292;&#27604;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22823;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.10276</link><description>&lt;p&gt;
&#36830;&#38145;&#31526;&#21495;&#25552;&#31034;&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#30340;&#34394;&#25311;&#31354;&#38388;&#29615;&#22659;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#23427;&#30001;&#19968;&#32452;&#26032;&#39062;&#30340;&#20219;&#21153;&#32452;&#25104;&#65306;Brick World&#12289;&#22522;&#20110;NLVR&#30340;&#25805;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27969;&#34892;&#30340;LLMs&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#32570;&#20047;&#22797;&#26434;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;LLMs&#26159;&#21542;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#29615;&#22659;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#65292;&#25110;&#32773;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#31526;&#21495;&#34920;&#31034;&#65289;&#26159;&#21542;&#26356;&#21152;&#31616;&#21333;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#34987;LLMs&#29702;&#35299;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoS&#65288;Chain-of-Symbol Prompting&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38142;&#24335;&#20013;&#38388;&#24605;&#32771;&#27493;&#39588;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;CoS&#26131;&#20110;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#39069;&#22806;&#30340;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10270</link><description>&lt;p&gt;
&#22686;&#24378;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20004;&#32452;&#24120;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#30340;&#29305;&#24449;&#65288;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#65288;HoG&#65289;&#65289;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#30340;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of phone classification in the context of speech recognition, and explore several sets of local spectro-temporal features that can be used for phone classification. In particular, we present some preliminary results for phone classification using two sets of features that are commonly used for object detection: Haar features and SVM-classified Histograms of Gradients (HoG)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32763;&#35793;&#33021;&#21147;&#20013;&#30340;&#24847;&#22806;&#21452;&#35821;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;PaLM&#27169;&#22411;&#21033;&#29992;&#24847;&#22806;&#21452;&#35821;&#20869;&#23481;&#21487;&#20197;&#25913;&#21892;&#38646;-shot&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10266</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25628;&#32034;&#38024;&#30340;&#20316;&#29992;&#65306;&#25506;&#31350;&#24847;&#22806;&#21452;&#35821;&#23545;&#20110;PaLM&#32763;&#35793;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability. (arXiv:2305.10266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32763;&#35793;&#33021;&#21147;&#20013;&#30340;&#24847;&#22806;&#21452;&#35821;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;PaLM&#27169;&#22411;&#21033;&#29992;&#24847;&#22806;&#21452;&#35821;&#20869;&#23481;&#21487;&#20197;&#25913;&#21892;&#38646;-shot&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20174;&#26410;&#35265;&#36807;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#25552;&#20379;&#30340;&#26377;&#24847;&#30340;&#32763;&#35793;&#26679;&#20363;&#65292;&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#38646;&#25110;&#23569;&#37327;&#26679;&#20363;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#24847;&#22806;&#21452;&#35821;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32763;&#35793;&#33021;&#21147;&#30340;&#35299;&#37322;&#20316;&#29992;-&#21253;&#25324;&#26377;&#24847;&#25552;&#20379;&#30340;&#32763;&#35793;&#26679;&#20363;&#22312;&#20869;&#30340;&#21452;&#35821;&#20449;&#21495;&#30340;&#38750;&#24847;&#22806;&#28040;&#36153;&#65292;&#20197;Pathways&#35821;&#35328;&#27169;&#22411;&#65288;PaLM&#65289;&#20026;&#26696;&#20363;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#29702;&#35299;&#35268;&#27169;&#19978;&#30340;&#24847;&#22806;&#21452;&#35821;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PaLM&#26292;&#38706;&#20110;&#33267;&#23569;44&#31181;&#35821;&#35328;&#20013;&#30340;&#36229;&#36807;3000&#19975;&#20010;&#32763;&#35793;&#23545;&#12290;&#27492;&#22806;&#65292;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#24847;&#22806;&#21452;&#35821;&#20869;&#23481;&#37327;&#19982;&#35813;&#35821;&#35328;&#30340;&#21333;&#35821;&#20869;&#35821;&#35328;&#20869;&#23481;&#37327;&#39640;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#23558;&#24847;&#22806;&#21452;&#35821;&#20869;&#23481;&#19982;&#38646;-shot&#25552;&#31034;&#30456;&#20851;&#32852;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#34987;&#29992;&#20110;&#25366;&#25496;&#26032;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;PaLM&#30340;&#33521;&#35821;&#20197;&#22806;&#30340;&#38646;-shot&#32763;&#35793;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism -- the unintentional consumption of bilingual signals, including translation examples -- in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM's out-of-English zero-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#22269;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;M3KE&#65292;&#25910;&#38598;&#20102;20,477&#20010;&#38382;&#39064;&#20197;&#35206;&#30422;&#20013;&#22269;&#25945;&#32946;&#20307;&#31995;&#30340;&#25152;&#26377;&#20027;&#35201;&#23618;&#27425;&#21644;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#27979;&#35797;&#27861;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;GPT-2&#65292;RoBERTa&#65292;ERNIE&#21644;ELECTRA&#23545;&#22810;&#28304;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10263</link><description>&lt;p&gt;
M3KE:&#19968;&#31181;&#38754;&#21521;&#20013;&#22269;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#22269;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;M3KE&#65292;&#25910;&#38598;&#20102;20,477&#20010;&#38382;&#39064;&#20197;&#35206;&#30422;&#20013;&#22269;&#25945;&#32946;&#20307;&#31995;&#30340;&#25152;&#26377;&#20027;&#35201;&#23618;&#27425;&#21644;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#27979;&#35797;&#27861;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;GPT-2&#65292;RoBERTa&#65292;ERNIE&#21644;ELECTRA&#23545;&#22810;&#28304;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#65292;&#25351;&#20196;&#36981;&#24490;&#31561;&#12290;&#20840;&#38754;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3KE&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#36890;&#36807;&#27979;&#35797;&#38646;&#21644;&#20960;&#20010;&#31034;&#20363;&#35774;&#32622;&#19979;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#26469;&#34913;&#37327;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;71&#20010;&#20219;&#21153;&#30340;20,477&#20010;&#38382;&#39064;&#12290;&#36873;&#25321;&#28085;&#30422;&#20102;&#20013;&#22269;&#25945;&#32946;&#20307;&#31995;&#30340;&#25152;&#26377;&#20027;&#35201;&#23618;&#27425;&#65292;&#20174;&#23567;&#23398;&#21040;&#22823;&#23398;&#65292;&#20197;&#21450;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#20154;&#25991;&#65292;&#21382;&#21490;&#65292;&#25919;&#27835;&#65292;&#27861;&#24459;&#65292;&#25945;&#32946;&#65292;&#24515;&#29702;&#65292;&#31185;&#23398;&#65292;&#25216;&#26415;&#65292;&#33402;&#26415;&#21644;&#23447;&#25945;&#12290;&#25152;&#26377;&#38382;&#39064;&#37117;&#26159;&#22235;&#20010;&#36873;&#39033;&#30340;&#22810;&#36873;&#39064;&#65292;&#22240;&#27492;&#20445;&#35777;&#20102;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#27969;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-2&#65292;RoBERTa&#65292;ERNIE&#21644;ELECTRA&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#32467;&#26524;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;M3KE&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#25972;&#21512;&#21644;&#21033;&#29992;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chines
&lt;/p&gt;</description></item><item><title>MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10250</link><description>&lt;p&gt;
MemoryBank: &#29992;&#38271;&#26399;&#35760;&#24518;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10250
&lt;/p&gt;
&lt;p&gt;
MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#19981;&#36275;&#20043;&#22788;&#26159;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#36825;&#22312;&#38656;&#35201;&#25345;&#32493;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20363;&#22914;&#20010;&#20154;&#20276;&#20387;&#31995;&#32479;&#21644;&#24515;&#29702;&#21672;&#35810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoryBank&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;LLM&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#12290;MemoryBank&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#24182;&#26377;&#36873;&#25321;&#22320;&#20445;&#23384;&#35760;&#24518;&#65292;MemoryBank&#37319;&#29992;&#20102;&#21463;Ebbinghaus&#36951;&#24536;&#26354;&#32447;&#29702;&#35770;&#21551;&#21457;&#30340;&#35760;&#24518;&#26356;&#26032;&#26426;&#21046;&#65292;&#36825;&#26679;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#21644;&#35760;&#24518;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#36951;&#24536;&#21644;&#21152;&#24378;&#35760;&#24518;&#65292;&#20174;&#32780;&#20026;LLM&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
&lt;/p&gt;</description></item><item><title>OpenSLU&#26159;&#19968;&#20010;&#32479;&#19968;&#12289;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#35821;&#38899;&#29702;&#35299;&#24037;&#20855;&#21253;&#65292;&#23558;10&#31181;&#38024;&#23545;&#21333;&#24847;&#22270;&#21644;&#22810;&#24847;&#22270;&#22330;&#26223;&#30340;&#35821;&#38899;&#29702;&#35299;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#65292;&#21516;&#26102;&#25903;&#25345;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#39640;&#24230;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.10231</link><description>&lt;p&gt;
OpenSLU: &#19968;&#20010;&#32479;&#19968;&#12289;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#35821;&#38899;&#29702;&#35299;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding. (arXiv:2305.10231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10231
&lt;/p&gt;
&lt;p&gt;
OpenSLU&#26159;&#19968;&#20010;&#32479;&#19968;&#12289;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#35821;&#38899;&#29702;&#35299;&#24037;&#20855;&#21253;&#65292;&#23558;10&#31181;&#38024;&#23545;&#21333;&#24847;&#22270;&#21644;&#22810;&#24847;&#22270;&#22330;&#26223;&#30340;&#35821;&#38899;&#29702;&#35299;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#65292;&#21516;&#26102;&#25903;&#25345;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#39640;&#24230;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29702;&#35299;&#26159;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#19968;&#65292;&#26088;&#22312;&#25552;&#21462;&#29992;&#25143;&#26597;&#35810;&#65288;&#20363;&#22914;&#24847;&#22270;&#21644;&#27133;&#20301;&#65289;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenSLU&#65292;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#12289;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#35821;&#38899;&#29702;&#35299;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;OpenSLU&#23558;10&#31181;&#38024;&#23545;&#21333;&#24847;&#22270;&#21644;&#22810;&#24847;&#22270;&#22330;&#26223;&#30340;&#35821;&#38899;&#29702;&#35299;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#65292;&#21516;&#26102;&#25903;&#25345;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;OpenSLU&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26550;&#26500;&#12289;&#25512;&#29702;&#21644;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#21487;&#37325;&#29992;&#27169;&#22359;&#65292;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20197;&#39640;&#24230;&#28789;&#27963;&#30340;&#37197;&#32622;&#24555;&#36895;&#35774;&#32622;&#35821;&#38899;&#29702;&#35299;&#23454;&#39564;&#12290;OpenSLU&#22522;&#20110;PyTorch&#23454;&#29616;&#65292;&#24182;&#22312;\url{https://github.com/LightChen233/OpenSLU}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible toolkit for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models for both single-intent and multi-intent scenarios, which support both non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is highly modularized and extensible by decomposing the model architecture, inference, and learning process into reusable modules, which allows researchers to quickly set up SLU experiments with highly flexible configurations. OpenSLU is implemented based on PyTorch, and released at \url{https://github.com/LightChen233/OpenSLU}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;&#26799;&#24230;&#22522;&#30784;&#25237;&#24433;&#65288;IGBP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21024;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#26576;&#20010;&#25935;&#24863;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#19968;&#20010;&#36229;&#24179;&#38754;&#19978;&#65292;&#20351;&#24471;&#20998;&#31867;&#22120;&#23545;&#30446;&#26631;&#23646;&#24615;&#21464;&#24471;&#26080;&#24847;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#28040;&#38500;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.10204</link><description>&lt;p&gt;
&#25252;&#30462;&#24335;&#34920;&#31034;&#65306;&#36890;&#36807;&#36845;&#20195;&#22522;&#20110;&#26799;&#24230;&#30340;&#25237;&#24433;&#20445;&#25252;&#25935;&#24863;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection. (arXiv:2305.10204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;&#26799;&#24230;&#22522;&#30784;&#25237;&#24433;&#65288;IGBP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21024;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#26576;&#20010;&#25935;&#24863;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#19968;&#20010;&#36229;&#24179;&#38754;&#19978;&#65292;&#20351;&#24471;&#20998;&#31867;&#22120;&#23545;&#30446;&#26631;&#23646;&#24615;&#21464;&#24471;&#26080;&#24847;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#28040;&#38500;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20542;&#21521;&#20110;&#23398;&#20064;&#21644;&#32534;&#30721;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35299;&#20915;&#27492;&#31867;&#20559;&#24046;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#28040;&#38500;&#27169;&#22411;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21024;&#38500;&#32447;&#24615;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;&#26799;&#24230;&#22522;&#30784;&#25237;&#24433;&#65288;IGBP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21024;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#25105;&#20204;&#35201;&#28040;&#38500;&#30340;&#29305;&#23450;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#19968;&#20010;&#36229;&#24179;&#38754;&#19978;&#65292;&#20351;&#24471;&#20998;&#31867;&#22120;&#23545;&#30446;&#26631;&#23646;&#24615;&#21464;&#24471;&#26080;&#24847;&#35782;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#38500;&#24615;&#21035;&#21644;&#31181;&#26063;&#20449;&#24687;&#20316;&#20026;&#25935;&#24863;&#23646;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;IGBP&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#22312;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#38646;&#20195;&#35789;&#32763;&#35793;&#65288;ZPT&#65289;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#25512;&#24191;&#21518;&#30340;&#37325;&#35201;&#24037;&#20316;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#20219;&#21153;&#25110;&#36801;&#31227;&#23398;&#20064;&#37117;&#21487;&#20197;&#23454;&#29616;ZPT&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.10196</link><description>&lt;p&gt;
&#38646;&#20195;&#35789;&#32763;&#35793;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Zero Pronoun Translation. (arXiv:2305.10196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#38646;&#20195;&#35789;&#32763;&#35793;&#65288;ZPT&#65289;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#25512;&#24191;&#21518;&#30340;&#37325;&#35201;&#24037;&#20316;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#20219;&#21153;&#25110;&#36801;&#31227;&#23398;&#20064;&#37117;&#21487;&#20197;&#23454;&#29616;ZPT&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#20195;&#35789;&#65288;ZP&#65289;&#36890;&#24120;&#22312;&#31867;&#20284;&#20013;&#25991;&#12289;&#21256;&#29273;&#21033;&#35821;&#21644;&#21360;&#22320;&#35821;&#36825;&#26679;&#30340;&#20002;&#30465;&#30053;&#65292;&#32780;&#22312;&#38750;&#20002;&#22833;&#30465;&#20221;&#35832;&#22914;&#33521;&#35821;&#20013;&#65292;&#24212;&#24403;&#36827;&#34892;&#22238;&#24212;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#24456;&#38590;&#30830;&#23450;&#20195;&#35789;&#30340;&#27491;&#30830;&#20808;&#34892;&#35789;&#65292;&#36825;&#26159;MT&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#25512;&#23637;&#20043;&#21518;&#22312;&#38646;&#20195;&#35789;&#32763;&#35793;&#65288;ZPT&#65289;&#26041;&#38754;&#25152;&#20570;&#30340;&#37325;&#35201;&#24037;&#20316;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#26681;&#25454;&#28436;&#21464;&#12289;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20221;&#25991;&#29486;&#32452;&#32455;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27604;&#36739;&#21644;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#31454;&#20105;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#25366;&#25496;&#20102;&#19968;&#20123;&#26377;&#30410;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#65306;1&#65289;ZPT&#31526;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#36235;&#21183;&#65307;2&#65289;&#25968;&#25454;&#38480;&#21046;&#20250;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#20013;&#20135;&#29983;&#23398;&#20064;&#20559;&#24046;&#65307;3&#65289;&#36890;&#36807;&#22810;&#20219;&#21153;&#25110;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21160;&#26426;&#35775;&#35848;&#31574;&#30053;&#37325;&#26032;&#34920;&#36798;&#22312;&#32447;&#24515;&#29702;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#21709;&#24212;&#31867;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#21709;&#24212;&#30340;&#31526;&#21512;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10195</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#26426;&#35775;&#35848;&#31574;&#30053;&#25552;&#21319;&#24515;&#29702;&#21387;&#21147;&#25903;&#25345;&#23545;&#35805;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy. (arXiv:2305.10195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21160;&#26426;&#35775;&#35848;&#31574;&#30053;&#37325;&#26032;&#34920;&#36798;&#22312;&#32447;&#24515;&#29702;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#21709;&#24212;&#31867;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#21709;&#24212;&#30340;&#31526;&#21512;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25104;&#20026;&#32531;&#35299;&#24515;&#29702;&#21387;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#32570;&#20047;&#24515;&#29702;&#27835;&#30103;&#25968;&#25454;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20174;&#22312;&#32447;&#21516;&#20276;&#25903;&#25345;&#35770;&#22363;&#25235;&#21462;&#30340;&#23545;&#35805;&#26469;&#35757;&#32451;&#23427;&#20204;&#12290;&#20294;&#30001;&#20110;&#27492;&#31867;&#24179;&#21488;&#19978;&#30340;&#21709;&#24212;&#24182;&#38750;&#30001;&#19987;&#19994;&#20154;&#22763;&#25552;&#20379;&#65292;&#22240;&#27492;&#23427;&#20204;&#26082;&#21253;&#21547;&#31526;&#21512;&#26631;&#20934;&#30340;&#21709;&#24212;&#21448;&#21253;&#21547;&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#28304;&#33258;&#21517;&#20026;&#21160;&#26426;&#35775;&#35848;&#27835;&#30103;&#23436;&#25972;&#24615;&#65288;MITI&#65289;&#20195;&#30721;&#30340;&#34892;&#20026;&#32534;&#30721;&#26041;&#26696;&#25152;&#36866;&#24212;&#30340;&#26631;&#31614;&#26469;&#35782;&#21035;&#22312;&#32447;&#24515;&#29702;&#25903;&#25345;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#31526;&#21512;&#26631;&#20934;&#21644;&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;&#21709;&#24212;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#23558;&#26576;&#20123;&#21709;&#24212;&#31867;&#22411;&#37325;&#26032;&#34920;&#36798;&#20026;&#26356;&#31526;&#21512;&#21160;&#26426;&#35775;&#35848;&#26631;&#20934;&#30340;&#24418;&#24335;&#65292;&#20174;&#32780;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#21709;&#24212;&#26356;&#31526;&#21512;&#21160;&#26426;&#35775;&#35848;&#31574;&#30053;&#12290;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;Blender&#21644;GPT3&#36827;&#34892;&#24494;&#35843;&#26469;&#26500;&#24314;&#20960;&#20010;&#37325;&#26032;&#34920;&#36798;&#22120;&#65292;&#23558;MI&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;&#8220;&#26410;&#32463;&#20801;&#35768;&#24314;&#35758;&#8221;&#21709;&#24212;&#36716;&#21270;&#20026;&#8220;&#32463;&#36807;&#35768;&#21487;&#30340;&#24314;&#35758;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20511;&#21161;&#20154;&#26426;&#21512;&#20316;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24182;&#22312;&#22312;&#32447;&#24515;&#29702;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28304;&#33258;MI&#26694;&#26550;&#30340;&#37325;&#26032;&#34920;&#36798;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#24515;&#29702;&#21387;&#21147;&#25903;&#25345;&#23545;&#35805;&#20013;&#32842;&#22825;&#26426;&#22120;&#20154;&#21709;&#24212;&#30340;&#31526;&#21512;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. In this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted from a well-established behavioral coding scheme named Motivational Interviewing Treatment Integrity (MITI) code and show how some response types could be rephrased into a more MI adherent form that can, in turn, enable chatbot responses to be more compliant with the MI strategy. As a proof of concept, we build several rephrasers by fine-tuning Blender and GPT3 to rephrase MI non-adherent "Advise without permission" responses into "Advise with permission". We show how this can be achieved with th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21464;&#38271;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#30340;&#23450;&#38271;&#34920;&#31034;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22266;&#23450;&#38271;&#24230;&#20013;&#38388;&#35821;&#34920;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#29305;&#23450;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#20013;&#25928;&#26524;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.10190</link><description>&lt;p&gt;
&#21464;&#38271;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#29992;&#20110;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Variable-length Neural Interlingua Representations for Zero-shot Neural Machine Translation. (arXiv:2305.10190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21464;&#38271;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#30340;&#23450;&#38271;&#34920;&#31034;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22266;&#23450;&#38271;&#24230;&#20013;&#38388;&#35821;&#34920;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#29305;&#23450;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#20013;&#25928;&#26524;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;MNMT&#65289;&#27169;&#22411;&#20013;&#32534;&#30721;&#34920;&#31034;&#30340;&#35821;&#35328;&#26080;&#20851;&#24615;&#23545;&#20854;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#24037;&#20316;&#20013;&#24341;&#20837;&#30340;&#23450;&#38271;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#21487;&#33021;&#20250;&#38480;&#21046;&#20854;&#28789;&#27963;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#30340;&#38271;&#24230;&#21464;&#21270;&#26469;&#22686;&#24378;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#23450;&#38271;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;OPUS&#12289;IWSLT&#21644;Europarl&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#38271;&#24230;&#31070;&#32463;&#20013;&#38388;&#35821;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31283;&#23450;&#30340;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#20248;&#31168;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32763;&#35793;&#26576;&#20123;&#28304;&#35821;&#35328;&#26102;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
The language-independency of encoded representations within multilingual neural machine translation (MNMT) models is crucial for their generalization ability on zero-shot translation. Neural interlingua representations have been shown as an effective method for achieving this. However, fixed-length neural interlingua representations introduced in previous work can limit its flexibility and representation ability. In this study, we introduce a novel method to enhance neural interlingua representations by making their length variable, thereby overcoming the constraint of fixed-length neural interlingua representations. Our empirical results on zero-shot translation on OPUS, IWSLT, and Europarl datasets demonstrate stable model convergence and superior zero-shot translation results compared to fixed-length neural interlingua representations. However, our analysis reveals the suboptimal efficacy of our approach in translating from certain source languages, wherein we pinpoint the defective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#28151;&#21512;&#20027;&#21160;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#23454;&#38469;&#26696;&#20363;&#30693;&#35782;&#26469;&#29983;&#25104;&#28151;&#21512;&#20027;&#21160;&#21709;&#24212;&#65292;&#24182;&#22312;&#20849;&#24773;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10172</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#24773;&#24863;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations. (arXiv:2305.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#28151;&#21512;&#20027;&#21160;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#23454;&#38469;&#26696;&#20363;&#30693;&#35782;&#26469;&#29983;&#25104;&#28151;&#21512;&#20027;&#21160;&#21709;&#24212;&#65292;&#24182;&#22312;&#20849;&#24773;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20849;&#24773;&#23545;&#35805;&#19981;&#21516;&#65292;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22312;&#23433;&#24944;&#27714;&#21161;&#32773;&#30340;&#21516;&#26102;&#20027;&#21160;&#24110;&#21161;&#25506;&#32034;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#20027;&#21160;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#21644;&#31995;&#32479;&#37117;&#21487;&#20197;&#22312;&#23545;&#35805;&#20013;&#37319;&#21462;&#20027;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#28151;&#21512;&#20027;&#21160;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#26032;&#22411;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#20010;&#24773;&#24863;&#25903;&#25345;&#25351;&#26631;&#26469;&#35780;&#20215;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#12290;&#20998;&#26512;&#25581;&#31034;&#20102;&#26500;&#24314;&#28151;&#21512;&#20027;&#21160;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#21644;&#25361;&#25112;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#26694;&#26550;&#65288;KEMI&#65289;&#65292;&#35813;&#26694;&#26550;&#20174;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#23454;&#38469;&#26696;&#20363;&#30693;&#35782;&#26469;&#29983;&#25104;&#28151;&#21512;&#20027;&#21160;&#21709;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;KEMI&#22312;&#20849;&#24773;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#21644;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#26694;&#26550;&#30340;&#21464;&#20307;sRSA&#65292;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#39046;&#22495;&#20013;&#30340;&#23454;&#29992;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#39068;&#33394;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#37319;&#29992;sRSA&#30340;&#20195;&#29702;&#27604;&#20256;&#32479;RSA&#21644;&#20165;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.10167</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#20013;&#30340;&#23454;&#29992;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Reasoning in Structured Signaling Games. (arXiv:2305.10167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#21644;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#26694;&#26550;&#30340;&#21464;&#20307;sRSA&#65292;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#39046;&#22495;&#20013;&#30340;&#23454;&#29992;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#39068;&#33394;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#37319;&#29992;sRSA&#30340;&#20195;&#29702;&#27604;&#20256;&#32479;RSA&#21644;&#20165;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#24102;&#26377;&#30456;&#20284;&#24615;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#26694;&#26550;&#30340;&#21464;&#20307;&#65292;&#31216;&#20026;&#32467;&#26500;&#21270;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#65288;sRSA&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#23454;&#29992;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#39068;&#33394;&#39046;&#22495;&#20013;&#37319;&#29992;sRSA&#30340;&#29702;&#24615;&#26234;&#33021;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#37319;&#29992;World Color Survey&#24471;&#20986;&#30340;&#35821;&#20041;&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#20195;&#29702;&#27604;&#20256;&#32479;RSA&#21644;&#20165;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#19988;&#32463;&#36807;1&#25110;2&#27425;&#36882;&#24402;&#30340;&#35757;&#32451;&#23601;&#33021;&#22815;&#36798;&#21040;&#25928;&#29575;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23454;&#29992;&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;sRSA&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21457;&#23637;&#20986;&#30340;&#36890;&#20449;&#31574;&#30053;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we introduce a structured signaling game, an extension of the classical signaling game with a similarity structure between meanings in the context, along with a variant of the Rational Speech Act (RSA) framework which we call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We explore the behavior of the sRSA in the domain of color and show that pragmatic agents using sRSA on top of semantic representations, derived from the World Color Survey, attain efficiency very close to the information theoretic limit after only 1 or 2 levels of recursion. We also explore the interaction between pragmatic reasoning and learning in multi-agent reinforcement learning framework. Our results illustrate that artificial agents using sRSA develop communication closer to the information theoretic frontier compared to agents using RSA and just reinforcement learning. We also find that the ambiguity of the semantic representation increases as the pragmatic agents are allowe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#36866;&#29992;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#27979;&#35797;&#25968;&#25454;&#65292;&#20165;&#20801;&#35768;&#29305;&#23450;&#27966;&#29983;&#21457;&#24067;&#65307;&#65288;2&#65289;&#23545;&#20110;API&#25345;&#26377;&#26041;&#65292;&#35201;&#27714;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#20445;&#25252;&#27979;&#35797;&#25968;&#25454;&#65292;&#19981;&#20572;&#27490;&#35780;&#20272;&#30452;&#21040;&#36798;&#21040;&#35201;&#27714;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#38656;&#36991;&#20813;&#26576;&#20123;&#32467;&#26524;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10160</link><description>&lt;p&gt;
&#19981;&#35201;&#29992;&#26126;&#25991;&#19978;&#20256;&#27979;&#35797;&#25968;&#25454;&#65306;&#20943;&#36731;&#25968;&#25454;&#22806;&#27844;&#23545;&#20110;&#35780;&#20272;&#22522;&#20934;&#30340;&#25345;&#32493;&#24433;&#21709;&#30340;&#23454;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#36866;&#29992;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#27979;&#35797;&#25968;&#25454;&#65292;&#20165;&#20801;&#35768;&#29305;&#23450;&#27966;&#29983;&#21457;&#24067;&#65307;&#65288;2&#65289;&#23545;&#20110;API&#25345;&#26377;&#26041;&#65292;&#35201;&#27714;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#20445;&#25252;&#27979;&#35797;&#25968;&#25454;&#65292;&#19981;&#20572;&#27490;&#35780;&#20272;&#30452;&#21040;&#36798;&#21040;&#35201;&#27714;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#38656;&#36991;&#20813;&#26576;&#20123;&#32467;&#26524;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#21160;&#29228;&#32593;&#36164;&#26009;&#24211;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#25968;&#25454;&#22806;&#27844;&#21464;&#24471;&#24120;&#35265;&#19988;&#37096;&#20998;&#38590;&#20197;&#24212;&#23545;&#12290;&#23545;&#20110;&#37027;&#20123;&#19981;&#20250;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20854;&#25968;&#25454;&#25104;&#20026;&#20102;&#21830;&#19994;&#26426;&#23494;&#65292;&#21363;&#20351;&#22312;&#20844;&#24320;&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#29305;&#23450;&#27979;&#35797;&#23454;&#20363;&#26159;&#21542;&#34987;&#27844;&#38706;&#20063;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#19977;&#20010;&#21487;&#34892;&#30340;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20351;&#29992;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#24182;&#38480;&#21046;&#27966;&#29983;&#21457;&#24067;&#30340;&#35768;&#21487;&#65307;&#65288;2&#65289;&#35201;&#27714;&#25345;&#26377;API&#35757;&#32451;&#25968;&#25454;&#30340;&#20844;&#21496;&#37319;&#29992;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#24182;&#25298;&#32477;&#35780;&#20272;&#65292;&#30452;&#21040;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#26080;&#35823;&#20026;&#27490;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#37027;&#20040;&#38656;&#36991;&#20813;&#22312;&#32593;&#32476;&#25628;&#32034;&#20013;&#20986;&#29616;&#21253;&#21547;&#27491;&#30830;&#25552;&#21462;&#37096;&#20998;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination has become especially prevalent and challenging with the rise of models pretrained on very large, automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to ascertain whether a particular test instance has been compromised. Strategies such as live leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met; (3) in case of test data based on internet text, avoid data which appears with its soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10156</link><description>&lt;p&gt;
&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#26159;&#38405;&#35835;&#25925;&#20107;&#30340;&#20851;&#38190;&#12290;&#38543;&#30528;&#35835;&#32773;&#19982;&#25925;&#20107;&#30340;&#20114;&#21160;&#65292;&#20182;&#20204;&#23545;&#19968;&#20010;&#20154;&#29289;&#30340;&#29702;&#35299;&#20250;&#26681;&#25454;&#26032;&#30340;&#20107;&#20214;&#21644;&#20449;&#24687;&#32780;&#28436;&#21464;&#65307;&#24182;&#19988;&#21487;&#20197;&#24863;&#30693;&#21040;&#22810;&#20010;&#31934;&#32454;&#30340;&#20010;&#24615;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#24773;&#22659;&#21644;&#31934;&#32454;&#30340;&#20010;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;NLP&#39046;&#22495;&#20013;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#27169;&#20223;&#38405;&#35835;&#36807;&#31243;&#30340;&#36866;&#24403;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#27880;&#37322;&#31574;&#30053;&#28041;&#21450;&#29992;&#22312;&#32447;&#38405;&#35835;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#31508;&#35760;&#20316;&#20026;&#21407;&#22987;&#20070;&#31821;&#30340;&#20195;&#29702;&#36827;&#34892;&#27880;&#37322;&#12290;&#23454;&#39564;&#21644;&#20154;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26082;&#26377;&#25928;&#21448;&#20934;&#30830;&#65307;&#25105;&#20204;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#38271;&#26399;&#30340;&#19978;&#19979;&#25991;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Gorov/personet_acl23&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#36845;&#20195;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#25991;&#21270;&#36827;&#21270;&#27169;&#22411;&#65292;&#23637;&#31034;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24182;&#25910;&#25947;&#20110;&#39640;&#25928;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#20041;&#31995;&#32479;&#21453;&#26144;&#25928;&#29575;&#21387;&#21147;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.10154</link><description>&lt;p&gt;
&#36845;&#20195;&#23398;&#20064;&#19982;&#20132;&#27969;&#20849;&#21516;&#35299;&#37322;&#20102;&#26377;&#25928;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Iterated learning and communication jointly explain efficient color naming systems. (arXiv:2305.10154v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#36845;&#20195;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#25991;&#21270;&#36827;&#21270;&#27169;&#22411;&#65292;&#23637;&#31034;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24182;&#25910;&#25947;&#20110;&#39640;&#25928;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#20041;&#31995;&#32479;&#21453;&#26144;&#25928;&#29575;&#21387;&#21147;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#26377;&#20154;&#35748;&#20026;&#65292;&#35821;&#20041;&#31995;&#32479;&#21453;&#26144;&#20102;&#25928;&#29575;&#30340;&#21387;&#21147;&#65292;&#19968;&#20010;&#24403;&#21069;&#30340;&#20105;&#35770;&#20851;&#27880;&#20110;&#20135;&#29983;&#36825;&#31181;&#27169;&#24335;&#30340;&#25991;&#21270;&#36827;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25928;&#29575;&#23454;&#29616;&#20026;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#24182;&#32467;&#21512;&#36845;&#20195;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#25991;&#21270;&#36827;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30340;&#27169;&#22411;&#25910;&#25947;&#20110;&#22312;IB&#24847;&#20041;&#19979;&#39640;&#25928;&#24182;&#19988;&#31867;&#20284;&#20110;&#20154;&#31867;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#20165;&#36845;&#20195;&#23398;&#20064;&#25110;&#32773;&#20165;&#20165;&#20132;&#27969;&#24182;&#19981;&#33021;&#20687;&#36825;&#20010;&#27169;&#22411;&#37027;&#26679;&#20135;&#29983;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been argued that semantic systems reflect pressure for efficiency, and a current debate concerns the cultural evolutionary process that produces this pattern. We consider efficiency as instantiated in the Information Bottleneck (IB) principle, and a model of cultural evolution that combines iterated learning and communication. We show that this model, instantiated in neural networks, converges to color naming systems that are efficient in the IB sense and similar to human color naming systems. We also show that iterated learning alone, and communication alone, do not yield the same outcome as clearly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#22810;&#31890;&#24230;&#30693;&#35782;&#26816;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#20307;&#36873;&#25321;&#22120;&#21644;&#23646;&#24615;&#36873;&#25321;&#22120;&#23454;&#29616;&#30693;&#35782;&#26816;&#32034;&#12289;&#21709;&#24212;&#29983;&#25104;&#35299;&#32806;&#65292;&#24182;&#19988;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25191;&#34892;&#30693;&#35782;&#26816;&#32034;&#20197;&#29983;&#25104;&#20449;&#24687;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.10149</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#22810;&#31890;&#24230;&#30693;&#35782;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog. (arXiv:2305.10149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10149
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#22810;&#31890;&#24230;&#30693;&#35782;&#26816;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#20307;&#36873;&#25321;&#22120;&#21644;&#23646;&#24615;&#36873;&#25321;&#22120;&#23454;&#29616;&#30693;&#35782;&#26816;&#32034;&#12289;&#21709;&#24212;&#29983;&#25104;&#35299;&#32806;&#65292;&#24182;&#19988;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25191;&#34892;&#30693;&#35782;&#26816;&#32034;&#20197;&#29983;&#25104;&#20449;&#24687;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#20986;&#21512;&#36866;&#30340;&#39046;&#22495;&#30693;&#35782;&#26159;&#29983;&#25104;&#20449;&#24687;&#21709;&#24212;&#30340;&#20219;&#21153;&#23548;&#21521;&#22411;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31995;&#32479;&#23558;&#30693;&#35782;&#26816;&#32034;&#19982;&#21709;&#24212;&#29983;&#25104;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36890;&#36807;&#24341;&#29992;&#21709;&#24212;&#30452;&#25509;&#30417;&#30563;&#20248;&#21270;&#65292;&#23548;&#33268;&#24403;&#30693;&#35782;&#24211;&#21464;&#24471;&#22823;&#35268;&#27169;&#26102;&#65292;&#26816;&#32034;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#30693;&#35782;&#26816;&#32034;&#20174;&#21709;&#24212;&#29983;&#25104;&#20013;&#35299;&#32806;&#65292;&#24182;&#24341;&#20837;&#22810;&#31890;&#24230;&#30693;&#35782;&#26816;&#32034;&#22120;&#65288;MAKER&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#23454;&#20307;&#36873;&#25321;&#22120;&#20197;&#25628;&#32034;&#30456;&#20851;&#23454;&#20307;&#21644;&#23646;&#24615;&#36873;&#25321;&#22120;&#20197;&#36807;&#28388;&#19981;&#30456;&#20851;&#23646;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#30446;&#26631;&#65292;&#20174;&#21709;&#24212;&#29983;&#25104;&#22120;&#20013;&#25552;&#21462;&#30417;&#30563;&#20449;&#21495;&#12290;&#22312;&#19977;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#25191;&#34892;&#30693;&#35782;&#26816;&#32034;&#65292;&#19988;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#21457;&#24067;&#22312;https://github.com/PaddlePaddle/Research/tree/master/NLP/End-to-End%20Conversation%20Model%20(MAKER)&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieving proper domain knowledge from an external database lies at the heart of end-to-end task-oriented dialog systems to generate informative responses. Most existing systems blend knowledge retrieval with response generation and optimize them with direct supervision from reference responses, leading to suboptimal retrieval performance when the knowledge base becomes large-scale. To address this, we propose to decouple knowledge retrieval from response generation and introduce a multi-grained knowledge retriever (MAKER) that includes an entity selector to search for relevant entities and an attribute selector to filter out irrelevant attributes. To train the retriever, we propose a novel distillation objective that derives supervision signals from the response generator. Experiments conducted on three standard benchmarks with both small and large-scale knowledge bases demonstrate that our retriever performs knowledge retrieval more effectively than existing methods. Our code has be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#21644;&#21453;&#39304;&#20114;&#30456;&#25552;&#39640;&#65292;&#22312;&#35848;&#21028;&#28216;&#25103;&#20013;&#36827;&#34892;&#35848;&#21028;&#65292;&#36798;&#25104;&#20132;&#26131;&#12290;&#20351;&#29992;&#21382;&#21490;&#35760;&#24405;&#21644;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#36845;&#20195;&#25913;&#36827;&#27169;&#22411;&#30340;&#35848;&#21028;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10142</link><description>&lt;p&gt;
&#33258;&#25105;&#21338;&#24328;&#19982;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#35848;&#21028;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. (arXiv:2305.10142v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#21644;&#21453;&#39304;&#20114;&#30456;&#25552;&#39640;&#65292;&#22312;&#35848;&#21028;&#28216;&#25103;&#20013;&#36827;&#34892;&#35848;&#21028;&#65292;&#36798;&#25104;&#20132;&#26131;&#12290;&#20351;&#29992;&#21382;&#21490;&#35760;&#24405;&#21644;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#36845;&#20195;&#25913;&#36827;&#27169;&#22411;&#30340;&#35848;&#21028;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21542;&#36890;&#36807;&#29609;&#32781;&#12289;&#21453;&#24605;&#21644;&#25209;&#35780;&#22312;&#35848;&#21028;&#28216;&#25103;&#20013;&#24444;&#27492;&#33258;&#20027;&#25913;&#36827;&#12290;&#22914;&#26524;LLM&#33021;&#22815;&#30456;&#20114;&#25552;&#39640;&#65292;&#21017;&#24847;&#21619;&#30528;&#21487;&#20197;&#22312;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#25105;&#20204;&#35753;&#20004;&#20010;LLM&#25198;&#28436;&#20080;&#26041;&#21644;&#21334;&#26041;&#35282;&#33394;&#36827;&#34892;&#21327;&#21830;&#65292;&#31532;&#19977;&#20010;LLM&#25198;&#28436;&#25209;&#35780;&#23478;&#65292;&#20026;&#19968;&#26041;&#25552;&#20379;&#21453;&#39304;&#20197;&#25913;&#36827;&#20854;&#35848;&#21028;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#21382;&#21490;&#20132;&#26131;&#35760;&#24405;&#21644;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#20316;&#20026;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#27169;&#22411;&#30340;&#35848;&#21028;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;LLM&#65288;GPT&#21644;Claude&#65289;&#26469;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#24182;&#20351;&#29992;&#20132;&#26131;&#20215;&#26684;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#25581;&#31034;&#20986;&#22810;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;&#25919;&#20826;&#31435;&#22330;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#25919;&#31574;&#22495;&#24863;&#30693;&#30340;&#25919;&#20826;&#30456;&#20284;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#32553;&#25918;&#25552;&#21462;&#21487;&#35299;&#37322;&#30340;&#20027;&#35201;&#25919;&#31574;&#36724;&#19978;&#30340;&#25919;&#20826;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2305.10136</link><description>&lt;p&gt;
&#12298;&#38468;&#21152;&#23459;&#35328;&#20998;&#35299;&#65306;&#19968;&#31181;&#29702;&#35299;&#25919;&#20826;&#31435;&#22330;&#30340;&#25919;&#31574;&#22495;&#24863;&#30693;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Additive manifesto decomposition: A policy domain aware method for understanding party positioning. (arXiv:2305.10136v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;&#25919;&#20826;&#31435;&#22330;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#25919;&#31574;&#22495;&#24863;&#30693;&#30340;&#25919;&#20826;&#30456;&#20284;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#32553;&#25918;&#25552;&#21462;&#21487;&#35299;&#37322;&#30340;&#20027;&#35201;&#25919;&#31574;&#36724;&#19978;&#30340;&#25919;&#20826;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#25919;&#27835;&#25991;&#26412;&#20013;&#25552;&#21462;&#25919;&#20826;&#65288;&#19981;&#65289;&#30456;&#20284;&#24615;&#22312;&#35745;&#31639;&#25919;&#27835;&#23398;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#20165;&#38480;&#20110;&#38024;&#23545;&#20840;&#23616;&#25919;&#20826;&#65288;&#19981;&#65289;&#30456;&#20284;&#24615;&#65306;&#23427;&#20204;&#23558;&#20004;&#20010;&#25919;&#20826;&#20043;&#38388;&#30340;&#20851;&#31995;&#21387;&#32553;&#20026;&#21333;&#20010;&#25968;&#23383;&#65292;&#21363;&#30456;&#20284;&#24615;&#65292;&#26080;&#27861;&#25552;&#20379;&#26377;&#20851;&#25919;&#20826;&#22312;&#21738;&#20123;&#39046;&#22495;&#19978;&#36798;&#25104;&#20849;&#35782;&#25110;&#19981;&#19968;&#33268;&#30340;&#20219;&#20309;&#23450;&#24615;&#27934;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;&#25919;&#31574;&#22495;&#24863;&#30693;&#30340;&#25919;&#20826;&#30456;&#20284;&#24615;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#65288;a&#65289;&#23450;&#20041;&#21512;&#36866;&#30340;&#25919;&#31574;&#39046;&#22495;&#65307;&#65288;b&#65289;&#22914;&#26524;&#27809;&#26377;&#25163;&#21160;&#26631;&#31614;&#65292;&#21017;&#33258;&#21160;&#26631;&#35760;&#39046;&#22495;&#65307;&#65288;c&#65289;&#35745;&#31639;&#39046;&#22495;&#32423;&#30456;&#20284;&#24615;&#24182;&#22312;&#20840;&#23616;&#32423;&#21035;&#36827;&#34892;&#27719;&#24635;&#65307;&#65288;d&#65289;&#36890;&#36807;&#22810;&#32500;&#32553;&#25918;&#25552;&#21462;&#21487;&#35299;&#37322;&#30340;&#20027;&#35201;&#25919;&#31574;&#36724;&#19978;&#30340;&#25919;&#20826;&#31435;&#22330;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic extraction of party (dis)similarities from texts such as party election manifestos or parliamentary speeches plays an increasing role in computational political science. However, existing approaches are fundamentally limited to targeting only global party (dis)-similarity: they condense the relationship between a pair of parties into a single figure, their similarity. In aggregating over all policy domains (e.g., health or foreign policy), they do not provide any qualitative insights into which domains parties agree or disagree on. This paper proposes a workflow for estimating policy domain aware party similarity that overcomes this limitation. The workflow covers (a) definition of suitable policy domains; (b) automatic labeling of domains, if no manual labels are available; (c) computation of domain-level similarities and aggregation at a global level; (d) extraction of interpretable party positions on major policy axes via multidimensional scaling. We evaluate our workflow 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#24247;&#21345;&#23612;&#35821;&#21475;&#33108;&#21644;&#40763;&#33108;&#20803;&#38899;&#30340;&#22768;&#23398;-&#35821;&#38899;&#23398;&#29305;&#24615;&#65292;&#20026;&#24247;&#21345;&#23612;&#35821;&#35328;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#21644;&#20803;&#38899;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.10122</link><description>&lt;p&gt;
&#24247;&#21345;&#23612;&#35821;&#21475;&#33108;&#21644;&#40763;&#33108;&#20803;&#38899;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of Oral and Nasal Vowels of Konkani. (arXiv:2305.10122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#24247;&#21345;&#23612;&#35821;&#21475;&#33108;&#21644;&#40763;&#33108;&#20803;&#38899;&#30340;&#22768;&#23398;-&#35821;&#38899;&#23398;&#29305;&#24615;&#65292;&#20026;&#24247;&#21345;&#23612;&#35821;&#35328;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#21644;&#20803;&#38899;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#21345;&#23612;&#35821;&#26159;&#19968;&#31181;&#39640;&#24230;&#40763;&#38899;&#21270;&#30340;&#35821;&#35328;&#65292;&#36825;&#20351;&#20854;&#22312;&#21360;&#24230;-&#38597;&#21033;&#23433;&#35821;&#35328;&#20013;&#20855;&#26377;&#29420;&#29305;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24247;&#21345;&#23612;&#35821;&#21475;&#33108;&#21644;&#40763;&#33108;&#20803;&#38899;&#30340;&#22768;&#23398;-&#35821;&#38899;&#23398;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25910;&#38598;&#20102;6&#21517;&#21457;&#35328;&#20154;&#65288;3&#30007;&#24615;&#21644;3&#22899;&#24615;&#65289;&#30340;&#35821;&#38899;&#26679;&#26412;&#12290;&#20849;&#20351;&#29992;&#20102;74&#20010;&#21807;&#19968;&#21477;&#23376;&#20316;&#20026;&#24405;&#21046;&#33050;&#26412;&#65292;&#20998;&#21035;&#20026;&#21475;&#33108;&#21644;&#40763;&#33108;&#20803;&#38899;37&#20010;&#12290;&#26368;&#32456;&#25968;&#25454;&#38598;&#30001;1135&#20010;&#20803;&#38899;&#38899;&#32032;&#32452;&#25104;&#12290;&#25552;&#20379;&#20102;&#24247;&#21345;&#23612;&#35821;&#21475;&#33108;&#21644;&#40763;&#33108;&#20803;&#38899;&#30340;&#27604;&#36739;F1-F2&#22270;&#20197;&#21450;&#23454;&#39564;&#32467;&#26524;&#21644;&#20849;&#25391;&#23792;&#20998;&#26512;&#12290;&#25152;&#26377;&#40763;&#33108;&#21644;&#21475;&#33108;&#20803;&#38899;&#30340;&#24179;&#22343;F1&#65292;F2&#21644;F3&#20540;&#20063;&#39318;&#27425;&#36890;&#36807;&#23454;&#39564;&#25253;&#21578;&#12290;&#35813;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#38024;&#23545;&#24247;&#21345;&#23612;&#35821;&#35328;&#30340;&#20803;&#38899;&#21644;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Konkani is a highly nasalised language which makes it unique among Indo-Aryan languages. This work investigates the acoustic-phonetic properties of Konkani oral and nasal vowels. For this study, speech samples from six speakers (3 male and 3 female) were collected. A total of 74 unique sentences were used as a part of the recording script, 37 each for oral and nasal vowels, respectively. The final data set consisted of 1135 vowel phonemes. A comparative F1-F2 plot of Konkani oral and nasal vowels is presented with an experimental result and formant analysis. The average F1, F2 and F3 values are also reported for the first time through experimentation for all nasal and oral vowels. This study can be helpful for the linguistic research on vowels and speech synthesis systems specific to the Konkani language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31227;&#24773;&#21453;&#24212;&#24847;&#22270;&#20998;&#31867;&#27861;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#31070;&#32463;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#22238;&#24212;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#21487;&#25511;&#21644;&#21487;&#35299;&#37322;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.10096</link><description>&lt;p&gt;
&#20351;&#29992;&#31227;&#24773;&#21453;&#24212;&#24847;&#22270;&#20998;&#31867;&#27861;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#31070;&#32463;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots. (arXiv:2305.10096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31227;&#24773;&#21453;&#24212;&#24847;&#22270;&#20998;&#31867;&#27861;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#31070;&#32463;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#22238;&#24212;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#21487;&#25511;&#21644;&#21487;&#35299;&#37322;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#35753;&#23427;&#20204;&#33021;&#22815;&#23545;&#24773;&#24863;&#25552;&#31034;&#36827;&#34892;&#21516;&#24773;&#24335;&#30340;&#23545;&#35805;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#36981;&#24490;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#35201;&#20040;&#22312;&#30456;&#20284;&#30340;&#24773;&#24863;&#26631;&#31614;&#19978;&#36827;&#34892;&#26465;&#20214;&#21453;&#24212;&#20197;&#20135;&#29983;&#20849;&#24773;&#24335;&#30340;&#22238;&#31572;&#12290;&#20294;&#20849;&#24773;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#20010;&#20307;&#23545;&#21478;&#19968;&#20010;&#20154;&#35266;&#23519;&#21040;&#30340;&#32463;&#21382;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#21453;&#24212;&#65292;&#23427;&#27604;&#21333;&#32431;&#30340;&#24773;&#24863;&#27169;&#20223;&#26356;&#21152;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#36890;&#29992;&#24773;&#24863;&#22806;&#65292;&#36824;&#38656;&#35201;&#35782;&#21035;&#22797;&#26434;&#30340;&#20154;&#31867;&#23545;&#35805;&#31574;&#30053;&#21644;&#21160;&#24577;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20849;&#24773;&#22238;&#24212;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20843;&#31181;&#20849;&#24773;&#21453;&#24212;&#24847;&#22270;&#30340;&#20998;&#31867;&#27861;&#20197;&#21450;&#36890;&#29992;&#24773;&#24863;&#31867;&#21035;&#26469;&#24314;&#31435;&#19968;&#20010;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#21487;&#25511;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20135;&#29983;&#20849;&#24773;&#22238;&#24212;&#12290;&#23427;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;1&#65289;&#21709;&#24212;&#24773;&#24863;/&#24847;&#22270;&#39044;&#27979;&#27169;&#22359;&#65307;&#20197;&#21450;2&#65289;&#21709;&#24212;&#29983;&#25104;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent trend in the domain of open-domain conversational agents is enabling them to converse empathetically to emotional prompts. Current approaches either follow an end-to-end approach or condition the responses on similar emotion labels to generate empathetic responses. But empathy is a broad concept that refers to the cognitive and emotional reactions of an individual to the observed experiences of another and it is more complex than mere mimicry of emotion. Hence, it requires identifying complex human conversational strategies and dynamics in addition to generic emotions to control and interpret empathetic responding capabilities of chatbots. In this work, we make use of a taxonomy of eight empathetic response intents in addition to generic emotion categories in building a dialogue response generation model capable of generating empathetic responses in a controllable and interpretable manner. It consists of two modules: 1) a response emotion/intent prediction module; and 2) a res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20301;&#32622;&#20449;&#24687;&#30340;&#20351;&#29992;&#65292;&#34920;&#26126;&#27169;&#22411;&#23384;&#22312;&#20301;&#32622;&#20449;&#24687;&#65292;&#20294;&#19981;&#33021;&#24456;&#22909;&#22320;&#21033;&#29992;&#23427;&#36827;&#34892;&#22270;&#20687; - &#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#20449;&#24687;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#21305;&#37197;&#30340;PI&#23545;&#27604;&#24230;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#38598;&#19978;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10046</link><description>&lt;p&gt;
&#25506;&#32034;&#20301;&#32622;&#20449;&#24687;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Probing the Role of Positional Information in Vision-Language Models. (arXiv:2305.10046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20301;&#32622;&#20449;&#24687;&#30340;&#20351;&#29992;&#65292;&#34920;&#26126;&#27169;&#22411;&#23384;&#22312;&#20301;&#32622;&#20449;&#24687;&#65292;&#20294;&#19981;&#33021;&#24456;&#22909;&#22320;&#21033;&#29992;&#23427;&#36827;&#34892;&#22270;&#20687; - &#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#20449;&#24687;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#21305;&#37197;&#30340;PI&#23545;&#27604;&#24230;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#38598;&#19978;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VL&#65289;&#20013;&#65292;&#29702;&#35299;&#22270;&#20687;&#32467;&#26500;&#38656;&#35201;&#27880;&#20837;&#26377;&#20851;&#22270;&#20687;&#20013;&#29289;&#20307;&#20301;&#32622;&#20449;&#24687;&#65288;PI&#65289;&#12290;&#22312;&#25105;&#20204;&#23545;LXMERT&#36827;&#34892;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;VL&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;PI&#22312;&#34920;&#31034;&#20013;&#30340;&#20351;&#29992;&#21450;&#20854;&#23545;&#35270;&#35273;&#38382;&#31572;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#33021;&#21033;&#29992;PI&#22788;&#29702;&#20165;&#20301;&#32622;&#19981;&#21516;&#30340;&#25361;&#25112;&#38598;&#19978;&#30340;&#22270;&#20687; - &#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#26597;&#23454;&#39564;&#35777;&#23454;&#20102;&#34920;&#31034;&#20013;&#30830;&#23454;&#23384;&#22312;PI&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20301;&#32622;&#20449;&#24687;&#39044;&#35757;&#32451;&#21644;&#65288;ii&#65289;&#20351;&#29992;&#36328;&#27169;&#24577;&#21305;&#37197;&#30340;PI&#23545;&#27604;&#24230;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27169;&#22411;&#21487;&#20197;&#27491;&#30830;&#20998;&#31867;&#20855;&#26377;&#35814;&#32454;PI&#38472;&#36848;&#30340;&#22270;&#20687;&#26159;&#21542;&#21305;&#37197;&#12290;&#38500;&#20102;&#26469;&#33258;&#36793;&#30028;&#26694;&#30340;2D&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#29289;&#20307;&#28145;&#24230;&#20316;&#20026;&#26032;&#29305;&#24449;&#65292;&#20197;&#22312;&#31354;&#38388;&#20013;&#26356;&#22909;&#22320;&#23450;&#20301;&#23545;&#35937;&#12290;&#23613;&#31649;&#25105;&#20204;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#25925;&#20107;&#20013;&#20173;&#26377;&#36827;&#19968;&#27493;&#30340;&#25361;&#25112;&#65292;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10037</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#22270;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#19968;&#20123;&#20855;&#26377;&#38544;&#24335;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25110;&#30693;&#35782;&#25506;&#32034;&#12289;&#32467;&#26500;&#21270;&#24120;&#35782;&#25512;&#29702;&#31561;&#31561;&#12290;&#34429;&#28982;LLM&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;LLM&#26159;&#21542;&#33021;&#22815;&#26174;&#24335;&#22788;&#29702;&#22270;&#24418;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#22522;&#20110;&#27010;&#24565;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#25191;&#34892;&#32467;&#26500;&#21270;&#25805;&#20316;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#23427;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#20840;&#38754;&#27979;&#35797;&#12290;NLGraph&#21253;&#21547;29,370&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#36830;&#25509;&#21644;&#26368;&#30701;&#36335;&#24452;&#21040;&#22797;&#26434;&#30340;&#26368;&#22823;&#27969;&#21644;&#27169;&#25311;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#20219;&#21153;&#19981;&#31561;&#12290;&#25105;&#20204;&#22312;NLGraph&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LLM(GPT-3/4)&#65292;&#24182;&#21457;&#29616;1)&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65307;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#65292;&#24182;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;</title><link>http://arxiv.org/abs/2305.10036</link><description>&lt;p&gt;
&#20320;&#22312;&#25220;&#25105;&#30340;&#27169;&#22411;&#21527;&#65311;&#22522;&#20110;&#21518;&#38376;&#27700;&#21360;&#30340;&#20445;&#25252;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#65292;&#24182;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#20844;&#21496;&#24050;&#32463;&#24320;&#22987;&#22522;&#20110;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23884;&#20837;&#24335;&#26381;&#21153; (EaaS)&#65292;&#21487;&#20197;&#20026;&#23458;&#25143;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#24102;&#26469;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS &#26131;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545; LLM &#30340;&#25152;&#26377;&#32773;&#36896;&#25104;&#24040;&#22823;&#25439;&#22833;&#65292;&#22240;&#20026;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#20445;&#25252; EaaS &#30340; LLM &#30340;&#29256;&#26435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#19968;&#32452;&#20013;&#31561;&#39057;&#29575;&#30340;&#21333;&#35789;&#65292;&#24418;&#25104;&#35302;&#21457;&#38598;&#65292;&#28982;&#21518;&#36873;&#25321;&#19968;&#20010;&#30446;&#26631;&#23884;&#20837;&#20316;&#20026;&#27700;&#21360;&#65292;&#24182;&#23558;&#20854;&#25554;&#20837;&#21253;&#21547;&#35302;&#21457;&#35789;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#20013;&#20316;&#20026;&#21518;&#38376;&#12290;&#25554;&#20837;&#30340;&#37325;&#37327;&#19982;&#21253;&#21547;&#22312;&#25991;&#26412;&#20013;&#30340;&#35302;&#21457;&#35789;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#36825;&#20351;&#24471;&#27700;&#21360;&#21518;&#38376;&#21487;&#20197;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#65292;&#32780;&#19981;&#24433;&#21709; LLM &#22312;&#21508;&#31181; NLP &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GDFO&#65292;&#23558;&#26799;&#24230;&#19979;&#38477;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21327;&#35843;&#22320;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36830;&#32493;&#25552;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10013</link><description>&lt;p&gt;
&#24403;&#26799;&#24230;&#19979;&#38477;&#36935;&#21040;&#26080;&#23548;&#25968;&#20248;&#21270;&#65306;&#40657;&#30418;&#22330;&#26223;&#19979;&#30340;&#23436;&#32654;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario. (arXiv:2305.10013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;GDFO&#65292;&#23558;&#26799;&#24230;&#19979;&#38477;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21327;&#35843;&#22320;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36830;&#32493;&#25552;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22240;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#35299;&#20915;&#24191;&#27867;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#28508;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36816;&#34892;&#36825;&#20123;PLMs&#30340;&#25104;&#26412;&#21487;&#33021;&#26159;&#31105;&#27490;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21830;&#19994;&#32771;&#34385;&#21644;&#28508;&#22312;&#30340;&#35823;&#29992;&#39118;&#38505;&#65288;&#20363;&#22914;GPT-3&#65289;&#65292;PLMs&#21487;&#33021;&#26410;&#24320;&#25918;&#28304;&#20195;&#30721;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26080;&#23548;&#25968;&#20248;&#21270;&#65288;DFO&#65289;&#25552;&#20986;&#20102;&#40657;&#30418;&#35843;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#36830;&#32493;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20123;&#26080;&#26799;&#24230;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#26174;&#30528;&#24046;&#36317;&#12290;&#26412;&#25991;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#26799;&#24230;&#19979;&#38477;&#24341;&#20837;&#40657;&#30418;&#35843;&#25972;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;GDFO&#65292;&#23558;&#26799;&#24230;&#19979;&#38477;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#34701;&#21512;&#21040;&#19968;&#36215;&#65292;&#20197;&#21327;&#35843;&#30340;&#26041;&#24335;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36830;&#32493;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807; Integrated Gradients &#25506;&#32034;&#25945;&#24072;&#27169;&#22411;&#30340;&#21407;&#29702;&#65292;&#23558;&#24402;&#22240;&#30693;&#35782;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#22312; GLUE &#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;BERT&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10010</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#30693;&#35782;&#33976;&#39311;&#65306;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression. (arXiv:2305.10010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807; Integrated Gradients &#25506;&#32034;&#25945;&#24072;&#27169;&#22411;&#30340;&#21407;&#29702;&#65292;&#23558;&#24402;&#22240;&#30693;&#35782;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#22312; GLUE &#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;BERT&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36817;&#26469;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21387;&#32553;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#19968;&#26159;&#23398;&#29983;&#27169;&#22411;&#20165;&#27169;&#20223;&#25945;&#24072;&#30340;&#34892;&#20026;&#32780;&#24573;&#30053;&#20854;&#28508;&#22312;&#30340;&#25512;&#29702;&#36807;&#31243;&#65307;&#20108;&#26159;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#27169;&#22411;&#29305;&#23450;&#30340;&#22797;&#26434;&#30693;&#35782;&#36716;&#31227;&#65292;&#20294;&#24573;&#35270;&#25968;&#25454;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24402;&#22240;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807; Integrated Gradients(IG) &#25506;&#32034;&#25945;&#24072;&#27169;&#22411;&#32972;&#21518;&#30340;&#20196;&#29260;&#32423;&#21035;&#30340;&#21407;&#29702;&#65292;&#24182;&#23558;&#24402;&#22240;&#30693;&#35782;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#25512;&#29702;&#21644;&#27867;&#21270;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#25945;&#24072;&#30340;&#25152;&#26377;&#28508;&#22312;&#20915;&#31574;&#30340;&#22810;&#35270;&#35282;&#24402;&#22240;&#33976;&#39311;&#12290;&#25105;&#20204;&#36824;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;BERT&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation has attracted a great deal of interest recently to compress pre-trained language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher's behavior while ignoring the underlying reasoning. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to sev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EfficientSCI&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#31264;&#23494;&#36830;&#25509;&#21644;&#26102;&#31354;&#20998;&#35299;&#26426;&#21046;&#26469;&#24314;&#31435;&#35270;&#39057;SCI&#20013;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.10006</link><description>&lt;p&gt;
EfficientSCI: &#31264;&#23494;&#36830;&#25509;&#32593;&#32476;&#19982;&#26102;&#31354;&#20998;&#35299;&#30456;&#32467;&#21512;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#24555;&#29031;&#21387;&#32553;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EfficientSCI&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#31264;&#23494;&#36830;&#25509;&#21644;&#26102;&#31354;&#20998;&#35299;&#26426;&#21046;&#26469;&#24314;&#31435;&#35270;&#39057;SCI&#20013;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24555;&#29031;&#21387;&#32553;&#25104;&#20687; (SCI) &#20351;&#29992;&#20108;&#32500;&#26816;&#27979;&#22120;&#22312;&#21333;&#20010;&#26333;&#20809;&#26102;&#38388;&#20869;&#25429;&#33719;&#36830;&#32493;&#35270;&#39057;&#24103;&#12290;&#28982;&#21518;&#38656;&#35201;&#35774;&#35745;&#39640;&#25928;&#30340;&#37325;&#24314;&#31639;&#27861;&#26469;&#37325;&#24314;&#25152;&#38656;&#30340;&#35270;&#39057;&#24103;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#37325;&#24314;&#31639;&#27861;&#24050;&#32463;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#36807;&#24230;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;GPU&#20869;&#23384;&#38480;&#21046;&#12290;&#20854;&#20013;&#65292;1)&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;2)&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#22312;&#39640;&#21387;&#32553;&#27604;&#19979;&#37325;&#24314;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#24103;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#39057;SCI&#32593;&#32476;&#65292;&#20351;&#29992;&#21333;&#20010;&#27531;&#24046;&#22359;&#20869;&#30340;&#31264;&#23494;&#36830;&#25509;&#21644;&#26102;&#31354;&#20998;&#35299;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;EfficientSCI&#12290; EfficientSCI&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22312;&#31354;&#38388;&#22495;&#20013;&#20351;&#29992;&#21367;&#31215;&#21644;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#36716;&#25442;&#22495;&#31232;&#30095;&#21270;&#65288;TDS&#65289;&#26469;&#24456;&#22909;&#22320;&#24314;&#31435;&#31354;&#38388; - &#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;EfficientSCI&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#35270;&#39057;SCI&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video snapshot compressive imaging (SCI) uses a two-dimensional detector to capture consecutive video frames during a single exposure time. Following this, an efficient reconstruction algorithm needs to be designed to reconstruct the desired video frames. Although recent deep learning-based state-of-the-art (SOTA) reconstruction algorithms have achieved good results in most tasks, they still face the following challenges due to excessive model complexity and GPU memory limitations:  1) these models need high computational cost, and  2) they are usually unable to reconstruct large-scale video frames at high compression ratios.  To address these issues, we develop an {\bf{\em efficient network}} for video SCI by using {\bf {\em dense connections and space-time factorization mechanism}} within a single residual block, dubbed {\bf \emph{EfficientSCI}}. The EfficientSCI network can well establish spatial-temporal correlation by using {\bf {\em convolution in the spatial domain and Transform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DinoSR&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#31561;&#27010;&#24565;&#65292;&#33021;&#22815;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#20135;&#29983;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.10005</link><description>&lt;p&gt;
DinoSR&#65306;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning. (arXiv:2305.10005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DinoSR&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#31561;&#27010;&#24565;&#65292;&#33021;&#22815;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#20135;&#29983;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#29992;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;DinoSR&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#36825;&#20123;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#30456;&#20114;&#34917;&#20805;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;DinoSR&#39318;&#20808;&#20351;&#29992;&#25945;&#24072;&#32593;&#32476;&#20174;&#36755;&#20837;&#38899;&#39057;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#21270;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#21521;&#37327;&#19978;&#36816;&#34892;&#22312;&#32447;&#32858;&#31867;&#31995;&#32479;&#20197;&#20135;&#29983;&#26426;&#22120;&#21457;&#29616;&#30340;&#38899;&#32032;&#24211;&#23384;&#65292;&#26368;&#21518;&#20351;&#29992;&#24050;&#31163;&#25955;&#21270;&#30340;&#26631;&#35760;&#25351;&#23548;&#23398;&#29983;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DinoSR&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#21644;&#23398;&#20064;&#31163;&#25955;&#21333;&#20803;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#21311;&#21517;&#26399;&#32467;&#26463;&#21518;&#65292;&#25105;&#20204;&#23558;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce self-distillation and online clustering for self-supervised speech representation learning (DinoSR) which combines masked language modeling, self-distillation, and online clustering. We show that these concepts complement each other and result in a strong representation learning model for speech. DinoSR first extracts contextualized embeddings from the input audio with a teacher network, then runs an online clustering system on the embeddings to yield a machine-discovered phone inventory, and finally uses the discretized tokens to guide a student network. We show that DinoSR surpasses previous state-of-the-art performance in several downstream tasks, and provide a detailed analysis of the model and the learned discrete units. The source code will be made available after the anonymity period.
&lt;/p&gt;</description></item><item><title>Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09993</link><description>&lt;p&gt;
Reprompting: &#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#33258;&#21160;&#25512;&#26029;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09993
&lt;/p&gt;
&lt;p&gt;
Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Reprompting&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#32473;&#23450;&#20219;&#21153;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#25105;&#20204;&#25512;&#26029;&#36866;&#29992;&#20110;&#19968;&#32452;&#35757;&#32451;&#26679;&#20363;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#37319;&#26679;&#30340;&#35299;&#20316;&#20026;&#29238;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#37319;&#26679;&#26032;&#30340;&#37197;&#26041;&#26469;&#35299;&#20915;&#20854;&#20182;&#35757;&#32451;&#38382;&#39064;&#12290;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#20116;&#20010;Big-Bench Hard&#20219;&#21153;&#20013;&#65292;Reprompting&#30340;&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#22522;&#32447;&#12290;Reprompting&#36824;&#21487;&#20197;&#20419;&#36827;&#30693;&#35782;&#20174;&#19968;&#20010;&#26356;&#24378;&#30340;&#27169;&#22411;&#21040;&#19968;&#20010;&#36739;&#24369;&#30340;&#27169;&#22411;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Reprompting&#30456;&#23545;&#20110;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24102;&#26469;&#20102;&#39640;&#36798;+17&#20010;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#30693;&#35782;&#32452;&#21512;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#30693;&#35782;&#32452;&#21512;&#26426;&#21046;&#21644;&#34920;&#31034;&#32423;&#35268;&#33539;&#21270;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#30340;&#20851;&#38190;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#21709;&#24212;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.09990</link><description>&lt;p&gt;
&#21452;&#37325;&#35821;&#20041;&#30693;&#35782;&#32452;&#21512;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Dual Semantic Knowledge Composed Multimodal Dialog Systems. (arXiv:2305.09990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09990
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#30693;&#35782;&#32452;&#21512;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#30693;&#35782;&#32452;&#21512;&#26426;&#21046;&#21644;&#34920;&#31034;&#32423;&#35268;&#33539;&#21270;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#30340;&#20851;&#38190;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#26159;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#20016;&#30805;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;1&#65289;&#27880;&#37325;&#23646;&#24615;&#30693;&#35782;&#65292;&#20294;&#24573;&#30053;&#20102;&#21487;&#20197;&#25581;&#31034;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#20851;&#31995;&#30693;&#35782;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#21709;&#24212;&#29983;&#25104;&#65307;2&#65289;&#21482;&#36827;&#34892;&#22522;&#20110;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#36755;&#20986;&#32423;&#30417;&#30563;&#65292;&#32570;&#20047;&#34920;&#31034;&#32423;&#30340;&#35268;&#33539;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65288;&#21517;&#20026;MDS-S2&#65289;&#12290;&#29305;&#21035;&#26159;&#65292;MDS-S2&#39318;&#20808;&#21516;&#26102;&#20174;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#30693;&#35782;&#65292;&#20854;&#20013;&#38750;&#30452;&#35266;&#30340;&#20851;&#31995;&#30693;&#35782;&#26159;&#36890;&#36807;n&#36339;&#22270;&#24418;&#36941;&#21382;&#25552;&#21462;&#30340;&#12290;&#27492;&#21518;&#65292;&#32771;&#34385;&#21040;&#23646;&#24615;&#30693;&#35782;&#21644;&#20851;&#31995;&#30693;&#35782;&#21487;&#20197;&#26377;&#21161;&#20110;&#21709;&#24212;&#19981;&#21516;&#32423;&#21035;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32423;&#30693;&#35782;&#32452;&#21512;&#26426;&#21046;&#26469;&#26377;&#25928;&#22320;&#34701;&#21512;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24378;&#21046;&#23646;&#24615;&#20851;&#31995;&#19968;&#33268;&#24615;&#24182;&#25552;&#39640;&#21709;&#24212;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#32423;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#21452;&#37325;&#35821;&#20041;&#30693;&#35782;&#32452;&#21512;&#30340;&#27169;&#22411;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual response generation is an essential task for multimodal task-oriented dialog systems.Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation}, and 2) only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20889;&#20316;&#36741;&#21161;&#20013;&#30340;&#8220;&#26234;&#33021;&#35789;&#27719;&#24314;&#35758;&#8221;&#65288;SWS&#65289;&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#21644;&#26356;&#21152;&#29616;&#23454;&#30340;&#20889;&#20316;&#36741;&#21161;&#22330;&#26223;&#12290;&#23427;&#21253;&#21547;&#19968;&#32452;&#25968;&#25454;&#38598;&#21644;&#25552;&#20379;&#26367;&#25442;&#24314;&#35758;&#30340;&#38590;&#39064;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#22522;&#30784;&#21450;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.09975</link><description>&lt;p&gt;
&#20889;&#20316;&#36741;&#21161;&#26234;&#33021;&#35789;&#27719;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Smart Word Suggestions for Writing Assistance. (arXiv:2305.09975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20889;&#20316;&#36741;&#21161;&#20013;&#30340;&#8220;&#26234;&#33021;&#35789;&#27719;&#24314;&#35758;&#8221;&#65288;SWS&#65289;&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#21644;&#26356;&#21152;&#29616;&#23454;&#30340;&#20889;&#20316;&#36741;&#21161;&#22330;&#26223;&#12290;&#23427;&#21253;&#21547;&#19968;&#32452;&#25968;&#25454;&#38598;&#21644;&#25552;&#20379;&#26367;&#25442;&#24314;&#35758;&#30340;&#38590;&#39064;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#22522;&#30784;&#21450;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#35789;&#27719;&#20351;&#29992;&#26159;&#20889;&#20316;&#36741;&#21161;&#20013;&#19968;&#39033;&#37325;&#35201;&#21151;&#33021;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#8220;&#26234;&#33021;&#35789;&#27719;&#24314;&#35758;&#8221;&#65288;SWS&#65289;&#20219;&#21153;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#65292;SWS&#24378;&#35843;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#24182;&#25552;&#20986;&#26356;&#21152;&#29616;&#23454;&#30340;&#20889;&#20316;&#36741;&#21161;&#22330;&#26223;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#35782;&#21035;&#38656;&#35201;&#25913;&#36827;&#30340;&#21333;&#35789;&#25110;&#30701;&#35821;&#65292;&#24182;&#25552;&#20379;&#26367;&#25442;&#24314;&#35758;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#34987;&#20154;&#24037;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#30340;&#22823;&#22411;&#36965;&#24863;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#35780;&#20272;&#26694;&#26550;&#12290;&#27979;&#35797;&#25968;&#25454;&#21253;&#25324;1000&#20010;&#30001;&#33521;&#35821;&#23398;&#20064;&#32773;&#20889;&#25104;&#30340;&#21477;&#23376;&#65292;&#38468;&#24102;10&#20010;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#27880;&#37322;&#30340;&#36229;&#36807;16000&#20010;&#26367;&#25442;&#24314;&#35758;&#12290;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#36229;&#36807;370&#19975;&#20010;&#21477;&#23376;&#21644;&#36890;&#36807;&#35268;&#21017;&#29983;&#25104;&#30340;1270&#19975;&#20010;&#24314;&#35758;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SWS&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;SWS&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing word usage is a desired feature for writing assistance. To further advance research in this area, this paper introduces "Smart Word Suggestions" (SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end evaluation and presents a more realistic writing assistance scenario. This task involves identifying words or phrases that require improvement and providing substitution suggestions. The benchmark includes human-labeled data for testing, a large distantly supervised dataset for training, and the framework for evaluation. The test data includes 1,000 sentences written by English learners, accompanied by over 16,000 substitution suggestions annotated by 10 native speakers. The training dataset comprises over 3.7 million sentences and 12.7 million suggestions generated through rules. Our experiments with seven baselines demonstrate that SWS is a challenging task. Based on experimental analysis, we suggest potential directions for future research on SWS. The dataset 
&lt;/p&gt;</description></item><item><title>CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09955</link><description>&lt;p&gt;
CooK: &#29992;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#30693;&#35782;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09955
&lt;/p&gt;
&lt;p&gt;
CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#21644;&#35821;&#22659;&#20013;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#25110;&#29983;&#25104;&#30693;&#35782;&#25552;&#31034;&#26469;&#25913;&#21892;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21453;&#26144;&#30693;&#35782;&#20016;&#23500;&#27169;&#22411;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#30693;&#35782;&#24212;&#35813;&#26159;&#27169;&#22359;&#21270;&#65292;&#19981;&#26029;&#22686;&#38271;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65307;&#30693;&#35782;&#33719;&#21462;&#21644;&#29983;&#25104;&#24212;&#35813;&#26159;&#21327;&#20316;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773; contribue &#26032;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; CooK&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20026;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#26469;&#28304;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#22312;&#24191;&#27867;&#39046;&#22495;&#21644;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#36825;&#20123;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#30693;&#35782;&#24211;&#65292;&#21518;&#26469;&#34987;&#25552;&#31034;&#29983;&#25104;&#36890;&#29992;&#30340; LLM &#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#30693;&#35782;&#36807;&#28388;&#22120;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#28304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#36129;&#29486;&#32773;&#32452;&#20214;&#65292;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#20026;&#31995;&#32479;&#36129;&#29486;&#29305;&#23450;&#20110;&#22495;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; CooK &#22312;&#19968;&#32452;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09900</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#26159;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697; \cite{basu2022equi} &#21644; \cite{kaba2022equivariance} &#20998;&#21035;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#32676;&#21464;&#25442;&#36755;&#20837;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#32676;&#24179;&#22343;&#20540;&#65288;\textit{equitune}&#65289;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20174;&#19981;&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#21462;&#31561;&#21464;&#36755;&#20986;&#12290;&#34429;&#28982; \cite{kaba2022equivariance} &#21482;&#20851;&#27880;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#33391;&#22909;&#30340;&#24494;&#35843;&#32467;&#26524;&#19979;&#65292;\textit{equitune} &#22312;&#31561;&#21464;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#26576;&#20123;&#36716;&#25442;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#65292;&#32780;&#23545;&#20854;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#30340;$\lambda$-\textit{equitune} &#26041;&#27861;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#37325;&#26032;&#25490;&#21015;&#30828;&#20214;&#24179;&#34913;&#20102;&#35789;&#27719;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#20197;&#32531;&#35299;&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#30340;&#26292;&#38706;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09898</link><description>&lt;p&gt;
&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#24179;&#34913;&#35789;&#27719;&#21644;&#35821;&#20041;&#36136;&#37327;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balancing Lexical and Semantic Quality in Abstractive Summarization. (arXiv:2305.09898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#37325;&#26032;&#25490;&#21015;&#30828;&#20214;&#24179;&#34913;&#20102;&#35789;&#27719;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#20197;&#32531;&#35299;&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#30340;&#26292;&#38706;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#26333;&#20809;&#20559;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20960;&#24180;&#19968;&#30452;&#20351;&#29992;&#37325;&#26032;&#25490;&#24207;&#31995;&#32479;&#12290;&#23613;&#31649;&#26377;&#20123;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#36825;&#20010;&#26041;&#27861;&#20173;&#28982;&#19981;&#22826;&#25104;&#29087;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#36890;&#36807;ROUGE&#20998;&#25968;&#21644;&#23545;&#40784;&#20505;&#36873;&#25688;&#35201;&#26469;&#25351;&#23450;&#25490;&#21517;&#65292;&#20294;&#35789;&#27719;&#37325;&#21472;&#25351;&#26631;&#21644;&#35821;&#20041;&#30456;&#20284;&#24230;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#37325;&#26032;&#25490;&#21015;&#31243;&#24207;&#24179;&#34913;&#35789;&#27719;&#21644;&#35821;&#20041;&#36136;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37325;&#26032;&#23450;&#20041;&#20102;&#25490;&#21517;&#20013;&#30340;&#20551;&#38451;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#23427;&#20204;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;&#22312;CNN / DailyMail&#21644;XSum&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20272;&#35745;&#25688;&#35201;&#30340;&#21547;&#20041;&#65292;&#32780;&#19981;&#20250;&#20005;&#37325;&#38477;&#20302;&#35789;&#27719;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;CNN / DailyMail&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;89.67&#30340;BERTScore&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have mostly specified the rank through the ROUGE score and aligned candidate summaries, but there can be quite a large gap between the lexical overlap metric and semantic similarity. In this paper, we propose a novel training method in which a re-ranker balances the lexical and semantic quality. We further newly define false positives in ranking and present a strategy to reduce their influence. Experiments on the CNN/DailyMail and XSum datasets show that our method can estimate the meaning of summaries without seriously degrading the lexical aspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail dataset, reaching new state-of-the-art performance. Our code is publicly availa
&lt;/p&gt;</description></item><item><title>ClusterNS &#26159;&#19968;&#31181;&#23558;&#32858;&#31867;&#20449;&#24687;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340; K &#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#38590;&#36127;&#20363;&#24182;&#35782;&#21035;&#38169;&#35823;&#36127;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.09892</link><description>&lt;p&gt;
&#38754;&#21521;&#32858;&#31867;&#30340;&#36127;&#37319;&#26679;&#29992;&#20110;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clustering-Aware Negative Sampling for Unsupervised Sentence Representation. (arXiv:2305.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09892
&lt;/p&gt;
&lt;p&gt;
ClusterNS &#26159;&#19968;&#31181;&#23558;&#32858;&#31867;&#20449;&#24687;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340; K &#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#38590;&#36127;&#20363;&#24182;&#35782;&#21035;&#38169;&#35823;&#36127;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#22312;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#26089;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27491;&#20363;&#30340;&#26500;&#24314;&#19978;&#65292;&#32780; batch &#20869;&#30340;&#26679;&#26412;&#36890;&#24120;&#34987;&#35270;&#20026;&#36127;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#36873;&#25321;&#21512;&#36866;&#30340;&#36127;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#38590;&#36127;&#20363;&#30340;&#31232;&#32570;&#24615;&#21644;&#38169;&#35823;&#36127;&#20363;&#30340;&#21253;&#21547;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ClusterNS&#65288;&#38754;&#21521;&#32858;&#31867;&#30340;&#36127;&#37319;&#26679;&#65289;&#65292;&#19968;&#31181;&#23558;&#32858;&#31867;&#20449;&#24687;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24212;&#29992;&#25913;&#36827;&#30340; K &#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#26469;&#25552;&#20379;&#38590;&#36127;&#20363;&#24182;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#36127;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340; ClusterNS &#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#19978;&#20256;&#21040; https://github.com/xxxxxx&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; GIS&amp;T BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#20197;&#35299;&#20915;&#25163;&#21160;&#23450;&#20041;&#35805;&#39064;&#20851;&#31995;&#24102;&#26469;&#30340;&#19981;&#23436;&#25972;&#35780;&#20272;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#37327;&#35805;&#39064;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.09877</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#8212;&#8212;&#20197;UCGIS GIS&amp;T&#30693;&#35782;&#20307;&#31995;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&amp;T Body of Knowledge. (arXiv:2305.09877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; GIS&amp;T BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#20197;&#35299;&#20915;&#25163;&#21160;&#23450;&#20041;&#35805;&#39064;&#20851;&#31995;&#24102;&#26469;&#30340;&#19981;&#23436;&#25972;&#35780;&#20272;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#37327;&#35805;&#39064;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GIS&amp;T &#30693;&#35782;&#20307;&#31995;&#26159;&#30001;&#22320;&#29702;&#20449;&#24687;&#31185;&#23398;&#19982;&#25216;&#26415;&#30456;&#20851;&#22242;&#20307;&#21457;&#36215;&#30340;&#19968;&#20010;&#31038;&#21306;&#39033;&#30446;&#65292;&#26088;&#22312;&#23450;&#20041;&#12289;&#24320;&#21457;&#21644;&#35760;&#24405;&#22320;&#29702;&#20449;&#24687;&#31185;&#23398;&#19982;&#25216;&#26415;&#30456;&#20851;&#35805;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035; BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182; NLP &#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#19988;&#20934;&#30830;&#22320;&#24230;&#37327;&#35805;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initiated by the University Consortium of Geographic Information Science (UCGIS), GIS&amp;T Body of Knowledge (BoK) is a community-driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&amp;T). In recent years, GIS&amp;T BoK has undergone rigorous development in terms of its topic re-organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationship. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from te
&lt;/p&gt;</description></item><item><title>Jaseci&#32534;&#31243;&#33539;&#24335;&#21644;&#36816;&#34892;&#26102;&#22534;&#26632;&#30340;&#35774;&#35745;&#21407;&#21017;&#22312;&#20110;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#21160;&#20248;&#21270;&#23613;&#21487;&#33021;&#22810;&#30340;&#35268;&#27169;&#21270;&#25968;&#25454;&#31649;&#29702;&#12289;&#24494;&#26381;&#21153;&#32452;&#20214;&#21270;&#21644;&#23454;&#26102;&#26356;&#26032;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25277;&#35937;&#27700;&#24179;&#65292;&#38477;&#20302;&#20102;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#38590;&#24230;&#21644;&#37096;&#32626;&#38376;&#27099;&#12290;</title><link>http://arxiv.org/abs/2305.09864</link><description>&lt;p&gt;
Jaseci&#32534;&#31243;&#33539;&#24335;&#21644;&#36816;&#34892;&#26102;&#22534;&#26632;&#65306;&#36731;&#26494;&#24555;&#36895;&#26500;&#24314;&#35268;&#27169;&#21270;&#29983;&#20135;&#24212;&#29992;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
The Jaseci Programming Paradigm and Runtime Stack: Building Scale-out Production Applications Easy and Fast. (arXiv:2305.09864v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09864
&lt;/p&gt;
&lt;p&gt;
Jaseci&#32534;&#31243;&#33539;&#24335;&#21644;&#36816;&#34892;&#26102;&#22534;&#26632;&#30340;&#35774;&#35745;&#21407;&#21017;&#22312;&#20110;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#21160;&#20248;&#21270;&#23613;&#21487;&#33021;&#22810;&#30340;&#35268;&#27169;&#21270;&#25968;&#25454;&#31649;&#29702;&#12289;&#24494;&#26381;&#21153;&#32452;&#20214;&#21270;&#21644;&#23454;&#26102;&#26356;&#26032;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25277;&#35937;&#27700;&#24179;&#65292;&#38477;&#20302;&#20102;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#38590;&#24230;&#21644;&#37096;&#32626;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#29983;&#20135;&#35268;&#27169;&#21270;&#24212;&#29992;&#31243;&#24207;&#21253;&#25324;&#35768;&#22810;&#23376;&#24212;&#29992;&#31243;&#24207;&#32452;&#20214;&#65292;&#20363;&#22914;&#23384;&#20648;&#21518;&#31471;&#12289;&#26085;&#24535;&#22522;&#30784;&#35774;&#26045;&#21644;AI&#27169;&#22411;&#12290;&#36825;&#20123;&#32452;&#20214;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#29305;&#24615;&#65292;&#38656;&#35201;&#21327;&#21516;&#24037;&#20316;&#65292;&#24182;&#20316;&#20026;&#24494;&#26381;&#21153;&#19982;&#24444;&#27492;&#25509;&#21475;&#12290;&#36825;&#23548;&#33268;&#22312;&#24320;&#21457;&#12289;&#20248;&#21270;&#12289;&#37197;&#32622;&#21644;&#37096;&#32626;&#35268;&#27169;&#21270;&#24212;&#29992;&#31243;&#24207;&#26041;&#38754;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#25552;&#39640;&#20102;&#22823;&#22810;&#25968;&#20010;&#20154;&#21644;&#23567;&#22242;&#38431;&#30340;&#20934;&#20837;&#38376;&#27099;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#35774;&#35745;&#36816;&#34892;&#26102;&#31995;&#32479;Jaseci&#21644;&#32534;&#31243;&#35821;&#35328;Jac&#65292;&#26088;&#22312;&#20943;&#23569;&#36825;&#31181;&#22797;&#26434;&#24615;&#12290;Jaseci&#35774;&#35745;&#21644;&#24320;&#21457;&#30340;&#20851;&#38190;&#35774;&#35745;&#21407;&#21017;&#26159;&#36890;&#36807;&#23558;&#23613;&#21487;&#33021;&#22810;&#30340;&#35268;&#27169;&#21270;&#25968;&#25454;&#31649;&#29702;&#12289;&#24494;&#26381;&#21153;&#32452;&#20214;&#21270;&#21644;&#23454;&#26102;&#26356;&#26032;&#22797;&#26434;&#24615;&#31227;&#20837;&#36816;&#34892;&#26102;&#22534;&#26632;&#20197;&#36827;&#34892;&#33258;&#21160;&#21270;&#21644;&#33258;&#21160;&#20248;&#21270;&#26469;&#25552;&#39640;&#25277;&#35937;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#26469;&#23637;&#31034;Jaseci&#22312;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#21644;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#26041;&#38754;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's production scale-out applications include many sub-application components, such as storage backends, logging infrastructure and AI models. These components have drastically different characteristics, are required to work in collaboration, and interface with each other as microservices. This leads to increasingly high complexity in developing, optimizing, configuring, and deploying scale-out applications, raising the barrier to entry for most individuals and small teams. We developed a novel co-designed runtime system, Jaseci, and programming language, Jac, which aims to reduce this complexity. The key design principle throughout Jaseci's design is to raise the level of abstraction by moving as much of the scale-out data management, microservice componentization, and live update complexity into the runtime stack to be automated and optimized automatically. We use real-world AI applications to demonstrate Jaseci's benefit for application performance and developer productivity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09863</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#19981;&#36879;&#26126;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#19968;&#20010;&#8220;&#25991;&#26412;&#27169;&#22359;&#8221;&#26159;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#26631;&#37327;&#36830;&#32493;&#20540;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#20363;&#22914;LLM&#20869;&#30340;&#23376;&#27169;&#22359;&#25110;&#22823;&#33041;&#21306;&#22495;&#30340;&#25311;&#21512;&#27169;&#22411;&#12290;&#8220;&#40657;&#30418;&#8221;&#34920;&#31034;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#27169;&#22359;&#30340;&#36755;&#20837;/&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Summarize and Score&#65288;SASC&#65289;&#26041;&#27861;&#65292;&#23427;&#25509;&#21463;&#25991;&#26412;&#27169;&#22359;&#24182;&#36820;&#22238;&#27169;&#22359;&#36873;&#25321;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;SASC&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#27169;&#22359;&#19978;&#35780;&#20272;SASC&#65292;&#24182;&#21457;&#29616;&#23427;&#32463;&#24120;&#24674;&#22797;&#22522;&#26412;&#30495;&#30456;&#35828;&#26126;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SASC&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#65292;&#20351;&#24471;&#26816;&#26597;BERT&#30340;&#27169;&#22359;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.09860</link><description>&lt;p&gt;
Epsilon Sampling Rocks: &#30740;&#31350;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#24050;&#32463;&#26174;&#31034;&#20986;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26102;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;MBR&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20363;&#22914;&#31062;&#20808;&#37319;&#26679;&#65292;&#26680;&#37319;&#26679;&#21644;top-k&#37319;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#23427;&#20204;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;epsilon&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25152;&#26377;&#23567;&#20110;epsilon&#30340;&#26631;&#35760;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#33719;&#24471;&#20844;&#24179;&#30340;&#27010;&#29575;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;epsilon&#37319;&#26679;&#30340;MBR&#35299;&#30721;&#26174;&#33879;&#20248;&#20110;&#19981;&#20165;&#26159;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#32780;&#19988;&#36824;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;MBR&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09858</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65306;&#20197; LLMS &#22312;&#30005;&#21830;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20026;&#20363;&#30340;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#22686;&#24378;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#20135;&#21697;&#25110;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#30340;&#20114;&#34917;&#25110;&#26367;&#20195;&#20851;&#31995;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#21160;&#24577;&#24615;&#21644;&#20154;&#21147;&#25104;&#26412;&#30456;&#20851;&#30340;&#21407;&#22240;&#65292;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#20046;&#24847;&#26009;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110; LLM &#22312;&#30005;&#23376;&#21830;&#21153;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181; LLM&#65292;&#21253;&#25324; PaLM &#21644; GPT-3.5&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#20851;&#31995;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
&lt;/p&gt;</description></item><item><title>CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09857</link><description>&lt;p&gt;
CoEdIT&#65306;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09857
&lt;/p&gt;
&lt;p&gt;
CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#25110;&#20462;&#35746;&#26159;&#20154;&#31867;&#20889;&#20316;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#21151;&#33021;&#12290;&#29702;&#35299;LLMs&#22312;&#36827;&#34892;&#39640;&#36136;&#37327;&#20462;&#35746;&#21644;&#19982;&#20154;&#31867;&#20889;&#20316;&#32773;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#26500;&#24314;&#26377;&#25928;&#20889;&#20316;&#21161;&#25163;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;LLMs&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#20808;&#21069;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20462;&#35746;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CoEdIT&#65292;&#36825;&#26159;&#19968;&#27454;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#12290;CoEdIT&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#25351;&#20196;&#65292;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#8220;&#20351;&#21477;&#23376;&#26356;&#31616;&#21333;&#8221;&#25110;&#8220;&#20197;&#26356;&#20013;&#31435;&#30340;&#39118;&#26684;&#20889;&#20316;&#8221;&#65292;&#24182;&#36755;&#20986;&#32534;&#36753;&#21518;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;1&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#65288;2&#65289;&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text editing or revision is an essential function of the human writing process. Understanding the capabilities of LLMs for making high-quality revisions and collaborating with human writers is a critical step toward building effective writing assistants. With the prior success of LLMs and instruction tuning, we leverage instruction-tuned LLMs for text revision to improve the quality of user-generated text and improve the efficiency of the process. We introduce CoEdIT, a state-of-the-art text editing model for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as "Make the sentence simpler" or "Write it in a more neutral style," and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;CPL-NoViD&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#36829;&#35268;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09846</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#36829;&#35268;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities. (arXiv:2305.09846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;CPL-NoViD&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#36829;&#35268;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#21306;&#20013;&#26816;&#27979;&#36829;&#35268;&#34892;&#20026;&#23545;&#20110;&#32500;&#25252;&#20581;&#24247;&#21644;&#23433;&#20840;&#30340;&#22312;&#32447;&#35752;&#35770;&#31354;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20043;&#38388;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22240;&#20026;&#20026;&#36825;&#31181;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#23398;&#20064;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#35268;&#21017;&#19979;&#30340;&#36829;&#35268;&#34892;&#20026;&#65288;CPL-NoViD&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;CPL-NoViD&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#35268;&#21017;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#19981;&#20165;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#36829;&#35268;&#26816;&#27979;&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting norm violations in online communities is critical to maintaining healthy and safe spaces for online discussions. Existing machine learning approaches often struggle to adapt to the diverse rules and interpretations across different communities due to the inherent challenges of fine-tuning models for such context-specific tasks. In this paper, we introduce Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a novel method that employs prompt-based learning to detect norm violations across various types of rules. CPL-NoViD outperforms the baseline by incorporating context through natural language prompts and demonstrates improved performance across different rule types. Significantly, it not only excels in cross-rule-type and cross-community norm violation detection but also exhibits adaptability in few-shot learning scenarios. Most notably, it establishes a new state-of-the-art in norm violation detection, surpassing existing benchmarks. Our work high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09807</link><description>&lt;p&gt;
&#20851;&#20110;transformer&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#21487;&#36801;&#31227;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26597;&#35810;&#23545;&#27169;&#22411;&#23398;&#20064;&#26368;&#26377;&#30410;&#30340;&#31034;&#20363;&#26469;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#20110;&#24494;&#35843;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#19981;&#28165;&#26970;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#20027;&#21160;&#23398;&#20064;&#25910;&#30410;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36866;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#31215;&#26497;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#22312;&#20351;&#29992;&#19981;&#21516;PLM&#35757;&#32451;&#26102;&#33021;&#21542;&#20445;&#25345;AL&#25910;&#30410;&#12290;&#25105;&#20204;&#23558;AL&#25968;&#25454;&#38598;&#30340;&#21487;&#36801;&#31227;&#24615;&#19982;&#19981;&#21516;PLMs&#26597;&#35810;&#21040;&#30340;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#34920;&#26126;&#20855;&#26377;&#31867;&#20284;&#33719;&#21462;&#24207;&#21015;&#30340;AL&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#20855;&#26377;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#33719;&#21462;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#26356;&#21463;&#21040;AL&#26041;&#27861;&#30340;&#36873;&#25321;&#32780;&#38750;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
&lt;/p&gt;</description></item><item><title>&#25514;&#36766;&#23545;&#20449;&#24687;&#21442;&#19982;&#21644;&#20915;&#31574;&#21046;&#23450;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20449;&#24687;&#21442;&#19982;&#26159;&#30001;&#20449;&#24687;&#26412;&#36523;&#30340;&#34920;&#36798;&#25152;&#39537;&#21160;&#21644;&#22521;&#32946;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.09798</link><description>&lt;p&gt;
&#35789;&#35821;&#30340;&#26041;&#24335;&#65306;&#35789;&#35821;&#36873;&#25321;&#23545;&#20449;&#24687;&#21442;&#19982;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Ways of Words: The Impact of Word Choice on Information Engagement and Decision Making. (arXiv:2305.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09798
&lt;/p&gt;
&lt;p&gt;
&#25514;&#36766;&#23545;&#20449;&#24687;&#21442;&#19982;&#21644;&#20915;&#31574;&#21046;&#23450;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20449;&#24687;&#21442;&#19982;&#26159;&#30001;&#20449;&#24687;&#26412;&#36523;&#30340;&#34920;&#36798;&#25152;&#39537;&#21160;&#21644;&#22521;&#32946;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25514;&#36766;&#65292;&#29305;&#21035;&#26159;&#35789;&#35821;&#36873;&#25321;&#65292;&#23545;&#20449;&#24687;&#21442;&#19982;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#24433;&#21709;&#12290;&#32508;&#21512;&#20102;&#29992;&#25143;&#21442;&#19982;&#29702;&#35770;&#21644;&#20449;&#24687;&#34892;&#20026;&#29702;&#35770;&#20004;&#20010;&#29702;&#35770;&#27169;&#22411;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#20102;&#20551;&#35774;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#22312;&#34913;&#37327;&#35789;&#35821;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20449;&#24687;&#21442;&#19982;&#30340;&#24863;&#30693;&#12289;&#21442;&#19982;&#24230;&#21644;&#22362;&#25345;&#24230;&#31561;&#19977;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#21442;&#19982;&#19981;&#21516;&#20110;&#20854;&#20182;&#24418;&#24335;&#30340;&#21442;&#19982;&#65292;&#23427;&#26159;&#30001;&#20449;&#24687;&#26412;&#36523;&#30340;&#34920;&#36798;&#25152;&#39537;&#21160;&#21644;&#22521;&#32946;&#30340;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#20449;&#24687;&#31995;&#32479;&#26469;&#26597;&#30475;&#12289;&#20132;&#20114;&#21644;&#20351;&#29992;&#20449;&#24687;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25514;&#36766;&#21487;&#20197;&#23545;&#20449;&#24687;&#21442;&#19982;&#21644;&#20915;&#31574;&#21046;&#23450;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Little research has explored how information engagement (IE), the degree to which individuals interact with and use information in a manner that manifests cognitively, behaviorally, and affectively. This study explored the impact of phrasing, specifically word choice, on IE and decision making. Synthesizing two theoretical models, User Engagement Theory UET and Information Behavior Theory IBT, a theoretical framework illustrating the impact of and relationships among the three IE dimensions of perception, participation, and perseverance was developed and hypotheses generated. The framework was empirically validated in a large-scale user study measuring how word choice impacts the dimensions of IE. The findings provide evidence that IE differs from other forms of engagement in that it is driven and fostered by the expression of the information itself, regardless of the information system used to view, interact with, and use the information. The findings suggest that phrasing can have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09785</link><description>&lt;p&gt;
&#20174;&#23545;&#27604;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35821;&#20041;&#27010;&#24565;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models. (arXiv:2305.09785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25429;&#25417;&#27010;&#24565;&#21547;&#20041;&#30340;&#21521;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#33267;&#20170;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20165;&#22312;&#23545;&#36825;&#31181;&#27010;&#24565;&#23884;&#20837;&#30340;&#36136;&#37327;&#26041;&#38754;&#20135;&#29983;&#20102;&#26377;&#38480;&#30340;&#25552;&#39640;&#12290;&#30446;&#21069;&#30340;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#36890;&#24120;&#36890;&#36807;&#22312;&#26576;&#31181;&#35821;&#26009;&#24211;&#20013;&#24179;&#22343;&#34920;&#31034;&#19968;&#20010;&#27010;&#24565;&#22312;&#20854;&#25552;&#21450;&#20013;&#30340;&#35821;&#22659;&#21270;&#34920;&#31034;&#26469;&#34920;&#31034;&#19968;&#20010;&#27010;&#24565;&#12290;&#36825;&#22312;&#33267;&#23569;&#20004;&#20010;&#26041;&#38754;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#39318;&#20808;&#65292;&#35821;&#22659;&#21270;&#30340;&#21333;&#35789;&#21521;&#37327;&#20855;&#26377;&#24322;&#24120;&#30340;&#20960;&#20309;&#24615;&#65292;&#36825;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#27010;&#24565;&#23884;&#20837;&#24212;&#35813;&#25429;&#25417;&#27010;&#24565;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#32780;&#35821;&#22659;&#21270;&#30340;&#21333;&#35789;&#21521;&#37327;&#20063;&#21463;&#21040;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#28857;&#65292;&#27599;&#24403;&#20004;&#20010;&#21477;&#23376;&#26174;&#31034;&#30456;&#20284;&#30340;&#23646;&#24615;&#26102;&#65292;&#30456;&#24212;&#30340;&#35821;&#22659;&#21270;&#21521;&#37327;&#20063;&#24212;&#35813;&#30456;&#20284;&#12290;&#19968;&#31181;&#31574;&#30053;&#26159;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#65292;&#20272;&#35745;&#22312;&#19968;&#20010;&#21477;&#23376;&#20013;&#34920;&#36798;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.09782</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35270;&#35273;&#38382;&#31572;&#31639;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29702;&#35299;&#24182;&#22238;&#31572;&#38382;&#39064;&#12290; VQA &#23545;&#35270;&#35273;&#21463;&#25439;&#32773;&#26377;&#24110;&#21161;&#65292;&#21487;&#29992;&#20110;&#23433;&#20840;&#30417;&#25511;&#31995;&#32479;&#21644;&#20174;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290; &#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23398;&#20064;&#38382;&#39064;&#30340;&#35821;&#20041;&#24182;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#12290; &#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#29992;&#20110;&#20197;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25152;&#38382;&#38382;&#39064;&#28041;&#21450;&#30340;&#29289;&#20307;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#34920;&#31034;&#12290; &#27880;&#24847;&#21147;&#27169;&#22411;&#35797;&#22270;&#27169;&#20223;&#20154;&#31867;&#26681;&#25454;&#35821;&#22659;&#20851;&#27880;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#34892;&#20026;&#12290; &#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340; VQA &#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#29983;&#25104;&#25991;&#26412;&#35821;&#20041;&#65292;&#35782;&#21035;&#23545;&#35937;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#21069;&#21521;&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;&#26080;&#24212;&#29992;&#25351;&#23548;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;&#20197;&#24110;&#21161;&#35774;&#22791;&#19978;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20811;&#26381;&#36895;&#24230;&#12289;&#30913;&#30424;&#21644;&#20869;&#23384;&#31561;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.09764</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#26080;&#24212;&#29992;&#35821;&#35328;&#24314;&#27169;&#25351;&#23548;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Application-Agnostic Language Modeling for On-Device ASR. (arXiv:2305.09764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#21069;&#21521;&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;&#26080;&#24212;&#29992;&#25351;&#23548;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;&#20197;&#24110;&#21161;&#35774;&#22791;&#19978;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20811;&#26381;&#36895;&#24230;&#12289;&#30913;&#30424;&#21644;&#20869;&#23384;&#31561;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#31995;&#32479;&#30456;&#27604;&#65292;&#35774;&#22791;&#19978;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;&#23427;&#20204;&#24517;&#39035;&#22312;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#36895;&#24230;&#12289;&#30913;&#30424;&#22823;&#23567;&#21644;&#20869;&#23384;&#30340;&#38480;&#21046;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#24517;&#39035;&#20026;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#37197;&#30340;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#26381;&#21153;&#65292;&#20363;&#22914;&#19982;&#34394;&#25311;&#21161;&#25163;&#36890;&#20449;&#21644;&#35821;&#38899;&#36716;&#25991;&#26412;&#31561;&#12290;&#20026;&#20102;&#20026;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#26381;&#21153;&#65292;&#26368;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#26500;&#24314;&#29305;&#23450;&#20110;&#24212;&#29992;&#31243;&#24207;&#30340;(&#35821;&#35328;)&#27169;&#22411;&#65292;&#20294;&#36825;&#20250;&#22686;&#21152;&#20869;&#23384;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#21644;&#26550;&#26500;&#39537;&#21160;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#24212;&#29992;&#25351;&#23548;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#21069;&#21521;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#25214;&#21040;&#22312;&#19981;&#21516;&#35774;&#22791;&#38480;&#21046;&#20043;&#38388;&#30340;&#26368;&#20339;&#25240;&#34935;&#12290;&#19982;&#29305;&#23450;&#20110;&#24212;&#29992;&#31243;&#24207;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#30913;&#30424;&#22823;&#23567;&#20943;&#21322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21407;&#27169;&#22411;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several applications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build application-specific (language) models, but this leads to an increase in memory. Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model. We propose two novel feed-forward architectures that find an optimal trade off between different on-device constraints. In comparison to the application-specific solution, one of our novel approaches reduces the disk size by half, while maintaining speed and accuracy of the original model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#32423;&#21035;&#30340;&#22810;&#23618;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TM-HGNN&#65289;&#65292;&#36890;&#36807;&#31508;&#35760;&#21644;&#20998;&#31867;&#32423;&#21035;&#30340;&#36229;&#36793;&#32452;&#35013;&#26377;&#29992;&#30340;&#20013;&#24615;&#35789;&#21644;&#31232;&#26377;&#20851;&#38190;&#35789;&#20197;&#20445;&#30041;&#20020;&#24202;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09756</link><description>&lt;p&gt;
&#24739;&#32773;&#30340;&#20020;&#24202;&#31508;&#35760;&#20855;&#26377;&#33258;&#24049;&#30340;&#23618;&#27425;&#32467;&#26500;&#65306;&#22810;&#23618;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24739;&#32773;&#32423;&#21035;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning. (arXiv:2305.09756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#32423;&#21035;&#30340;&#22810;&#23618;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TM-HGNN&#65289;&#65292;&#36890;&#36807;&#31508;&#35760;&#21644;&#20998;&#31867;&#32423;&#21035;&#30340;&#36229;&#36793;&#32452;&#35013;&#26377;&#29992;&#30340;&#20013;&#24615;&#35789;&#21644;&#31232;&#26377;&#20851;&#38190;&#35789;&#20197;&#20445;&#30041;&#20020;&#24202;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#30693;&#35782;&#26469;&#39044;&#27979;&#24739;&#32773;&#30340;&#30149;&#24773;&#23545;&#20110;&#25552;&#20379;&#36866;&#24403;&#30340;&#25252;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#24739;&#32773;&#30340;&#20020;&#24202;&#31508;&#35760;&#21253;&#21547;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#20854;&#38590;&#20197;&#29702;&#35299;&#30340;&#20869;&#23481;&#21644;&#22797;&#26434;&#30340;&#23618;&#27425;&#32467;&#26500;&#32780;&#34987;&#20302;&#20272;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#25991;&#26723;&#20998;&#31867;&#12290;&#30452;&#25509;&#37319;&#29992;&#29616;&#26377;&#30340;&#36229;&#22270;&#26041;&#27861;&#22788;&#29702;&#24739;&#32773;&#30340;&#20020;&#24202;&#31508;&#35760;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24739;&#32773;&#30340;&#23618;&#27425;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#36890;&#36807;(1) &#39057;&#32321;&#20351;&#29992;&#30340;&#20013;&#24615;&#35789;&#21644;(2) &#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#23618;&#27425;&#32467;&#26500;&#38477;&#20302;&#20020;&#24202;&#35821;&#20041;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#31867;&#32423;&#21035;&#30340;&#22810;&#23618;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TM-HGNN&#65289;&#65292;&#20854;&#20013;&#22810;&#23618;&#36229;&#22270;&#36890;&#36807;&#31508;&#35760;&#21644;&#20998;&#31867;&#32423;&#21035;&#30340;&#36229;&#36793;&#32452;&#35013;&#26377;&#29992;&#30340;&#20013;&#24615;&#35789;&#21644;&#31232;&#26377;&#20851;&#38190;&#35789;&#65292;&#20197;&#20445;&#30041;&#20020;&#24202;&#35821;&#20041;&#20449;&#24687;&#12290;&#26500;&#24314;&#30340;&#24739;&#32773;&#36229;&#22270;&#36755;&#20837;&#20998;&#23618;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#20854;&#20013;&#28040;&#24687;&#20256;&#36882;&#22312;&#21516;&#19968;&#32423;&#21035;&#20869;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#30001;&#36229;&#36793;&#36830;&#25509;&#25351;&#23548;&#12290;&#22312;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TM-HGNN&#22312;&#24739;&#32773;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging knowledge from electronic health records (EHRs) to predict a patient's condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize the hierarchy information of the patient, which can degrade clinical semantic information by (1) frequent neutral words and (2) hierarchies with imbalanced distribution. Thus, we propose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where multi-level hypergraphs assemble useful neutral words with rare keywords via note and taxonomy level hyperedges to retain the clinical semantic information. The constructed patient hypergraphs are fed into hierarchical message passing layers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#35782;&#21035;&#21644;&#20219;&#21153;&#23398;&#20064;&#20004;&#31181;&#26041;&#24335;&#34920;&#24449;&#20102;ICL&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;LLMs&#21033;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#20219;&#21153;&#30340;&#35299;&#20915;&#65292;TR&#20027;&#35201;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;TL&#21017;&#20855;&#22791;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09731</link><description>&lt;p&gt;
&#22312;&#35821;&#22659;&#20013;&#23398;&#20064;&#65306;&#8220;&#23398;&#20064;&#8221;&#35821;&#22659;&#20013;&#30340;&#20219;&#21153;&#35782;&#21035;&#21644;&#20219;&#21153;&#23398;&#20064;&#30340;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#35782;&#21035;&#21644;&#20219;&#21153;&#23398;&#20064;&#20004;&#31181;&#26041;&#24335;&#34920;&#24449;&#20102;ICL&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;LLMs&#21033;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#20219;&#21153;&#30340;&#35299;&#20915;&#65292;TR&#20027;&#35201;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;TL&#21017;&#20855;&#22791;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35821;&#22659;&#20013;&#30340;&#23398;&#20064;&#26469;&#35299;&#20915;&#21482;&#26377;&#23569;&#25968;&#28436;&#31034;&#30340;&#20219;&#21153;&#65292;&#20294;&#20854;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;LLMs&#20165;&#22238;&#24518;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#24050;&#23398;&#27010;&#24565;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#26263;&#31034;ICL&#25191;&#34892;&#28436;&#31034;&#30340;&#38544;&#21547;&#23398;&#20064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#35782;&#21035;(TR)&#21644;&#20219;&#21153;&#23398;&#20064;(TL)&#20004;&#31181;&#26041;&#24335;&#34920;&#24449;&#20102;ICL&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;LLM&#31995;&#21015;&#65288;GPT-3&#12289;LLaMA&#21644;OPT&#65289;&#36827;&#34892;&#25511;&#21046;&#23454;&#39564;&#65292;&#22312;ICL&#20013;&#21306;&#20998;TR&#21644;TL&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21482;&#20351;&#29992;TR&#23601;&#33021;&#21462;&#24471;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#65292;TR&#19981;&#20250;&#38543;&#30528;&#26356;&#22823;&#30340;&#27169;&#22411;&#25110;&#26356;&#22810;&#30340;&#28436;&#31034;&#32780;&#36827;&#19968;&#27493;&#25913;&#21892;&#65307;&#65288;2&#65289;LLMs&#33021;&#22815;&#36890;&#36807;TL&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#65292;&#32780;TR&#21017;&#20027;&#35201;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#26684;&#39044;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#34920;&#26684;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#65292;&#24182;&#21487;&#20197;&#19982;&#22810;&#20010;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09696</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#34920;&#26684;&#39044;&#35757;&#32451;&#22686;&#24378;&#20102;&#34920;&#26684;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#26684;&#39044;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#34920;&#26684;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#65292;&#24182;&#21487;&#20197;&#19982;&#22810;&#20010;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34920;&#26684;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30340;&#28909;&#28857;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#34920;&#26684;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#34920;&#26684;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#34920;&#26684;&#39044;&#27979;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;&#22312;&#23545;&#22823;&#37327;&#23454;&#38469;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#65292;TapTap&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#34920;&#26684;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#65292;&#21253;&#25324;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#36164;&#28304;&#29615;&#22659;&#12289;&#32570;&#22833;&#20540;&#25554;&#34917;&#21644;&#22833;&#34913;&#20998;&#31867;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#21508;&#31181;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21253;&#25324;LightGBM&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;Transformer&#12290;&#27492;&#22806;&#65292;&#22312;&#34920;&#26684;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;TapTap&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#29978;&#33267;&#21487;&#20197;&#19982;&#20351;&#29992;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#30340;&#27169;&#22411;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or
&lt;/p&gt;</description></item><item><title>OOD-Speech &#26159;&#29992;&#20110; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30001;&#20247;&#21253;&#25910;&#38598;&#20102;&#27597;&#35821;&#20026; Bengali &#30340; 22,645 &#21517;&#35828;&#35805;&#32773;&#24405;&#21046;&#30340; 1177.94 &#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#32463;&#36807;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547; 17 &#31181;&#19981;&#21516;&#30340;&#36164;&#28304;&#65292;&#22914; Bengali &#30005;&#35270;&#21095;&#12289;&#26377;&#22768;&#35835;&#29289;&#12289;&#33073;&#21475;&#31168;&#12289;&#22312;&#32447;&#25945;&#23398;&#20197;&#21450;&#20234;&#26031;&#20848;&#35762;&#36947;&#31561;&#65292;&#21487;&#20316;&#20026; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.09688</link><description>&lt;p&gt;
OOD-Speech: &#29992;&#20110; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking. (arXiv:2305.09688v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09688
&lt;/p&gt;
&lt;p&gt;
OOD-Speech &#26159;&#29992;&#20110; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30001;&#20247;&#21253;&#25910;&#38598;&#20102;&#27597;&#35821;&#20026; Bengali &#30340; 22,645 &#21517;&#35828;&#35805;&#32773;&#24405;&#21046;&#30340; 1177.94 &#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#32463;&#36807;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547; 17 &#31181;&#19981;&#21516;&#30340;&#36164;&#28304;&#65292;&#22914; Bengali &#30005;&#35270;&#21095;&#12289;&#26377;&#22768;&#35835;&#29289;&#12289;&#33073;&#21475;&#31168;&#12289;&#22312;&#32447;&#25945;&#23398;&#20197;&#21450;&#20234;&#26031;&#20848;&#35762;&#36947;&#31561;&#65292;&#21487;&#20316;&#20026; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; OOD-Speech&#65292;&#36825;&#26159; Bengali &#30340;&#31532;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#20840;&#29699;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#65292;Bengali &#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#26041;&#35328;&#21644;&#38901;&#24459;&#29305;&#24449;&#65292;&#36825;&#35201;&#27714; ASR &#26694;&#26550;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20363;&#22914;&#65292;Bengali &#20013;&#30340;&#20234;&#26031;&#20848;&#23447;&#25945;&#35762;&#36947;&#26159;&#29992;&#26126;&#26174;&#19981;&#21516;&#30340;&#35821;&#35843;&#36827;&#34892;&#30340;&#65292;&#36825;&#20063;&#25104;&#20026;&#20102;&#20998;&#24067;&#21464;&#21270;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#22312;&#32447;&#20247;&#21253;&#27963;&#21160;&#25910;&#38598;&#24182;&#31579;&#36873;&#32780;&#26469;&#65292;&#20849;&#25910;&#38598;&#20102;&#26469;&#33258;&#21335;&#20122;&#30340; 22,645 &#21517;&#27597;&#35821;&#20026; Bengali &#30340;&#35828;&#35805;&#32773;&#25152;&#24405;&#21046;&#30340; 1177.94 &#23567;&#26102;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#21017;&#21253;&#25324;&#26469;&#33258; 17 &#20010;&#19981;&#21516;&#36164;&#28304;&#65288;&#22914; Bengali &#30005;&#35270;&#21095;&#12289;&#26377;&#22768;&#35835;&#29289;&#12289;&#33073;&#21475;&#31168;&#12289;&#22312;&#32447;&#25945;&#23398;&#20197;&#21450;&#20234;&#26031;&#20848;&#35762;&#36947;&#31561;&#65289;&#30340; 23.03 &#23567;&#26102;&#35821;&#38899;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#20063;&#37117;&#32463;&#36807;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;OOD-Speech &#26082;&#26159;&#24403;&#21069;&#20844;&#24320;&#30340;&#26368;&#22823;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159; Bengali &#35821;&#38899;&#35782;&#21035;&#30340;&#31532;&#19968;&#20010;&#36234;&#22495;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Being one of the most spoken languages globally, Bengali portrays large diversity in dialects and prosodic features, which demands ASR frameworks to be robust towards distribution shifts. For example, islamic religious sermons in Bengali are delivered with a tonality that is significantly different from regular speech. Our training dataset is collected via massively online crowdsourcing campaigns which resulted in 1177.94 hours collected and curated from $22,645$ native Bengali speakers from South Asia. Our test dataset comprises 23.03 hours of speech collected and manually annotated from 17 different sources, e.g., Bengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to name a few. OOD-Speech is jointly the largest publicly available speech dataset, as well as the first out-of-distribution ASR benchmarking dataset for Bengali.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09656</link><description>&lt;p&gt;
&#22768;&#26126;&#25552;&#31034;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#28857;&#65306;&#31532;&#19968;&#65292;&#22768;&#26126;&#24615;&#35268;&#33539;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#25509;&#36817;&#38382;&#39064;&#25551;&#36848;&#65292;&#22240;&#27492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35299;&#26512;&#23427;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#22996;&#25176;&#32473;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
&lt;/p&gt;</description></item><item><title>&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09550</link><description>&lt;p&gt;
PII&#30340;&#29983;&#21629;--&#19968;&#31181;PII&#28151;&#28102;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09550
&lt;/p&gt;
&lt;p&gt;
&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#19990;&#30028;&#20013;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25200;&#21160;&#25216;&#26415;&#26469;&#20943;&#23569;(&#25935;&#24863;)&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#20449;&#24687;(PII)&#25968;&#25454;&#30340;&#36807;&#24230;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;&#25968;&#25454;&#25200;&#21160;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#26174;&#30528;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;PII&#30340;&#29983;&#21629;&#8221;--&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;API&#26469;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#36827;&#34892;&#25509;&#21475;&#65292;&#19968;&#20010;&#22522;&#20110;&#37197;&#32622;&#30340;&#28151;&#28102;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;LLMs&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#19978;&#19979;&#25991;&#20445;&#23384;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#21407;&#22987;PII&#21644;&#20854;&#36716;&#25442;&#21518;&#30340;&#20154;&#36896;PII&#23545;&#24212;&#30340;&#26144;&#23556;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
&lt;/p&gt;</description></item><item><title>GIFT&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#23558;&#22270;&#24863;&#30693;&#20449;&#24687;&#38598;&#25104;&#21040;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#39034;&#24207;&#25991;&#26412;&#22788;&#29702;&#30340;PLM&#12290;</title><link>http://arxiv.org/abs/2305.09360</link><description>&lt;p&gt;
GIFT: &#22522;&#20110;&#22270;&#24863;&#30693;&#24494;&#35843;&#30340;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09360
&lt;/p&gt;
&lt;p&gt;
GIFT&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#23558;&#22270;&#24863;&#30693;&#20449;&#24687;&#38598;&#25104;&#21040;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#39034;&#24207;&#25991;&#26412;&#22788;&#29702;&#30340;PLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#35841;&#19982;&#35841;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#35828;&#20102;&#20160;&#20040;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#26041;&#27861;&#36890;&#24120;&#23558;&#35828;&#35805;&#32773;&#21644;&#35805;&#35821;&#23884;&#20837;&#21040;&#39034;&#24207;&#20449;&#24687;&#27969;&#20013;&#65292;&#25110;&#20165;&#21033;&#29992;&#22810;&#26041;&#23545;&#35805;&#20013;&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#34920;&#23618;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24863;&#30693;&#24494;&#35843;&#65288;GIFT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36890;&#29992;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#26222;&#36890;Transformer&#20013;&#65292;&#35805;&#35821;&#20043;&#38388;&#30340;&#20840;&#31561;&#36830;&#25509;&#20250;&#24573;&#30053;&#19968;&#20010;&#35805;&#35821;&#23545;&#21478;&#19968;&#20010;&#35805;&#35821;&#30340;&#31232;&#30095;&#20294;&#26377;&#21306;&#21035;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#21306;&#20998;&#35805;&#35821;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#20197;&#23558;&#22270;&#24863;&#30693;&#20449;&#21495;&#38598;&#25104;&#21040;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#20197;&#25913;&#36827;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#39034;&#24207;&#25991;&#26412;&#30340;PLMs&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GIFT&#23454;&#29616;&#21040;&#19977;&#20010;PLMs&#24182;&#23545;&#20854;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;GIFT&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09329</link><description>&lt;p&gt;
BERTTM: &#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#36827;&#34892;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20027;&#39064;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#34955;&#65288;BoW&#65289;&#20449;&#24687;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#36824;&#26159;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#23427;&#20204;&#22312;&#22788;&#29702;&#26032;&#25991;&#26723;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#21333;&#35789;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#22312;&#35789;&#20041;&#28040;&#27495;&#30340;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;OOV&#21333;&#35789;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#65292;&#20197;&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#35780;&#20272;NLG&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25351;&#26631;&#21450;&#20854;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08566</link><description>&lt;p&gt;
&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#65306;&#19968;&#31181;&#32463;&#39564;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#65292;&#20197;&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#35780;&#20272;NLG&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25351;&#26631;&#21450;&#20854;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;NLG&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#26159;&#21542;&#23558;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#29992;&#20316;&#19978;&#19979;&#25991;&#25110;&#30446;&#26631;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20998;&#20026;&#65288;i&#65289;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#65288;ii&#65289;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#20316;&#20026;&#35780;&#20272;&#33258;&#21160;&#25351;&#26631;&#22312;&#19977;&#20010;NLG&#20219;&#21153;&#20013;&#30340;&#37492;&#21035;&#21147;&#30340;&#26694;&#26550;&#65306;&#25991;&#26412;&#25688;&#35201;&#65292;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#21644;&#21463;&#25511;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we analyze NLG automatic metrics based on whether human evaluation aspect is used as context or objective to compute the metrics: (i) Task-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remains unclear.  We present metric preference checklist as a framework to assess the discriminative power of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. We show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BER
&lt;/p&gt;</description></item><item><title>C-Eval&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22871;&#20214;&#65292;&#28085;&#30422;52&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;&#22810;&#32423;&#21035;&#36873;&#25321;&#39064;&#21644;&#25361;&#25112;&#24615;&#31185;&#30446;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;GPT-4&#33021;&#22815;&#36798;&#21040;&#36229;&#36807;60&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36824;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.08322</link><description>&lt;p&gt;
C-Eval: &#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#32423;&#22810;&#23398;&#31185;&#20013;&#25991;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. (arXiv:2305.08322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08322
&lt;/p&gt;
&lt;p&gt;
C-Eval&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22871;&#20214;&#65292;&#28085;&#30422;52&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;&#22810;&#32423;&#21035;&#36873;&#25321;&#39064;&#21644;&#25361;&#25112;&#24615;&#31185;&#30446;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;GPT-4&#33021;&#22815;&#36798;&#21040;&#36229;&#36807;60&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36824;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#32423;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;C-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#35821;&#22659;&#19979;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#35780;&#20272;&#22871;&#20214;&#12290;C-Eval&#21253;&#21547;&#22235;&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#36873;&#25321;&#39064;&#65306;&#21021;&#20013;&#12289;&#39640;&#20013;&#12289;&#22823;&#23398;&#21644;&#19987;&#19994;&#27700;&#24179;&#12290;&#36825;&#20123;&#39064;&#30446;&#28085;&#30422;52&#20010;&#19981;&#21516;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#20154;&#25991;&#12289;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#31185;&#12290;C-Eval&#36824;&#37197;&#22791;&#20102;C-Eval Hard&#65292;&#36825;&#26159;C-Eval&#20013;&#19968;&#20123;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#31185;&#30446;&#65292;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#25165;&#33021;&#35299;&#20915;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;&#33521;&#25991;&#21644;&#20013;&#25991;&#27169;&#22411;&#22312;&#20869;&#30340;&#26368;&#20808;&#36827;&#30340;LLM&#22312;C-Eval&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;GPT-4&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;60&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;LLM&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#26399;&#26395;C-Eval&#23558;&#26377;&#21161;&#20110;&#20998;&#26512;&#37325;&#35201;&#30340;&#20248;&#21183;&#21644;&#30701;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07961</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#23454;&#26102;&#30340;&#22810;&#36718;&#23545;&#35805;&#20351;&#29992;&#25143;&#26356;&#21152;&#36879;&#26126;&#21644;&#25484;&#25511;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#23545;&#35805;&#33258;&#28982;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19990;&#30028;&#30693;&#35782;&#21644;&#24120;&#35782;&#25512;&#29702;&#34701;&#20837;&#21040;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#21253;&#25324;&#36866;&#24403;&#22320;&#29702;&#35299;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#23545;&#35805;&#21644;&#20174;&#22806;&#37096;&#20449;&#24687;&#28304;&#26816;&#32034;&#12290;&#30001;&#20110;&#22823;&#32780;&#19981;&#26029;&#22686;&#38271;&#30340;&#39033;&#30446;&#35821;&#26009;&#24211;&#21644;&#32570;&#20047;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#38382;&#39064;&#21152;&#21095;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#20559;&#22909;&#29702;&#35299;&#12289;&#28789;&#27963;&#30340;&#23545;&#35805;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#20316;&#20026;&#25972;&#20010;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#30340;&#26032;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#23383;&#20856;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.06575</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23383;&#20856;&#38142;&#25552;&#31034;&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06575
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#23383;&#20856;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#20063;&#33021;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#32763;&#35793;&#31232;&#26377;&#35789;&#27719;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#65292;&#24456;&#38590;&#26816;&#32034;&#21040;&#30456;&#20851;&#31034;&#33539;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#38480;&#21046;&#20102;LLMs&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#8212;&#8212;&#25105;&#20204;&#35813;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;CoD&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23383;&#20856;&#38142;&#20026;&#19968;&#37096;&#20998;&#36755;&#20837;&#21333;&#35789;&#22686;&#21152;LLMs&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#20174;&#32780;&#20419;&#36827;LLMs&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;FLORES-200&#20840;&#24320;&#21457;&#27979;&#35797;&#38598;&#19978;&#65292;&#36890;&#36807;&#23558;CoD&#21644;ChatGPT&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;13&#20493;&#30340;MNMT ChrF++&#20998;&#25968;&#30340;&#25910;&#30410;&#65288;&#33521;&#35821;&#21040;&#22622;&#23572;&#32500;&#20122;&#35821;&#65292;&#35199;&#37324;&#23572;&#23383;&#27597;&#20070;&#20889;&#65292;ChrF ++&#20998;&#25968;&#20174;3.08&#22686;&#21152;&#21040;42.63&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#24182;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#20026;&#38889;&#35821;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.06335</link><description>&lt;p&gt;
K-UniMorph&#65306;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#21450;&#20854;&#29305;&#24449;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#24182;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#20026;&#38889;&#35821;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20043;&#21069;&#65292;&#38889;&#35821;&#22312;&#25968;&#30334;&#31181;&#22810;&#26679;&#30340;&#19990;&#30028;&#35821;&#35328;&#20013;&#30340;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20013;&#19968;&#30452;&#22788;&#20110;&#23569;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#30340;&#36890;&#29992;&#35789;&#24418;&#23398;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;K-UniMorph&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#27010;&#36848;&#20102;&#27599;&#20010;&#35821;&#27861;&#26631;&#20934;&#30340;&#21160;&#35789;&#32467;&#23614;&#65292;&#24182;&#38416;&#26126;&#22914;&#20309;&#25552;&#21462;&#21464;&#24418;&#24418;&#24335;&#20197;&#21450;&#22914;&#20309;&#29983;&#25104;&#35789;&#24418;&#27169;&#24335;&#12290;&#27492;&#25968;&#25454;&#38598;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#65288;2015&#65289;&#21644;Sylak-Glassman&#65288;2016&#65289;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#32780;&#25105;&#20204;&#20174;Sejong&#24418;&#24577;&#20998;&#26512;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21464;&#24418;&#24418;&#24335;&#65292;&#36825;&#26159;&#38889;&#35821;&#26368;&#22823;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;&#22312;&#25968;&#25454;&#21019;&#24314;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#35843;&#26597;&#20174;Sejong&#35821;&#26009;&#24211;&#20013;&#30340;&#36716;&#25442;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#21464;&#24418;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from Sylak-Glassman et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three di
&lt;/p&gt;</description></item><item><title>DAMO-NLP&#22242;&#38431;&#30340;U-RaNER&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#24102;&#26377;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#23384;&#22312;&#30340;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#21644;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03688</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#20013;&#30340;DAMO-NLP: &#19968;&#31181;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32479;&#19968;&#26816;&#32034;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03688
&lt;/p&gt;
&lt;p&gt;
DAMO-NLP&#22242;&#38431;&#30340;U-RaNER&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#24102;&#26377;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#23384;&#22312;&#30340;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#21644;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MultiCoNER 2&#20849;&#20139;&#20219;&#21153;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32454;&#31890;&#24230;&#21644;&#22024;&#26434;&#24773;&#20917;&#65292;&#24182;&#32487;&#25215;&#20102;MultiCoNER 1&#20219;&#21153;&#30340;&#35821;&#20041;&#27495;&#20041;&#21644;&#20302;&#19978;&#19979;&#25991;&#29615;&#22659;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;MultiCoNER 1&#20013;&#30340;&#21069;&#20960;&#20010;&#39030;&#23574;&#31995;&#32479;&#35201;&#20040;&#32435;&#20837;&#30693;&#35782;&#24211;&#25110;&#19987;&#26377;&#21517;&#35789;&#34920;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#20197;&#21450;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;DAMO-NLP&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32479;&#19968;&#26816;&#32034;&#22686;&#24378;&#31995;&#32479;&#65288;U-RaNER&#65289;&#12290;&#25105;&#20204;&#23545;&#19978;&#36848;&#20960;&#20010;&#39030;&#23574;&#31995;&#32479;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#29942;&#39048;&#22312;&#20110;&#30693;&#35782;&#19981;&#36275;&#65292;&#32780;&#19988;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#20351;&#24471;&#26816;&#32034;&#30693;&#35782;&#23545;&#27169;&#22411;&#19981;&#21487;&#35265;&#12290;&#20026;&#20102;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#65292;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#25299;&#23485;&#19978;&#19979;&#25991;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To cope with these problems, the previous top systems in the MultiCoNER \RNum{1} either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#21457;&#29616;&#22312;&#25112;&#20105;&#29190;&#21457;&#21069;&#24050;&#32463;&#20986;&#29616;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#36716;&#21464;&#30340;&#36235;&#21183;&#65292;&#32780;&#25112;&#20105;&#29190;&#21457;&#21518;&#36825;&#31181;&#36235;&#21183;&#21152;&#36895;&#20102;&#65292;&#24182;&#19988;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;</title><link>http://arxiv.org/abs/2305.02770</link><description>&lt;p&gt;
&#35821;&#35328;&#36873;&#25321;&#30340;&#25919;&#27835;&#65306;&#20420;&#20044;&#25112;&#20105;&#22914;&#20309;&#24433;&#21709;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Politics of Language Choice: How the Russian-Ukrainian War Influences Ukrainians' Language Use on Twitter. (arXiv:2305.02770v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#21457;&#29616;&#22312;&#25112;&#20105;&#29190;&#21457;&#21069;&#24050;&#32463;&#20986;&#29616;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#36716;&#21464;&#30340;&#36235;&#21183;&#65292;&#32780;&#25112;&#20105;&#29190;&#21457;&#21518;&#36825;&#31181;&#36235;&#21183;&#21152;&#36895;&#20102;&#65292;&#24182;&#19988;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20351;&#29992;&#22825;&#29983;&#26159;&#25919;&#27835;&#30340;&#65292;&#24182;&#32463;&#24120;&#29992;&#20316;&#25991;&#21270;&#36523;&#20221;&#30340;&#36733;&#20307;&#65292;&#21516;&#26102;&#20063;&#26159;&#22269;&#23478;&#24314;&#35774;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#65288;2020&#24180;1&#26376;&#33267;2022&#24180;10&#26376;&#65289;&#65292;&#22522;&#20110;&#36229;&#36807;62,000&#20301;&#29992;&#25143;&#21457;&#24067;&#30340;400&#19975;&#26465;&#22320;&#29702;&#26631;&#35760;&#25512;&#25991;&#20013;&#65292;&#20044;&#20811;&#20848;&#20844;&#27665;&#30340;&#35821;&#35328;&#36873;&#25321;&#21644;&#25512;&#25991;&#27963;&#21160;&#12290;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#65292;&#21306;&#20998;&#20102;Twitter&#19978;&#29992;&#25143;&#30340;&#27969;&#20837;&#27969;&#20986;&#25152;&#24341;&#36215;&#30340;&#26679;&#26412;&#25928;&#24212;&#21644;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#25152;&#24341;&#36215;&#30340;&#34892;&#20026;&#25928;&#24212;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#25112;&#20105;&#29190;&#21457;&#20043;&#21069;&#24050;&#32463;&#26377;&#19968;&#20010;&#31283;&#23450;&#30340;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#30340;&#36716;&#21464;&#65292;&#32780;&#36825;&#19968;&#36807;&#31243;&#22312;&#25112;&#20105;&#29190;&#21457;&#21518;&#36805;&#36895;&#21152;&#36895;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21464;&#21270;&#20027;&#35201;&#24402;&#22240;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#25913;&#21464;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#20250;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of language is innately political and often a vehicle of cultural identity as well as the basis for nation building. Here, we examine language choice and tweeting activity of Ukrainian citizens based on more than 4 million geo-tagged tweets from over 62,000 users before and during the Russian-Ukrainian War, from January 2020 to October 2022. Using statistical models, we disentangle sample effects, arising from the in- and outflux of users on Twitter, from behavioural effects, arising from behavioural changes of the users. We observe a steady shift from the Russian language towards the Ukrainian language already before the war, which drastically speeds up with its outbreak. We attribute these shifts in large part to users' behavioural changes. Notably, we find that many Russian-tweeting users perform a hard-switch to Ukrainian as a result of the war.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02437</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20256;&#32479;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#36845;&#20195;&#20154;&#31867;&#32534;&#20889;&#30340;&#21442;&#32771;&#24211;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20986;&#30456;&#24212;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#25991;&#26412;&#12290;&#20294;&#24403;&#21069;&#25991;&#29486;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#36136;&#37327;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#35760;&#24518;&#22686;&#24378;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Selfmem&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#37319;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22120;&#33258;&#36523;&#20197;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#33258;&#25105;&#35760;&#24518;&#27744;&#65292;&#24182;&#20351;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#20026;&#19979;&#19968;&#36718;&#29983;&#25104;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#30340;&#35760;&#24518;&#12290;&#30456;&#32467;&#21512;&#65292;&#36825;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#25552;&#20986;&#20102;&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01711</link><description>&lt;p&gt;
&#19981;&#20572;&#27490;&#39044;&#35757;&#32451;&#65311;&#35753;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26356;&#21152;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;NLP&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;LM&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#24615;&#33021;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#22312;&#21322;&#30417;&#30563;&#21644;&#20840;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#20843;&#20010;&#21333;&#21477;&#20219;&#21153;&#21644;&#20843;&#20010;&#21477;&#23545;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#23545;&#21477;&#23545;&#20219;&#21153;&#25110;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#24335;&#26102;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65288;PCP&#65289;&#65292;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#24605;&#24819;&#19982;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#30446;&#26631;&#20043;&#21069;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;FT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the targ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37325;&#25490;&#30340;&#37327;&#21270;&#26041;&#27861;RPTQ&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37327;&#21270;&#26102;&#30001;&#20110;&#20449;&#36947;&#28608;&#27963;&#33539;&#22260;&#19981;&#21516;&#32780;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#23454;&#29616;&#35813;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#23558;LLL&#27169;&#22411;&#25512;&#21160;&#21040;3&#20301;&#28608;&#27963;&#12290;</title><link>http://arxiv.org/abs/2304.01089</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#25490;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RPTQ: Reorder-based Post-training Quantization for Large Language Models. (arXiv:2304.01089v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37325;&#25490;&#30340;&#37327;&#21270;&#26041;&#27861;RPTQ&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37327;&#21270;&#26102;&#30001;&#20110;&#20449;&#36947;&#28608;&#27963;&#33539;&#22260;&#19981;&#21516;&#32780;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#23454;&#29616;&#35813;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#23558;LLL&#27169;&#22411;&#25512;&#21160;&#21040;3&#20301;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#32780;&#24341;&#21457;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;LLL&#27169;&#22411;&#37327;&#21270;&#30340;&#20027;&#35201;&#38590;&#28857;&#22312;&#20110;&#20449;&#36947;&#20043;&#38388;&#19981;&#21516;&#30340;&#28608;&#27963;&#33539;&#22260;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31163;&#32676;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#25490;&#30340;&#37327;&#21270;&#26041;&#27861;RPTQ&#65292;&#29992;&#20110;&#35299;&#20915;LLL&#27169;&#22411;&#37327;&#21270;&#38382;&#39064;&#12290;RPTQ&#36890;&#36807;&#37325;&#26032;&#25490;&#21015;&#28608;&#27963;&#20013;&#30340;&#20449;&#36947;&#65292;&#24182;&#25353;&#31751;&#37327;&#21270;&#20449;&#36947;&#65292;&#20174;&#32780;&#20943;&#23569;&#20449;&#36947;&#33539;&#22260;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#37325;&#25490;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#23454;&#29616;&#20102;&#35813;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;LLL&#27169;&#22411;&#25512;&#21160;&#21040;3&#20301;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models (LLMs) have demonstrated outstanding performance on various tasks, but their deployment poses challenges due to their enormous model size. In this paper, we identify that the main challenge in quantizing LLMs stems from the different activation ranges between the channels, rather than just the issue of outliers.We propose a novel reorder-based quantization approach, RPTQ, that addresses the issue of quantizing the activations of LLMs. RPTQ rearranges the channels in the activations and then quantizing them in clusters, thereby reducing the impact of range difference of channels. In addition, we reduce the storage and computation overhead by avoiding explicit reordering. By implementing this approach, we achieved a significant breakthrough by pushing LLM models to 3 bit activation for the first time.
&lt;/p&gt;</description></item><item><title>UKP-SQuARE v3&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;QA&#30740;&#31350;&#24179;&#21488;&#65292;&#19982;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#30456;&#27604;&#65292;&#32467;&#21512;&#19987;&#23478;&#26234;&#33021;&#20307;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.18120</link><description>&lt;p&gt;
UKP-SQuARE v3&#65306;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;QA&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
UKP-SQuARE v3: A Platform for Multi-Agent QA Research. (arXiv:2303.18120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18120
&lt;/p&gt;
&lt;p&gt;
UKP-SQuARE v3&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;QA&#30740;&#31350;&#24179;&#21488;&#65292;&#19982;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#30456;&#27604;&#65292;&#32467;&#21512;&#19987;&#23478;&#26234;&#33021;&#20307;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#21457;&#23637;&#24050;&#24341;&#36215;&#30740;&#31350;&#30028;&#23545;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#20851;&#27880;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#30340;&#35268;&#24459;&#24182;&#38450;&#27490;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#36807;&#24230;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#32447;&#20195;&#30721;&#24211;&#65288;&#22914;GitHub&#25110;Hugging Face&#65289;&#20013;QA&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#27491;&#22312;&#21464;&#24471;&#21487;&#34892;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#19987;&#23478;&#26234;&#33021;&#20307;&#21487;&#20197;&#27604;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#33719;&#24471;&#26356;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#20102;&#26041;&#20415;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;UKP-SQuARE&#25193;&#23637;&#20026;&#25903;&#25345;&#19977;&#31181;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;i&#65289;&#26234;&#33021;&#20307;&#36873;&#25321;&#65292;ii&#65289;&#26234;&#33021;&#20307;&#30340;&#26089;&#26399;&#34701;&#21512;&#65292;&#20197;&#21450;iii&#65289;&#26234;&#33021;&#20307;&#30340;&#21518;&#26399;&#34701;&#21512;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#26029;&#36895;&#24230;&#65292;&#24182;&#19982;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#19982;&#36895;&#24230;&#26435;&#34913;&#30340;&#35752;&#35770;&#12290;UKP-SQuARE&#26159;&#24320;&#28304;&#30340;&#65292;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous development of Question Answering (QA) datasets has drawn the research community's attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert agents can yield large performance gains over multi-dataset models. To ease research in multi-agent models, we extend UKP-SQuARE, an online platform for QA research, to support three families of multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii) late-fusion of agents. We conduct experiments to evaluate their inference speed and discuss the performance vs. speed trade-off compared to multi-dataset models. UKP-SQuARE is open-source and publicly available at this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2303.12112</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#20687;&#35270;&#39057;&#26631;&#39064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;CLIP&#27169;&#22411;&#22312;&#24456;&#22810;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#32467;&#26500;&#20013;&#29983;&#25104;&#30340;&#26631;&#39064;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#37197;&#26041;&#65292;&#21363;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#24230;&#23398;&#20064;&#20998;&#25968;&#65288;PAC-S&#65289;&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#23545;&#27604;&#24230;&#35270;&#35273;-&#35821;&#20041;&#31354;&#38388;&#30340;&#23398;&#20064;&#21644;&#31574;&#23637;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#28155;&#21152;&#12290;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25351;&#26631;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20248;&#20110;&#29616;&#26377;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CIDEr&#21644;SPICE&#65289;&#21644;&#26080;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CLIP-Score&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#22270;&#20687;&#26631;&#39064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#19981;&#21516;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11719</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#32435;&#20837;&#25991;&#26723;&#25688;&#35201;&#29983;&#25104;&#20013;&#65306;&#22522;&#20110;GPT-2&#30340;&#21069;&#32512;&#35843;&#25972;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#25991;&#26723;&#25688;&#35201;&#25216;&#26415;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#26159;&#29983;&#25104;&#30340;&#25688;&#35201;&#21644;&#21407;&#22987;&#25991;&#26412;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#20173;&#28982;&#26102;&#26377;&#21457;&#29983;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#25552;&#31034;&#26469;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#21069;&#32512;&#35843;&#25972;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#21069;&#32512;&#25552;&#31034;&#21644;&#31163;&#25955;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35757;&#32451;&#30340;&#21069;&#32512;&#21487;&#20197;&#24110;&#21161;&#25688;&#35201;&#27169;&#22411;&#20934;&#30830;&#22320;&#20174;&#31163;&#25955;&#25552;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#22312;&#20107;&#23454;&#19978;&#19982;&#31163;&#25955;&#25552;&#31034;&#19968;&#33268;&#12290;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;ROUGE&#25913;&#36827;&#34920;&#26126;&#65292;&#23558;&#20107;&#23454;&#30693;&#35782;&#26126;&#30830;&#22320;&#28155;&#21152;&#21040;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26415;&#35821;&#20197;&#21450;&#19968;&#20010;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20803;&#32032;&#30340;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#24615;&#22320;&#25429;&#25417;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#19979;&#30340;&#25991;&#26412;&#27010;&#24565;&#24046;&#24322;&#12290;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;NLP&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05604</link><description>&lt;p&gt;
&#21253;&#23481;&#24615;&#25991;&#26412;&#27010;&#24565;&#35770;
&lt;/p&gt;
&lt;p&gt;
An Inclusive Notion of Text. (arXiv:2211.05604v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05604
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26415;&#35821;&#20197;&#21450;&#19968;&#20010;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20803;&#32032;&#30340;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#24615;&#22320;&#25429;&#25417;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#19979;&#30340;&#25991;&#26412;&#27010;&#24565;&#24046;&#24322;&#12290;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;NLP&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#20070;&#38754;&#25991;&#26412;&#24320;&#21457;&#35821;&#27861;&#12289;&#24847;&#20041;&#21644;&#20132;&#27969;&#27169;&#22411;&#12290;&#30001;&#20110;&#20219;&#21153;&#21644;&#25968;&#25454;&#30340;&#24046;&#24322;&#65292;&#34987;&#35748;&#20026;&#26159;&#25991;&#26412;&#30340;&#20869;&#23481;&#22312;&#30740;&#31350;&#20013;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#25429;&#25417;&#36825;&#20123;&#24046;&#24322;&#30340;&#27010;&#24565;&#26694;&#26550;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#28548;&#28165;&#25991;&#26412;&#27010;&#24565;&#23545;&#20110;&#21487;&#37325;&#22797;&#21644;&#21487;&#25512;&#24191;&#30340;NLP&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#26415;&#35821;&#26469;&#35752;&#35770;&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#20135;&#21644;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20108;&#23618;&#27425;&#30340;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20803;&#32032;&#30340;&#20998;&#31867;&#27861;&#65292;&#36825;&#20123;&#20803;&#32032;&#22312;&#25991;&#26412;&#26469;&#28304;&#20013;&#21487;&#29992;&#20110;NLP&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#27492;&#20998;&#31867;&#27861;&#24212;&#29992;&#20110;&#35843;&#26597;&#23558;&#25991;&#26412;&#27010;&#24565;&#25193;&#23637;&#21040;&#20445;&#23432;&#30340;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#20197;&#22806;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#27491;&#22312;&#20852;&#36215;&#30340;NLP&#20013;&#21253;&#23481;&#24615;&#25991;&#26412;&#26041;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#21644;&#25361;&#25112;&#65292;&#24182;&#24314;&#35758;&#31038;&#21306;&#32423;&#21035;&#30340;&#25253;&#21578;&#20316;&#20026;&#24041;&#22266;&#35752;&#35770;&#30340;&#20851;&#38190;&#19979;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) researchers develop models of grammar, meaning and communication based on written text. Due to task and data differences, what is considered text can vary substantially across studies. A conceptual framework for systematically capturing these differences is lacking. We argue that clarity on the notion of text is crucial for reproducible and generalizable NLP. Towards that goal, we propose common terminology to discuss the production and transformation of textual data, and introduce a two-tier taxonomy of linguistic and non-linguistic elements that are available in textual sources and can be used in NLP modeling. We apply this taxonomy to survey existing work that extends the notion of text beyond the conservative language-centered view. We outline key desiderata and challenges of the emerging inclusive approach to text in NLP, and suggest community-level reporting as a crucial next step to consolidate the discussion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; KGLM &#26550;&#26500;&#65292;&#23558;&#26032;&#30340;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#23618;&#25972;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02744</link><description>&lt;p&gt;
KGLM: &#23558;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#25972;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; KGLM &#26550;&#26500;&#65292;&#23558;&#26032;&#30340;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#23618;&#25972;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#34920;&#31034;&#22797;&#26434;&#20851;&#31995;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#23384;&#22312;&#20449;&#24687;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#12290;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#25928;&#26524;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24573;&#30053;&#20102;&#30693;&#35782;&#22270;&#35889;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLM&#65288;Knowledge Graph Language Model&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#23618;&#65292;&#23427;&#23398;&#20064;&#21306;&#20998;&#19981;&#21516;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#36825;&#20123;&#39069;&#22806;&#23884;&#20837;&#23618;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#37319;&#29992;&#21518;&#32493;&#30340; link prediction &#20219;&#21153;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMP&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.06210</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMP&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#20110;&#21442;&#25968;&#21270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24191;&#27867;&#22320;&#20351;&#29992;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#30452;&#25509;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#26435;&#37325;&#12290;&#20808;&#21069;&#30340;&#19968;&#38454;&#26041;&#27861;&#25104;&#21151;&#22320;&#23558;PLMs&#21387;&#32553;&#21040;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#34920;&#29616;&#20960;&#20046;&#19981;&#19979;&#38477;&#65292;&#22914;&#36816;&#21160;&#21098;&#26525;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19968;&#38454;&#20449;&#24687;&#26469;&#21098;&#26525;PLMs&#65292;&#21516;&#26102;&#24494;&#35843;&#20854;&#20313;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#19968;&#38454;&#21098;&#26525;&#65292;&#24494;&#35843;&#26159;&#22810;&#20313;&#30340;&#65292;&#22240;&#20026;&#19968;&#38454;&#21098;&#26525;&#36275;&#20197;&#23558;PLMs&#25910;&#25947;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#36825;&#20010;&#21021;&#34935;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#20351;PLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#33945;&#29256;&#20989;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;SMP&#12290;&#22823;&#37327;&#21508;&#31181;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SMP&#27604;&#19968;&#38454;&#21644;&#38646;&#38454;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order met
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#27861;&#24459;&#39046;&#22495;&#35770;&#28857;&#25366;&#25496;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#28145;&#20837;&#27861;&#24459;&#35770;&#35777;&#30740;&#31350;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#26032;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#24182;&#32534;&#35793;&#20102;&#21253;&#21547;373&#39033;&#35009;&#20915;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2208.06178</link><description>&lt;p&gt;
&#25366;&#25496;&#27861;&#24237;&#35009;&#20915;&#20013;&#30340;&#27861;&#24459;&#35770;&#28857;
&lt;/p&gt;
&lt;p&gt;
Mining Legal Arguments in Court Decisions. (arXiv:2208.06178v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#27861;&#24459;&#39046;&#22495;&#35770;&#28857;&#25366;&#25496;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#28145;&#20837;&#27861;&#24459;&#35770;&#35777;&#30740;&#31350;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#26032;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#24182;&#32534;&#35793;&#20102;&#21253;&#21547;373&#39033;&#35009;&#20915;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#35770;&#28857;&#25366;&#25496;&#39046;&#22495;&#30340;&#20135;&#29983;&#20197;&#26469;&#65292;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#20998;&#26512;&#27861;&#24459;&#35805;&#35821;&#20013;&#30340;&#35770;&#28857;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#23545;&#27861;&#24237;&#20915;&#23450;&#20013;&#30340;&#35770;&#28857;&#36827;&#34892;&#24314;&#27169;&#21644;&#27880;&#37322;&#30340;&#26041;&#24335;&#19982;&#27861;&#24459;&#19987;&#23478;&#29702;&#35299;&#21644;&#20998;&#26512;&#27861;&#24459;&#35770;&#35777;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;&#34429;&#28982;&#35745;&#31639;&#26041;&#27861;&#36890;&#24120;&#23558;&#35770;&#28857;&#31616;&#21270;&#20026;&#36890;&#29992;&#21069;&#25552;&#21644;&#20027;&#24352;&#65292;&#20294;&#27861;&#24459;&#30740;&#31350;&#20013;&#30340;&#35770;&#28857;&#36890;&#24120;&#21576;&#29616;&#20986;&#20016;&#23500;&#30340;&#31867;&#22411;&#65292;&#36825;&#23545;&#20110;&#28145;&#20837;&#20102;&#35299;&#29305;&#23450;&#26696;&#20363;&#21644;&#27861;&#24459;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20570;&#20986;&#20102;&#19968;&#20123;&#23454;&#36136;&#24615;&#30340;&#36129;&#29486;&#65292;&#20197;&#25512;&#21160;&#35813;&#39046;&#22495;&#21521;&#21069;&#21457;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#28145;&#26893;&#26681;&#20110;&#27861;&#24459;&#35770;&#35777;&#30740;&#31350;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#27431;&#27954;&#20154;&#26435;&#35009;&#21028;&#25152;&#65288;ECHR&#65289;&#35785;&#35772;&#20013;&#27861;&#24459;&#35770;&#28857;&#30340;&#26032;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#32534;&#35793;&#21644;&#27880;&#37322;&#20102;&#19968;&#20010;&#21253;&#25324;373&#39033;&#35009;&#20915;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying, classifying, and analyzing arguments in legal discourse has been a prominent area of research since the inception of the argument mining field. However, there has been a major discrepancy between the way natural language processing (NLP) researchers model and annotate arguments in court decisions and the way legal experts understand and analyze legal argumentation. While computational approaches typically simplify arguments into generic premises and claims, arguments in legal research usually exhibit a rich typology that is important for gaining insights into the particular case and applications of law in general. We address this problem and make several substantial contributions to move the field forward. First, we design a new annotation scheme for legal arguments in proceedings of the European Court of Human Rights (ECHR) that is deeply rooted in the theory and practice of legal argumentation research. Second, we compile and annotate a large corpus of 373 court decision
&lt;/p&gt;</description></item></channel></rss>