<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01293</link><description>&lt;p&gt;
MLLMs&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can MLLMs Perform Text-to-Image In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#21040;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#25512;&#21160;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;ICL&#19978;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;ICL&#65288;T2I-ICL&#65289;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23569;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;T2I-ICL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CoBSAT&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#20219;&#21153;&#30340;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#65292;&#25105;&#20204;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01744</link><description>&lt;p&gt;
Octopus v2&#65306;&#29992;&#20110;&#36229;&#32423;&#20195;&#29702;&#30340;&#35774;&#22791;&#19978;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Octopus v2: On-device language model for super agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#36171;&#20104;&#36229;&#36234;GPT-4&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#24615;&#33021;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;95&#65285;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20989;&#25968;&#35843;&#29992;&#20013;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#19982;&#33258;&#21160;&#24037;&#20316;&#27969;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#35843;&#29992;&#20989;&#25968;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#21019;&#24314;AI&#20195;&#29702;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20113;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#30528;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#29992;&#20110;&#20989;&#25968;&#35843;&#29992;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#38754;&#20020;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#20855;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35774;&#22791;&#19978;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;GPT-4&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#32553;&#20943;&#20102;95%&#12290;&#19982;&#22522;&#20110;RAG&#30340;&#20989;&#25968;&#35843;&#29992;&#26426;&#21046;&#30340;Llama-7B&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;35&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;&#36866;&#21512;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#27700;&#24179;&#19978;&#65292;&#31526;&#21512;&#24615;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2404.00685</link><description>&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Properties of Speech Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#26088;&#22312;&#20174;&#21407;&#22987;&#38899;&#39057;&#20013;&#23398;&#20064;&#35821;&#35328;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36164;&#28304;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25105;&#20204;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24369;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#23545;&#35821;&#38899;&#27169;&#24577;&#25104;&#31435;&#65292;&#36825;&#20123;&#33021;&#21147;&#23558;&#38543;&#30528;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#26469;&#20272;&#35745;&#25105;&#20204;&#24403;&#21069;&#26041;&#27861;&#23558;&#20135;&#29983;&#20855;&#26377;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33521;&#35821;&#29087;&#32451;&#24230;&#30340;SLM&#30340;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#30340;&#21453;&#24212;&#65292;&#35774;&#35745;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;&#25552;&#31034;&#27169;&#26495;&#65292;&#25552;&#21319;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19346</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#27627;&#26080;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Unconscious of Unreasonability in Math Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#30340;&#21453;&#24212;&#65292;&#35774;&#35745;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;&#25552;&#31034;&#27169;&#26495;&#65292;&#25552;&#21319;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#32473;&#20986;&#21253;&#21547;&#19981;&#21512;&#29702;&#38169;&#35823;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#38754;&#23545;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#23427;&#20204;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;(UMP)&#22522;&#20934;&#26469;&#26816;&#26597;LLMs&#30340;&#38169;&#35823;&#26816;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#26816;&#27979;&#21040;&#19981;&#21512;&#29702;&#38169;&#35823;&#65292;&#20294;&#20173;&#28982;&#22312;&#29983;&#25104;&#38750;&#24187;&#35273;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#12290;&#20026;&#20102;&#25913;&#21892;&#23427;&#20204;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;(CCC)&#30340;&#25112;&#30053;&#25552;&#31034;&#27169;&#26495;&#12290;&#36890;&#36807;CCC&#65292;LLMs&#21487;&#20197;&#26356;&#22909;&#22320;&#33258;&#25105;&#35780;&#20272;&#24182;&#26816;&#27979;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#19981;&#21512;&#29702;&#38169;&#35823;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#26356;&#21487;&#38752;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19346v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs. Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.
&lt;/p&gt;</description></item><item><title>&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.16852</link><description>&lt;p&gt;
&#22312;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20013;&#36808;&#21521;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability in Legal Outcome Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16852
&lt;/p&gt;
&lt;p&gt;
&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411; - &#27861;&#24459;NLP&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998; - &#19981;&#33021;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#20154;&#31867;&#27861;&#24459;&#20027;&#20307;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#22312;&#26222;&#36890;&#27861;&#26696;&#20363;&#20013;&#65292;&#27861;&#24459;&#20174;&#19994;&#32773;&#36890;&#36807;&#21442;&#32771;&#34987;&#31216;&#20026;&#20808;&#20363;&#30340;&#36807;&#21435;&#26696;&#20363;&#27861;&#24459;&#25512;&#29702;&#21040;&#26696;&#20214;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20808;&#20363;&#22240;&#27492;&#25104;&#20026;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21046;&#23450;&#27861;&#24459;&#20808;&#20363;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#20154;&#31867;&#27861;&#23448;&#21644;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20182;&#20204;&#20381;&#36182;&#30340;&#19981;&#21516;&#31867;&#22411;&#20808;&#20363;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#27169;&#22411;&#23398;&#20250;&#20102;&#21512;&#29702;&#22320;&#39044;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20351;&#29992;&#30340;&#20808;&#20363;&#26041;&#24335;&#19981;&#21516;&#20110;&#20154;&#31867;&#27861;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15822</link><description>&lt;p&gt;
&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Computational Sentence-level Metrics Predicting Human Sentence Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#21333;&#35789;&#22788;&#29702;&#19978;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#12290;&#24320;&#21457;&#30340;&#24230;&#37327;&#21253;&#25324;&#21477;&#23376;&#24847;&#22806;&#24615;&#21644;&#21477;&#23376;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#32463;&#36807;&#27979;&#35797;&#21644;&#27604;&#36739;&#20197;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#36328;&#35821;&#35328;&#25972;&#20307;&#29702;&#35299;&#21477;&#23376;&#12290;&#36825;&#20123;&#24230;&#37327;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#35745;&#31639;&#30340;&#21477;&#23376;&#32423;&#24230;&#37327;&#22312;&#39044;&#27979;&#21644;&#38416;&#26126;&#35835;&#32773;&#22312;&#29702;&#35299;&#25972;&#20307;&#21477;&#23376;&#26102;&#36935;&#21040;&#30340;&#22788;&#29702;&#22256;&#38590;&#26041;&#38754;&#24322;&#24120;&#26377;&#25928;&#65292;&#21487;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12290;&#23427;&#20204;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20026;&#26410;&#26469;&#22312;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15822v1 Announce Type: new  Abstract: The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07175</link><description>&lt;p&gt;
&#37325;&#24314;ROME: &#35299;&#20915;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;Rank-One Model Editing (ROME)&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#19968;&#20123;&#20107;&#23454;&#34920;&#26126;&#35813;&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#32534;&#36753;&#32780;&#19981;&#30772;&#22351;&#27169;&#22411;&#12290;&#36825;&#20123;&#32534;&#36753;&#20197;&#21069;&#34987;&#31216;&#20026;&#31105;&#29992;&#32534;&#36753;&#12290;&#36825;&#20123;&#31105;&#29992;&#32534;&#36753;&#20250;&#23548;&#33268;&#31435;&#21363;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#38480;&#21046;&#20102;ROME&#29992;&#20110;&#39034;&#24207;&#32534;&#36753;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;CounterFact&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#36753;&#26102;&#65292;ROME&#20165;&#22312;&#27492;&#26102;&#21457;&#29983;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#22312;&#20351;&#29992;zsRE&#25968;&#25454;&#38598;&#26102;&#19981;&#20250;&#21457;&#29983;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#31105;&#29992;&#32534;&#36753;&#26159;ROME&#21407;&#22987;&#23454;&#29616;&#30340;&#20135;&#29289;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#31283;&#23450;&#30340;&#23454;&#29616;ROME&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;r-ROME&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#22312;&#20351;&#29992;ROME&#36827;&#34892;&#22823;&#35268;&#27169;&#39034;&#24207;&#32534;&#36753;&#26102;&#19981;&#20877;&#35266;&#23519;&#21040;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
&lt;/p&gt;</description></item><item><title>ICE-PIXIU&#27169;&#22411;&#23558;&#20013;&#25991;&#21644;&#33521;&#25991;&#37329;&#34701;&#20998;&#26512;&#32479;&#19968;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25552;&#21319;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06249</link><description>&lt;p&gt;
&#27809;&#26377;&#23396;&#23707;&#35821;&#35328;:&#32479;&#19968;&#20013;&#33521;&#25991;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#23548;&#25968;&#25454;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06249
&lt;/p&gt;
&lt;p&gt;
ICE-PIXIU&#27169;&#22411;&#23558;&#20013;&#25991;&#21644;&#33521;&#25991;&#37329;&#34701;&#20998;&#26512;&#32479;&#19968;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25552;&#21319;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#37329;&#34701;&#20998;&#26512;&#65292;&#20294;&#23427;&#20204;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#21333;&#19968;&#35821;&#35328;&#39046;&#22495;&#65292;&#26410;&#20805;&#20998;&#24320;&#21457;&#20013;&#33521;&#25991;&#21452;&#35821;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837; ICE-PIXIU&#65292;&#26080;&#32541;&#34701;&#21512; ICE-INTENT &#27169;&#22411;&#21644; ICE-FLARE &#21452;&#35821;&#37329;&#34701;&#20998;&#26512;&#22522;&#20934;&#12290;ICE-PIXIU &#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#20013;&#25991;&#20219;&#21153;&#65292;&#20197;&#21450;&#32763;&#35793;&#21644;&#21407;&#22987;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#12290;&#23427;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#26080;&#38480;&#35775;&#38382;&#26435;&#38480;&#65292;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#36328;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#22823;&#37327;&#32534;&#35793;&#65292;&#20197;&#21450;&#19968;&#20010;&#20855;&#26377;&#19987;&#23478;&#27880;&#37322;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324; 10 &#20010; NLP &#20219;&#21153;&#65292;20 &#20010;&#21452;&#35821;&#19987;&#29992;&#20219;&#21153;&#65292;&#20849;&#35745;1,185k &#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24443;&#24213;&#35780;&#20272;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#21452;&#35821;&#25968;&#25454;&#38598;&#32435;&#20837;&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#22312;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06249v1 Announce Type: cross  Abstract: While the progression of Large Language Models (LLMs) has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and multi-modal instruction data, and an evaluation benchmark with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06149</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#21160;&#35780;&#20998;&#20889;&#20316;&#25991;&#31456;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Automatically Score Proficiency of Written Essays?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#36807;&#21435;50&#24180;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#35780;&#20998;&#20316;&#25991;&#65288;AES&#65289;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25928;&#26524;&#26041;&#38754;&#20173;&#26377;&#35768;&#22810;&#19981;&#36275;&#20043;&#22788;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#37492;&#20110;&#23427;&#20204;&#24378;&#22823;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#26469;&#20998;&#26512;&#21644;&#26377;&#25928;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;ChatGPT&#21644;Llama&#12290;&#25105;&#20204;&#26088;&#22312;&#26816;&#26597;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#21363;&#22312;&#25972;&#20307;&#19978;&#21644;&#22312;&#20010;&#20307;&#20889;&#20316;&#29305;&#24449;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#35774;&#35745;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#30340;&#26368;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25968;&#25454;&#38598;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#65292;&#21033;&#29992;&#36866;&#21512;&#27604;&#36739;&#24230;&#37327;&#38598;&#21512;&#30340;Jaccard&#25351;&#25968;&#30340;&#29256;&#26412;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#20419;&#36827;&#38271;&#26399;&#20869;&#30340;&#22810;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03909</link><description>&lt;p&gt;
&#29992;&#20110;&#36879;&#26126;&#27604;&#36739;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25968;&#25454;&#38598;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#65292;&#21033;&#29992;&#36866;&#21512;&#27604;&#36739;&#24230;&#37327;&#38598;&#21512;&#30340;Jaccard&#25351;&#25968;&#30340;&#29256;&#26412;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#20419;&#36827;&#38271;&#26399;&#20869;&#30340;&#22810;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22411;&#23398;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#21019;&#24314;&#26469;&#36861;&#36394;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#36890;&#24120;&#34987;&#27979;&#37327;&#20026;&#26679;&#26412;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#25110;&#35821;&#35328;&#23478;&#26063;&#30340;&#25968;&#37327;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#24182;&#26410;&#32771;&#34385;&#25152;&#21253;&#21547;&#35821;&#35328;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#19982;&#21442;&#32771;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#35780;&#20272;&#65292;&#20316;&#20026;&#26368;&#22823;&#31243;&#24230;&#22320;&#20419;&#36827;&#38271;&#26399;&#20869;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#34920;&#31034;&#20026;&#29305;&#24449;&#38598;&#65292;&#24182;&#24212;&#29992;&#36866;&#29992;&#20110;&#27604;&#36739;&#24230;&#37327;&#38598;&#21512;&#30340;Jaccard&#25351;&#25968;&#30340;&#29256;&#26412;&#12290;&#38500;&#20102;&#20174;&#31867;&#22411;&#23398;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#31181;&#33258;&#21160;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#20811;&#26381;&#25163;&#21160;&#25910;&#38598;&#29305;&#24449;&#20013;&#24050;&#30693;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#30340;&#22810;&#26679;&#24615;&#35780;&#20998;&#22312;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#35782;&#21035;&#35821;&#35328;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03909v1 Announce Type: new  Abstract: Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;</title><link>https://arxiv.org/abs/2403.01373</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#65306;&#19968;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#30456;&#20851;&#25361;&#25112;&#26041;&#38754;&#30340;&#26174;&#33879;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#24187;&#35273;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#24187;&#35273;&#65292;&#31216;&#20026;&#25968;&#23383;&#24187;&#35273;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#35782;&#21035;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#25968;&#23383;&#24187;&#35273;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#22312;&#20027;&#27969;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#20013;&#30340;&#26126;&#26174;&#26222;&#36941;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#30456;&#20851;&#35270;&#35282;&#28145;&#20837;&#20998;&#26512;&#20102;&#25968;&#23383;&#24187;&#35273;&#65292;&#32771;&#23519;&#20102;&#20869;&#22312;&#21644;&#22806;&#22312;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26159;&#25968;&#23383;&#24187;&#35273;&#30340;&#19968;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#20316;&#20026;&#20943;&#36731;&#27492;&#31867;&#24187;&#35273;&#30340;&#25163;&#27573;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;8%&#30340;&#24179;&#22343;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01373v1 Announce Type: new  Abstract: Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination, which achieves an average improvement of 8\%
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cause and Effect: Can Large Language Models Truly Understand Causality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#35299;&#35835;&#21644;&#35299;&#37322;&#35821;&#35328;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#28982;&#32780;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20855;&#26377;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#22686;&#24378;&#65288;CARE CA&#65289;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22240;&#26524;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558; ConceptNet &#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#20013;&#30340;&#26126;&#30830;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#20197;&#21450;&#36890;&#36807;LLMs&#36827;&#34892;&#30340;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#19968;&#23618;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#19968;&#27493;&#31361;&#20986;LLMs&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;ConceptNet &#20013;&#30340;&#30693;&#35782;&#25552;&#39640;&#20102;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#65292;&#24182;&#25581;&#31034;&#20102;&#20215;&#20540;&#23545;&#40784;&#33021;&#21147;&#21487;&#20197;&#34987;&#36328;&#35821;&#35328;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18120</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65306;&#20215;&#20540;&#35266;&#40784;&#25972;&#24615;&#12289;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#21644;&#21487;&#25511;&#24615;&#26159;&#21542;&#20855;&#26377;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#65292;&#24182;&#25581;&#31034;&#20102;&#20215;&#20540;&#23545;&#40784;&#33021;&#21147;&#21487;&#20197;&#34987;&#36328;&#35821;&#35328;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#22312;&#34920;&#31034;&#24037;&#31243;&#39046;&#22495;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#20854;&#34920;&#31034;&#31354;&#38388;&#20013;&#32534;&#30721;&#27010;&#24565;&#65292;&#20027;&#35201;&#22260;&#32469;&#33521;&#35821;&#23637;&#24320;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#29702;&#24565;&#25193;&#23637;&#21040;&#22810;&#35821;&#22659;&#22330;&#26223;&#65292;&#28145;&#20837;&#25506;&#35752;LLMs&#20013;&#30340;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#27010;&#24565;&#12290;&#36890;&#36807;&#25105;&#20204;&#23545;7&#31181;&#20154;&#31867;&#20215;&#20540;&#12289;16&#31181;&#35821;&#35328;&#20197;&#21450;3&#20010;&#20855;&#26377;&#26126;&#26174;&#22810;&#35821;&#29305;&#24615;&#30340;LLM&#31995;&#21015;&#36827;&#34892;&#30340;&#20840;&#38754;&#25506;&#32034;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#35777;&#23454;&#20102;LLMs&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#12290;&#23545;&#36825;&#20123;&#27010;&#24565;&#30340;&#36328;&#35821;&#35328;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#24046;&#24322;&#32780;&#20135;&#29983;&#30340;3&#20010;&#29305;&#24449;&#65306;&#36328;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#12289;&#25197;&#26354;&#30340;&#35821;&#35328;&#20851;&#31995;&#20197;&#21450;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#22312;&#20154;&#31867;&#20215;&#20540;&#27010;&#24565;&#26041;&#38754;&#30340;&#21333;&#21521;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#21033;&#29992;&#20027;&#23548;&#35821;&#35328;&#20316;&#20026;&#20449;&#24687;&#28304;&#23454;&#29616;&#23545;LLMs&#20215;&#20540;&#35266;&#40784;&#25972;&#24615;&#30340;&#36328;&#35821;&#35328;&#25511;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18120v1 Announce Type: new  Abstract: Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source 
&lt;/p&gt;</description></item><item><title>LLM&#20351;&#29992;&#22312;&#21360;&#23612;&#35821;&#26041;&#38754;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#24061;&#20182;&#35821;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.17302</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#29983;&#25104;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#24120;&#35782;&#24615;&#38382;&#31572;&#25968;&#25454;&#65311;&#21360;&#23612;&#21644;&#24061;&#20182;&#35821;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17302
&lt;/p&gt;
&lt;p&gt;
LLM&#20351;&#29992;&#22312;&#21360;&#23612;&#35821;&#26041;&#38754;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#24061;&#20182;&#35821;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#34701;&#20837;&#35821;&#35328;&#20013;&#30693;&#35782;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#30340;&#39640;&#36136;&#37327;&#38382;&#31572;(QA)&#25968;&#25454;&#38598;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21360;&#23612;&#35821;&#21644;&#24061;&#20182;&#35821;&#25991;&#21270;&#30456;&#20851;&#24120;&#35782;&#24615;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#25324;LLMs&#21644;&#20154;&#31867;&#26631;&#27880;&#32773;&#22312;&#20869;&#30340;&#21508;&#31181;&#26041;&#27861;&#20026;&#36825;&#20123;&#35821;&#35328;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30446;&#21069;&#24615;&#33021;&#26368;&#20339;&#30340;LLM&#65292;GPT-4 Turbo&#65292;&#33021;&#22815;&#29983;&#25104;&#21360;&#23612;&#35821;&#20013;&#20855;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#24061;&#20182;&#35821;&#20013;&#21364;&#19981;&#34892;&#65292;&#31361;&#20986;&#20102;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17302v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators. Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also benchmark various LLMs on our generated datasets and find that they perform better on 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.15861</link><description>&lt;p&gt;
MATHWELL: &#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#25945;&#32946;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
MATHWELL: Generating Educational Math Word Problems at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15861
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#39064;&#22312;K-8&#25945;&#32946;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#32534;&#20889;&#23427;&#20204;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35268;&#27169;&#21270;&#38382;&#39064;&#26469;&#25903;&#25345;K-8&#25968;&#23398;&#25945;&#32946;&#12290;&#20026;&#20102;&#25945;&#32946;&#24615;&#65292;&#29983;&#25104;&#30340;&#38382;&#39064;&#24517;&#39035;&#26159;1&#65289;&#21487;&#35299;&#20915;&#30340;&#65292;2&#65289;&#20934;&#30830;&#30340;&#65292;3&#65289;&#36866;&#24403;&#30340;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#26410;&#26631;&#35760;&#36825;&#20123;&#26631;&#20934;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#35757;&#32451;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MATHWELL&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#30340;70B Llama-2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;K-8&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#20511;&#21161;MATHWELL&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#12290;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#30340;3,484&#20010;&#38382;&#39064;&#21457;&#29616;&#65292;MATHWELL&#25317;&#26377;&#27604;&#20854;&#20182;&#36873;&#25321;&#26356;&#39640;&#30340;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#21644;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#30340;&#38382;&#39064;&#20221;&#39069;&#39640;&#20986;40&#65285;&#65292;&#20854;&#20013;74&#65285;&#30340;&#38382;&#39064;&#20855;&#26377;&#21487;&#35299;&#30340;&#12289;&#20934;&#30830;&#30340;&#21644;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
&lt;/p&gt;</description></item><item><title>APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;</title><link>https://arxiv.org/abs/2402.14866</link><description>&lt;p&gt;
APTQ: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14866
&lt;/p&gt;
&lt;p&gt;
APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#36127;&#36733;&#21644;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#23545;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;APTQ&#65288;Attention-aware Post-Training Mixed-Precision Quantization&#65289;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#32780;&#19988;&#39318;&#27425;&#32771;&#34385;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;&#23545;&#25972;&#20010;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;Hessian&#36857;&#20316;&#20026;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25935;&#24863;&#24230;&#24230;&#37327;&#65292;&#30830;&#20445;&#32463;&#36807;&#29702;&#24615;&#30340;&#31934;&#24230;&#38477;&#20302;&#33021;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;APTQ&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;C4&#25968;&#25454;&#38598;&#20013;&#20197;&#24179;&#22343;4&#20301;&#23485;&#24230;&#33719;&#24471;5.22&#22256;&#24785;&#24230;&#65292;&#20960;&#20046;&#31561;&#25928;&#20110;&#20840;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;APTQ&#22312;LLaMa-7B&#21644;LLaMa-1&#20013;&#20197;&#24179;&#22343;3.8&#20301;&#23485;&#24230;&#36798;&#21040;&#20102;68.24&#65285;&#21644;70.48&#65285;&#30340;&#26368;&#20808;&#36827;&#38646;-shot&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#36127;&#38754;&#31034;&#20363;&#21644;&#36866;&#24403;&#30340;&#25968;&#25454;&#28165;&#27927;&#19982;&#24494;&#35843;&#31574;&#30053;&#65292;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20316;&#20026;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11651</link><description>&lt;p&gt;
&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65306;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#25972;&#21512;&#36127;&#38754;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11651
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#36127;&#38754;&#31034;&#20363;&#21644;&#36866;&#24403;&#30340;&#25968;&#25454;&#28165;&#27927;&#19982;&#24494;&#35843;&#31574;&#30053;&#65292;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20316;&#20026;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20805;&#24403;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#24037;&#20855;&#65288;&#22914;&#25628;&#32034;&#24341;&#25806;&#65289;&#26102;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35757;&#32451;&#25110;&#23545;&#40784;&#36807;&#31243;&#20013;&#24182;&#26410;&#19987;&#38376;&#38024;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#20248;&#21270;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25910;&#38598;&#20102;GPT-4&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#36712;&#36857;&#65292;&#24182;&#29992;&#23427;&#20204;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20316;&#20026;&#36825;&#19968;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26631;&#20934;&#26041;&#27861;&#36890;&#24120;&#26159;&#31616;&#21333;&#22320;&#20002;&#24323;&#26410;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#30340;&#36712;&#36857;&#65292;&#36825;&#19968;&#26041;&#38754;&#23548;&#33268;&#20102;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#26174;&#33879;&#28010;&#36153;&#65292;&#21478;&#19968;&#26041;&#38754;&#26377;&#21487;&#33021;&#38480;&#21046;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#36335;&#24452;&#12290;&#26412;&#25991;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#24494;&#35843;&#31574;&#30053;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21644;&#25112;&#30053;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11651v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11512</link><description>&lt;p&gt;
&#20174;&#20559;&#35265;&#21040;&#24179;&#31561;&#65306;&#21435;&#20559;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#35789;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#23427;&#20204;&#26159;&#36825;&#20123;&#27169;&#22411;&#25226;&#25569;&#19978;&#19979;&#25991;&#20851;&#31995;&#12289;&#20419;&#36827;&#26356;&#32454;&#33268;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#22312;&#35768;&#22810;&#38656;&#35201;&#23545;&#20154;&#31867;&#35821;&#35328;&#26377;&#22522;&#26412;&#29702;&#35299;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#22522;&#30707;&#12290;&#37492;&#20110;&#36825;&#20123;&#23884;&#20837;&#24448;&#24448;&#33258;&#36523;&#21453;&#26144;&#25110;&#23637;&#31034;&#20559;&#35265;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20063;&#20250;&#26080;&#24847;&#20013;&#23398;&#20064;&#36825;&#31181;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#24320;&#21019;&#24615;&#21069;&#20154;&#30740;&#31350;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;DeepSoftDebias&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#8220;&#36719;&#21435;&#20559;&#8221;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31867;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#20855;&#26377;&#25361;&#25112;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;DeepSoftDebias&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;&#20849;&#24773;&#35780;&#20272;&#26694;&#26550;&#65292;&#21516;&#26102;&#34913;&#37327;&#20102;&#35828;&#35805;&#32773;&#35282;&#24230;&#30340;&#34920;&#36798;&#24847;&#22270;&#21644;&#21548;&#32773;&#35282;&#24230;&#30340;&#24863;&#30693;&#20849;&#24773;&#65292;&#30740;&#31350;&#21457;&#29616;&#20108;&#32773;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#24863;&#30693;&#20849;&#24773;&#19982;&#23545;&#35805;&#20250;&#35805;&#30340;&#28385;&#24847;&#27700;&#24179;&#21576;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11409</link><description>&lt;p&gt;
&#22810;&#32500;&#24230;&#35780;&#20272;&#20849;&#24773;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional Evaluation of Empathetic Dialog Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;&#20849;&#24773;&#35780;&#20272;&#26694;&#26550;&#65292;&#21516;&#26102;&#34913;&#37327;&#20102;&#35828;&#35805;&#32773;&#35282;&#24230;&#30340;&#34920;&#36798;&#24847;&#22270;&#21644;&#21548;&#32773;&#35282;&#24230;&#30340;&#24863;&#30693;&#20849;&#24773;&#65292;&#30740;&#31350;&#21457;&#29616;&#20108;&#32773;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#24863;&#30693;&#20849;&#24773;&#19982;&#23545;&#35805;&#20250;&#35805;&#30340;&#28385;&#24847;&#27700;&#24179;&#21576;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#26377;&#25928;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#23545;&#35805;&#27807;&#36890;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#28982;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#34913;&#37327;&#34920;&#36798;&#30340;&#27807;&#36890;&#24847;&#22270;&#19978;&#8212;&#8212;&#21363;&#20849;&#24773;&#26159;&#22914;&#20309;&#34920;&#36798;&#30340;&#65292;&#24573;&#30053;&#20102;&#23545;&#35805;&#20063;&#26159;&#19968;&#31181;&#28041;&#21450;&#35828;&#35805;&#32773;&#21644;&#32838;&#21548;&#32773;&#30340;&#21327;&#20316;&#23454;&#36341;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;&#20849;&#24773;&#35780;&#20272;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#20197;&#34913;&#37327;&#20174;&#35828;&#35805;&#32773;&#35282;&#24230;&#34920;&#36798;&#30340;&#24847;&#22270;&#20197;&#21450;&#20174;&#21548;&#32773;&#35282;&#24230;&#24863;&#30693;&#21040;&#30340;&#20849;&#24773;&#12290;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#20998;&#26512;&#25105;&#20204;&#20869;&#37096;&#30340;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#32500;&#24230;&#65288;&#34920;&#36798;&#30340;&#24847;&#22270;&#31867;&#22411;&#21644;&#24863;&#30693;&#21040;&#30340;&#20849;&#24773;&#65289;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#32780;&#24863;&#30693;&#21040;&#30340;&#20849;&#24773;&#19982;&#23545;&#35805;&#20250;&#35805;&#30340;&#28385;&#24847;&#27700;&#24179;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#25552;&#20986;&#30340;&#26694;&#26550;&#20173;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#21592;&#30340;&#20027;&#35266;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11409v1 Announce Type: new  Abstract: Empathy is a critical element of effective and satisfactory conversational communication, yet previous studies in measuring conversational empathy mostly focus on expressed communicative intents -- in which way empathy is expressed, ignoring the fact that conversation is also a collaborative practice involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework that extends upon existing work to measure both expressed intents from the speaker's perspective and perceived empathy from the listener's perspective. Applying the proposed framework to analyzing our internal customer-service dialogue shows that the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlation with the satisfactory level of dialogue sessions. This proposed framework still requires subjective assessments from trained annotators, which can be non-triv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25171;&#24320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#21283;&#23376;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#19979;&#32780;&#19978;&#30340;&#26426;&#26800;&#35299;&#37322;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#34920;&#31034;&#24037;&#31243;&#35270;&#35282;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#24212;&#29992;LLMs&#30340;&#34892;&#20026;&#21644;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.10688</link><description>&lt;p&gt;
&#25171;&#24320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#65306;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#30340;&#20004;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25171;&#24320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#21283;&#23376;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#19979;&#32780;&#19978;&#30340;&#26426;&#26800;&#35299;&#37322;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#34920;&#31034;&#24037;&#31243;&#35270;&#35282;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#24212;&#29992;LLMs&#30340;&#34892;&#20026;&#21644;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20154;&#20204;&#23545;&#28508;&#22312;&#20260;&#23475;(&#22914;&#27602;&#24615;&#12289;&#19981;&#20844;&#24179;&#21644;&#24187;&#35273;)&#30340;&#25285;&#24551;&#23041;&#32961;&#21040;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#36890;&#36807;&#27169;&#22411;&#23545;&#40784;&#30830;&#20445;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#26377;&#30410;&#22865;&#21512;&#22240;&#27492;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;LLMs&#30340;&#34892;&#20026;&#21644;&#26426;&#21046;&#26377;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19968;&#20010;&#28085;&#30422;&#20114;&#34917;&#30340;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#35270;&#35282;&#30340;&#25972;&#20307;&#35299;&#37322;&#26694;&#26550;&#26469;&#25171;&#24320;LLMs&#30340;&#40657;&#21283;&#23376;&#12290;&#33258;&#19979;&#32780;&#19978;&#35270;&#35282;&#30001;&#26426;&#26800;&#35299;&#37322;&#33021;&#21147;&#23454;&#29616;&#65292;&#20391;&#37325;&#20110;&#32452;&#20214;&#21151;&#33021;&#21644;&#35757;&#32451;&#21160;&#24577;&#12290;&#33258;&#19978;&#32780;&#19979;&#35270;&#35282;&#21033;&#29992;&#34920;&#31034;&#24037;&#31243;&#36890;&#36807;&#38544;&#34255;&#34920;&#31034;&#20998;&#26512;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#21608;&#22260;&#20851;&#20110;&#26426;&#26800;&#35299;&#37322;&#33021;&#21147;&#21644;&#34920;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#65292;&#24635;&#32467;&#20102;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#36798;&#21040;&#30340;&#26410;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10688v1 Announce Type: new  Abstract: As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65306;&#30001;LLMs&#39537;&#21160;&#30340;&#35821;&#38899;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26032;&#19968;&#20195;&#36719;&#20214;&#30340;&#35806;&#29983;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#24037;&#19994;&#30028;&#26080;&#25968;&#24212;&#29992;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#24341;&#25806;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20998;&#31867;&#26368;&#26377;&#21487;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#35782;&#21035;&#25152;&#38656;&#30340;UI&#32452;&#20214;&#65292;&#24182;&#38543;&#21518;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#23558;&#38745;&#24577;UI&#31995;&#32479;&#21457;&#23637;&#25104;&#39640;&#24230;&#21160;&#24577;&#21644;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#20837;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#39046;&#22495;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#29992;&#25143;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#26497;&#22823;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.18463</link><description>&lt;p&gt;
PeTailor&#65306;&#36890;&#36807;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18463
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#26088;&#22312;&#33258;&#21160;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#29702;&#35299;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#21477;&#23376;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#31283;&#20581;&#30340;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#30340;&#24320;&#21457;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;PeTailor&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#25105;&#20204;&#39044;&#20808;&#26500;&#24314;&#30340;&#22810;&#26679;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26174;&#24335;&#22320;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#29983;&#25104;&#30456;&#24212;&#30340;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23614;&#23454;&#20307;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;GM-CIHT&#65292;&#19968;&#31181;&#19987;&#23478;&#26631;&#27880;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical triple extraction systems aim to automatically extract biomedical entities and relations between entities. While current unified information extraction models showcase state-of-the-art performance, they face challenges in understanding relationships between entities within intricate biomedical sentences. Furthermore, the absence of a high-quality biomedical triple extraction dataset impedes the progress in developing robust triple extraction systems. To tackle these challenges, we propose a novel retrieval-based framework for biomedical triple extraction, namely PeTailor, which explicitly retrieves the relevant document from our pre-built diverse chunk database using a novel tailored chunk scorer and integrates the retrieved information into the input of a Large Language Model (LLM) to generate the corresponding triple (head entity, relation, tail entity) for the input sentence. Additionally, we present GM-CIHT, an expert-annotated biomedical triple extraction dataset that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#24847;&#22806;&#24773;&#20917;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26410;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#21477;&#23376;&#32467;&#26500;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.14212</link><description>&lt;p&gt;
&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#25913;&#36827;&#20102;&#24847;&#22806;&#24773;&#20917;&#19979;&#30340;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations. (arXiv:2401.14212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#24847;&#22806;&#24773;&#20917;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26410;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#21477;&#23376;&#32467;&#26500;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#35782;&#21035;&#35270;&#35273;&#23454;&#20307;&#24182;&#23558;&#23427;&#20204;&#25490;&#21015;&#22312;&#20108;&#32500;&#31354;&#38388;&#24067;&#23616;&#20013;&#65292;&#38656;&#35201;&#23545;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#32452;&#21512;&#29702;&#35299;&#12290;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#23616;&#37096;&#21644;&#21463;&#25511;&#30340;&#20462;&#22797;&#12290;&#36890;&#36807;&#27604;&#36739;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#38544;&#24335;&#25110;&#26174;&#24335;&#32534;&#30721;&#21477;&#23376;&#35821;&#27861;&#30340;&#35821;&#35328;&#34920;&#31034;&#20013;&#39044;&#27979;&#24067;&#23616;&#65292;&#22914;&#26524;&#21477;&#23376;&#25552;&#21040;&#30340;&#23454;&#20307;&#20851;&#31995;&#19982;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#12290;&#20026;&#20102;&#27979;&#35797;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30001;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#24067;&#23616;&#32452;&#25104;&#30340;&#27979;&#35797;&#38598;&#65292;&#25551;&#36848;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#26410;&#26366;&#35265;&#36807;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22312;&#36825;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#27169;&#22411;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#29702;&#35299;&#36755;&#20837;&#21477;&#23376;&#30340;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#25439;&#22833;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#24378;&#21270;&#20102;&#21477;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10768</link><description>&lt;p&gt;
&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65306;&#36890;&#36807;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#40784;&#21518;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#21487;&#33021;&#20135;&#29983;&#19982;&#19978;&#19979;&#25991;&#25110;&#19990;&#30028;&#30693;&#35782;&#33258;&#20449;&#30683;&#30462;&#30340;&#21709;&#24212;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32487;&#25215;&#30340;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#23545;&#40784;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#33258;&#21160;&#21046;&#23450;&#32771;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#20110;&#21253;&#21547;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#65292;KCA&#23454;&#26045;&#20102;&#20960;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#21644;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;KCA&#26041;&#27861;&#22312;&#32531;&#35299;&#24187;&#35273;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.02797</link><description>&lt;p&gt;
PeFoMed&#65306;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#36827;&#21270;&#25193;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24212;&#23545;&#36229;&#36234;&#32431;&#25991;&#26412;&#24212;&#29992;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#20102;&#20808;&#21069;&#32534;&#30721;&#22312;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;MLLMs&#36866;&#24212;&#20026;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Med-VQA&#65289;&#20219;&#21153;&#30340;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Med-VQA&#24212;&#29992;&#29305;&#21035;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#20026;&#20102;&#20934;&#30830;&#34913;&#37327;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#19978;&#36798;&#21040;&#20102;81.9&#65285;&#65292;&#19988;&#20854;&#30456;&#23545;&#20110;GPT-4v&#27169;&#22411;&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.07989</link><description>&lt;p&gt;
&#32479;&#19968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#35270;&#35282;&#65306;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code. (arXiv:2311.07989v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#26368;&#36817;&#22312;&#20195;&#30721;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#22788;&#29702;&#27169;&#22411;&#20998;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#31995;&#21015;&#65289;&#21644;&#19987;&#38376;&#22312;&#20195;&#30721;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#20855;&#26377;&#19987;&#38376;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#36825;&#27491;&#26159;NLP&#20063;&#32463;&#21382;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#65292;&#22914;AST&#12289;CFG&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#35757;&#32451;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#36825;&#20221;&#32508;&#36848;&#20445;&#25345;&#24320;&#25918;&#65292;&#24182;&#22312;GitHub&#19978;&#26356;&#26032;&#65292;&#32593;&#22336;&#20026;https://github.com/codefuse-ai/Awesome-Code-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 170+ datasets, and 700+ related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01041</link><description>&lt;p&gt;
&#23398;&#20250;&#25298;&#32477;&#65306;&#36890;&#36807;&#30693;&#35782;&#33539;&#22260;&#38480;&#21046;&#21644;&#25298;&#32477;&#26426;&#21046;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21487;&#25511;&#21644;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22238;&#31572;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#32463;&#24120;&#20135;&#29983;&#21547;&#26377;&#38169;&#35823;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#22238;&#31572;&#12290;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#24187;&#35273;&#65292;&#20351;&#24471;LLMs&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#19981;&#21487;&#38752;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;LLMs&#20013;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25298;&#32477;&#26426;&#21046;&#65292;&#25351;&#23548;LLMs&#25298;&#32477;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;Learn to Refuse (L2R)&#65292;&#23427;&#23558;&#25298;&#32477;&#26426;&#21046;&#32435;&#20837;&#21040;LLMs&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#37027;&#20123;&#23427;&#20204;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#26469;&#34920;&#31034;&#25152;&#26377;LLMs&#25152;&#38656;&#35201;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.17722</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#35270;&#35273;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;(LLaRP)&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#30340;LLM&#35843;&#25972;&#20026;&#25509;&#25910;&#25991;&#26412;&#25351;&#20196;&#21644;&#35270;&#35273;&#33258;&#25105;&#20013;&#24515;&#35266;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#30452;&#25509;&#22312;&#29615;&#22659;&#20013;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;LLaRP&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#30475;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLaRP&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;1,000&#20010;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;42%&#65292;&#26159;&#20854;&#20182;&#24120;&#35265;&#23398;&#20064;&#22522;&#32447;&#25110;&#38646;&#26679;&#26412;&#24212;&#29992;&#30340;1.7&#20493;&#25104;&#21151;&#29575;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#31038;&#21306;&#30740;&#31350;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;AI&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#65292;&#21253;&#25324;150,000&#20010;&#35757;&#32451;&#20219;&#21153;&#21644;1,000&#20010;&#27979;&#35797;&#20219;&#21153;&#65292;&#29992;&#20110;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07023</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25366;&#25496;&#23439;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23439;&#20219;&#21153;&#26159;&#25105;&#20204;&#26085;&#24120;&#25163;&#26426;&#27963;&#21160;&#30340;&#26500;&#24314;&#22359;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#8220;&#30331;&#24405;&#8221;&#25110;&#8220;&#39044;&#23450;&#33322;&#29677;&#8221;&#65289;&#12290;&#26377;&#25928;&#22320;&#25552;&#21462;&#23439;&#20219;&#21153;&#23545;&#20110;&#29702;&#35299;&#31227;&#21160;&#20132;&#20114;&#21644;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23439;&#20219;&#21153;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24456;&#38590;&#25552;&#21462;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#21516;&#26102;&#21448;&#38544;&#34255;&#22312;&#24212;&#29992;&#30340;&#32534;&#31243;&#32452;&#20214;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20174;&#38543;&#26426;&#21644;&#29992;&#25143;&#31574;&#21010;&#30340;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23439;&#20219;&#21153;&#33258;&#21160;&#26631;&#35760;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#19988;&#21487;&#20197;&#23436;&#20840;&#25191;&#34892;&#12290;&#20026;&#20102;&#26816;&#39564;&#25552;&#21462;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#65292;&#21253;&#25324;&#29992;&#25143;&#35780;&#20272;&#12289;&#19982;&#20154;&#24037;&#31574;&#21010;&#20219;&#21153;&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#23545;&#36825;&#20123;&#23439;&#20219;&#21153;&#30340;&#33258;&#21160;&#25191;&#34892;&#12290;&#36825;&#20123;&#23454;&#39564;&#21644;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>X-PARADE&#26159;&#31532;&#19968;&#20010;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;&#26469;&#28304;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#19978;&#30340;&#27573;&#33853;&#23545;&#40784;&#65292;&#26631;&#27880;&#32773;&#35780;&#20272;&#20102;&#30446;&#26631;&#35821;&#35328;&#27573;&#33853;&#19982;&#28304;&#35821;&#35328;&#27573;&#33853;&#20043;&#38388;&#30340;&#20449;&#24687;&#26159;&#21542;&#30456;&#21516;&#12289;&#26032;&#30340;&#25110;&#32773;&#21487;&#20197;&#25512;&#26029;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.08873</link><description>&lt;p&gt;
X-PARADE: &#36328;&#35821;&#35328;&#25991;&#26412;&#34164;&#21547;&#21644;&#27573;&#33853;&#20043;&#38388;&#30340;&#20449;&#24687;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs. (arXiv:2309.08873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08873
&lt;/p&gt;
&lt;p&gt;
X-PARADE&#26159;&#31532;&#19968;&#20010;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;&#26469;&#28304;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#19978;&#30340;&#27573;&#33853;&#23545;&#40784;&#65292;&#26631;&#27880;&#32773;&#35780;&#20272;&#20102;&#30446;&#26631;&#35821;&#35328;&#27573;&#33853;&#19982;&#28304;&#35821;&#35328;&#27573;&#33853;&#20043;&#38388;&#30340;&#20449;&#24687;&#26159;&#21542;&#30456;&#21516;&#12289;&#26032;&#30340;&#25110;&#32773;&#21487;&#20197;&#25512;&#26029;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20004;&#27573;&#25991;&#26412;&#26159;&#21542;&#20256;&#36798;&#30456;&#21516;&#30340;&#20449;&#24687;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35768;&#22810;&#23376;&#38382;&#39064;&#30340;&#30446;&#26631;&#65292;&#21253;&#25324;&#25991;&#26412;&#34164;&#21547;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#24403;&#36825;&#20004;&#27573;&#25991;&#26412;&#22788;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;X-PARADE&#65288;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#30340;&#20998;&#27495;&#21644;&#34164;&#21547;&#20998;&#26512;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26631;&#27880;&#32773;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#20197;&#36328;&#24230;&#32423;&#21035;&#26631;&#27880;&#27573;&#33853;&#65292;&#24182;&#19982;&#28304;&#35821;&#35328;&#20013;&#30340;&#30456;&#24212;&#27573;&#33853;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#31034;&#32473;&#23450;&#30340;&#20449;&#24687;&#26159;&#21542;&#30456;&#21516;&#12289;&#26032;&#30340;&#65292;&#25110;&#32773;&#26032;&#30340;&#20294;&#21487;&#20197;&#25512;&#26029;&#12290;&#36825;&#20010;&#27010;&#24565;&#19982;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#23545;&#40784;&#30340;&#27573;&#33853;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#20998;&#27495;&#12290;&#20973;&#20511;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20196;&#29260;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;LLM&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#29983;&#25104;&#24335;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10261</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;LLM&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#29983;&#25104;&#24335;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#22312;ML&#31038;&#21306;&#24341;&#36215;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#20197;BERT&#12289;RoBERTa&#21644;GPT-2&#31561;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;Transformer&#27169;&#22411;&#25506;&#32034;&#20102;OOD&#26816;&#27979;&#65292;&#20294;&#22312;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#33539;&#24335;&#26041;&#38754;&#30340;&#26126;&#26174;&#24046;&#24322;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#21457;&#29616;&#22312;LLM&#20013;&#30340;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#22312;LLMs&#39046;&#22495;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;7B&#21040;65B&#22823;&#23567;&#30340;LLaMA&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23457;&#26597;&#20102;&#23427;&#20204;&#22312;&#38646;&#26799;&#24230;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#20043;&#21069;&#30340;&#21028;&#21035;&#24335;&#30340;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#25913;&#20026;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20351;LLM&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19982;&#20043;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLM
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65288;SCoTD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#20986;&#30340;&#21512;&#29702;&#21270;&#35299;&#37322;&#26469;&#35757;&#32451;&#25968;&#37327;&#32423;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23567;&#27169;&#22411;&#20063;&#33021;&#21463;&#30410;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14050</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65306;&#23567;&#27169;&#22411;&#20063;&#33021;&#36880;&#27493;&#8220;&#24605;&#32771;&#8221;
&lt;/p&gt;
&lt;p&gt;
Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65288;SCoTD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#20986;&#30340;&#21512;&#29702;&#21270;&#35299;&#37322;&#26469;&#35757;&#32451;&#25968;&#37327;&#32423;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23567;&#27169;&#22411;&#20063;&#33021;&#21463;&#30410;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;&#20363;&#22914;&#8220;&#25105;&#20204;&#26469;&#36880;&#27493;&#24605;&#32771;&#8221;&#65289;&#20250;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#36827;&#34892;&#21512;&#29702;&#21270;&#35299;&#37322;&#12290;&#34429;&#28982;&#24605;&#32500;&#38142;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#30410;&#22788;&#20284;&#20046;&#20165;&#36866;&#29992;&#20110;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#65288;&#36229;&#36807;50&#20159;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#37327;&#32423;&#36739;&#23567;&#65288;125M-1.3B&#21442;&#25968;&#65289;&#30340;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#20174;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31526;&#21495;&#21270;&#24605;&#32500;&#38142;&#25552;&#28860;&#65288;SCoTD&#65289;&#65292;&#19968;&#31181;&#23558;&#36739;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#20986;&#30340;&#21512;&#29702;&#21270;&#35299;&#37322;&#29992;&#20110;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#20960;&#20010;&#24120;&#35782;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;1&#65289;SCoTD&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#30417;&#30563;&#23398;&#20064;&#36824;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#38598;&#26041;&#38754;&#12290;2&#65289;&#20174;&#25945;&#24072;&#27169;&#22411;&#20013;&#25277;&#26679;&#22810;&#20010;&#25512;&#29702;&#38142;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;3&#65289;&#22312;&#25552;&#28860;&#21518;&#65292;&#34429;&#28982;&#21442;&#25968;&#23569;&#24471;&#22810;&#65292;&#20294;&#23398;&#29983;&#24605;&#32500;&#38142;&#19982;&#25945;&#24072;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.14502</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#39034;&#24207;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;RetICL
&lt;/p&gt;
&lt;p&gt;
RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#37117;&#38598;&#20013;&#22312;&#20419;&#20351;&#23427;&#20204;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#65288;&#21487;&#33021;&#26159;&#26032;&#30340;&#65289;&#29983;&#25104;/&#39044;&#27979;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31034;&#20363;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22909;&#30340;&#31034;&#20363;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#65292;&#22240;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#32452;&#30340;&#23450;&#20041;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29420;&#31435;&#22320;&#23545;&#31034;&#20363;&#36827;&#34892;&#35780;&#20998;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31034;&#20363;&#30340;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;In-Context Learning&#30340;&#26816;&#32034;RetICL&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#36880;&#27493;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#25226;&#39034;&#24207;&#31034;&#20363;&#36873;&#25321;&#30340;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#36845;&#20195;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#25991;&#21270;&#36827;&#21270;&#27169;&#22411;&#65292;&#23637;&#31034;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24182;&#25910;&#25947;&#20110;&#39640;&#25928;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#20041;&#31995;&#32479;&#21453;&#26144;&#25928;&#29575;&#21387;&#21147;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.10154</link><description>&lt;p&gt;
&#36845;&#20195;&#23398;&#20064;&#19982;&#20132;&#27969;&#20849;&#21516;&#35299;&#37322;&#20102;&#26377;&#25928;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Iterated learning and communication jointly explain efficient color naming systems. (arXiv:2305.10154v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32467;&#21512;&#36845;&#20195;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#25991;&#21270;&#36827;&#21270;&#27169;&#22411;&#65292;&#23637;&#31034;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24182;&#25910;&#25947;&#20110;&#39640;&#25928;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#20041;&#31995;&#32479;&#21453;&#26144;&#25928;&#29575;&#21387;&#21147;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#26377;&#20154;&#35748;&#20026;&#65292;&#35821;&#20041;&#31995;&#32479;&#21453;&#26144;&#20102;&#25928;&#29575;&#30340;&#21387;&#21147;&#65292;&#19968;&#20010;&#24403;&#21069;&#30340;&#20105;&#35770;&#20851;&#27880;&#20110;&#20135;&#29983;&#36825;&#31181;&#27169;&#24335;&#30340;&#25991;&#21270;&#36827;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25928;&#29575;&#23454;&#29616;&#20026;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#24182;&#32467;&#21512;&#36845;&#20195;&#23398;&#20064;&#21644;&#20132;&#27969;&#30340;&#25991;&#21270;&#36827;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30340;&#27169;&#22411;&#25910;&#25947;&#20110;&#22312;IB&#24847;&#20041;&#19979;&#39640;&#25928;&#24182;&#19988;&#31867;&#20284;&#20110;&#20154;&#31867;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#30340;&#39068;&#33394;&#21629;&#21517;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#20165;&#36845;&#20195;&#23398;&#20064;&#25110;&#32773;&#20165;&#20165;&#20132;&#27969;&#24182;&#19981;&#33021;&#20687;&#36825;&#20010;&#27169;&#22411;&#37027;&#26679;&#20135;&#29983;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been argued that semantic systems reflect pressure for efficiency, and a current debate concerns the cultural evolutionary process that produces this pattern. We consider efficiency as instantiated in the Information Bottleneck (IB) principle, and a model of cultural evolution that combines iterated learning and communication. We show that this model, instantiated in neural networks, converges to color naming systems that are efficient in the IB sense and similar to human color naming systems. We also show that iterated learning alone, and communication alone, do not yield the same outcome as clearly.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09446</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#36755;&#20837;&#25511;&#21046;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#29983;&#25104;&#39640;&#24230;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#25509;&#21475;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29992;&#25143;&#25110;&#25163;&#21160;&#25506;&#32034;&#19981;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25110;&#32773;&#36153;&#21147;&#22320;&#27880;&#37322;&#25968;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23454;&#20363;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MICVAE)&#65292;&#23427;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32534;&#30721;&#31232;&#30095;&#30340;&#38901;&#24459;&#29305;&#24449;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MICVAE&#34920;&#29616;&#20986;&#20102;&#31232;&#30095;&#30340;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#26426;&#21046;&#25152;&#38656;&#30340;&#33391;&#22909;&#21697;&#36136;&#65306;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#12290;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;(~4)&#65292;MICVAE&#20063;&#33021;&#35753;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
&lt;/p&gt;</description></item></channel></rss>