<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;EvalWeb&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#20013;&#25991;&#25991;&#26412;&#12290;&#36890;&#36807;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#31579;&#38500;&#22122;&#38899;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#35780;&#20272;&#27169;&#22411;&#20026;&#27599;&#20010;&#25991;&#26412;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01149</link><description>&lt;p&gt;
Chinesewebtext: &#29992;&#26377;&#25928;&#30340;&#35780;&#20272;&#27169;&#22411;&#25552;&#21462;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#32593;&#32476;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;EvalWeb&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#20013;&#25991;&#25991;&#26412;&#12290;&#36890;&#36807;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#31579;&#38500;&#22122;&#38899;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#35780;&#20272;&#27169;&#22411;&#20026;&#27599;&#20010;&#25991;&#26412;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#36136;&#37327;&#23545;&#20110;&#22609;&#36896;LLM&#30340;&#33021;&#21147;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#21152;&#24555;LLM&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;C4 [1]&#12289;Pile [2]&#12289;RefinedWeb [3]&#21644;WanJuan [4]&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#21457;&#24067;&#30340;&#35821;&#26009;&#24211;&#20027;&#35201;&#20851;&#27880;&#33521;&#25991;&#65292;&#20173;&#28982;&#32570;&#20047;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;&#26469;&#20174;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#24178;&#20928;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#23545;&#35821;&#26009;&#24211;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#20363;&#22914;&#27599;&#20010;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;EvalWeb&#65292;&#29992;&#20110;&#20174;&#22024;&#26434;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#20013;&#25991;&#24178;&#20928;&#30340;&#25991;&#26412;&#12290;&#39318;&#20808;&#65292;&#31867;&#20284;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20351;&#29992;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#26469;&#20002;&#24323;&#21407;&#22987;&#29228;&#21462;&#30340;&#32593;&#32476;&#20869;&#23481;&#20013;&#30340;&#26126;&#30830;&#22024;&#26434;&#30340;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#35780;&#20272;&#27169;&#22411;&#26469;&#35780;&#20272;&#21097;&#20313;&#30456;&#23545;&#24178;&#20928;&#30340;&#25968;&#25454;&#65292;&#24182;&#20026;&#27599;&#20010;&#25991;&#26412;&#20998;&#37197;&#19968;&#20010;&#29305;&#23450;&#30340;&#36136;&#37327;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;EvalWeb&#24037;&#20855;&#38142;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2310.19531</link><description>&lt;p&gt;
&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#30340;&#20449;&#24687;&#29109;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19531
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#23569;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23545;&#24120;&#35265;&#21644;&#26131;&#23398;&#26631;&#35760;&#30340;&#20559;&#22909;&#65292;&#20351;&#20854;&#26356;&#20851;&#27880;&#19981;&#24120;&#35265;&#21644;&#38590;&#23398;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#39044;&#27979;&#19978;&#19968;&#20010;&#26631;&#35760;&#65288;&#23376;&#35789;/&#35789;/&#30701;&#35821;&#65289;&#32473;&#20986;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#24120;&#24573;&#35270;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#21363;&#39057;&#32321;&#26631;&#35760;&#21644;&#19981;&#32463;&#24120;&#20986;&#29616;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#34987;&#24120;&#35265;&#19988;&#26131;&#23398;&#30340;&#26631;&#35760;&#25152;&#20027;&#23548;&#65292;&#20174;&#32780;&#24573;&#35270;&#19981;&#32463;&#24120;&#20986;&#29616;&#19988;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29109;&#25439;&#22833;&#65288;InfoEntropy Loss&#65289;&#20989;&#25968;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#30456;&#24212;&#30340;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#20449;&#24687;&#29109;&#21160;&#24577;&#35780;&#20272;&#24453;&#23398;&#20064;&#26631;&#35760;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#28982;&#21518;&#65292;&#23427;&#36866;&#24212;&#22320;&#35843;&#25972;&#35757;&#32451;&#25439;&#22833;&#65292;&#35797;&#22270;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#23398;&#20064;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15823</link><description>&lt;p&gt;
Rosetta Stone&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;&#65306;&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#30340;&#36291;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35789;&#20856;&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#23450;&#20041;&#12289;&#21547;&#20041;&#25110;&#25551;&#36848;&#26469;&#21457;&#29616;&#19968;&#20010;&#35789;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21487;&#20197;&#24110;&#21161;&#25484;&#25569;&#19968;&#20010;&#35789;&#30340;&#25551;&#36848;&#32780;&#19981;&#30693;&#20854;&#36523;&#20221;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#24182;&#20351;&#23547;&#27714;&#31934;&#30830;&#26415;&#35821;&#30340;&#20889;&#20316;&#32773;&#21463;&#30410;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#28085;&#30422;&#34987;&#31216;&#20026;&#8220;&#33292;&#23574;&#19978;&#30340;&#35789;&#8221;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#21453;&#21521;&#35789;&#20856;&#20849;&#20139;&#20219;&#21153;&#20013;&#33719;&#32988;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#37325;&#28857;&#26159;&#20174;&#20276;&#38543;&#30340;&#25551;&#36848;&#20013;&#25512;&#23548;&#20986;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#19968;&#20010;&#38463;&#25289;&#20271;&#23450;&#20041;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#21017;&#20351;&#29992;&#19968;&#20010;&#33521;&#25991;&#23450;&#20041;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#32463;&#36807;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#12290;&#26368;&#32456;&#34920;&#31034;&#26159;&#36890;&#36807;&#23545;&#27599;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;BRAINTEASER&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27178;&#21521;&#24605;&#32500;&#21644;&#36829;&#21453;&#40664;&#35748;&#24120;&#35782;&#32852;&#31995;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#27178;&#21521;&#24605;&#32500;&#22522;&#20934;&#65292;&#20016;&#23500;&#38382;&#39064;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#65292;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.05057</link><description>&lt;p&gt;
BRAINTEASER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32500;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. (arXiv:2310.05057v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;BRAINTEASER&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27178;&#21521;&#24605;&#32500;&#21644;&#36829;&#21453;&#40664;&#35748;&#24120;&#35782;&#32852;&#31995;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#27178;&#21521;&#24605;&#32500;&#22522;&#20934;&#65292;&#20016;&#23500;&#38382;&#39064;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#65292;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#28608;&#21169;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20851;&#27880;&#38656;&#35201;&#38544;&#21547;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20381;&#36182;&#20110;&#31867;&#20154;&#30340;&#24120;&#35782;&#26426;&#21046;&#12290;&#34429;&#28982;&#36825;&#20123;&#22402;&#30452;&#24605;&#32500;&#20219;&#21153;&#30456;&#23545;&#36739;&#21463;&#27426;&#36814;&#65292;&#20294;&#27178;&#21521;&#24605;&#32500;&#38590;&#39064;&#21364;&#40092;&#26377;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;BRAINTEASER&#65306;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#34920;&#29616;&#20986;&#27178;&#21521;&#24605;&#32500;&#21644;&#36829;&#21453;&#40664;&#35748;&#24120;&#35782;&#32852;&#31995;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#31243;&#24207;&#26469;&#21019;&#24314;&#31532;&#19968;&#20010;&#27178;&#21521;&#24605;&#32500;&#22522;&#20934;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#24178;&#25200;&#39033;&#29983;&#25104;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#65292;&#20849;&#26377;1,100&#20010;&#20855;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#27178;&#21521;&#25512;&#29702;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#22522;&#20110;&#38382;&#39064;&#30340;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#37325;&#24314;&#26469;&#20016;&#23500;BRAINTEASER&#12290;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#21644;&#24120;&#35782;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model
&lt;/p&gt;</description></item><item><title>Wordification&#26159;&#19968;&#31181;&#26032;&#30340;&#25945;&#25480;&#33521;&#35821;&#25340;&#20889;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20840;&#29699;&#33539;&#22260;&#20869;&#23384;&#22312;&#30340;&#35782;&#23383;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#38738;&#23569;&#24180;&#29359;&#32618;&#21644;&#25945;&#32946;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.12981</link><description>&lt;p&gt;
Wordification:&#19968;&#31181;&#26032;&#30340;&#25945;&#25480;&#33521;&#35821;&#25340;&#20889;&#27169;&#24335;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wordification: A New Way of Teaching English Spelling Patterns. (arXiv:2309.12981v1 [cs.OH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12981
&lt;/p&gt;
&lt;p&gt;
Wordification&#26159;&#19968;&#31181;&#26032;&#30340;&#25945;&#25480;&#33521;&#35821;&#25340;&#20889;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20840;&#29699;&#33539;&#22260;&#20869;&#23384;&#22312;&#30340;&#35782;&#23383;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#38738;&#23569;&#24180;&#29359;&#32618;&#21644;&#25945;&#32946;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35835;&#20889;&#33021;&#21147;&#26159;&#29983;&#27963;&#21644;&#31038;&#20250;&#20013;&#25104;&#21151;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25454;&#20272;&#35745;&#65292; 85% &#30340;&#26410;&#25104;&#24180;&#29359;&#32618;&#31995;&#32479;&#30340;&#20154;&#26080;&#27861;&#36275;&#22815;&#22320;&#38405;&#35835;&#21644;&#20070;&#20889;&#65292;&#36229;&#36807;&#19968;&#21322;&#26377;&#28389;&#29992;&#29289;&#36136;&#38382;&#39064;&#30340;&#20154;&#22312;&#38405;&#35835;&#25110;&#20070;&#20889;&#26041;&#38754;&#26377;&#22256;&#38590;&#65292;&#32780;&#26410;&#23436;&#25104;&#39640;&#20013;&#23398;&#19994;&#30340;&#19977;&#20998;&#20043;&#20108;&#32570;&#20047;&#36866;&#24403;&#30340;&#35835;&#20889;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19981;&#20855;&#22791;&#19982;&#22235;&#24180;&#32423;&#21305;&#37197;&#30340;&#38405;&#35835;&#33021;&#21147;&#30340;&#24188;&#20799;&#22823;&#32422;&#26377;80%&#30340;&#21487;&#33021;&#24615;&#26681;&#26412;&#26080;&#27861;&#36214;&#19978;&#12290;&#35768;&#22810;&#20154;&#21487;&#33021;&#35748;&#20026;&#22312;&#21457;&#36798;&#22269;&#23478;&#22914;&#32654;&#22269;&#65292;&#35782;&#23383;&#33021;&#21147;&#19981;&#20877;&#26159;&#19968;&#20010;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#31181;&#21361;&#38505;&#30340;&#35823;&#35299;&#12290;&#20840;&#29699;&#27599;&#24180;&#22240;&#35782;&#23383;&#38382;&#39064;&#25439;&#22833;&#32422;1.19&#19975;&#20159;&#32654;&#20803;&#65307;&#22312;&#32654;&#22269;&#65292;&#25439;&#22833;&#32422;&#20026;3000&#20159;&#32654;&#20803;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#29616;&#22312;&#21807;&#19968;&#30340;&#24037;&#20855;&#26159;
&lt;/p&gt;
&lt;p&gt;
Literacy, or the ability to read and write, is a crucial indicator of success in life and greater society. It is estimated that 85% of people in juvenile delinquent systems cannot adequately read or write, that more than half of those with substance abuse issues have complications in reading or writing and that two-thirds of those who do not complete high school lack proper literacy skills. Furthermore, young children who do not possess reading skills matching grade level by the fourth grade are approximately 80% likely to not catch up at all. Many may believe that in a developed country such as the United States, literacy fails to be an issue; however, this is a dangerous misunderstanding. Globally an estimated 1.19 trillion dollars are lost every year due to issues in literacy; in the USA, the loss is an estimated 300 billion. To put it in more shocking terms, one in five American adults still fail to comprehend basic sentences. Making matters worse, the only tools available now to c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#26816;&#32034;&#26041;&#27861;&#21644;&#35821;&#20041;&#25628;&#32034;&#26469;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;SHAP&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#38598;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#21512;&#27880;&#37322;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#38271;&#26399;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.14899</link><description>&lt;p&gt;
&#20026;&#35299;&#20915;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;&#25991;&#26412;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification. (arXiv:2307.14899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#26816;&#32034;&#26041;&#27861;&#21644;&#35821;&#20041;&#25628;&#32034;&#26469;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;SHAP&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#38598;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#21512;&#27880;&#37322;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#38271;&#26399;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#36873;&#25321;&#19968;&#32452;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#20154;&#21147;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#27880;&#37322;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20108;&#20803;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21363;&#27491;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#27880;&#37322;&#26159;&#22312;&#36739;&#38271;&#30340;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#30340;&#65292;&#21487;&#20197;&#23558;&#35201;&#27880;&#37322;&#30340;&#25991;&#26412;&#25209;&#37327;&#36873;&#25321;&#65292;&#20197;&#21069;&#38754;&#30340;&#27880;&#37322;&#26469;&#25351;&#23548;&#19979;&#19968;&#32452;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;SHAP&#26500;&#24314;Elasticsearch&#21644;&#35821;&#20041;&#25628;&#32034;&#30340;&#39640;&#36136;&#37327;&#26597;&#35810;&#38598;&#65292;&#20197;&#23581;&#35797;&#35782;&#21035;&#19968;&#32452;&#26368;&#20248;&#25991;&#26412;&#26469;&#24110;&#21161;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#25551;&#36848;&#21487;&#33021;&#30340;&#26410;&#26469;&#20107;&#20214;&#30340;&#25552;&#32434;&#25991;&#26412;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35813;&#25991;&#26412;&#38598;&#30001;&#21442;&#19982;&#32933;&#32982;&#21644;&#31958;&#23615;&#30149;&#31649;&#29702;&#30740;&#31350;&#30340;&#21442;&#19982;&#32773;&#26500;&#24314;&#12290;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of selecting of a set of texts for annotation in text classification using retrieval methods when there are limits on the number of annotations due to constraints on human resources. An additional challenge addressed is dealing with binary categories that have a small number of positive instances, reflecting severe class imbalance. In our situation, where annotation occurs over a long time period, the selection of texts to be annotated can be made in batches, with previous annotations guiding the choice of the next set. To address these challenges, the paper proposes leveraging SHAP to construct a quality set of queries for Elasticsearch and semantic search, to try to identify optimal sets of texts for annotation that will help with class imbalance. The approach is tested on sets of cue texts describing possible future events, constructed by participants involved in studies aimed to help with the management of obesity and diabetes. We introduce an effec
&lt;/p&gt;</description></item><item><title>M3Exam&#26159;&#19968;&#20010;&#26469;&#28304;&#20110;&#30495;&#23454;&#20154;&#31867;&#32771;&#35797;&#39064;&#30446;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#20013;&#30340;&#26222;&#36866;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05179</link><description>&lt;p&gt;
M3Exam: &#19968;&#31181;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#23618;&#27425;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05179
&lt;/p&gt;
&lt;p&gt;
M3Exam&#26159;&#19968;&#20010;&#26469;&#28304;&#20110;&#30495;&#23454;&#20154;&#31867;&#32771;&#35797;&#39064;&#30446;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#20013;&#30340;&#26222;&#36866;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#30528;&#21508;&#31181;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#32771;&#35797;&#26356;&#36866;&#21512;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36866;&#26234;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#22218;&#25324;&#20102;&#26356;&#24191;&#27867;&#30340;&#33021;&#21147;&#38656;&#27714;&#65292;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; M3Exam&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#21644;&#23448;&#26041;&#20154;&#31867;&#32771;&#35797;&#39064;&#30446;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#20013;&#35780;&#20272; LLM&#12290;M3Exam &#20855;&#26377;&#19977;&#20010;&#29420;&#29305;&#29305;&#28857;:&#65288;1&#65289;&#22810;&#35821;&#35328;&#24615;&#65292;&#28085;&#30422;&#22810;&#20010;&#22269;&#23478;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#25991;&#21270;&#30693;&#35782;&#65307;&#65288;2&#65289;&#22810;&#27169;&#24577;&#65292;&#32771;&#34385;&#21040;&#35768;&#22810;&#32771;&#35797;&#38382;&#39064;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65307;&#65288;3&#65289;&#22810;&#23618;&#27425;&#32467;&#26500;&#65292;&#29305;&#21035;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#25945;&#32946;&#38454;&#27573;&#30340;&#32771;&#35797;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#25945;&#32946;&#27700;&#24179;&#19978;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>VCoT&#26159;&#19968;&#31181;&#20351;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#36923;&#36753;&#24046;&#36317;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.02317</link><description>&lt;p&gt;
&#35270;&#35273;&#24605;&#32500;&#38142;&#65306;&#22810;&#27169;&#24577;&#22635;&#20805;&#25216;&#26415;&#24357;&#21512;&#36923;&#36753;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02317
&lt;/p&gt;
&lt;p&gt;
VCoT&#26159;&#19968;&#31181;&#20351;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#36923;&#36753;&#24046;&#36317;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#20197;&#20154;&#31867;&#26041;&#24335;&#20998;&#35299;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#33539;&#20363;&#30001;&#20110;&#20854;&#21333;&#27169;&#24577;&#24615;&#36136;&#24182;&#19988;&#20027;&#35201;&#24212;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#23558;&#35270;&#35273;&#22686;&#24378;&#20869;&#23481;&#32435;&#20837;&#25512;&#29702;&#26159;&#24517;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#22797;&#26434;&#24819;&#35937;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VCoT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#26469;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#65292;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#65292;&#21516;&#26102;&#25552;&#20379;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23558;VCoT&#24212;&#29992;&#20110;&#35270;&#35273;&#21465;&#20107;&#21644;WikiHow&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02754</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32467;&#26500;&#34920;&#29616;&#30340;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#20197;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#34987;&#29992;&#20316;&#30740;&#31350;&#24515;&#29702;&#21644;&#33041;&#37096;&#27010;&#24565;&#34920;&#24449;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#20195;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#20960;&#20046;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;&#27010;&#24565;&#34920;&#24449;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#21644;&#27604;&#36739;&#20154;&#31867;&#21644;&#19968;&#20010;&#33879;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#30340;DaVinci&#21464;&#20307;&#65289;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#24378;&#22823;&#19988;&#40065;&#26834;&#65292;&#19981;&#21463;&#25991;&#21270;&#12289;&#35821;&#35328;&#21644;&#20272;&#31639;&#26041;&#27861;&#30340;&#24046;&#24322;&#24433;&#21709;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#30456;&#23545;&#31283;&#23450;&#65292;&#20294;&#20855;&#20307;&#21462;&#20915;&#20110;&#20219;&#21153;&#26412;&#36523;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#21487;&#38752;&#65292;&#20294;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#35748;&#30693;&#22788;&#29702;&#30456;&#20851;&#25512;&#26029;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language AIs, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses two common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from AI behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.10186</link><description>&lt;p&gt;
&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#26500;&#24819;&#20102;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#19968;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#65292;&#32780;&#26080;&#38656;&#20013;&#38388;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#20154;&#19982;&#35745;&#31639;&#26426;&#30340;&#23545;&#35805;&#20013;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#22914;&#22995;&#21517;&#12289;&#37038;&#25919;&#22320;&#22336;&#21644;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#23545;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#21487;&#35835;&#24615;&#24378;&#30340;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25991;&#26412;&#36716;&#24405;&#12290;&#25105;&#20204;&#35828;&#26126;&#36825;&#31181;&#30452;&#25509;&#26041;&#27861;&#20248;&#21270;&#20102;&#32534;&#30721;&#22120;&#65292;&#20197;&#20165;&#36716;&#24405;&#35821;&#38899;&#20013;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#37096;&#20998;&#65292;&#24573;&#30053;&#20102;&#22810;&#20313;&#30340;&#37096;&#20998;&#65292;&#22914;&#25645;&#26723;&#35821;&#25110;&#23454;&#20307;&#25340;&#20889;&#12290;&#22312;&#20225;&#19994;&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#27493;&#27861;&#30340;&#26041;&#27861;&#20248;&#20110;&#20856;&#22411;&#30340;&#20004;&#27493;&#27861;&#65292;&#21363;&#39318;&#20808;&#20135;&#29983;&#35789;&#27719;&#36716;&#24405;&#65292;&#28982;&#21518;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#23454;&#20307;&#25552;&#21462;&#20197;&#35782;&#21035;&#21475;&#35821;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reimagines some aspects of speech processing using speech encoders, specifically about extracting entities directly from speech, with no intermediate textual representation. In human-computer conversations, extracting entities such as names, postal addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech, ignoring the superfluous portions such as carrier phrases and spellings of entities. In the context of dialogs from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step cascade of first generating lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
&lt;/p&gt;</description></item></channel></rss>