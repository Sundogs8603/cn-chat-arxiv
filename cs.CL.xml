<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.15054</link><description>&lt;p&gt;
&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#25991;&#26412;&#30340;&#23454;&#20540;&#34920;&#31034;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#20123;&#34920;&#31034;&#21253;&#21547;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#35821;&#35328;&#23646;&#24615;&#21644;&#22522;&#20110;&#24615;&#21035;&#30340;&#20154;&#21475;&#20559;&#35265;&#31561;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#22312;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#27491;&#20132;&#25237;&#24433;&#26469;&#33719;&#24471;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#20449;&#24687;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#20026;&#36825;&#39033;&#30740;&#31350;&#36129;&#29486;&#20102;&#26032;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#26041;&#27861;&#26469;&#36991;&#20813;&#34394;&#20551;&#30456;&#20851;&#30340;&#22833;&#25928;&#27169;&#24335;&#65292;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#23376;&#31354;&#38388;&#20013;&#30340;&#20998;&#37327;&#21644;&#20854;&#27491;&#20132;&#34917;&#31354;&#38388;&#20013;&#30340;&#20998;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23376;&#31354;&#38388;&#20013;&#30340;&#21453;&#20107;&#23454;&#20449;&#24687;&#27010;&#24565;&#26159;&#30001;&#19968;&#20010;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#24178;&#39044;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25805;&#20316;&#26469;&#23581;&#35797;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.15043</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#36890;&#29992;&#21644;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#24341;&#36215;&#21453;&#24863;&#30340;&#20869;&#23481;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#29983;&#25104;&#12290;&#23613;&#31649;&#22312;&#35268;&#36991;&#36825;&#20123;&#25514;&#26045;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#25152;&#35859;&#30340;&#23545;LLMs&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38656;&#35201;&#20154;&#20026;&#30340;&#24039;&#24605;&#65292;&#23454;&#38469;&#19978;&#24182;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#21518;&#32512;&#65292;&#24403;&#38468;&#21152;&#21040;&#21508;&#31181;&#26597;&#35810;&#19978;&#65292;&#20379;LLM&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#26102;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#20135;&#29983;&#32943;&#23450;&#22238;&#31572;&#65288;&#32780;&#19981;&#26159;&#25298;&#32477;&#22238;&#31572;&#65289;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#36825;&#20123;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#19988;&#22312;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;SuperCLUE&#65292;&#21253;&#25324;&#23454;&#38469;&#29992;&#25143;&#30340;&#26597;&#35810;&#21644;&#35780;&#32423;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22635;&#34917;&#20102;&#30446;&#21069;&#23545;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#21147;&#29702;&#35299;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.15020</link><description>&lt;p&gt;
SuperCLUE:&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;SuperCLUE&#65292;&#21253;&#25324;&#23454;&#38469;&#29992;&#25143;&#30340;&#26597;&#35810;&#21644;&#35780;&#32423;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22635;&#34917;&#20102;&#30446;&#21069;&#23545;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#21147;&#29702;&#35299;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26174;&#31034;&#20986;&#23558;&#20854;&#25972;&#21512;&#21040;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#20559;&#22909;&#26159;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#26368;&#20851;&#38190;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#22810;&#36873;&#39064;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;SuperCLUE&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#20197;&#21478;&#19968;&#20010;&#27969;&#34892;&#30340;&#20013;&#25991;LLM&#22522;&#20934;&#27979;&#35797;CLUE&#21629;&#21517;&#12290;SuperCLUE&#21253;&#25324;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#26469;&#33258;&#19968;&#20010;LLM&#23545;&#25112;&#24179;&#21488;(CArena)&#30340;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#21644;&#35780;&#32423;&#65292;&#26377;&#21333;&#20010;&#21644;&#22810;&#36718;&#23545;&#35805;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;(OPEN)&#65292;&#20197;&#21450;&#19982;&#24320;&#25918;&#24615;&#21333;&#36718;&#38382;&#39064;&#30456;&#21516;&#33550;&#30340;&#23553;&#38381;&#24615;&#38382;&#39064;(CLOSE)&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23553;&#38381;&#24615;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#24615;&#19981;&#36275;&#20197;&#21453;&#26144;&#22312;&#24320;&#25918;&#24615;&#38382;&#39064;&#19978;&#23454;&#29616;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20114;&#34917;&#22320;&#39044;&#27979;&#23454;&#38469;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user prefe
&lt;/p&gt;</description></item><item><title>Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15002</link><description>&lt;p&gt;
Gzip&#19982;KNN&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15002
&lt;/p&gt;
&lt;p&gt;
Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;KNN&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#21387;&#32553;&#36317;&#31163;&#65288;gzip&#65289;&#30340;&#26377;&#25928;&#24615;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#38656;&#35201;&#25991;&#26412;&#21387;&#32553;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#8220;&#35789;&#34955;&#8221;&#21305;&#37197;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransNormerLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;softmax&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#23884;&#20837;&#12289;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#12289;&#38376;&#25511;&#26426;&#21046;&#31561;&#20808;&#36827;&#25913;&#36827;&#65292;&#24182;&#21033;&#29992;Lightning Attention&#25216;&#26415;&#21152;&#36895;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#37319;&#29992;&#24352;&#37327;&#24402;&#19968;&#21270;&#26041;&#26696;&#21152;&#36895;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;TransNormer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14995</link><description>&lt;p&gt;
&#23558;TransNormer&#25193;&#23637;&#21040;1750&#20159;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransNormerLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;softmax&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#23884;&#20837;&#12289;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#12289;&#38376;&#25511;&#26426;&#21046;&#31561;&#20808;&#36827;&#25913;&#36827;&#65292;&#24182;&#21033;&#29992;Lightning Attention&#25216;&#26415;&#21152;&#36895;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#37319;&#29992;&#24352;&#37327;&#24402;&#19968;&#21270;&#26041;&#26696;&#21152;&#36895;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;TransNormer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TransNormerLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20004;&#26041;&#38754;&#37117;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;softmax&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;TransNormerLLM&#20174;&#20043;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#26550;&#26500;TransNormer&#21457;&#23637;&#32780;&#26469;&#65292;&#36890;&#36807;&#24341;&#20837;&#20301;&#32622;&#23884;&#20837;&#12289;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#12289;&#38376;&#25511;&#26426;&#21046;&#12289;&#24352;&#37327;&#24402;&#19968;&#21270;&#12289;&#25512;&#29702;&#21152;&#36895;&#21644;&#31283;&#23450;&#21270;&#31561;&#20808;&#36827;&#25913;&#36827;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LRPE&#19982;&#25351;&#25968;&#34928;&#20943;&#32467;&#21512;&#65292;&#26082;&#36991;&#20813;&#20102;&#27880;&#24847;&#21147;&#31232;&#37322;&#38382;&#39064;&#65292;&#21448;&#20351;&#27169;&#22411;&#20445;&#30041;&#20102;&#26631;&#35760;&#20043;&#38388;&#30340;&#20840;&#23616;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lightning Attention&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#32447;&#24615;&#27880;&#24847;&#21147;&#21152;&#36895;&#36229;&#36807;&#20004;&#20493;&#65292;&#24182;&#23558;&#20869;&#23384;&#20351;&#29992;&#37327;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;TransNormer&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#24179;&#28369;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#24352;&#37327;&#24402;&#19968;&#21270;&#26041;&#26696;&#21152;&#36895;&#27169;&#22411;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#37327;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#20013;&#38388;&#20540;&#24182;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#36755;&#20837;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.14988</link><description>&lt;p&gt;
&#22686;&#37327;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#30340;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#37327;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#20013;&#38388;&#20540;&#24182;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#36755;&#20837;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#29992;&#25143;&#36755;&#20837;&#65289;&#26102;&#24120;&#38754;&#20020;&#30528;&#39640;&#25928;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#35745;&#31639;&#26469;&#36866;&#24212;&#36755;&#20837;&#21464;&#21270;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#26469;&#31163;&#25955;&#21270;&#32593;&#32476;&#20013;&#30340;&#20013;&#38388;&#20540;&#65292;&#24182;&#36807;&#28388;&#22122;&#22768;&#21644;&#19981;&#24517;&#35201;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#20462;&#25913;&#65292;&#20174;&#32780;&#20419;&#36827;&#20540;&#30340;&#37325;&#29992;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Transformer&#26550;&#26500;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22686;&#37327;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RRTF&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;PanGu-Coder2&#26159;&#35813;&#26694;&#26550;&#30340;&#23454;&#29616;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#20808;&#21069;&#30340;Code LLMs&#12290;</title><link>http://arxiv.org/abs/2307.14936</link><description>&lt;p&gt;
PanGu-Coder2&#65306;&#21033;&#29992;&#25490;&#21517;&#21453;&#39304;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RRTF&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;PanGu-Coder2&#26159;&#35813;&#26694;&#26550;&#30340;&#23454;&#29616;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#20808;&#21069;&#30340;Code LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#27599;&#21608;&#37117;&#26377;&#26032;&#30340;&#24378;&#22823;&#27169;&#22411;&#21457;&#24067;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#35757;&#32451;&#30340;Code LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#12289;&#25351;&#20196;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RRTF&#65288;Rank Responses to align Test&amp;Teacher Feedback&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#25552;&#21319;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PanGu-Coder2&#65292;&#22312;OpenAI HumanEval&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;62.20%&#30340;&#19968;&#32423;&#36890;&#36807;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;CoderEval&#21644;LeetCode&#22522;&#20934;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PanGu-Coder2&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;Code LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20316;&#32773;&#20889;&#20316;&#39118;&#26684;&#26816;&#27979;&#12290;&#20316;&#32773;&#23558;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#27573;&#33853;&#20043;&#38388;&#30340;&#36807;&#28193;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#29256;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36798;&#21040;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14913</link><description>&lt;p&gt;
PAN 2023&#19978;&#30340;ARC-NLP&#65306;&#22522;&#20110;&#36716;&#25442;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#29992;&#20110;&#20889;&#20316;&#39118;&#26684;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection. (arXiv:2307.14913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20316;&#32773;&#20889;&#20316;&#39118;&#26684;&#26816;&#27979;&#12290;&#20316;&#32773;&#23558;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#27573;&#33853;&#20043;&#38388;&#30340;&#36807;&#28193;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#29256;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36798;&#21040;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20316;&#32773;&#20889;&#20316;&#39118;&#26684;&#26816;&#27979;&#20219;&#21153;&#26088;&#22312;&#21457;&#29616;&#32473;&#23450;&#25991;&#26412;&#20013;&#30340;&#20889;&#20316;&#39118;&#26684;&#21464;&#21270;&#20301;&#32622;&#12290;&#25105;&#20204;&#23558;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#20004;&#20010;&#36830;&#32493;&#27573;&#33853;&#37197;&#23545;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#27573;&#33853;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#22312;&#20219;&#21153;&#20013;&#25130;&#26029;&#36755;&#20837;&#26631;&#35760;&#12290;&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#19981;&#21516;&#32534;&#30721;&#22120;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#39044;&#28909;&#38454;&#27573;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#27169;&#22411;&#29256;&#26412;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#22522;&#32447;&#21644;&#20854;&#20182;&#25552;&#20986;&#30340;&#27169;&#22411;&#29256;&#26412;&#12290;&#23545;&#20110;&#31616;&#21333;&#21644;&#20013;&#31561;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20132;&#20102;&#22522;&#20110;DeBERTa&#21644;&#39044;&#28909;&#35757;&#32451;&#30340;&#22522;&#20110;&#36716;&#25442;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#23545;&#20110;&#22256;&#38590;&#35774;&#32622;&#65292;&#20351;&#29992;&#30456;&#21516;&#27169;&#22411;&#20294;&#26080;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of multi-author writing style detection aims at finding any positions of writing style change in a given text document. We formulate the task as a natural language inference problem where two consecutive paragraphs are paired. Our approach focuses on transitions between paragraphs while truncating input tokens for the task. As backbone models, we employ different Transformer-based encoders with warmup phase during training. We submit the model version that outperforms baselines and other proposed model versions in our experiments. For the easy and medium setups, we submit transition-focused natural language inference based on DeBERTa with warmup training, and the same model without transition for the hard setup.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ARC-NLP&#22312;PAN 2023&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#21270;&#27169;&#22411;&#26469;&#36827;&#34892;&#35302;&#21457;&#35789;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#31881;&#19997;&#23567;&#35828;&#25991;&#26723;&#20013;&#22810;&#20010;&#35302;&#21457;&#35789;&#30340;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;LSTM&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.14912</link><description>&lt;p&gt;
ARC-NLP&#22312;PAN 2023&#19978;&#30340;&#24212;&#29992;&#65306;&#29992;&#20110;&#35302;&#21457;&#35789;&#26816;&#27979;&#30340;&#23618;&#27425;&#21270;&#38271;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ARC-NLP&#22312;PAN 2023&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#21270;&#27169;&#22411;&#26469;&#36827;&#34892;&#35302;&#21457;&#35789;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#31881;&#19997;&#23567;&#35828;&#25991;&#26723;&#20013;&#22810;&#20010;&#35302;&#21457;&#35789;&#30340;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;LSTM&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31881;&#19997;&#23567;&#35828;&#26159;&#19968;&#31181;&#22312;&#24050;&#24314;&#31435;&#30340;&#34394;&#26500;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#21019;&#36896;&#24615;&#20889;&#20316;&#24418;&#24335;&#65292;&#24050;&#32463;&#22312;&#32593;&#32476;&#19978;&#25317;&#26377;&#22823;&#37327;&#30340;&#36861;&#38543;&#32773;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#21442;&#19982;&#32773;&#30340;&#31119;&#31049;&#21644;&#23433;&#20840;&#24050;&#32463;&#25104;&#20026;&#35813;&#31038;&#21306;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35835;&#32773;&#24773;&#24863;&#22256;&#25200;&#25110;&#21019;&#20260;&#30340;&#21050;&#28608;&#24615;&#20869;&#23481;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;PAN CLEF 2023&#30340;&#35302;&#21457;&#35789;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#26816;&#27979;&#32473;&#23450;&#31881;&#19997;&#23567;&#35828;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#35302;&#21457;&#35789;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#36882;&#24402;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38271;&#25991;&#26723;&#25286;&#20998;&#20026;&#36739;&#23567;&#30340;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;Transformer&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#30340;Transformer&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#29992;&#20316;&#22810;&#26631;&#31614;&#35774;&#32622;&#20013;&#29992;&#20110;&#35302;&#21457;&#35789;&#26816;&#27979;&#30340;&#22810;&#20010;LSTM&#27169;&#22411;&#30340;&#35757;&#32451;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;F1-macro sco
&lt;/p&gt;
&lt;p&gt;
Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following. However, ensuring the well-being and safety of participants has become a critical concern in this community. The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge. In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document. For this, we build a hierarchical model that uses recurrence over Transformer-based language models. In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model. Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting. Our model achieves an F1-macro sco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#26816;&#32034;&#26041;&#27861;&#21644;&#35821;&#20041;&#25628;&#32034;&#26469;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;SHAP&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#38598;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#21512;&#27880;&#37322;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#38271;&#26399;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.14899</link><description>&lt;p&gt;
&#20026;&#35299;&#20915;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;&#25991;&#26412;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification. (arXiv:2307.14899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#26816;&#32034;&#26041;&#27861;&#21644;&#35821;&#20041;&#25628;&#32034;&#26469;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;SHAP&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#38598;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#21512;&#27880;&#37322;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#38271;&#26399;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#36873;&#25321;&#19968;&#32452;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#20154;&#21147;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#27880;&#37322;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20108;&#20803;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21363;&#27491;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#27880;&#37322;&#26159;&#22312;&#36739;&#38271;&#30340;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#30340;&#65292;&#21487;&#20197;&#23558;&#35201;&#27880;&#37322;&#30340;&#25991;&#26412;&#25209;&#37327;&#36873;&#25321;&#65292;&#20197;&#21069;&#38754;&#30340;&#27880;&#37322;&#26469;&#25351;&#23548;&#19979;&#19968;&#32452;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;SHAP&#26500;&#24314;Elasticsearch&#21644;&#35821;&#20041;&#25628;&#32034;&#30340;&#39640;&#36136;&#37327;&#26597;&#35810;&#38598;&#65292;&#20197;&#23581;&#35797;&#35782;&#21035;&#19968;&#32452;&#26368;&#20248;&#25991;&#26412;&#26469;&#24110;&#21161;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#25551;&#36848;&#21487;&#33021;&#30340;&#26410;&#26469;&#20107;&#20214;&#30340;&#25552;&#32434;&#25991;&#26412;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35813;&#25991;&#26412;&#38598;&#30001;&#21442;&#19982;&#32933;&#32982;&#21644;&#31958;&#23615;&#30149;&#31649;&#29702;&#30740;&#31350;&#30340;&#21442;&#19982;&#32773;&#26500;&#24314;&#12290;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of selecting of a set of texts for annotation in text classification using retrieval methods when there are limits on the number of annotations due to constraints on human resources. An additional challenge addressed is dealing with binary categories that have a small number of positive instances, reflecting severe class imbalance. In our situation, where annotation occurs over a long time period, the selection of texts to be annotated can be made in batches, with previous annotations guiding the choice of the next set. To address these challenges, the paper proposes leveraging SHAP to construct a quality set of queries for Elasticsearch and semantic search, to try to identify optimal sets of texts for annotation that will help with class imbalance. The approach is tested on sets of cue texts describing possible future events, constructed by participants involved in studies aimed to help with the management of obesity and diabetes. We introduce an effec
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#38598;&#25193;&#23637;&#25968;&#25454;&#38598;MESED&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#30340;&#36127;&#38754;&#23454;&#20307;&#12289;&#21516;&#20041;&#23454;&#20307;&#12289;&#22810;&#20041;&#23454;&#20307;&#21644;&#38271;&#23614;&#23454;&#20307;&#31561;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#38598;&#25104;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#34920;&#31034;&#23454;&#20307;&#65292;&#33021;&#22815;&#25552;&#20379;&#20114;&#34917;&#20449;&#24687;&#12289;&#32479;&#19968;&#20449;&#21495;&#21644;&#24378;&#22823;&#30340;&#23545;&#40784;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.14878</link><description>&lt;p&gt;
MESED&#65306;&#19968;&#20010;&#20855;&#26377;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21644;&#22256;&#38590;&#36127;&#38754;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#38598;&#25193;&#23637;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities. (arXiv:2307.14878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14878
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#38598;&#25193;&#23637;&#25968;&#25454;&#38598;MESED&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#30340;&#36127;&#38754;&#23454;&#20307;&#12289;&#21516;&#20041;&#23454;&#20307;&#12289;&#22810;&#20041;&#23454;&#20307;&#21644;&#38271;&#23614;&#23454;&#20307;&#31561;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#38598;&#25104;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#34920;&#31034;&#23454;&#20307;&#65292;&#33021;&#22815;&#25552;&#20379;&#20114;&#34917;&#20449;&#24687;&#12289;&#32479;&#19968;&#20449;&#21495;&#21644;&#24378;&#22823;&#30340;&#23545;&#40784;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;ESE&#65289;&#20219;&#21153;&#26088;&#22312;&#29992;&#23646;&#20110;&#30456;&#21516;&#35821;&#20041;&#31867;&#30340;&#26032;&#23454;&#20307;&#25193;&#23637;&#19968;&#23567;&#37096;&#20998;&#31181;&#23376;&#23454;&#20307;&#12290;&#20256;&#32479;&#30340;ESE&#26041;&#27861;&#22522;&#20110;&#21333;&#19968;&#27169;&#24577;&#65288;&#21363;&#25991;&#23383;&#27169;&#24577;&#65289;&#65292;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22797;&#26434;&#23454;&#20307;&#65292;&#22914;&#65306;&#65288;1&#65289;&#20855;&#26377;&#32454;&#31890;&#24230;&#35821;&#20041;&#24046;&#24322;&#30340;&#36127;&#38754;&#23454;&#20307;&#12290; &#65288;2&#65289;&#21516;&#20041;&#23454;&#20307;&#12290; &#65288;3&#65289;&#22810;&#20041;&#23454;&#20307;&#12290; &#65288;4&#65289;&#38271;&#23614;&#23454;&#20307;&#12290;&#36825;&#20123;&#25361;&#25112;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;MESE&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#38598;&#25104;&#20102;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#26469;&#34920;&#31034;&#23454;&#20307;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;ESE&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#65306;&#65288;1&#65289;&#19981;&#21516;&#30340;&#27169;&#24577;&#21487;&#20197;&#25552;&#20379;&#20114;&#34917;&#30340;&#20449;&#24687;&#12290;&#65288;2&#65289;&#22810;&#27169;&#24577;&#20449;&#24687;&#36890;&#36807;&#20849;&#21516;&#30340;&#35270;&#35273;&#29305;&#24615;&#25552;&#20379;&#20102;&#21516;&#19968;&#20010;&#35821;&#20041;&#31867;&#25110;&#23454;&#20307;&#30340;&#32479;&#19968;&#20449;&#21495;&#12290; &#65288;3&#65289;&#22810;&#27169;&#24577;&#20449;&#24687;&#20026;&#21516;&#20041;&#23454;&#20307;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23545;&#40784;&#20449;&#21495;&#12290;&#20026;&#20102;&#35780;&#20272;MESE&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Entity Set Expansion (ESE) task aims to expand a handful of seed entities with new entities belonging to the same semantic class. Conventional ESE methods are based on mono-modality (i.e., literal modality), which struggle to deal with complex entities in the real world such as: (1) Negative entities with fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous entities. (4) Long-tailed entities. These challenges prompt us to propose Multi-modal Entity Set Expansion (MESE), where models integrate information from multiple modalities to represent entities. Intuitively, the benefits of multi-modal information for ESE are threefold: (1) Different modalities can provide complementary information. (2) Multi-modal information provides a unified signal via common visual properties for the same semantic class or entity. (3) Multi-modal information offers robust alignment signal for synonymous entities. To assess the performance of model in MESE and facilitate further rese
&lt;/p&gt;</description></item><item><title>Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.14856</link><description>&lt;p&gt;
&#21457;&#25381;Seq2Seq&#27169;&#22411;&#20316;&#20026;&#31283;&#20581;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14856
&lt;/p&gt;
&lt;p&gt;
Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#21482;&#26377;&#35299;&#30721;&#22120;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#32780;&#32534;&#30721;-&#35299;&#30721;&#65288;&#21363;Seq2Seq&#65289;&#27169;&#22411;&#22312;&#20381;&#36182;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;Seq2Seq&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#36825;&#20165;&#38480;&#20110;&#19982;Seq2Seq&#20307;&#31995;&#32467;&#26500;&#30456;&#21305;&#37197;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#21644;&#32763;&#35793;&#12290;&#21463;&#21040;&#36825;&#20123;&#21021;&#22987;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26356;&#26377;&#25928;&#22320;&#24341;&#21457;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65306;&#30446;&#26631;&#23545;&#40784;&#25552;&#31034;&#21644;&#22522;&#20110;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#19968;&#20010;&#20307;&#31215;&#26159;&#20854;&#20845;&#20493;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#24120;&#35268;Seq2Seq&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a var
&lt;/p&gt;</description></item><item><title>ArcGPT&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#26723;&#26696;&#39046;&#22495;&#23450;&#21046;&#30340;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#26723;&#26696;&#39046;&#22495;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ArcGPT&#22312;&#30495;&#23454;&#26723;&#26696;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26631;&#24535;&#30528;&#26377;&#25928;&#26723;&#26696;&#25968;&#25454;&#31649;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2307.14852</link><description>&lt;p&gt;
ArcGPT&#65306;&#19968;&#20010;&#19987;&#20026;&#29616;&#23454;&#19990;&#30028;&#26723;&#26696;&#24212;&#29992;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArcGPT: A Large Language Model Tailored for Real-world Archival Applications. (arXiv:2307.14852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14852
&lt;/p&gt;
&lt;p&gt;
ArcGPT&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#26723;&#26696;&#39046;&#22495;&#23450;&#21046;&#30340;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#26723;&#26696;&#39046;&#22495;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ArcGPT&#22312;&#30495;&#23454;&#26723;&#26696;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26631;&#24535;&#30528;&#26377;&#25928;&#26723;&#26696;&#25968;&#25454;&#31649;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26723;&#26696;&#22312;&#20449;&#24687;&#21644;&#30693;&#35782;&#20445;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#38656;&#35201;&#39640;&#25928;&#21644;&#33258;&#21160;&#21270;&#30340;&#24037;&#20855;&#26469;&#31649;&#29702;&#21644;&#21033;&#29992;&#26723;&#26696;&#20449;&#24687;&#36164;&#28304;&#12290;&#26723;&#26696;&#24212;&#29992;&#28041;&#21450;&#31649;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#38590;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36824;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#38754;&#21521;&#26723;&#26696;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ArcGPT&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#26723;&#26696;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#22312;&#30495;&#23454;&#26723;&#26696;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;ArcGPT&#22312;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#30340;&#26723;&#26696;&#39046;&#22495;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38500;ArcGPT&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;AMBLE&#65292;&#19968;&#20010;&#21253;&#21547;&#22235;&#20010;&#30495;&#23454;&#26723;&#26696;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;&#22312;AMBLE&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ArcGPT&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#26377;&#25928;&#26723;&#26696;&#25968;&#25454;&#31649;&#29702;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26368;&#32456;&#65292;ArcGPT&#26088;&#22312;&#26356;&#22909;&#22320;&#26381;&#21153;&#26723;&#26696;&#30028;&#65292;&#21161;&#21147;&#25913;&#21892;&#26723;&#26696;&#25968;&#25454;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Archives play a crucial role in preserving information and knowledge, and the exponential growth of such data necessitates efficient and automated tools for managing and utilizing archive information resources. Archival applications involve managing massive data that are challenging to process and analyze. Although LLMs have made remarkable progress in diverse domains, there are no publicly available archives tailored LLM. Addressing this gap, we introduce ArcGPT, to our knowledge, the first general-purpose LLM tailored to the archival field. To enhance model performance on real-world archival tasks, ArcGPT has been pre-trained on massive and extensive archival domain data. Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing state-of-the-art models, marking a substantial step forward in effective archival data management. Ultimately, ArcGPT aims to better serve the archival community, aidi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14850</link><description>&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;Native Language Identification&#65292;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#12290;NLI &#26159;&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;NLI&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#33521;&#35821;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#20854;&#33539;&#22260;&#25193;&#23637;&#21040;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#26500;&#24314;&#30340;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#65288;CFG &#20135;&#29983;&#35268;&#21017;&#65292;&#35789;&#24615;n-gram&#21644;&#20989;&#25968;&#35789;&#65289;&#19982;L2&#25991;&#26412;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22909;&#30340;&#25913;&#20889;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#65292;&#26159;&#21542;&#21487;&#20197;&#20165;&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#26469;&#35780;&#20272;&#25913;&#20889;&#30340;&#36136;&#37327;&#12290;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21644;&#19987;&#23478;&#35780;&#20272;&#24471;&#20986;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.14818</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19968;&#20010;&#22909;&#30340;&#25913;&#20889;&#65306;&#33258;&#21160;&#35780;&#20272;&#26159;&#21542;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes a Good Paraphrase: Do Automated Evaluations Work?. (arXiv:2307.14818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22909;&#30340;&#25913;&#20889;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#65292;&#26159;&#21542;&#21487;&#20197;&#20165;&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#26469;&#35780;&#20272;&#25913;&#20889;&#30340;&#36136;&#37327;&#12290;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21644;&#19987;&#23478;&#35780;&#20272;&#24471;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#26159;&#29992;&#19981;&#21516;&#30340;&#35789;&#35821;&#34920;&#36798;&#19968;&#20010;&#37325;&#35201;&#24605;&#24819;&#25110;&#21547;&#20041;&#30340;&#20219;&#21153;&#12290;&#20294;&#26159;&#20026;&#20102;&#34987;&#35748;&#20026;&#26159;&#21487;&#25509;&#21463;&#30340;&#25913;&#20889;&#65292;&#36825;&#20123;&#35789;&#35821;&#24212;&#35813;&#26377;&#22810;&#20040;&#19981;&#21516;&#65311;&#25105;&#20204;&#21487;&#20197;&#20165;&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#25913;&#20889;&#30340;&#36136;&#37327;&#21527;&#65311;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#20010;&#24503;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#33258;&#21160;&#21644;&#19987;&#23478;&#35821;&#35328;&#35780;&#20272;&#26469;&#23581;&#35797;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing is the task of expressing an essential idea or meaning in different words. But how different should the words be in order to be considered an acceptable paraphrase? And can we exclusively use automated metrics to evaluate the quality of a paraphrase? We attempt to answer these questions by conducting experiments on a German data set and performing automatic and expert linguistic evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30446;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#21442;&#32771;&#29983;&#25104;&#27169;&#22411;&#65292;&#21457;&#29616;GREC&#19981;&#20877;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#24182;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14817</link><description>&lt;p&gt;
&#21442;&#32771;&#29983;&#25104;&#27169;&#22411;&#65306;&#23427;&#20204;&#26159;&#21542;&#32463;&#24471;&#36215;&#26102;&#38388;&#30340;&#32771;&#39564;&#65311;
&lt;/p&gt;
&lt;p&gt;
Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30446;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#21442;&#32771;&#29983;&#25104;&#27169;&#22411;&#65292;&#21457;&#29616;GREC&#19981;&#20877;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#24182;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20165;&#20851;&#27880;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#26412;&#25991;&#20174;&#35821;&#35328;&#21644;&#31185;&#23398;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#29983;&#25104;&#25351;&#20195;&#34920;&#36798;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20197;GREC&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;GREC&#26159;&#21313;&#22810;&#24180;&#21069;&#20851;&#20110;&#35813;&#20027;&#39064;&#30340;&#19968;&#31995;&#21015;&#33521;&#35821;&#20849;&#20139;&#20219;&#21153;&#30340;&#32508;&#21512;&#38598;&#21512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#26524;&#25105;&#20204;&#20197;&#26356;&#29616;&#23454;&#30340;&#25968;&#25454;&#38598;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#20250;&#22914;&#20309;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#29305;&#24449;&#36873;&#25321;&#23454;&#39564;&#26469;&#27979;&#35797;&#27169;&#22411;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#36873;&#25321;&#23545;&#32467;&#26524;&#20135;&#29983;&#20102;&#24456;&#22823;&#24433;&#21709;&#65292;GREC&#19981;&#20877;&#34987;&#35270;&#20026;&#21487;&#38752;&#35780;&#20272;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#21442;&#32771;&#29983;&#25104;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#30340;&#36873;&#25321;&#19981;&#22826;&#20381;&#36182;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, many NLP studies have focused solely on performance improvement. In this work, we focus on the linguistic and scientific aspects of NLP. We use the task of generating referring expressions in context (REG-in-context) as a case study and start our analysis from GREC, a comprehensive set of shared tasks in English that addressed this topic over a decade ago. We ask what the performance of models would be if we assessed them (1) on more realistic datasets, and (2) using more advanced methods. We test the models using different evaluation metrics and feature selection experiments. We conclude that GREC can no longer be regarded as offering a reliable assessment of models' ability to mimic human reference production, because the results are highly impacted by the choice of corpus and evaluation metrics. Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#27169;&#22411;&#25552;&#21319;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31471;&#21040;&#31471;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#27169;&#22411;&#26377;&#25928;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#33521;&#35821;&#21644;&#25463;&#20811;&#35821;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14785</link><description>&lt;p&gt;
&#25552;&#39640;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#31471;&#21040;&#31471;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model. (arXiv:2307.14785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#27169;&#22411;&#25552;&#21319;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31471;&#21040;&#31471;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#27169;&#22411;&#26377;&#25928;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#33521;&#35821;&#21644;&#25463;&#20811;&#35821;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20174;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#25552;&#39640;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#33719;Transformer&#38544;&#34255;&#29366;&#24577;&#20013;&#30340;&#22823;&#37096;&#20998;&#32467;&#26500;&#21270;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#25105;&#20204;&#26032;&#25552;&#20986;&#30340;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;ELECTRA-small&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#25463;&#20811;&#35821;&#20004;&#31181;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32452;&#21512;&#27169;&#22411;&#22312;&#20004;&#31181;&#35821;&#35328;&#30340;ABSA&#24615;&#33021;&#19978;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25463;&#20811;&#35821;ABSA&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a series of approaches aimed at enhancing the performance of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic information from a Semantic Role Labeling (SRL) model. We propose a novel end-to-end Semantic Role Labeling model that effectively captures most of the structured semantic information within the Transformer hidden state. We believe that this end-to-end model is well-suited for our newly proposed models that incorporate semantic information. We evaluate the proposed models in two languages, English and Czech, employing ELECTRA-small models. Our combined models improve ABSA performance in both languages. Moreover, we achieved new state-of-the-art results on the Czech ABSA.
&lt;/p&gt;</description></item><item><title>Emotion4MIDI&#26159;&#19968;&#20010;&#21253;&#21547;12k&#20010;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#26469;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14783</link><description>&lt;p&gt;
Emotion4MIDI&#65306;&#22522;&#20110;&#27468;&#35789;&#30340;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14783
&lt;/p&gt;
&lt;p&gt;
Emotion4MIDI&#26159;&#19968;&#20010;&#21253;&#21547;12k&#20010;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#26469;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#24102;&#24773;&#24863;&#26631;&#31614;&#30340;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12k&#20010;MIDI&#27468;&#26354;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#21322;&#22823;&#23567;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;MIDI&#25968;&#25454;&#38598;&#30340;&#27468;&#35789;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#65292;&#20026;&#25506;&#32034;&#38899;&#20048;&#21644;&#24773;&#24863;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23588;&#20854;&#26159;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#24863;&#29983;&#25104;&#38899;&#20048;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#21487;&#20197;&#22312;&#32447;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Whisper&#30340;&#23454;&#26102;&#35821;&#38899;&#36716;&#24405;&#21644;&#32763;&#35793;&#31995;&#32479;Whisper-Streaming&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21327;&#35758;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#27969;&#24335;&#36716;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Whisper-Streaming&#22312;&#26410;&#20998;&#21106;&#30340;&#38271;&#31687;&#28436;&#35762;&#36716;&#24405;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#20013;&#30340;&#24212;&#29992;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14743</link><description>&lt;p&gt;
&#23558;Whisper&#36716;&#21270;&#20026;&#23454;&#26102;&#36716;&#24405;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Whisper&#30340;&#23454;&#26102;&#35821;&#38899;&#36716;&#24405;&#21644;&#32763;&#35793;&#31995;&#32479;Whisper-Streaming&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21327;&#35758;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#27969;&#24335;&#36716;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Whisper-Streaming&#22312;&#26410;&#20998;&#21106;&#30340;&#38271;&#31687;&#28436;&#35762;&#36716;&#24405;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#20013;&#30340;&#24212;&#29992;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#26368;&#36817;&#30340;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#23427;&#24182;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#36716;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Whisper&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;Whisper-Streaming&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#35821;&#38899;&#36716;&#24405;&#21644;&#32763;&#35793;&#23454;&#29616;Whisper&#31867;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;Whisper-Streaming&#20351;&#29992;&#26412;&#22320;&#21327;&#35758;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#27969;&#24335;&#36716;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Whisper-Streaming&#22312;&#26410;&#20998;&#21106;&#30340;&#38271;&#31687;&#28436;&#35762;&#36716;&#24405;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;&#39640;&#36136;&#37327;&#21644;3.3&#31186;&#30340;&#24310;&#36831;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#19968;&#20010;&#22810;&#35821;&#31181;&#20250;&#35758;&#23454;&#26102;&#36716;&#24405;&#26381;&#21153;&#32452;&#20214;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is one of the recent state-of-the-art multilingual speech recognition and translation models, however, it is not designed for real time transcription. In this paper, we build on top of Whisper and create Whisper-Streaming, an implementation of real-time speech transcription and translation of Whisper-like models. Whisper-Streaming uses local agreement policy with self-adaptive latency to enable streaming transcription. We show that Whisper-Streaming achieves high quality and 3.3 seconds latency on unsegmented long-form speech transcription test set, and we demonstrate its robustness and practical usability as a component in live transcription service at a multilingual conference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#25991;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#24182;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#22256;&#38590;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#23481;&#26131;&#29983;&#25104;&#24102;&#26377;&#24187;&#35273;&#25110;&#26080;&#20851;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.14712</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#25991;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#25991;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#24182;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#22256;&#38590;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#23481;&#26131;&#29983;&#25104;&#24102;&#26377;&#24187;&#35273;&#25110;&#26080;&#20851;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#25991;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;LLM&#30340;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36164;&#28304;&#21644;&#27880;&#37322;&#24037;&#20316;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22270;&#25968;&#25454;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3&#21644;ChatGPT&#22312;&#20004;&#20010;&#22270;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#24494;&#35843;&#30340;LLM&#27169;&#22411;&#65288;&#22914;T5&#21644;BART&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#24182;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#22312;AGENDA&#21644;WebNLG&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;10.57&#21644;11.08&#30340;BLEU&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;&#19988;&#23427;&#20204;&#36824;&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#24187;&#35273;&#25110;&#26080;&#20851;&#20449;&#24687;&#30340;&#25991;&#26412;&#12290;&#20316;&#20026;&#38169;&#35823;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;BERT&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#23439;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#35821;&#35328;&#23398;&#30693;&#35782;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#31454;&#20105;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.14666</link><description>&lt;p&gt;
&#25913;&#36827;&#22522;&#20110;Transformer&#27169;&#22411;&#21644;&#35821;&#35328;&#23398;&#39044;&#35757;&#32451;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#35821;&#35328;&#23398;&#30693;&#35782;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#31454;&#20105;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#38463;&#25289;&#20271;&#25991;&#26412;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#30683;&#30462;&#26816;&#27979;&#12290;&#38463;&#25289;&#20271;&#35821;&#34987;&#35748;&#20026;&#26159;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#24847;&#21619;&#30528;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#23548;&#33268;NLP&#26041;&#27861;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#20844;&#24320;&#21487;&#29992;&#36164;&#28304;&#20013;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#24212;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#35821;&#35328;&#23398;&#30693;&#35782;&#39044;&#35757;&#32451;&#26041;&#27861;&#26102;&#65292;&#19968;&#31181;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#27169;&#22411;&#65288;AraBERT&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#20013;&#36825;&#19968;&#20219;&#21153;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#20063;&#26159;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#22312;&#27492;&#29615;&#22659;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the classification of Arabic text data in the field of Natural Language Processing (NLP), with a particular focus on Natural Language Inference (NLI) and Contradiction Detection (CD). Arabic is considered a resource-poor language, meaning that there are few data sets available, which leads to limited availability of NLP methods. To overcome this limitation, we create a dedicated data set from publicly available resources. Subsequently, transformer-based machine learning models are being trained and evaluated. We find that a language-specific model (AraBERT) performs competitively with state-of-the-art multilingual approaches, when we apply linguistically informed pre-training methods such as Named Entity Recognition (NER). To our knowledge, this is the first large-scale evaluation for this task in Arabic, as well as the first application of multi-task pre-training in this context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MBL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39030;&#32423;SARI&#24471;&#20998;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#22312;&#36739;&#22823;&#22411;&#30340;GPT&#27169;&#22411;&#19978;&#33719;&#24471;&#26368;&#20339;&#34920;&#29616;&#12290;&#32780;&#22312;&#36739;&#23567;&#22411;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36739;&#39640;&#21387;&#32553;&#27604;&#20363;&#30340;&#31034;&#20363;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.14632</link><description>&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#25991;&#26412;&#31616;&#21270;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MBL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39030;&#32423;SARI&#24471;&#20998;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#22312;&#36739;&#22823;&#22411;&#30340;GPT&#27169;&#22411;&#19978;&#33719;&#24471;&#26368;&#20339;&#34920;&#29616;&#12290;&#32780;&#22312;&#36739;&#23567;&#22411;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36739;&#39640;&#21387;&#32553;&#27604;&#20363;&#30340;&#31034;&#20363;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#36873;&#25321;ICL&#31034;&#20363;&#30340;&#26368;&#20339;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#32467;&#26524;&#21487;&#20197;&#22240;&#20351;&#29992;&#30340;&#31034;&#20363;&#30340;&#36136;&#37327;&#12289;&#25968;&#37327;&#21644;&#39034;&#24207;&#32780;&#21464;&#21270;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110;&#25991;&#26412;&#31616;&#21270;&#65288;TS&#65289;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#25506;&#35752;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#21644;&#26368;&#20581;&#22766;&#30340;ICL&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#29992;&#30340;TS&#24230;&#37327;&#65288;&#22914;SARI&#12289;&#21387;&#32553;&#27604;&#20363;&#21644;BERT-Precision&#65289;&#36827;&#34892;&#36873;&#25321;&#30340;&#22522;&#20110;&#24230;&#37327;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;MBL&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#26631;&#20934;&#30340;TS&#22522;&#20934;&#65288;&#22914;TurkCorpus&#21644;ASSET&#65289;&#19978;&#20351;&#29992;&#21508;&#31181;&#35268;&#27169;&#30340;GPT&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-175B&#65289;&#19978;&#36873;&#25321;&#30340;&#31034;&#20363;&#36890;&#36807;&#39030;&#32423;SARI&#24471;&#20998;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#21387;&#32553;&#27604;&#20363;&#36890;&#24120;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-13B&#21644;GPT-6.7B&#65289;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;MBL&#36890;&#24120;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#24739;&#26377;ADHD&#12289;&#35829;&#35835;&#22256;&#38590;&#25110;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;&#26356;&#39640;&#25928;&#22320;&#28040;&#21270;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#31639;&#27861;&#21644;T5&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#22788;&#29702;&#21644;&#25688;&#35201;&#20219;&#21153;&#23454;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#24037;&#20855;&#36824;&#24212;&#29992;&#20102;&#20223;&#29983;&#38405;&#35835;&#21407;&#21017;&#26469;&#22686;&#24378;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14544</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#65292;&#29992;&#20110;&#36741;&#21161;&#24739;&#26377;ADHD&#12289;&#35829;&#35835;&#22256;&#38590;&#25110;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;
&lt;/p&gt;
&lt;p&gt;
Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#24739;&#26377;ADHD&#12289;&#35829;&#35835;&#22256;&#38590;&#25110;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;&#26356;&#39640;&#25928;&#22320;&#28040;&#21270;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#31639;&#27861;&#21644;T5&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#22788;&#29702;&#21644;&#25688;&#35201;&#20219;&#21153;&#23454;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#24037;&#20855;&#36824;&#24212;&#29992;&#20102;&#20223;&#29983;&#38405;&#35835;&#21407;&#21017;&#26469;&#22686;&#24378;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#24739;&#26377;&#35829;&#35835;&#22256;&#38590;&#12289;ADHD&#21644;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;&#30340;&#23398;&#29983;&#26356;&#39640;&#25928;&#22320;&#28040;&#21270;&#20219;&#20309;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#31639;&#27861;&#36827;&#34892;&#22797;&#26434;&#25991;&#26412;&#22788;&#29702;&#21644;&#25688;&#35201;&#20219;&#21153;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;Hugging Face&#30340;T5&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#22120;&#65289;&#27169;&#22411;&#65292;&#23558;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35270;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#27169;&#22411;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;NLTK&#30340;Punkt&#21477;&#23376;&#20998;&#35789;&#22120;&#29992;&#20110;&#23558;&#25991;&#26412;&#20998;&#21106;&#20026;&#21477;&#23376;&#21015;&#34920;&#12290;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#36731;&#37327;&#32423;Web&#26381;&#21153;&#22120;&#21644;&#26694;&#26550;Flask&#25552;&#20379;&#12290;&#35813;&#24037;&#20855;&#36824;&#24212;&#29992;&#20102;&#20223;&#29983;&#38405;&#35835;&#30340;&#21407;&#21017;&#26469;&#22686;&#24378;&#21487;&#35835;&#24615;&#65292;&#21253;&#25324;&#21152;&#31895;&#21151;&#33021;&#21644;&#23545;&#34892;&#12289;&#21333;&#35789;&#21644;&#23383;&#31526;&#38388;&#36317;&#30340;&#35843;&#25972;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36895;&#35835;&#24037;&#20855;&#30340;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to assist students with dyslexia, ADHD, and short attention span in digesting any text-based information more efficiently. The proposed solution utilizes the Multilayer Perceptron (MLP) algorithm for complex text processing and summarization tasks. The tool leverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face, which treats every NLP task as a text generation task. The model is fine-tuned on specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer is used to divide a text into a list of sentences. The application is served using Flask, a lightweight web server and framework. The tool also applies principles from Bionic Reading to enhance readability, which includes a bolding function and adjustments to line, word, and character spacing. The paper discusses the methodology, implementation, and results of the AI-based speed reading tool.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#25506;&#35752;&#20102;&#23558;&#29616;&#25104;&#32452;&#20214;&#25554;&#20837;&#22810;&#27169;&#22411;&#27169;&#22411;&#20013;&#30340;&#28431;&#27934;&#21644;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#22810;&#27169;&#22411;&#31995;&#32479;&#26435;&#37325;&#21644;&#21442;&#25968;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#20301;&#20110;&#39044;&#35757;&#32451;&#32452;&#20214;&#23884;&#20837;&#31354;&#38388;&#21361;&#38505;&#21306;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.14539</link><description>&lt;p&gt;
&#25554;&#20837;&#24182;&#31048;&#31095;&#65306;&#21033;&#29992;&#22810;&#27169;&#22411;&#27169;&#22411;&#30340;&#29616;&#25104;&#32452;&#20214;&#36827;&#34892;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#25506;&#35752;&#20102;&#23558;&#29616;&#25104;&#32452;&#20214;&#25554;&#20837;&#22810;&#27169;&#22411;&#27169;&#22411;&#20013;&#30340;&#28431;&#27934;&#21644;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#22810;&#27169;&#22411;&#31995;&#32479;&#26435;&#37325;&#21644;&#21442;&#25968;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#20301;&#20110;&#39044;&#35757;&#32451;&#32452;&#20214;&#23884;&#20837;&#31354;&#38388;&#21361;&#38505;&#21306;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39069;&#22806;&#30340;&#27169;&#24577;&#65288;&#22914;&#35270;&#35273;&#65289;&#21152;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#26085;&#30410;&#21463;&#27426;&#36814;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#31181;&#27169;&#24577;&#30340;&#25193;&#23637;&#31867;&#20284;&#20110;&#22312;&#25151;&#23376;&#19978;&#22686;&#21152;&#26356;&#22810;&#30340;&#38376;&#65292;&#26080;&#24847;&#20013;&#20026;&#23545;&#25239;&#24615;&#25915;&#20987;&#21019;&#24314;&#20102;&#22810;&#20010;&#35775;&#38382;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#65292;&#25105;&#20204;&#24378;&#35843;&#22810;&#27169;&#22411;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#65292;&#36825;&#20123;&#28431;&#27934;&#28304;&#20110;&#20197;&#25554;&#25300;&#26041;&#24335;&#23558;&#29616;&#25104;&#32452;&#20214;&#65288;&#22914;&#20844;&#20849;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65289;&#24341;&#20837;&#36825;&#20123;&#31995;&#32479;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#22810;&#27169;&#22411;&#31995;&#32479;&#30340;&#26435;&#37325;&#25110;&#21442;&#25968;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#36825;&#20123;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#24222;&#22823;&#19988;&#26410;&#23436;&#20840;&#24320;&#21457;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23884;&#20837;&#31354;&#38388;&#25915;&#20987;&#26159;&#36890;&#36807;&#23547;&#25214;&#20301;&#20110;&#36825;&#20123;&#39044;&#35757;&#32451;&#32452;&#20214;&#30340;&#24191;&#27867;&#23884;&#20837;&#31354;&#38388;&#30340;&#21361;&#38505;&#25110;&#30446;&#26631;&#21306;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#26469;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial 
&lt;/p&gt;</description></item><item><title>CliniDigest&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#24037;&#20855;&#65292;&#33021;&#22815;&#23454;&#26102;&#12289;&#30495;&#23454;&#21644;&#20840;&#38754;&#22320;&#23558;&#38271;&#31687;&#35797;&#39564;&#25551;&#36848;&#21387;&#32553;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.14522</link><description>&lt;p&gt;
CliniDigest: &#22522;&#20110;&#22823;&#35268;&#27169;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14522
&lt;/p&gt;
&lt;p&gt;
CliniDigest&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#24037;&#20855;&#65292;&#33021;&#22815;&#23454;&#26102;&#12289;&#30495;&#23454;&#21644;&#20840;&#38754;&#22320;&#23558;&#38271;&#31687;&#35797;&#39564;&#25551;&#36848;&#21387;&#32553;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#35780;&#20272;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#24178;&#39044;&#25514;&#26045;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#35774;&#35745;&#26032;&#30340;&#35797;&#39564;&#65292;&#30740;&#31350;&#20154;&#21592;&#20174;&#24403;&#21069;&#21644;&#24050;&#23436;&#25104;&#30340;&#35797;&#39564;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;2022&#24180;&#65292;&#27599;&#22825;&#24179;&#22343;&#26377;100&#22810;&#20010;&#20020;&#24202;&#35797;&#39564;&#25552;&#20132;&#21040;ClinicalTrials.gov&#65292;&#27599;&#20010;&#35797;&#39564;&#24179;&#22343;&#26377;&#32422;1500&#20010;&#21333;&#35789;&#12290;&#36825;&#20960;&#20046;&#19981;&#21487;&#33021;&#21450;&#26102;&#36319;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21019;&#24314;&#20102;&#19968;&#20010;&#25209;&#37327;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#24037;&#20855;CliniDigest&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CliniDigest&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23454;&#26102;&#12289;&#30495;&#23454;&#21644;&#20840;&#38754;&#30340;&#20020;&#24202;&#35797;&#39564;&#25688;&#35201;&#30340;&#24037;&#20855;&#12290;CliniDigest&#21487;&#20197;&#23558;&#22810;&#36798;85&#20010;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#65288;&#32422;10500&#20010;&#21333;&#35789;&#65289;&#32553;&#20943;&#20026;&#19968;&#20010;&#31616;&#27905;&#30340;200&#20010;&#35789;&#30340;&#25688;&#35201;&#65292;&#24102;&#26377;&#21442;&#32771;&#25991;&#29486;&#21644;&#26377;&#38480;&#30340;&#34394;&#26500;&#20869;&#23481;&#12290;&#25105;&#20204;&#24050;&#32463;&#27979;&#35797;&#20102;CliniDigest&#22312;27&#20010;&#21307;&#23398;&#23376;&#39046;&#22495;&#20013;&#28041;&#21450;&#30340;457&#20010;&#35797;&#39564;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#65292;CliniDigest&#29983;&#25104;&#30340;&#25688;&#35201;&#24179;&#22343;&#20026;153&#20010;&#21333;&#35789;&#65292;&#26631;&#20934;&#24046;&#20026;69&#20010;&#21333;&#35789;&#65292;&#20854;&#20013;&#27599;&#20010;&#25688;&#35201;&#24179;&#22343;&#20351;&#29992;54&#20010;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
A clinical trial is a study that evaluates new biomedical interventions. To design new trials, researchers draw inspiration from those current and completed. In 2022, there were on average more than 100 clinical trials submitted to ClinicalTrials.gov every day, with each trial having a mean of approximately 1500 words [1]. This makes it nearly impossible to keep up to date. To mitigate this issue, we have created a batch clinical trial summarizer called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first tool able to provide real-time, truthful, and comprehensive summaries of clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references and limited hallucinations. We have tested CliniDigest on its ability to summarize 457 trials divided across 27 medical subdomains. For each field, CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which utilizes $\mu=54\
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35748;&#30693;&#20559;&#24046;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#20934;&#30830;&#20195;&#34920;&#26680;&#24515;&#24605;&#24819;&#12289;&#26131;&#20110;&#29702;&#35299;&#12289;&#33021;&#22815;&#24341;&#21457;&#24773;&#24863;&#21453;&#24212;&#24182;&#19988;&#24120;&#35265;&#30340;&#21516;&#20041;&#35789;&#33021;&#22815;&#20419;&#36827;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14511</link><description>&lt;p&gt;
&#35789;&#35821;&#30340;&#30041;&#23384;&#65306;&#20351;&#29992;&#35748;&#30693;&#20559;&#24046;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#39044;&#27979;&#20915;&#31574;&#21644;&#21516;&#20041;&#35789;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics. (arXiv:2307.14511v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14511
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35748;&#30693;&#20559;&#24046;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#20934;&#30830;&#20195;&#34920;&#26680;&#24515;&#24605;&#24819;&#12289;&#26131;&#20110;&#29702;&#35299;&#12289;&#33021;&#22815;&#24341;&#21457;&#24773;&#24863;&#21453;&#24212;&#24182;&#19988;&#24120;&#35265;&#30340;&#21516;&#20041;&#35789;&#33021;&#22815;&#20419;&#36827;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20511;&#37492;&#20102;&#35748;&#30693;&#24515;&#29702;&#23398;&#21644;&#20449;&#24687;&#31995;&#32479;&#30740;&#31350;&#30340;&#25104;&#26524;&#65292;&#26088;&#22312;&#39044;&#27979;&#25968;&#23383;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#21442;&#19982;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#35748;&#30693;&#20559;&#24046;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#29992;&#25143;&#19982;&#25968;&#23383;&#20869;&#23481;&#20013;&#30340;&#21516;&#20041;&#35789;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20195;&#34920;&#24615;&#12289;&#26131;&#29992;&#24615;&#12289;&#24773;&#24863;&#21644;&#20998;&#24067;&#36825;&#22235;&#31181;&#35748;&#30693;&#20559;&#24046;&#34701;&#21512;&#21040;&#20102;READ&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#29992;&#25143;&#35843;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20934;&#30830;&#20195;&#34920;&#26680;&#24515;&#24605;&#24819;&#12289;&#26131;&#20110;&#29702;&#35299;&#12289;&#33021;&#22815;&#24341;&#21457;&#24773;&#24863;&#21453;&#24212;&#24182;&#19988;&#24120;&#35265;&#30340;&#21516;&#20041;&#35789;&#33021;&#22815;&#20419;&#36827;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20154;&#26426;&#20132;&#20114;&#12289;&#25968;&#23383;&#34892;&#20026;&#21644;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#35748;&#30693;&#20559;&#24046;&#20316;&#20026;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26377;&#21147;&#25351;&#26631;&#30340;&#28508;&#21147;&#65292;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#35774;&#35745;&#26377;&#25928;&#30340;&#25968;&#23383;&#24179;&#21488;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research draws upon cognitive psychology and information systems studies to anticipate user engagement and decision-making on digital platforms. By employing natural language processing (NLP) techniques and insights from cognitive bias research, we delve into user interactions with synonyms within digital content. Our methodology synthesizes four cognitive biasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ model. Through a comprehensive user survey, we assess the model's ability to predict user engagement, discovering that synonyms that accurately represent core ideas, are easy to understand, elicit emotional responses, and are commonly encountered, promote greater user engagement. Crucially, our work offers a fresh lens on human-computer interaction, digital behaviors, and decision-making processes. Our results highlight the promise of cognitive biases as potent indicators of user engagement, underscoring their significance in designing effective digital
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#30340;&#26032;&#27169;&#22411;READ&#65292;&#36890;&#36807;&#34701;&#21512;&#35748;&#30693;&#20559;&#24046;&#12289;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#25991;&#23383;&#30340;&#21442;&#19982;&#27700;&#24179;&#65292;&#24182;&#21306;&#20998;&#20102;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#27719;&#12290;</title><link>http://arxiv.org/abs/2307.14500</link><description>&lt;p&gt;
&#19968;&#20010;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#30340;&#39044;&#27979;&#27169;&#22411;&#65306;&#36890;&#36807;&#34701;&#21512;&#35748;&#30693;&#20559;&#24046;&#12289;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#29992;&#25143;&#23545;&#33521;&#25991;&#23383;&#30340;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing. (arXiv:2307.14500v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#30340;&#26032;&#27169;&#22411;READ&#65292;&#36890;&#36807;&#34701;&#21512;&#35748;&#30693;&#20559;&#24046;&#12289;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#25991;&#23383;&#30340;&#21442;&#19982;&#27700;&#24179;&#65292;&#24182;&#21306;&#20998;&#20102;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#32463;&#36807;&#23454;&#35777;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#20449;&#24687;&#21442;&#19982;&#65288;IE&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;READ&#27169;&#22411;&#65292;&#20854;&#20195;&#34920;&#20102;&#21442;&#19982;&#24230;&#39640;&#30340;&#20449;&#24687;&#30340;&#22235;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#20195;&#34920;&#24615;&#12289;&#26131;&#29992;&#24615;&#12289;&#24773;&#24863;&#21644;&#20998;&#24067;&#12290;&#22312;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#30340;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#35813;&#27169;&#22411;&#23558;&#20851;&#38190;&#30340;&#35748;&#30693;&#20559;&#24046;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#23637;&#20986;&#23545;&#20449;&#24687;&#21442;&#19982;&#30340;&#22810;&#32500;&#24230;&#35270;&#35282;&#12290;&#30740;&#31350;&#37319;&#29992;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#21327;&#35758;&#65292;&#36873;&#21462;&#20102;WordNet&#25968;&#25454;&#24211;&#20013;&#30340;50&#23545;&#38543;&#26426;&#21516;&#20041;&#35789;&#65288;&#20849;100&#20010;&#35789;&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#35843;&#26597;&#65288;n = 80,500&#65289;&#35780;&#20272;&#20102;&#36825;&#20123;&#35789;&#30340;&#21442;&#19982;&#27700;&#24179;&#65292;&#24471;&#20986;&#20102;&#23454;&#35777;IE&#25351;&#26631;&#12290;&#28982;&#21518;&#23545;&#27599;&#20010;&#35789;&#30340;READ&#23646;&#24615;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#26816;&#26597;&#20854;&#39044;&#27979;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;READ&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#35789;&#30340;IE&#27700;&#24179;&#65292;&#24182;&#21306;&#20998;&#20102;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engagi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36807;&#29983;&#25104;&#21644;&#25490;&#24207;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#35805;&#34892;&#20026;&#30340;&#21487;&#25511;&#29983;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#25490;&#21517;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#39640;&#35821;&#20041;&#20934;&#30830;&#24615;&#30340;&#22810;&#31181;&#23545;&#35805;&#34892;&#20026;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.14440</link><description>&lt;p&gt;
&#36890;&#36807;&#23569;&#26679;&#26412;&#21709;&#24212;&#29983;&#25104;&#21644;&#25490;&#24207;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#35805;&#34892;&#20026;&#30340;&#21487;&#25511;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking. (arXiv:2307.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36807;&#29983;&#25104;&#21644;&#25490;&#24207;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#35805;&#34892;&#20026;&#30340;&#21487;&#25511;&#29983;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#25490;&#21517;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#39640;&#35821;&#20041;&#20934;&#30830;&#24615;&#30340;&#22810;&#31181;&#23545;&#35805;&#34892;&#20026;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#20135;&#29983;&#20855;&#26377;&#39640;&#35821;&#20041;&#20445;&#30495;&#24230;&#30340;&#22810;&#31181;&#23545;&#35805;&#34892;&#20026;&#65288;DA&#65289;&#30340;&#21709;&#24212;&#12290;&#36807;&#21435;&#65292;&#23545;&#35805;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#22120;&#65288;NLG&#65289;&#26159;&#22312;&#22823;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#35813;&#35821;&#26009;&#24211;&#23558;&#29305;&#23450;&#39046;&#22495;&#30340;DA&#21450;&#20854;&#35821;&#20041;&#23646;&#24615;&#26144;&#23556;&#21040;&#36755;&#20986;&#35805;&#35821;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25511;&#21046;&#30340;NLG&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23569;&#26679;&#26412;&#36807;&#29983;&#25104;&#21644;&#25490;&#24207;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;DA&#30340;&#21487;&#25511;&#29983;&#25104;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20843;&#31181;&#23569;&#26679;&#26412;&#25552;&#31034;&#26679;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#20174;&#25991;&#26412;&#20266;&#21442;&#32771;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20845;&#31181;&#33258;&#21160;&#25490;&#21517;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#26102;&#35782;&#21035;&#20986;&#26082;&#20855;&#26377;&#27491;&#30830;DA&#21448;&#20855;&#26377;&#36739;&#39640;&#35821;&#20041;&#20934;&#30830;&#24615;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#21644;&#22235;&#20010;LLMs&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26426;&#22120;&#33258;&#21160;&#25490;&#21517;&#36755;&#20986;&#30340;&#23545;&#35805;&#31995;&#32479;NLG&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems need to produce responses that realize multiple types of dialogue acts (DAs) with high semantic fidelity. In the past, natural language generators (NLGs) for dialogue were trained on large parallel corpora that map from a domain-specific DA and its semantic attributes to an output utterance. Recent work shows that pretrained language models (LLMs) offer new possibilities for controllable NLG using prompt-based learning. Here we develop a novel few-shot overgenerate-and-rank approach that achieves the controlled generation of DAs. We compare eight few-shot prompt styles that include a novel method of generating from textual pseudo-references using a textual style transfer approach. We develop six automatic ranking functions that identify outputs with both the correct DA and high semantic accuracy at generation time. We test our approach on three domains and four LLMs. To our knowledge, this is the first work on NLG for dialogue that automatically ranks outputs using bot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#33719;&#24471;&#25216;&#33021;&#30340;&#26377;&#24207;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25216;&#33021;&#26102;&#20063;&#26377;&#19968;&#23450;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#20197;&#25913;&#21892;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.14430</link><description>&lt;p&gt;
Skill-it! &#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#33719;&#24471;&#25216;&#33021;&#30340;&#26377;&#24207;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25216;&#33021;&#26102;&#20063;&#26377;&#19968;&#23450;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#20197;&#25913;&#21892;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#22266;&#23450;&#30340;token&#39044;&#31639;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#36873;&#25321;&#33021;&#22815;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#33719;&#24471;&#33391;&#22909;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#20154;&#31867;&#22312;&#26377;&#24847;&#20041;&#30340;&#39034;&#24207;&#20013;&#33719;&#24471;&#30456;&#20114;&#20381;&#36182;&#30340;&#25216;&#33021;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#32452;&#25216;&#33021;&#26102;&#20063;&#20250;&#36981;&#24490;&#36825;&#26679;&#30340;&#39034;&#24207;&#12290;&#22914;&#26524;&#23384;&#22312;&#36825;&#26679;&#30340;&#39034;&#24207;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;&#21033;&#29992;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25216;&#33021;&#30340;&#27010;&#24565;&#21644;&#26377;&#24207;&#30340;&#25216;&#33021;&#38598;&#21512;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#30456;&#20851;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26377;&#24207;&#30340;&#25216;&#33021;&#38598;&#21512;&#30340;&#23384;&#22312;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23384;&#22312;&#20351;&#24471;&#22312;&#35757;&#32451;&#20854;&#20808;&#20915;&#26465;&#20214;&#25216;&#33021;&#26102;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#26356;&#39640;&#32423;&#30340;&#25216;&#33021;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#25454;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#21457;&#23637;&#36890;&#36807;&#24819;&#35937;&#35328;&#35821;&#36827;&#34892;&#20132;&#27969;&#30340;&#33041;-&#26426;&#25509;&#21475;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.14389</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#30721;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG. (arXiv:2307.14389v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#21457;&#23637;&#36890;&#36807;&#24819;&#35937;&#35328;&#35821;&#36827;&#34892;&#20132;&#27969;&#30340;&#33041;-&#26426;&#25509;&#21475;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#21644;&#20302;&#20449;&#22122;&#27604;&#12290;&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#20855;&#26377;&#21069;&#26223;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;DDPMs&#21644;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;Diff-E&#23545;&#24819;&#35937;&#35328;&#35821;&#30340;&#33041;&#30005;&#22270;&#36827;&#34892;&#35299;&#30721;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;Diff-E&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#24819;&#35937;&#35328;&#35821;&#33041;&#30005;&#22270;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;DDPMs&#21487;&#20197;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;&#24037;&#20855;&#65292;&#23545;&#20110;&#36890;&#36807;&#24819;&#35937;&#35328;&#35821;&#36827;&#34892;&#20132;&#27969;&#30340;&#33041;-&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.14385</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#39044;&#27979;&#24515;&#29702;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#25552;&#21319;&#20351;&#24471;&#22810;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#25913;&#36827;&#30740;&#31350;&#20960;&#20046;&#27809;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#22810;&#31181;LLM&#65288;&#21253;&#25324;Alpaca&#65292;Alpaca-LoRA&#21644;GPT-3.5&#65289;&#22312;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#22810;&#20010;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#12289;&#23569;-shot&#25552;&#31034;&#21644;&#25351;&#20196;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#38646;-shot&#21644;&#23569;-shot&#25552;&#31034;&#35774;&#35745;&#19978;&#22312;&#24515;&#29702;&#20581;&#24247;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#38480;&#20294;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;LLM&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;Mental-Alpaca&#65292;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#27604;GPT-3.5&#65288;&#20307;&#31215;&#22823;25&#20493;&#65289;&#39640;&#20986;16.7\%&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;&#25105;&#20204;&#24635;&#32467;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs' capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our find
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35774;&#35745;&#21644;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#20197;&#21450;&#20854;&#38480;&#21046;</title><link>http://arxiv.org/abs/2307.14377</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;&#21644;&#21046;&#36896;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14377
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35774;&#35745;&#21644;&#21046;&#36896;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#20197;&#21450;&#20854;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;GPT-4&#65292;&#20026;&#29983;&#25104;&#35774;&#35745;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#26426;&#20250;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25972;&#20010;&#35774;&#35745;&#21644;&#21046;&#36896;&#27969;&#31243;&#20013;&#24212;&#29992;&#35813;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;LLMs&#22312;&#35832;&#22914;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#36716;&#21270;&#20026;&#35774;&#35745;&#35268;&#33539;&#12289;&#23558;&#35774;&#35745;&#36716;&#21270;&#20026;&#21046;&#36896;&#25351;&#23548;&#12289;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#21644;&#35774;&#35745;&#21464;&#20307;&#12289;&#35745;&#31639;&#35774;&#35745;&#24615;&#33021;&#20197;&#21450;&#22522;&#20110;&#24615;&#33021;&#25628;&#32034;&#35774;&#35745;&#31561;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20363;&#23376;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#24403;&#21069;LLMs&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#25581;&#31034;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24076;&#26395;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#25913;&#36827;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Language Models (LLMs), including GPT-4, provides exciting new opportunities for generative design. We investigate the application of this tool across the entire design and manufacturing workflow. Specifically, we scrutinize the utility of LLMs in tasks such as: converting a text-based prompt into a design specification, transforming a design into manufacturing instructions, producing a design space and design variations, computing the performance of a design, and searching for designs predicated on performance. Through a series of examples, we highlight both the benefits and the limitations of the current LLMs. By exposing these limitations, we aspire to catalyze the continued improvement and progression of these models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.14367</link><description>&lt;p&gt;
Prot2Text: &#22522;&#20110;GNNs&#21644;Transformers&#30340;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#21151;&#33021;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20351;&#26576;&#20123;&#31185;&#23398;&#23478;&#23558;&#20854;&#29702;&#35299;&#24402;&#31867;&#20026;&#38590;&#20197;&#24819;&#35937;&#30340;&#20219;&#21153;&#12290;&#19981;&#21516;&#32423;&#21035;&#30340;&#25361;&#25112;&#20351;&#36825;&#39033;&#20219;&#21153;&#22797;&#26434;&#21270;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#24320;&#21457;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#20219;&#21153;&#34920;&#36848;&#20026;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#23558;&#39044;&#23450;&#20041;&#26631;&#31614;&#20998;&#37197;&#32473;&#34507;&#30333;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;Prot2Text&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#20801;&#35768;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#36827;&#34892;&#25972;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#24182;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;CIF-T&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14132</link><description>&lt;p&gt;
&#21578;&#21035;RNN-T Loss&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;CIF&#30340;&#36716;&#24405;&#22120;&#26550;&#26500;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#24182;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;CIF-T&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNN-T&#27169;&#22411;&#22312;ASR&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20381;&#38752;RNN-T Loss&#23454;&#29616;&#36755;&#20837;&#38899;&#39057;&#21644;&#30446;&#26631;&#24207;&#21015;&#30340;&#38271;&#24230;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RNN-T Loss&#30340;&#23454;&#29616;&#22797;&#26434;&#24615;&#21644;&#22522;&#20110;&#23545;&#40784;&#30340;&#20248;&#21270;&#30446;&#26631;&#23548;&#33268;&#35745;&#31639;&#20887;&#20313;&#21644;&#39044;&#27979;&#32593;&#32476;&#35282;&#33394;&#30340;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#65288;CIF-T&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23427;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#65288;CIF&#65289;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Funnel-CIF&#12289;Context Blocks&#12289;Unified Gating&#21644;Bilinear Pooling&#32852;&#21512;&#32593;&#32476;&#20197;&#21450;&#36741;&#21161;&#35757;&#32451;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;178&#23567;&#26102;&#30340;AISHELL-1&#21644;10000&#23567;&#26102;&#30340;WenetSpeech&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;RNN-T&#27169;&#22411;&#30456;&#27604;&#65292;CIF-T&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;32&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#25918;&#23556;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35780;&#20272;&#31354;&#30333;&#12290;&#35780;&#20272;&#32467;&#26524;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12289;&#20248;&#21183;&#21644;&#24369;&#28857;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#20026;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.13693</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#25918;&#23556;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Radiology Natural Language Processing. (arXiv:2307.13693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;32&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#25918;&#23556;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35780;&#20272;&#31354;&#30333;&#12290;&#35780;&#20272;&#32467;&#26524;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12289;&#20248;&#21183;&#21644;&#24369;&#28857;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#20026;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#26631;&#24535;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;LLMs&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#20016;&#23500;&#65292;&#24182;&#19988;&#20854;&#20013;&#35768;&#22810;&#27169;&#22411;&#20855;&#26377;&#21452;&#35821;&#33021;&#21147;&#65292;&#21487;&#20197;&#29087;&#32451;&#22788;&#29702;&#33521;&#25991;&#21644;&#20013;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20173;&#26377;&#24453;&#24320;&#23637;&#12290;&#22312;&#25918;&#23556;&#23398;NLP&#30340;&#32972;&#26223;&#19979;&#65292;&#23588;&#20854;&#26126;&#26174;&#32570;&#20047;&#36825;&#31181;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;32&#20010;LLMs&#22312;&#35299;&#37322;&#25918;&#23556;&#23398;&#25253;&#21578;&#26041;&#38754;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#35780;&#20272;&#20102;&#20174;&#24433;&#20687;&#23398;&#21457;&#29616;&#20013;&#24471;&#20986;&#21360;&#35937;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#35780;&#20272;&#30340;&#32467;&#26524;&#20026;&#36825;&#20123;LLMs&#30340;&#24615;&#33021;&#12289;&#20248;&#21183;&#21644;&#24369;&#28857;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#24182;&#20026;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP). LLMs have revolutionized a multitude of domains, and they have made a significant impact in the medical field. Large language models are now more abundant than ever, and many of these models exhibit bilingual capabilities, proficient in both English and Chinese. However, a comprehensive evaluation of these models remains to be conducted. This lack of assessment is especially apparent within the context of radiology NLP. This study seeks to bridge this gap by critically evaluating thirty two LLMs in interpreting radiology reports, a crucial component of radiology NLP. Specifically, the ability to derive impressions from radiologic findings is assessed. The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13365</link><description>&lt;p&gt;
&#29992;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#23558;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#37327;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#29616;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#20195;&#12290;&#26080;&#35770;&#36825;&#20123;&#27169;&#22411;&#33258;&#36523;&#30340;&#23481;&#37327;&#21644;&#32467;&#26500;&#22914;&#20309;&#65292;&#37117;&#23384;&#22312;&#23545;LLMs&#20855;&#26377;&#26356;&#38271;&#26356;&#22797;&#26434;&#19978;&#19979;&#25991;&#30340;&#22686;&#24378;&#29702;&#35299;&#30340;&#38656;&#27714;&#65292;&#32780;&#27169;&#22411;&#36890;&#24120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20854;&#29702;&#35299;&#33021;&#21147;&#33539;&#22260;&#30340;&#21477;&#23376;&#24207;&#21015;&#26102;&#20250;&#36935;&#21040;&#19978;&#38480;&#65292;&#23548;&#33268;&#20135;&#29983;&#31163;&#39064;&#25110;&#28151;&#20081;&#30340;&#22238;&#31572;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#20851;&#27880;&#8220;&#20026;&#20160;&#20040;&#27169;&#22411;&#26080;&#27861;&#33258;&#34892;&#24357;&#34917;&#25110;&#22686;&#24378;&#33258;&#24049;&#30340;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#24615;&#36136;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#26368;&#23567;&#21270;&#39069;&#22806;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#21033;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
&lt;/p&gt;</description></item><item><title>RRAML&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#24222;&#22823;&#25968;&#25454;&#24211;&#20013;&#30340;&#25903;&#25345;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12798</link><description>&lt;p&gt;
RRAML: &#24378;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RRAML: Reinforced Retrieval Augmented Machine Learning. (arXiv:2307.12798v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12798
&lt;/p&gt;
&lt;p&gt;
RRAML&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#24222;&#22823;&#25968;&#25454;&#24211;&#20013;&#30340;&#25903;&#25345;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20851;&#39046;&#22495;&#65292;&#22312;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25805;&#20316;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22522;&#20110;API&#30340;&#25991;&#26412;&#25552;&#31034;&#25552;&#20132;&#26469;&#20351;&#29992;&#23427;&#20204;&#20250;&#23384;&#22312;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#32422;&#26463;&#21644;&#22806;&#37096;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#24378;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;RRAML&#65289;&#12290;RRAML&#23558;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#30001;&#19987;&#29992;&#26816;&#32034;&#22120;&#20174;&#29992;&#25143;&#25552;&#20379;&#30340;&#24222;&#22823;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#25903;&#25345;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#32469;&#36807;&#20102;&#35775;&#38382;LLM&#26799;&#24230;&#30340;&#38656;&#27714;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;LLMs&#30340;&#36127;&#25285;&#65292;&#22240;&#20026;&#30001;&#20110;&#23545;&#27169;&#22411;&#21644;&#21512;&#20316;&#30340;&#35775;&#38382;&#21463;&#38480;&#65292;&#36825;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;RoBERTa&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.03952</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#20154;&#26684;&#35782;&#21035;&#22120;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03952
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#12289;RoBERTa&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#26684;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#20010;&#20154;&#22240;&#32032;&#65292;&#34987;&#32435;&#20837;&#20102;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#20135;&#21697;&#25512;&#33616;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35782;&#21035;&#20219;&#21153;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#25991;&#26412;&#35782;&#21035;&#20010;&#20307;&#30340;&#20154;&#26684;&#12290;&#32771;&#34385;&#21040;ChatGPT&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#20154;&#26684;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25506;&#32034;ChatGPT&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#26684;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#25105;&#20204;&#35774;&#35745;&#30340;&#38754;&#21521;&#23618;&#32423;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25351;&#23548;ChatGPT&#22312;&#25351;&#23450;&#23618;&#27425;&#20998;&#26512;&#32473;&#23450;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#23558;ChatGPT&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#24494;&#35843;&#30340;RoBERTa&#20197;&#21450;&#30456;&#24212;&#30340;&#26368;&#26032;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, personality has been regarded as a valuable personal factor being incorporated into numerous tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition task, which aims to identify an individual's personality based on given text. Considering that ChatGPT has recently exhibited remarkable abilities on various natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition task for generating effective personality data. Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level. We compare the performance of ChatGPT on two representative real-world datasets with traditional neural network, fine-tuned RoBERTa, and corresponding state-of-the-art task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Fraunhofer SIT&#22242;&#38431;&#22312;CLEF-2023 CheckThat!&#33521;&#35821;&#23454;&#39564;&#23460;&#20219;&#21153;1B&#20013;&#20351;&#29992;&#27169;&#22411;&#28151;&#21512;&#25216;&#26415;&#35299;&#20915;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#30340;&#26159;&#30830;&#23450;&#25919;&#27835;&#36777;&#35770;&#20013;&#30340;&#25991;&#26412;&#29255;&#27573;&#26159;&#21542;&#20540;&#24471;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#35780;&#20272;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2307.02377</link><description>&lt;p&gt;
Fraunhofer SIT&#22312;CheckThat! 2023&#20013;&#20351;&#29992;&#27169;&#22411;&#28151;&#21512;&#25216;&#26415;&#35299;&#20915;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#65306;&#20197;&#21487;&#26816;&#26597;&#24615;&#20998;&#31867;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification. (arXiv:2307.02377v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Fraunhofer SIT&#22242;&#38431;&#22312;CLEF-2023 CheckThat!&#33521;&#35821;&#23454;&#39564;&#23460;&#20219;&#21153;1B&#20013;&#20351;&#29992;&#27169;&#22411;&#28151;&#21512;&#25216;&#26415;&#35299;&#20915;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#30340;&#26159;&#30830;&#23450;&#25919;&#27835;&#36777;&#35770;&#20013;&#30340;&#25991;&#26412;&#29255;&#27573;&#26159;&#21542;&#20540;&#24471;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#35780;&#20272;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Fraunhofer SIT&#22242;&#38431;&#22312;CLEF-2023 CheckThat!&#33521;&#35821;&#23454;&#39564;&#23460;&#20219;&#21153;1B&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#27573;&#25919;&#27835;&#36777;&#35770;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#23545;&#20854;&#36827;&#34892;&#26816;&#26597;&#20215;&#20540;&#35780;&#20272;&#12290;&#26816;&#27979;&#21487;&#26816;&#26597;&#22768;&#26126;&#26088;&#22312;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20107;&#23454;&#26816;&#26597;&#20154;&#21592;&#24212;&#39318;&#20808;&#32771;&#34385;&#30340;&#22768;&#26126;&#26469;&#31616;&#21270;&#25163;&#21160;&#20107;&#23454;&#26816;&#26597;&#24037;&#20316;&#12290;&#23427;&#36824;&#21487;&#20197;&#34987;&#35270;&#20026;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#30340;&#20027;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#21033;&#29992;&#20102;&#20197;&#27169;&#22411;&#28151;&#21512;&#20026;&#20013;&#24515;&#30340;&#38598;&#25104;&#20998;&#31867;&#26041;&#26696;&#12290;&#22312;&#24212;&#29992;&#20110;&#33521;&#35821;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20132;&#30340;&#27169;&#22411;&#22312;&#27604;&#36187;&#20013;&#33719;&#24471;&#20102;0.878&#30340;&#25972;&#20307;F1&#24471;&#20998;&#65292;&#24182;&#34987;&#35780;&#20026;&#31532;&#20108;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the second-placed approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text snippet from a political debate, the aim of this task is to determine whether it should be assessed for check-worthiness. Detecting check-worthy statements aims to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. It can also be considered as primary step of a fact-checking system. Our best-performing method took advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall F1 score of 0.878 and was ranked as the second-best model in the competition.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#30340;&#32593;&#32476;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65292;&#36890;&#36807;&#24341;&#20837;&#26597;&#35810;&#27169;&#26495;&#26469;&#26126;&#30830;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#26469;&#24314;&#27169;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00866</link><description>&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#35805;&#35821;&#20013;&#25366;&#25496;&#32447;&#32034;&#65306;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#30340;&#32593;&#32476;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting. (arXiv:2307.00866v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00866
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#30340;&#32593;&#32476;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65292;&#36890;&#36807;&#24341;&#20837;&#26597;&#35810;&#27169;&#26495;&#26469;&#26126;&#30830;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#26469;&#24314;&#27169;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#19981;&#23436;&#25972;&#35805;&#35821;&#21644;&#37325;&#20889;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#20063;&#27809;&#26377;&#38544;&#24335;&#21644;&#19981;&#20805;&#20998;&#22320;&#24314;&#27169;&#35821;&#20041;&#32467;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26597;&#35810;&#32593;&#32476;&#65288;QUEEN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26597;&#35810;&#27169;&#26495;&#26126;&#30830;&#22320;&#24341;&#20837;&#20102;&#19981;&#23436;&#25972;&#35805;&#35821;&#21644;&#37325;&#20889;&#35805;&#35821;&#20043;&#38388;&#30340;&#25351;&#23548;&#24615;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#22312;&#21738;&#37324;&#36827;&#34892;&#21442;&#32771;&#25110;&#24674;&#22797;&#34987;&#30465;&#30053;&#30340;&#26631;&#35760;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#26469;&#24314;&#27169;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30001;&#20110;&#25552;&#20986;&#30340;&#26597;&#35810;&#27169;&#26495;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#32534;&#36753;&#25805;&#20316;&#35780;&#20998;&#32593;&#32476;&#30340;&#22909;&#22788;&#65292;QUEEN&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from proposed query template and the well-designed edit operation scoring network, QUEEN achieves state-of-the-art performance on several public datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21333;&#27169;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#65292;&#25104;&#21151;&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;CheckThat! 2023&#20219;&#21153;1A&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00610</link><description>&lt;p&gt;
Fraunhofer SIT&#22312;CheckThat! 2023&#20013;&#30340;&#36129;&#29486;&#65306;&#28151;&#21512;&#21333;&#27169;&#20998;&#31867;&#22120;&#20197;&#20272;&#35745;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets. (arXiv:2307.00610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21333;&#27169;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#65292;&#25104;&#21151;&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;CheckThat! 2023&#20219;&#21153;1A&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#25991;&#20214;&#30340;&#36873;&#39033;&#20026;&#21306;&#20998;&#32593;&#32476;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#27599;&#31186;&#20998;&#20139;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#26080;&#27861;&#36890;&#36807;&#35745;&#31639;&#26426;&#25110;&#20154;&#31867;&#19987;&#23478;&#23545;&#25152;&#26377;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#22240;&#27492;&#65292;&#21487;&#36890;&#36807;&#21487;&#38752;&#24615;&#20998;&#26512;&#20316;&#20026;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#65292;&#20197;&#21450;&#20316;&#20026;&#25552;&#39640;&#25928;&#29575;&#30340;&#36807;&#28388;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#22810;&#27169;&#24577;&#25512;&#25991;&#30340;&#21487;&#38752;&#24615;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#20010;&#22312;&#21333;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;OCR&#20998;&#26512;&#25552;&#21462;&#23884;&#20837;&#30340;&#25991;&#26412;&#34920;&#29616;&#26368;&#20339;&#12290;&#36890;&#36807;&#32452;&#21512;&#36825;&#20004;&#20010;&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#22312;CheckThat! 2023&#20219;&#21153;1A&#20013;&#36798;&#21040;&#20102;0.7297&#30340;F1&#20998;&#25968;&#65292;&#22312;&#31169;&#20154;&#27979;&#35797;&#38598;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16731</link><description>&lt;p&gt;
&#35782;&#21035;&#24773;&#24863;&#20307;&#39564;&#32773;&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35282;&#33394;&#26631;&#27880;&#26088;&#22312;&#25552;&#21462;&#25991;&#26412;&#20013;&#25551;&#36848;&#35841;&#32463;&#21382;&#24773;&#24863;&#12289;&#20026;&#20160;&#20040;&#20197;&#21450;&#23545;&#35841;&#30340;&#20449;&#24687;&#12290;&#36825;&#36890;&#24120;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#22914;&#26524;&#35201;&#22238;&#31572;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#35841;&#24863;&#21463;&#21040;&#20102;&#21738;&#31181;&#24773;&#24863;&#65292;&#36825;&#21487;&#33021;&#20250;&#36807;&#20110;&#22797;&#26434;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#38543;&#21518;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#21576;&#29616;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion role labeling aims at extracting who is described in text to experience an emotion, why, and towards whom. This is often a challenging modelling task which might be overly sophisticated if the main question to answer is who feels which emotion. Recently, Troiano et al. (2022) proposed a data set that focuses on assigning emotion labels and appraisal labels to individual entities in text and Wegge et al. (2022) presented the first modelling experiments. Their experiencer-specific emotion prediction model has, however, only been evaluated on gold-annotated experiencers, due to the unavailability of an automatic experiencer detection approach. We fill this gap with the first experiments to automatically detect emotion experiencers in text and, subsequently, assign them emotions. We show that experiencer detection in text is a challenging task, with a precision of .82 and a recall of .56 (F1 =.66). Consequently, the performance of the experiencer-specific emotion detection pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#27880;&#24847;&#21147;&#23618;&#21450;&#26368;&#20248;&#20256;&#36755;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#35821;&#38899;&#35782;&#21035;&#30196;&#21574;&#30151;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#25429;&#25417;&#20102;&#27169;&#24577;&#20869;&#37096;&#21644;&#27169;&#24577;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23454;&#29616;&#20102;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.16406</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#27880;&#24847;&#21147;&#23618;&#21450;&#26368;&#20248;&#20256;&#36755;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#35782;&#21035;&#33258;&#21457;&#35821;&#38899;&#20013;&#30340;&#30196;&#21574;&#30151;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Attention Layers coupled with Optimal Transport Domain Adaptation methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#27880;&#24847;&#21147;&#23618;&#21450;&#26368;&#20248;&#20256;&#36755;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#35821;&#38899;&#35782;&#21035;&#30196;&#21574;&#30151;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#25429;&#25417;&#20102;&#27169;&#24577;&#20869;&#37096;&#21644;&#27169;&#24577;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23454;&#29616;&#20102;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#31070;&#32463;&#35748;&#30693;&#30149;&#21464;&#65292;&#20063;&#26159;&#23548;&#33268;&#30196;&#21574;&#30151;&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#38024;&#23545;&#36890;&#36807;&#33258;&#21457;&#35821;&#38899;&#35786;&#26029;&#30196;&#21574;&#30151;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20998;&#21035;&#35757;&#32451;&#35821;&#35328;&#21644;&#22768;&#23398;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#20197;&#21450;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#22312;&#36755;&#20837;&#23618;&#36827;&#34892;&#32423;&#32852;&#25110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#32423;&#32852;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#65292;&#35745;&#31639;&#34920;&#31034;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#21040;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#37117;&#27809;&#26377;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;AD&#24739;&#32773;&#65292;&#25429;&#25417;&#20102;&#27169;&#24577;&#20869;&#37096;&#21644;&#27169;&#24577;&#38388;&#30340;&#20132;&#20114;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#38899;&#39057;&#25991;&#20214;&#36716;&#25442;&#20026;log-Mel&#20809;&#35889;&#22270;&#65292;&#23427;&#20204;&#30340;delta&#21644;delta-delta&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is the main cause of dementia. Although many studies have been proposed targeting at diagnosing dementia through spontaneous speech, there are still limitations. Existing state-of-the-art approaches, which propose multimodal methods, train separately language and acoustic models, employ majority-vote approaches, and concatenate the representations of the different modalities either at the input level, i.e., early fusion, or during training. Also, some of them employ self-attention layers, which calculate the dependencies between representations without considering the contextual information. In addition, no prior work has taken into consideration the model calibration. To address these limitations, we propose some new methods for detecting AD patients, which capture the intraand cross-modal interactions. First, we convert the audio files into log-Mel spectrograms, their delta, and delta-delta and create in this
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15299</link><description>&lt;p&gt;
&#22312;ChatGPT&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#31185;&#23398;&#65306;&#30740;&#31350;&#20262;&#29702;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20294;&#26377;&#20105;&#35758;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#26102;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#26088;&#22312;&#20026;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#22880;&#23450;&#26032;&#30340;&#21450;&#26102;&#22522;&#30784;&#12290;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#21644;&#30740;&#31350;&#23545;&#35937;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#35814;&#32454;&#23457;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31185;&#23398;&#23478;&#12289;&#21442;&#19982;&#32773;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#30340;&#26032;&#20852;&#23454;&#36341;&#65292;&#24182;&#32473;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20026;&#22312;AI&#26102;&#20195;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65288;CPLL&#65289;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#30495;&#23454;&#21518;&#39564;&#21644;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#21644;&#20869;&#23481;&#30340;&#32622;&#20449;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12485</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition. (arXiv:2305.12485v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65288;CPLL&#65289;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#30495;&#23454;&#21518;&#39564;&#21644;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#21644;&#20869;&#23481;&#30340;&#32622;&#20449;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#36890;&#36807;&#20247;&#21253;&#33719;&#24471;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#27880;&#31354;&#38388;&#30340;&#24191;&#27867;&#24615;&#21644;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#36890;&#36807;&#22810;&#20010;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#34920;&#20915;&#33719;&#24471;&#32479;&#19968;&#19988;&#27491;&#30830;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#30452;&#25509;&#21033;&#29992;&#21407;&#22987;&#30340;&#22810;&#26631;&#27880;&#32773;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;CPLL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#20808;&#39564;&#32622;&#20449;&#24230;&#21644;&#27169;&#22411;&#23398;&#20064;&#30340;&#21518;&#39564;&#32622;&#20449;&#24230;&#65292;&#29992;&#20110;&#32676;&#20307;&#27880;&#37322;&#30340;NER&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#21644;&#20869;&#23481;&#30340;&#32622;&#20449;&#24230;&#12290;&#30495;&#23454;&#21518;&#39564;&#20272;&#35745;&#22120;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#30495;&#23454;&#21518;&#39564;&#21644;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models for named entity recognition (NER) are mainly based on large-scale labeled datasets, which always obtain using crowdsourcing. However, it is hard to obtain a unified and correct label via majority voting from multiple annotators for NER due to the large labeling space and complexity of this task. To address this problem, we aim to utilize the original multi-annotator labels directly. Particularly, we propose a Confidence-based Partial Label Learning (CPLL) method to integrate the prior confidence (given by annotators) and posterior confidences (learned by models) for crowd-annotated NER. This model learns a token- and content-dependent confidence via an Expectation-Maximization (EM) algorithm by minimizing empirical risk. The true posterior estimator and confidence estimator perform iteratively to update the true posterior and confidence respectively. We conduct extensive experimental results on both real-world and synthetic datasets, which show that our model can impro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;In-Context RALM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#20214;&#20316;&#20026;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#65292;&#26080;&#38656;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21363;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#21644;&#28304;&#24402;&#22240;&#33021;&#21147;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;RALM&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#37096;&#32626;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00083</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;In-Context RALM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#20214;&#20316;&#20026;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#65292;&#26080;&#38656;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21363;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#21644;&#28304;&#24402;&#22240;&#33021;&#21147;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;RALM&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#37096;&#32626;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;(RALM)&#26041;&#27861;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#25991;&#20214;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#26469;&#19982;&#35821;&#35328;&#27169;&#22411;(LM)&#36827;&#34892;&#21327;&#21516;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#21487;&#20197;&#32531;&#35299;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#28304;&#24402;&#22240;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;RALM&#26041;&#27861;&#30528;&#37325;&#20110;&#20462;&#25913;LM&#26550;&#26500;&#20197;&#20415;&#20110;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#37096;&#32626;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;RALM&#65306;&#20445;&#25345;LM&#26550;&#26500;&#19981;&#21464;&#65292;&#24182;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#26816;&#32034;&#21040;&#30340;&#25991;&#20214;&#65292;&#26080;&#38656;&#23545;LM&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#29616;&#25104;&#30340;&#36890;&#29992;&#26816;&#32034;&#22120;&#30340;&#19978;&#19979;&#25991;RALM&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#19981;&#21516;&#35821;&#26009;&#24211;&#20013;&#33021;&#22815;&#25552;&#20379;&#20986;&#20154;&#24847;&#26009;&#30340;&#22823;&#24133;&#24230;&#30340;LM&#22686;&#30410;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#25991;&#20214;&#26816;&#32034;&#21644;&#25490;&#21517;&#26426;&#21046;&#21487;&#20197;&#38024;&#23545;RALM&#35774;&#32622;&#36827;&#34892;&#19987;&#38376;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to 
&lt;/p&gt;</description></item><item><title>ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.11596</link><description>&lt;p&gt;
ThoughtSource:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25968;&#25454;&#30340;&#20013;&#22830;&#26530;&#32445;&#12290;
&lt;/p&gt;
&lt;p&gt;
ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11596
&lt;/p&gt;
&lt;p&gt;
ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#19978;&#20173;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#19981;&#36879;&#26126;&#65292;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#20107;&#23454;&#65292;&#24182;&#19988;&#23384;&#22312;&#20854;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35753;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#34920;&#36798;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ThoughtSource&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#12290;ThoughtSource&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26469;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;ThoughtSource&#30340;&#39318;&#27425;&#21457;&#24067;&#38598;&#25104;&#20102;&#20845;&#20010;&#31185;&#23398;/&#21307;&#23398;&#12289;&#19977;&#20010;&#36890;&#29992;&#39046;&#22495;&#21644;&#20116;&#20010;&#25968;&#23398;&#39064;&#31572;&#26696;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30340;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20854;&#22238;&#31572;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;&#30340;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2211.08411</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#38271;&#23614;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Struggle to Learn Long-Tail Knowledge. (arXiv:2211.08411v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30340;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20854;&#22238;&#31572;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;&#30340;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#20013;&#21253;&#21547;&#30528;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20174;&#21382;&#21490;&#20154;&#29289;&#30340;&#29983;&#26085;&#21040;&#32534;&#31243;&#25945;&#31243;&#31561;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#21487;&#20197;&#30001;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26576;&#20123;&#20449;&#24687;&#22312;&#32593;&#19978;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#20182;&#20449;&#24687;&#30340;&#20986;&#29616;&#38750;&#24120;&#32597;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30340;&#30693;&#35782;&#19982;&#20174;&#32593;&#32476;&#25235;&#21462;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#19982;&#27492;&#38382;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#35745;&#31639;&#21253;&#21547;&#19982;&#32473;&#23450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30456;&#21516;&#23454;&#20307;&#30340;&#25991;&#26723;&#25968;&#37327;&#26469;&#35782;&#21035;&#36825;&#20123;&#30456;&#20851;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20247;&#22810;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914; TriviaQA&#65289;&#12289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914; ROOTS&#65289;&#21644;&#27169;&#22411;&#19978;&#65292;&#20934;&#30830;&#24615;&#19982;&#30456;&#20851;&#25991;&#26723;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21487;&#24494;&#20998;&#30340;Transformer&#22836;&#37096;&#23376;&#38598;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#23433;&#20840;&#22320;&#20462;&#21098;&#25481;&#22823;&#37096;&#20998;&#22836;&#37096;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#23567;&#26356;&#24555;&#65292;&#32780;&#19988;&#22312;&#31232;&#30095;&#31243;&#24230;&#19978;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.04657</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;Transformer&#22836;&#37096;&#23376;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.04657
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21487;&#24494;&#20998;&#30340;Transformer&#22836;&#37096;&#23376;&#38598;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#23433;&#20840;&#22320;&#20462;&#21098;&#25481;&#22823;&#37096;&#20998;&#22836;&#37096;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#23567;&#26356;&#24555;&#65292;&#32780;&#19988;&#22312;&#31232;&#30095;&#31243;&#24230;&#19978;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22836;&#27880;&#24847;&#21147;&#26159;Transformer&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#30001;&#22810;&#20010;&#29420;&#31435;&#20851;&#27880;&#36755;&#20837;&#19981;&#21516;&#37096;&#20998;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19981;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#20462;&#21098;&#25481;Transformer&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#22823;&#37096;&#20998;&#22836;&#37096;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#26356;&#23567;&#26356;&#24555;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22836;&#37096;&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#21487;&#24494;&#20998;&#23376;&#38598;&#20462;&#21098;&#12290;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23398;&#20064;&#27599;&#20010;&#22836;&#37096;&#30340;&#37325;&#35201;&#24615;&#21464;&#37327;&#65292;&#24182;&#23545;&#26410;&#20462;&#21098;&#22836;&#37096;&#30340;&#25968;&#37327;&#26045;&#21152;&#29992;&#25143;&#25351;&#23450;&#30340;&#30828;&#32422;&#26463;&#12290;&#37325;&#35201;&#24615;&#21464;&#37327;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#24471;&#21040;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#26426;&#22120;&#32763;&#35793;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21487;&#24494;&#20998;&#23376;&#38598;&#20462;&#21098;&#22312;&#31232;&#30095;&#31243;&#24230;&#19978;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#25552;&#20379;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer. Recent work has shown, however, that a large proportion of the heads in a Transformer's multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. Intuitively, our method learns per-head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. The importance variables are learned via stochastic gradient descent. We conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than previous works while offering precise control of the sparsity level.
&lt;/p&gt;</description></item></channel></rss>