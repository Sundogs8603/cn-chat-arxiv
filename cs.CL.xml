<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://rss.arxiv.org/abs/2311.05112</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#65306;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#21307;&#23398;&#20013;&#65292;LLMs&#22312;&#21327;&#21161;&#21307;&#29983;&#36827;&#34892;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;LLMs&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#20197;&#19979;&#20855;&#20307;&#38382;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26500;&#24314;&#21307;&#23398;LLMs&#65311;2&#65289;&#20160;&#20040;&#26159;&#21307;&#23398;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65311;3&#65289;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;4&#65289;&#20351;&#29992;&#21307;&#23398;LLMs&#20250;&#20986;&#29616;&#21738;&#20123;&#25361;&#25112;&#65311;5&#65289;&#22914;&#20309;&#26356;&#22909;&#22320;&#26500;&#24314;&#21644;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#21307;&#23398;&#20013;LLMs&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#24182;&#20316;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#30340;&#23454;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#32500;&#25252;&#24182;&#23450;&#26399;&#26356;&#26032;&#19968;&#20010;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.02333</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#21450;&#20854;&#22312;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#12289;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;KPDDS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#20851;&#38190;&#28857;&#21644;&#31034;&#20363;&#23545;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;KPDDS&#30830;&#20445;&#36890;&#36807;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#21644;&#22823;&#35268;&#27169;&#24615;&#33021;&#30340;&#29983;&#25104;&#26032;&#39062;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KPMath&#65292;&#36804;&#20170;&#20026;&#27490;&#37327;&#36523;&#23450;&#21046;&#30340;&#26368;&#24191;&#27867;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#30334;&#19975;&#20010;&#20197;&#19978;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#21033;&#29992;KPMath&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#25512;&#29702;&#23494;&#38598;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#25193;&#20805;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#12290;&#23558;Mistral-7B&#27169;&#22411;&#22312;KPMath-Plus&#19978;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;MATH&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#38646;-shot PASS@1&#31934;&#24230;&#36798;&#21040;39.3%&#65292;&#36825;&#26159;&#19968;&#39033;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02333v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance th
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20351;&#27169;&#22411;&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#24182;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02325</link><description>&lt;p&gt;
&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65306;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20351;&#27169;&#22411;&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#24182;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#22270;&#20687;&#20013;&#29305;&#21035;&#30456;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#23548;&#27169;&#22411;&#26356;&#23494;&#20999;&#22320;&#20851;&#27880;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#24320;&#28304;&#30340;VLMs&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;VL&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02325v1 Announce Type: cross  Abstract: Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a "visual prompt", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#26816;&#27979;&#26410;&#35760;&#24405;&#30340;&#35789;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35789;-&#19978;&#19979;&#25991;&#23884;&#20837;&#22120;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#26816;&#27979;&#21040;&#20855;&#26377;&#26410;&#35760;&#24405;&#35789;&#20041;&#30340;&#35789;&#35821;&#29992;&#27861;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.02285</link><description>&lt;p&gt;
&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#26816;&#27979;&#26410;&#35760;&#24405;&#30340;&#35789;&#20041;
&lt;/p&gt;
&lt;p&gt;
Detection of Non-recorded Word Senses in English and Swedish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#26816;&#27979;&#26410;&#35760;&#24405;&#30340;&#35789;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35789;-&#19978;&#19979;&#25991;&#23884;&#20837;&#22120;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#26816;&#27979;&#21040;&#20855;&#26377;&#26410;&#35760;&#24405;&#35789;&#20041;&#30340;&#35789;&#35821;&#29992;&#27861;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#36827;&#34892;&#26410;&#30693;&#35789;&#20041;&#26816;&#27979;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#29305;&#23450;&#35789;&#35821;&#29992;&#27861;&#30340;&#21547;&#20041;&#26159;&#21542;&#22312;&#35789;&#20856;&#20013;&#26377;&#35760;&#24405;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#22120;&#26469;&#27604;&#36739;&#35789;&#20041;&#26465;&#30446;&#19982;&#29616;&#20195;&#21644;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#35789;&#35821;&#29992;&#27861;&#65292;&#20174;&#32780;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24314;&#27169;&#36825;&#19968;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#19982;&#20174;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#26816;&#27979;&#21040;&#20855;&#26377;&#26410;&#35760;&#24405;&#35789;&#20041;&#30340;&#35789;&#35821;&#29992;&#27861;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02285v1 Announce Type: new  Abstract: This study addresses the task of Unknown Sense Detection in English and Swedish. The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not. For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a few-shot scenario. Additionally, we use human annotations to adapt and evaluate our models. Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20174;&#31038;&#20132;&#23186;&#20307;&#30340;&#25991;&#26412;&#20013;&#35745;&#31639;&#24773;&#32490;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#24515;&#29702;&#20581;&#24247;&#26465;&#20214;&#20013;&#20316;&#20026;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.02281</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#24773;&#32490;&#32454;&#31890;&#24230;&#65306;&#24515;&#29702;&#20581;&#24247;&#30340;&#27719;&#24635;&#32423;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20174;&#31038;&#20132;&#23186;&#20307;&#30340;&#25991;&#26412;&#20013;&#35745;&#31639;&#24773;&#32490;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#24515;&#29702;&#20581;&#24247;&#26465;&#20214;&#20013;&#20316;&#20026;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24773;&#32490;&#23545;&#22609;&#36896;&#25105;&#20204;&#30340;&#32463;&#21382;&#20013;&#26377;&#20849;&#21516;&#28857;&#65292;&#28982;&#32780;&#65292;&#27599;&#20010;&#20154;&#22312;&#22914;&#20309;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#34920;&#36798;&#24773;&#32490;&#26041;&#38754;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#22312;&#24515;&#29702;&#23398;&#20013;&#65292;&#20010;&#20307;&#21306;&#20998;&#24773;&#32490;&#27010;&#24565;&#30340;&#33021;&#21147;&#21464;&#21270;&#34987;&#31216;&#20026;&#24773;&#32490;&#32454;&#31890;&#24230;&#65288;&#36890;&#36807;&#20010;&#20307;&#23545;&#33258;&#24049;&#24773;&#32490;&#30340;&#33258;&#25105;&#25253;&#21578;&#26469;&#30830;&#23450;&#65289;&#12290;&#39640;&#24773;&#32490;&#32454;&#31890;&#24230;&#24050;&#19982;&#26356;&#22909;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#32852;&#31995;&#22312;&#19968;&#36215;&#65307;&#32780;&#20302;&#24773;&#32490;&#32454;&#31890;&#24230;&#24050;&#19982;&#24212;&#28608;&#24773;&#32490;&#35843;&#33410;&#31574;&#30053;&#21644;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#26102;&#38388;&#39034;&#24207;&#28436;&#35762;&#32773;&#35805;&#35821;&#20013;&#35745;&#31639;&#24773;&#32490;&#32454;&#31890;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#65288;&#20195;&#26367;&#21508;&#31181;&#20559;&#35265;&#30340;&#33258;&#25105;&#25253;&#21578;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#36825;&#31181;&#25991;&#26412;&#34893;&#29983;&#24773;&#32490;&#32454;&#31890;&#24230;&#25514;&#26045;&#22312;&#20316;&#20026;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#26465;&#20214;&#65288;MHCs&#65289;&#30340;&#26631;&#35760;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#24773;&#32490;&#32454;&#31890;&#24230;&#30340;&#22522;&#32447;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02281v1 Announce Type: new  Abstract: We are united in how emotions are central to shaping our experiences; and yet, individuals differ greatly in how we each identify, categorize, and express emotions. In psychology, variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity (determined through self-reports of one's emotions). High emotion granularity has been linked with better mental and physical health; whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes. In this work, we propose computational measures of emotion granularity derived from temporally-ordered speaker utterances in social media (in lieu of self-reports that suffer from various biases). We then investigate the effectiveness of such text-derived measures of emotion granularity in functioning as markers of various mental health conditions (MHCs). We establish baseline measures of emotion gran
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02271</link><description>&lt;p&gt;
RIFF: &#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#25913;&#20889;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#31934;&#30830;&#22320;&#20026;&#19979;&#28216;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20248;&#21270;&#36755;&#20837;&#25552;&#31034;&#25110;&#35843;&#25972;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914; LoRA&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21464;&#21407;&#22987;&#20219;&#21153;&#30340;&#36755;&#20837;&#25991;&#26412;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#37325;&#20889;&#36755;&#20837;&#25991;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#36229;&#20986;&#20102;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02271v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#25351;&#26631; FENICE&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02270</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#25351;&#26631; FENICE&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#22823;&#37327;&#33258;&#21160;&#29983;&#25104;&#30340;&#25688;&#35201;&#21576;&#29616;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#27604;&#22914;&#24187;&#35273;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#24341;&#20837;&#30340;&#24230;&#37327;&#26631;&#20934;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#19987;&#27880;&#20110;&#30701;&#25991;&#26723;&#25688;&#35201;&#65288;&#20363;&#22914;&#26032;&#38395;&#25991;&#31456;&#65289;&#20197;&#21450;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#65288;FENICE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21487;&#20449;&#24230;&#23548;&#21521;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02270v1 Announce Type: new  Abstract: Recent advancements in text summarization, particularly with the advent of Large Language Models (LLMs), have shown remarkable performance. However, a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations. In response to this issue, various approaches for the evaluation of consistency for summarization have emerged. Yet, these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), and computational impracticality, especially for LLM-based metrics. To address these shortcomings, we propose Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction (FENICE), a more interpretable and efficient factuality-oriented metric. FENICE leverages an NLI-based alignment between information in the source document and a set of atomic facts,
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25351;&#20986;&#22312;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#65292;&#28151;&#28102;&#20167;&#24680;&#21644;&#20882;&#29359;&#21487;&#33021;&#20250;&#20351;&#30740;&#31350;&#32467;&#26524;&#22833;&#25928;&#65292;&#21628;&#21505;&#26410;&#26469;&#24037;&#20316;&#38656;&#35201;&#20174;&#29702;&#35770;&#19978;&#23558;&#20167;&#24680;&#19982;&#20882;&#29359;&#27010;&#24565;&#36827;&#34892;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.02268</link><description>&lt;p&gt;
&#20027;&#35266; $\textit{Isms}$? &#35770;&#28151;&#28102;&#20167;&#24680;&#19982;&#20882;&#29359;&#22312;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#21361;&#38505;
&lt;/p&gt;
&lt;p&gt;
Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02268
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25351;&#20986;&#22312;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#65292;&#28151;&#28102;&#20167;&#24680;&#21644;&#20882;&#29359;&#21487;&#33021;&#20250;&#20351;&#30740;&#31350;&#32467;&#26524;&#22833;&#25928;&#65292;&#21628;&#21505;&#26410;&#26469;&#24037;&#20316;&#38656;&#35201;&#20174;&#29702;&#35770;&#19978;&#23558;&#20167;&#24680;&#19982;&#20882;&#29359;&#27010;&#24565;&#36827;&#34892;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02268v1 &#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25509;&#21463;&#27880;&#37322;&#32773;&#20027;&#35266;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#20027;&#35201;&#21463;&#21040;&#26631;&#35760;&#24046;&#24322;&#30340;&#39537;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#27599;&#20010;&#27880;&#37322;&#32773;&#30340;&#35266;&#28857;&#35270;&#20026;&#26377;&#25928;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;(&#22914;&#24773;&#24863;&#20998;&#26512;)&#21487;&#33021;&#38750;&#24120;&#36866;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20219;&#21153;&#65292;&#36825;&#31181;&#26500;&#36896;&#21487;&#33021;&#26159;&#19981;&#24688;&#24403;&#30340;&#65292;&#22240;&#20026;&#23427;&#36171;&#20104;&#20102;&#25152;&#26377;&#20851;&#20110;&#24615;&#21035;&#27495;&#35270;&#25110;&#31181;&#26063;&#20027;&#20041;&#31561;&#35758;&#39064;&#30340;&#31435;&#22330;&#30456;&#21516;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#20167;&#24680;&#21644;&#20882;&#29359;&#30340;&#28151;&#28102;&#21487;&#33021;&#20250;&#20351;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#32467;&#26524;&#26080;&#25928;&#65292;&#24182;&#21628;&#21505;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#32622;&#20110;&#29702;&#35770;&#26694;&#26550;&#20013;&#65292;&#23558;&#20167;&#24680;&#19982;&#20854;&#27491;&#20132;&#27010;&#24565;&#20882;&#29359;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02268v1 Announce Type: cross  Abstract: Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling. This approach understands each annotator's view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., sentiment analysis. However, this construction may be inappropriate for tasks such as hate speech detection, as it affords equal validity to all positions on e.g., sexism or racism. We argue that the conflation of hate and offence can invalidate findings on hate speech, and call for future work to be situated in theory, disentangling hate from its orthogonal concept, offence.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02253</link><description>&lt;p&gt;
KnowPhish&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20197;&#22686;&#24378;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24050;&#32473;&#20010;&#20154;&#21644;&#20225;&#19994;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#39640;&#25928;&#30340;&#33258;&#21160;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#65288;RBPDs&#65289;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#27604;&#36739;&#30446;&#26631;&#32593;&#39029;&#19978;&#30340;&#26631;&#24535;&#19982;&#24050;&#30693;&#26631;&#24535;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;RBPDs&#30340;&#20027;&#35201;&#23616;&#38480;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#25163;&#21160;&#26500;&#24314;&#30340;&#21697;&#29260;&#30693;&#35782;&#24211;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#21697;&#29260;&#65292;&#23548;&#33268;&#30001;&#20110;&#30693;&#35782;&#24211;&#20013;&#21697;&#29260;&#35206;&#30422;&#19981;&#36275;&#32780;&#20986;&#29616;&#20551;&#38452;&#24615;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#37319;&#29992;&#35813;&#27969;&#27700;&#32447;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21253;&#21547;20k&#20010;&#21697;&#29260;&#21644;&#27599;&#20010;&#21697;&#29260;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;KnowPhish&#21487;&#20197;&#29992;&#26469;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#25552;&#21319;&#29616;&#26377;RBPDs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
&lt;/p&gt;</description></item><item><title>Birbal&#26159;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#27169;&#22411;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;RTX 4090&#19978;&#36827;&#34892;16&#23567;&#26102;&#24494;&#35843;&#65292;&#25104;&#21151;&#22320;&#25972;&#29702;&#20102;&#39640;&#36136;&#37327;&#25351;&#20196;&#65292;&#20351;&#24615;&#33021;&#25552;&#39640;&#20102;35%&#12290;</title><link>https://arxiv.org/abs/2403.02247</link><description>&lt;p&gt;
Birbal: &#19968;&#31181;&#20351;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#39640;&#25928;7B&#25351;&#20196;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Birbal: An efficient 7B instruct-model fine-tuned with curated datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02247
&lt;/p&gt;
&lt;p&gt;
Birbal&#26159;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#27169;&#22411;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;RTX 4090&#19978;&#36827;&#34892;16&#23567;&#26102;&#24494;&#35843;&#65292;&#25104;&#21151;&#22320;&#25972;&#29702;&#20102;&#39640;&#36136;&#37327;&#25351;&#20196;&#65292;&#20351;&#24615;&#33021;&#25552;&#39640;&#20102;35%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMOps&#30001;&#20110;&#30828;&#20214;&#38656;&#27714;&#32780;&#20135;&#29983;&#37325;&#22823;&#25104;&#26412;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#21487;&#21450;&#24615;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#32570;&#20047;&#36879;&#26126;&#24230;&#23548;&#33268;&#22823;&#22810;&#25968;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;NeurIPS Workshop&#25512;&#20986;&#20102;LLM Efficiency Challenge&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21333;&#20010;GPU&#65288;RTX 4090&#25110;&#24102;&#26377;40GB&#30340;A100&#65289;&#19978;&#22312;24&#23567;&#26102;&#20869;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Birbal&#65292;&#25105;&#20204;&#22522;&#20110;Mistral-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#33719;&#32988;&#27169;&#22411;&#65292;&#24494;&#35843;&#26102;&#38388;&#20026;16&#23567;&#26102;&#12290;Birbal&#30340;&#25104;&#21151;&#22312;&#20110;&#25972;&#29702;&#20102;&#35206;&#30422;&#21508;&#31181;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#65292;&#20351;&#20854;&#24615;&#33021;&#27604;&#31532;&#20108;&#21517;Qwen-14B&#22522;&#20110;&#25552;&#20132;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;35%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02247v1 Announce Type: new  Abstract: LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible. To tackle these challenges, the LLM Efficiency Challenge was introduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe. In this system description paper, we introduce Birbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for 16 hours. Birbal's success lies in curating high-quality instructions covering diverse tasks, resulting in a 35% performance improvement over second-best Qwen-14B based submission.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#24341;&#21457;&#29305;&#23450;&#20154;&#26684;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#40657;&#26263;&#19977;&#21512;&#20250;&#30340;&#29305;&#36136;&#23545;&#22810;&#31181;LLMs&#22312;&#19981;&#21516;ToM&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22823;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.02246</link><description>&lt;p&gt;
PHAnToM&#65306;&#20154;&#26684;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#29702;&#35770;&#25512;&#29702;&#20135;&#29983;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02246
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24341;&#21457;&#29305;&#23450;&#20154;&#26684;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#40657;&#26263;&#19977;&#21512;&#20250;&#30340;&#29305;&#36136;&#23545;&#22810;&#31181;LLMs&#22312;&#19981;&#21516;ToM&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22823;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#19982;&#29978;&#33267;&#20248;&#20110;&#20154;&#31867;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#19968;&#36827;&#23637;&#65292;LLMs&#22312;&#31038;&#20250;&#35748;&#30693;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#19981;&#36275;&#65292;&#32780;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#22825;&#29983;&#23601;&#24456;&#25797;&#38271;&#12290;&#21463;&#21040;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#26576;&#20123;&#20154;&#26684;&#29305;&#36136;&#19982;&#24515;&#29702;&#29702;&#35770;&#65288;ToM&#65289;&#25512;&#29702;&#20043;&#38388;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#20851;&#20110;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#22312;&#24433;&#21709;LLMs&#33021;&#21147;&#26041;&#38754;&#30340;&#36229;&#25935;&#24863;&#24615;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#25552;&#31034;&#22312;LLMs&#20013;&#24341;&#21457;&#20154;&#26684;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;ToM&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#24341;&#21457;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;LLMs&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;ToM&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#26469;&#33258;&#40657;&#26263;&#19977;&#21512;&#20250;(Dark Triad)&#30340;&#29305;&#36136;&#23545;&#20110;&#20687;GPT-3.5&#12289;Llama 2&#21644;Mistral&#36825;&#26679;&#30340;LLMs&#22312;&#19981;&#21516;&#30340;ToM&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22823;&#30340;&#21464;&#37327;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#26576;&#20123;&#20154;&#26684;&#29305;&#36136;&#30340;LLMs&#22312;&#25191;&#34892;ToM&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02246v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) demonstrate that their capabilities are comparable, or even superior, to humans in many tasks in natural language processing. Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at. Drawing inspiration from psychological research on the links between certain personality traits and Theory-of-Mind (ToM) reasoning, and from prompt engineering research on the hyper-sensitivity of prompts in affecting LLMs capabilities, this study investigates how inducing personalities in LLMs using prompts affects their ToM reasoning capabilities. Our findings show that certain induced personalities can significantly affect the LLMs' reasoning capabilities in three different ToM tasks. In particular, traits from the Dark Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral across the different ToM tasks. We find that LLMs tha
&lt;/p&gt;</description></item><item><title>ChatGPT&#34987;&#24212;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#36801;&#31227;&#23398;&#20064;&#34701;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29983;&#25104;&#36731;&#37327;&#32423;&#20027;&#39064;&#21644;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#37327;&#21270;&#25237;&#36164;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.02185</link><description>&lt;p&gt;
ChatGPT&#20027;&#39064;&#21644;&#24773;&#24863;&#24314;&#27169;&#22312;&#37329;&#34701;&#20013;&#30340;&#24212;&#29992;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distilled ChatGPT Topic &amp; Sentiment Modeling with Applications in Finance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02185
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#34987;&#24212;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#36801;&#31227;&#23398;&#20064;&#34701;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29983;&#25104;&#36731;&#37327;&#32423;&#20027;&#39064;&#21644;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#37327;&#21270;&#25237;&#36164;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;ChatGPT&#21019;&#24314;&#20102;&#31616;&#21270;&#27169;&#22411;&#65292;&#29983;&#25104;&#26131;&#20110;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#35780;&#20272;&#20174;&#36130;&#25253;&#30005;&#35805;&#20250;&#35758;&#20013;&#24471;&#20986;&#30340;&#36130;&#21153;&#32467;&#26524;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#36731;&#37327;&#32423;&#30340;&#20027;&#39064;&#21644;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#65292;&#32780;&#20934;&#30830;&#29575;&#25439;&#22833;&#19981;&#22823;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#12290;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#20004;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#31361;&#26174;&#20102;&#29983;&#25104;&#30340;&#29305;&#24449;&#22914;&#20309;&#22312;&#37327;&#21270;&#25237;&#36164;&#22330;&#26223;&#20013;&#26377;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02185v1 Announce Type: cross  Abstract: In this study, ChatGPT is utilized to create streamlined models that generate easily interpretable features. These features are then used to evaluate financial outcomes from earnings calls. We detail a training approach that merges knowledge distillation and transfer learning, resulting in lightweight topic and sentiment classification models without significant loss in accuracy. These models are assessed through a dataset annotated by experts. The paper also delves into two practical case studies, highlighting how the generated features can be effectively utilized in quantitative investing scenarios.
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.02178</link><description>&lt;p&gt;
&#25513;&#38754;&#24605;&#24819;:&#31616;&#21333;&#22320;&#25513;&#30422;&#37096;&#20998;&#25512;&#29702;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02178
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#36731;&#24494;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#38543;&#26426;&#25513;&#30422;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#36825;&#31181;&#25216;&#26415;&#23545;&#25512;&#29702;&#20219;&#21153;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02178v1 Announce Type: cross  Abstract: In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#21010;-&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#34920;&#26684;&#19978;&#30340;&#21477;&#23376;&#32972;&#26223;&#20013;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#65292;&#36890;&#36807;&#23545;Llama-2-7B&#36827;&#34892;&#24494;&#35843;&#65292;&#26500;&#24314;&#20102;ProTrix&#27169;&#22411;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#34920;&#26684;&#20219;&#21153;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#21487;&#29983;&#25104;&#20934;&#30830;&#19988;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.02177</link><description>&lt;p&gt;
ProTrix: &#20351;&#29992;&#21477;&#23376;&#32972;&#26223;&#26500;&#24314;&#29992;&#20110;&#35268;&#21010;&#21644;&#25512;&#29702;&#34920;&#26684;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#21010;-&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#34920;&#26684;&#19978;&#30340;&#21477;&#23376;&#32972;&#26223;&#20013;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#65292;&#36890;&#36807;&#23545;Llama-2-7B&#36827;&#34892;&#24494;&#35843;&#65292;&#26500;&#24314;&#20102;ProTrix&#27169;&#22411;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#34920;&#26684;&#20219;&#21153;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#21487;&#29983;&#25104;&#20934;&#30830;&#19988;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#34920;&#26684;&#22312;&#20256;&#36798;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26159;&#32452;&#32455;&#21644;&#21576;&#29616;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#21010;-&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22238;&#31572;&#24102;&#26377;&#21477;&#23376;&#32972;&#26223;&#30340;&#34920;&#26684;&#19978;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#35268;&#21010;&#19978;&#19979;&#25991;&#20013;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#27493;&#39588;&#20998;&#37197;&#32473;&#22522;&#20110;&#31243;&#24207;&#25110;&#25991;&#26412;&#30340;&#25512;&#29702;&#65292;&#20197;&#36798;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#25105;&#20204;&#26681;&#25454;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#38598;TrixtInstruct&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#37027;&#20123;&#38656;&#35201;&#32467;&#21512;&#34920;&#26684;&#21644;&#21477;&#23376;&#20449;&#24687;&#26469;&#33719;&#24471;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31243;&#24207;&#26080;&#27861;&#35299;&#20915;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;TrixInstruct&#19978;&#30340;Llama-2-7B&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;ProTrix&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ProTrix&#23545;&#21508;&#31181;&#34920;&#26684;&#20219;&#21153;&#20855;&#26377;&#26222;&#36941;&#24615;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#19982;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ProTrix&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#26469;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02177v1 Announce Type: new  Abstract: Tables play a crucial role in conveying information in various domains, serving as indispensable tools for organizing and presenting data in a structured manner. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. We construct an instruction tuning set TrixInstruct following the framework. Our dataset cover queries that are program-unsolvable or need combining information from tables and sentences to obtain planning and reasoning abilities. We present ProTrix by finetuning Llama-2-7B on TrixInstruct. Our experiments show that ProTrix generalizes to diverse tabular tasks and achieves comparable performance to GPT-3.5-turbo. We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#26032;&#30340;&#38382;&#39064;-&#31572;&#26696;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#34920;&#31034;&#12289;&#31572;&#26696;&#20505;&#36873;&#39033;&#23884;&#20837;&#21644;&#20869;&#23384;&#25928;&#29575;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02176</link><description>&lt;p&gt;
EEE-QA: &#25506;&#32034;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#38382;&#39064;-&#31572;&#26696;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EEE-QA: Exploring Effective and Efficient Question-Answer Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#26032;&#30340;&#38382;&#39064;-&#31572;&#26696;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#34920;&#31034;&#12289;&#31572;&#26696;&#20505;&#36873;&#39033;&#23884;&#20837;&#21644;&#20869;&#23384;&#25928;&#29575;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22914;RoBERTa&#12290;&#36825;&#39033;&#24037;&#20316;&#25361;&#25112;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#32534;&#30721;&#32422;&#23450;&#65292;&#25506;&#32034;&#26356;&#31934;&#32454;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;&#20102;&#19982;&#20351;&#29992;&#21477;&#23376;&#24320;&#22836;&#26631;&#35760;&#20316;&#20026;&#38382;&#39064;&#34920;&#31034;&#30456;&#27604;&#65292;&#26356;&#22909;&#36136;&#37327;&#30340;&#21508;&#31181;&#27744;&#21270;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21516;&#26102;&#23558;&#25152;&#26377;&#31572;&#26696;&#20505;&#36873;&#39033;&#19982;&#38382;&#39064;&#23884;&#20837;&#30340;&#26426;&#20250;&#12290;&#36825;&#26679;&#21487;&#20197;&#20351;&#31572;&#26696;&#36873;&#25321;&#20043;&#38388;&#36827;&#34892;&#20132;&#21449;&#21442;&#32771;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#37327;&#26469;&#25552;&#39640;&#25512;&#26029;&#21534;&#21520;&#37327;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#20294;&#30446;&#21069;&#23578;&#26410;&#22312;&#24403;&#21069;&#26694;&#26550;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;PLMs&#65292;&#24182;&#22312;&#26159;&#21542;&#38598;&#25104;&#30693;&#35782;&#22270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#22312;&#24615;&#33021;&#19978;&#20960;&#20046;&#27809;&#26377;&#29306;&#29298;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25552;&#39640;&#20102;38-100%&#30340;&#21534;&#21520;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;26-65%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02176v1 Announce Type: new  Abstract: Current approaches to question answering rely on pre-trained language models (PLMs) like RoBERTa. This work challenges the existing question-answer encoding convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a question representation for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the question. This enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different PLMs, and with and without the integration of knowledge graphs. Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100% throughput with 26-65% speedups on consumer-grade G
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;LeBenchmark&#36825;&#20010;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#23545;&#27861;&#35821;&#21477;&#27861;&#20449;&#24687;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#23398;&#20064;&#21040;&#20102;&#19968;&#20123;&#21477;&#27861;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.02173</link><description>&lt;p&gt;
LeBenchmark&#23545;&#27861;&#35821;&#21477;&#27861;&#23398;&#20064;&#20102;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What has LeBenchmark Learnt about French Syntax?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02173
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;LeBenchmark&#36825;&#20010;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#23545;&#27861;&#35821;&#21477;&#27861;&#20449;&#24687;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#23398;&#20064;&#21040;&#20102;&#19968;&#20123;&#21477;&#27861;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25253;&#21578;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#25506;&#31350;LeBenchmark&#30340;&#23454;&#39564;&#65292;LeBenchmark&#26159;&#19968;&#20010;&#22312;7000&#23567;&#26102;&#30340;&#27861;&#35821;&#21475;&#35821;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#33719;&#21462;&#21477;&#27861;&#20449;&#24687;&#12290;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#19979;&#28216;&#35821;&#38899;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#21475;&#35821;&#29702;&#35299;&#25110;&#35821;&#38899;&#35299;&#26512;&#12290; &#23613;&#31649;&#23427;&#20204;&#26159;&#22312;&#38750;&#24120;&#20302;&#32423;&#30340;&#20449;&#24687;&#65288;&#21407;&#22987;&#35821;&#38899;&#20449;&#21495;&#65289;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#27809;&#26377;&#26126;&#30830;&#30340;&#35789;&#27719;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#35821;&#35328;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26032;&#20852;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#23545;&#21477;&#27861;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#20351;&#29992;Orf\'eo&#26641;&#24211;&#26469;&#25506;&#31350;LeBenchmark&#30340;&#27599;&#20010;&#34920;&#31034;&#23618;&#30340;&#21477;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#24050;&#32463;&#23398;&#20250;&#20102;&#19968;&#20123;&#21477;&#27861;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21477;&#27861;&#20449;&#24687;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#23618;&#26356;&#23481;&#26131;&#34987;&#25277;&#21462;&#20986;&#26469;&#65292;&#20043;&#21518;&#26159;&#19968;&#20010;&#38750;&#24120;&#38497;&#23789;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02173v1 Announce Type: new  Abstract: The paper reports on a series of experiments aiming at probing LeBenchmark, a pretrained acoustic model trained on 7k hours of spoken French, for syntactic information. Pretrained acoustic models are increasingly used for downstream speech tasks such as automatic speech recognition, speech translation, spoken language understanding or speech parsing. They are trained on very low level information (the raw speech signal), and do not have explicit lexical knowledge. Despite that, they obtained reasonable results on tasks that requires higher level linguistic knowledge. As a result, an emerging question is whether these models encode syntactic information. We probe each representation layer of LeBenchmark for syntax, using the Orf\'eo treebank, and observe that it has learnt some syntactic information. Our results show that syntactic information is more easily extractable from the middle layers of the network, after which a very sharp decre
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;</title><link>https://arxiv.org/abs/2403.02167</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#35782;&#21035;&#35821;&#38899;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition from voice messages recorded in the wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02167
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#34920;&#28436;&#25110;&#24341;&#21457;&#30340;&#35821;&#38899;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Emotional Voice Messages&#65288;EMOVOME&#65289;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#22312;&#28040;&#24687;&#24212;&#29992;&#20013;&#30340;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#30001;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#26631;&#27880;&#32773;&#20197;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#24773;&#24863;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;eGeMAPS&#29305;&#24449;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#35762;&#35805;&#32773;&#26080;&#20851;&#30340;SER&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#21442;&#32771;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20998;&#26512;&#20102;&#26631;&#27880;&#32773;&#21644;&#24615;&#21035;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#30340;Unispeech-L&#27169;&#22411;&#21450;&#20854;&#19982;eGeMAPS&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#22312;3&#31867;valence&#21644;arousal&#39044;&#27979;&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;61.64%&#21644;55.57%&#30340;Unweighted Accuracy&#65288;UA&#65289;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;&#23545;&#20110;&#24773;&#24863;&#31867;&#21035;&#65292;&#33719;&#24471;&#20102;42.58%&#30340;UA&#12290;EMOVOME&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02167v1 Announce Type: cross  Abstract: Emotion datasets used for Speech Emotion Recognition (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the Emotional Voice Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, transformer-based models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender fairness. The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.02130</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20135;&#21697;&#23646;&#24615;&#20540;
&lt;/p&gt;
&lt;p&gt;
Using LLMs for the Extraction and Normalization of Product Attribute Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#20135;&#21697;&#25552;&#20379;&#36890;&#24120;&#21253;&#25324;&#25991;&#26412;&#20135;&#21697;&#26631;&#39064;&#21644;&#25991;&#26412;&#20135;&#21697;&#25551;&#36848;&#12290;&#20026;&#20102;&#25552;&#20379;&#35832;&#22914;&#20998;&#38754;&#20135;&#21697;&#36807;&#28388;&#25110;&#22522;&#20110;&#20869;&#23481;&#30340;&#20135;&#21697;&#25512;&#33616;&#31561;&#21151;&#33021;&#65292;&#32593;&#31449;&#38656;&#35201;&#20174;&#38750;&#32467;&#26500;&#21270;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#23646;&#24615;&#20540;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WDC&#20135;&#21697;&#23646;&#24615;-&#20540;&#25552;&#21462;&#65288;WDC PAVE&#65289;&#25968;&#25454;&#38598;&#12290;WDC PAVE&#21253;&#21547;&#26469;&#33258;&#25552;&#20379;schema.org&#27880;&#37322;&#30340;87&#20010;&#32593;&#31449;&#30340;&#20135;&#21697;&#25552;&#20379;&#12290;&#36825;&#20123;&#25552;&#20379;&#23646;&#20110;&#20116;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#27599;&#20010;&#31867;&#21035;&#37117;&#20855;&#26377;&#19968;&#32452;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#20004;&#31181;&#24418;&#24335;&#25552;&#20379;&#25163;&#21160;&#39564;&#35777;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65306;&#65288;i&#65289;&#30452;&#25509;&#25552;&#21462;&#30340;&#20540;&#21644;&#65288;ii&#65289;&#35268;&#33539;&#21270;&#30340;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02130v1 Announce Type: new  Abstract: Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product recommendation, the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema.org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization 
&lt;/p&gt;</description></item><item><title>LOCR&#26159;&#19968;&#31181;&#38754;&#21521;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#36807;&#31243;&#20013;&#22312;transformer&#26550;&#26500;&#20013;&#38598;&#25104;&#20301;&#32622;&#24341;&#23548;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23398;&#26415;&#25991;&#26723;&#20013;&#30340;&#37325;&#22797;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.02127</link><description>&lt;p&gt;
LOCR&#65306;&#38754;&#21521;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#20301;&#32622;&#24341;&#23548;Transformer
&lt;/p&gt;
&lt;p&gt;
LOCR: Location-Guided Transformer for Optical Character Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02127
&lt;/p&gt;
&lt;p&gt;
LOCR&#26159;&#19968;&#31181;&#38754;&#21521;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#36807;&#31243;&#20013;&#22312;transformer&#26550;&#26500;&#20013;&#38598;&#25104;&#20301;&#32622;&#24341;&#23548;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23398;&#26415;&#25991;&#26723;&#20013;&#30340;&#37325;&#22797;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#25991;&#26723;&#20805;&#26021;&#30528;&#25991;&#26412;&#12289;&#26041;&#31243;&#24335;&#12289;&#34920;&#26684;&#21644;&#22270;&#24418;&#65292;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#12290;&#23613;&#31649;&#31471;&#21040;&#31471;OCR&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#22522;&#20110;&#24067;&#23616;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#22788;&#29702;&#37325;&#22797;&#24615;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#8220;&#39046;&#22495;&#22806;&#8221;&#65288;OOD&#65289;&#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#24067;&#23616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCR&#65292;&#19968;&#31181;&#23558;&#20301;&#32622;&#24341;&#23548;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;125K&#20010;&#23398;&#26415;&#25991;&#26723;&#39029;&#38754;&#30340;&#36229;&#36807;7700&#19975;&#20010;&#25991;&#26412;-&#20301;&#32622;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#21253;&#25324;&#21333;&#35789;&#12289;&#34920;&#26684;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#36793;&#30028;&#26694;&#12290;LOCR&#33021;&#29087;&#32451;&#22788;&#29702;&#21508;&#31181;&#26684;&#24335;&#20803;&#32032;&#24182;&#20197;Markdown&#35821;&#35328;&#29983;&#25104;&#20869;&#23481;&#12290;&#22312;&#25105;&#20204;&#20174;arXiv&#26500;&#24314;&#30340;&#27979;&#35797;&#38598;&#20013;&#65292;&#34913;&#37327;&#26041;&#24335;&#20026;&#32534;&#36753;&#36317;&#31163;&#12289;BLEU&#12289;METEOR&#21644;F-measure&#65292;LOCR&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;LOCR&#36824;&#23558;&#37325;&#22797;&#39057;&#29575;&#20174;4
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02127v1 Announce Type: cross  Abstract: Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also reduces repetition frequency from 4
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24369;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#25506;&#32034;&#20102;&#38646;&#27425;&#23398;&#20064;&#12289;&#19968;&#27425;&#23398;&#20064;&#21644;&#23569;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02121</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#20013;&#21033;&#29992;&#24369;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#21487;&#34892;&#24615;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24369;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#25506;&#32034;&#20102;&#38646;&#27425;&#23398;&#20064;&#12289;&#19968;&#27425;&#23398;&#20064;&#21644;&#23569;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#25512;&#21160;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#26631;&#27880;&#21644;&#35757;&#32451;&#37117;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#26368;&#36817;&#65292;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#25104;&#20026;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;&#22312;&#28151;&#21512;&#20195;&#30721;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#38382;&#39064;&#39046;&#22495;&#65292;LLM&#30340;&#20351;&#29992;&#22312;&#36825;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;100&#26465;YouTube&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#20013;&#30340;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#24615;&#21035;&#27495;&#35270;&#36827;&#34892;&#20102;&#24369;&#26631;&#27880;&#12290;&#30001;&#20110;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#27880;&#37322;&#65292;&#22240;&#27492;&#37319;&#29992;&#20102;&#24369;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#20102;&#38646;&#27425;&#23398;&#20064;&#12289;&#19968;&#27425;&#23398;&#20064;&#21644;&#23569;&#27425;&#23398;&#20064;&#20197;&#21450;&#25552;&#31034;&#26041;&#27861;&#26469;&#20026;&#35780;&#35770;&#20998;&#37197;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#24037;&#26631;&#35760;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02121v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has advanced the benchmark in various Natural Language Processing (NLP) tasks. However, large amounts of labelled training data are required to train LLMs. Furthermore, data annotation and training are computationally expensive and time-consuming. Zero and few-shot learning have recently emerged as viable options for labelling data using large pre-trained models. Hate speech detection in mix-code low-resource languages is an active problem area where the use of LLMs has proven beneficial. In this study, we have compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish. Weak annotation was applied due to the labor-intensive annotation process. Zero-shot learning, one-shot learning, and few-shot learning and prompting approaches have then been applied to assign labels to the comments and compare them to human-ass
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.02090</link><description>&lt;p&gt;
&#24314;&#27169;&#22810;&#27169;&#24577;&#31038;&#20132;&#20114;&#21160;&#65306;&#20855;&#26377;&#23494;&#38598;&#23545;&#40784;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28041;&#21450;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#31038;&#20132;&#20114;&#21160;&#23545;&#26377;&#25928;&#35299;&#37322;&#31038;&#20132;&#24773;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#27169;&#24577;&#31038;&#20132;&#32447;&#32034;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20154;&#34892;&#20026;&#19978;&#65292;&#25110;&#20381;&#36182;&#20110;&#19982;&#22810;&#26041;&#29615;&#22659;&#20013;&#30340;&#35805;&#35821;&#23494;&#20999;&#23545;&#40784;&#30340;&#25972;&#20307;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#20204;&#22312;&#24314;&#27169;&#22810;&#26041;&#20114;&#21160;&#30340;&#22797;&#26434;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65306;&#35805;&#35821;&#30446;&#26631;&#35782;&#21035;&#12289;&#20195;&#35789;&#25351;&#20195;&#28040;&#35299;&#21644;&#25552;&#21450;&#29609;&#23478;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#20013;&#30340;&#36825;&#20123;&#26032;&#25361;&#25112;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35805;&#35821;&#21516;&#27493;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-turbo 3.5&#33258;&#21160;&#29983;&#25104;&#33521;&#35821;&#35789;&#27719;&#22810;&#39033;&#36873;&#25321;&#22635;&#31354;&#39064;&#30446;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;VocaTT&#24341;&#25806;&#30340;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#39064;&#30446;&#26377;&#24456;&#39640;&#30340;&#33391;&#22909;&#24418;&#24335;&#27604;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.02078</link><description>&lt;p&gt;
&#20351;&#29992;GPT-turbo 3.5&#33258;&#21160;&#29983;&#25104;&#33521;&#35821;&#35789;&#27719;&#22810;&#39033;&#36873;&#25321;&#22635;&#31354;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-turbo 3.5&#33258;&#21160;&#29983;&#25104;&#33521;&#35821;&#35789;&#27719;&#22810;&#39033;&#36873;&#25321;&#22635;&#31354;&#39064;&#30446;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;VocaTT&#24341;&#25806;&#30340;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#39064;&#30446;&#26377;&#24456;&#39640;&#30340;&#33391;&#22909;&#24418;&#24335;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#22635;&#31354;&#39064;&#30446;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;VocaTT&#24341;&#25806;&#37319;&#29992;Python&#32534;&#20889;&#65292;&#21253;&#25324;&#19977;&#20010;&#22522;&#26412;&#27493;&#39588;&#65306;&#39044;&#22788;&#29702;&#30446;&#26631;&#35789;&#27719;&#21015;&#34920;&#12289;&#21033;&#29992;GPT&#29983;&#25104;&#21477;&#23376;&#21644;&#20505;&#36873;&#21333;&#35789;&#36873;&#39033;&#65292;&#26368;&#21518;&#36873;&#25321;&#21512;&#36866;&#30340;&#36873;&#39033;&#12290;&#36890;&#36807;&#29983;&#25104;60&#20010;&#20197;&#23398;&#26415;&#35789;&#27719;&#20026;&#30446;&#26631;&#30340;&#38382;&#39064;&#26469;&#27979;&#35797;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;75%&#30340;&#21477;&#23376;&#21644;66.85%&#30340;&#20505;&#36873;&#35789;&#36873;&#39033;&#24418;&#24335;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02078v1 Announce Type: new  Abstract: A common way of assessing language learners' mastery of vocabulary is via multiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of test items can be laborious for individual teachers or in large-scale language programs. In this paper, we evaluate a new method for automatically generating these types of questions using large language models (LLM). The VocaTT (vocabulary teaching and training) engine is written in Python and comprises three basic steps: pre-processing target word lists, generating sentences and candidate word options using GPT, and finally selecting suitable word options. To test the efficiency of this system, 60 questions were generated targeting academic words. The generated items were reviewed by expert reviewers who judged the well-formedness of the sentences and word options, adding comments to items judged not well-formed. Results showed a 75% rate of well-formedness for sentences and 66.85% rat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20027;&#39064;&#24863;&#30693;&#25506;&#31350;&#26041;&#27861;&#25506;&#35752;&#20102;Transformer-based&#27169;&#22411;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#26102;&#23545;&#20027;&#39064;&#20449;&#21495;&#30340;&#20027;&#35201;&#20381;&#36182;&#31243;&#24230;&#65292;&#24182;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20013;&#32534;&#30721;&#20102;&#20027;&#39064;&#20449;&#24687;&#21644;&#38750;&#20027;&#39064;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.02009</link><description>&lt;p&gt;
&#20027;&#39064;&#24863;&#30693;&#25506;&#31350;&#65306;&#20174;&#21477;&#23376;&#38271;&#24230;&#39044;&#27979;&#21040;&#20064;&#35821;&#35782;&#21035;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20027;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20027;&#39064;&#24863;&#30693;&#25506;&#31350;&#26041;&#27861;&#25506;&#35752;&#20102;Transformer-based&#27169;&#22411;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#26102;&#23545;&#20027;&#39064;&#20449;&#21495;&#30340;&#20027;&#35201;&#20381;&#36182;&#31243;&#24230;&#65292;&#24182;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20013;&#32534;&#30721;&#20102;&#20027;&#39064;&#20449;&#24687;&#21644;&#38750;&#20027;&#39064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#26102;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35789;&#24207;/&#21477;&#27861;&#25110;&#35789;&#20849;&#29616;/&#20027;&#39064;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;Transformer-based&#27169;&#22411;&#65288;&#22914;BERT&#21644;RoBERTa&#65289;&#22312;&#19968;&#31995;&#21015;&#33521;&#35821;&#25506;&#31350;&#20219;&#21153;&#65288;&#20174;&#31616;&#21333;&#30340;&#35789;&#27719;&#20219;&#21153;&#22914;&#21477;&#23376;&#38271;&#24230;&#39044;&#27979;&#21040;&#22797;&#26434;&#30340;&#35821;&#20041;&#20219;&#21153;&#22914;&#20064;&#35821;&#26631;&#35760;&#35782;&#21035;&#65289;&#19978;&#30340;&#34920;&#29616;&#19982;&#36825;&#20123;&#20219;&#21153;&#23545;&#20027;&#39064;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#65292;&#20026;&#36825;&#19968;&#20105;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#20027;&#39064;&#24863;&#30693;&#25506;&#31350;&#30340;&#26032;&#39062;&#25506;&#31350;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#38388;&#23618;&#20013;&#32534;&#30721;&#20102;&#20027;&#39064;&#20449;&#24687;&#21644;&#38750;&#20027;&#39064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02009v1 Announce Type: new  Abstract: Transformer-based Neural Language Models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models' (BERT and RoBERTa's) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call topic-aware probing. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;LMORT&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;LLM&#23450;&#21521;&#26816;&#32034;&#35843;&#33410;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23494;&#38598;&#26816;&#32034;&#65292;&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;DR&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38646;-shot&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01999</link><description>&lt;p&gt;
LLM-Oriented Retrieval Tuner
&lt;/p&gt;
&lt;p&gt;
LLM-Oriented Retrieval Tuner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01999
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;LMORT&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;LLM&#23450;&#21521;&#26816;&#32034;&#35843;&#33410;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23494;&#38598;&#26816;&#32034;&#65292;&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;DR&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38646;-shot&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#29616;&#22312;&#34987;&#35748;&#20026;&#26159;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT3&#21644;GPT-4&#35760;&#24518;&#33021;&#21147;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#34701;&#20837;&#22806;&#37096;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;DR&#20043;&#38388;&#30340;&#33539;&#24335;&#24046;&#24322;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#20219;&#21153;&#25972;&#21512;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;LLM&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;LLM&#23450;&#21521;&#26816;&#32034;&#35843;&#33410;&#22120;&#65292;&#21363;LMORT&#65292;&#23427;&#33021;&#22815;&#23558;DR&#23481;&#37327;&#19982;&#22522;&#30784;LLM&#35299;&#32806;&#65292;&#24182;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#22320;&#21327;&#35843;LLM&#30340;&#20248;&#21270;&#23545;&#40784;&#21644;&#32479;&#19968;&#23618;&#21521;&#32479;&#19968;&#30340;DR&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;DR&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;LLM&#26412;&#36523;&#12290;&#22312;&#20845;&#20010;BEIR&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;LLM&#29983;&#25104;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#19968;&#31995;&#21015;&#24378;&#22823;DR&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38646;-shot&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01999v1 Announce Type: new  Abstract: Dense Retrieval (DR) is now considered as a promising tool to enhance the memorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by incorporating external memories. However, due to the paradigm discrepancy between text generation of LLM and DR, it is still an open challenge to integrate the retrieval and generation tasks in a shared LLM. In this paper, we propose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which decouples DR capacity from base LLM and non-invasively coordinates the optimally aligned and uniform layers of the LLM towards a unified DR space, achieving an efficient and effective DR without tuning the LLM itself. The extensive experiments on six BEIR datasets show that our approach could achieve competitive zero-shot retrieval performance compared to a range of strong DR models while maintaining the generation ability of LLM.
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21464;&#21387;&#22120;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#39321;&#33609;&#21464;&#21387;&#22120;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#36801;&#31227;&#33021;&#21147;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#25351;&#20986;&#39321;&#33609;&#27169;&#22411;&#26159;&#36801;&#31227;&#33021;&#21147;&#30340;&#26377;&#25928;&#25945;&#24072;&#65292;&#25351;&#23548;MoE&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.01994</link><description>&lt;p&gt;
&#39321;&#33609;&#21464;&#21387;&#22120;&#26159;&#36801;&#31227;&#33021;&#21147;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
Vanilla Transformers are Transfer Capability Teachers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01994
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21464;&#21387;&#22120;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#39321;&#33609;&#21464;&#21387;&#22120;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#36801;&#31227;&#33021;&#21147;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#25351;&#20986;&#39321;&#33609;&#27169;&#22411;&#26159;&#36801;&#31227;&#33021;&#21147;&#30340;&#26377;&#25928;&#25945;&#24072;&#65292;&#25351;&#23548;MoE&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21464;&#21387;&#22120;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;MoE&#21464;&#21387;&#22120;&#30340;&#34920;&#29616;&#19981;&#21450;&#39321;&#33609;&#21464;&#21387;&#22120;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;MoE&#27169;&#22411;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#36801;&#31227;&#33021;&#21147;&#26159;&#24433;&#21709;&#20854;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#32852;&#21512;&#20915;&#23450;&#22240;&#32032;&#12290;&#19982;&#39321;&#33609;&#27169;&#22411;&#30456;&#27604;&#65292;MoE&#27169;&#22411;&#30340;&#36801;&#31227;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36801;&#31227;&#33021;&#21147;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#35748;&#20026;&#34429;&#28982;&#39321;&#33609;&#27169;&#22411;&#24615;&#33021;&#36739;&#24369;&#65292;&#20294;&#23427;&#20204;&#26159;&#36801;&#31227;&#33021;&#21147;&#30340;&#26377;&#25928;&#25945;&#24072;&#12290;&#30001;&#39321;&#33609;&#27169;&#22411;&#25351;&#23548;&#30340;MoE&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#36801;&#31227;&#33021;&#21147;&#65292;&#26368;&#32456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01994v1 Announce Type: new  Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately
&lt;/p&gt;</description></item><item><title>FakeNewsGPT4&#26159;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#20110;&#20266;&#36896;&#30340;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;LVLMs&#22312;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.01988</link><description>&lt;p&gt;
FakeNewsGPT4&#65306;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LVLMs&#25512;&#36827;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01988
&lt;/p&gt;
&lt;p&gt;
FakeNewsGPT4&#26159;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#20110;&#20266;&#36896;&#30340;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;LVLMs&#22312;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20419;&#20351;&#38656;&#35201;&#24191;&#20041;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#30340;&#23396;&#31435;&#24615;&#38480;&#21046;&#20102;&#20256;&#32479;&#26816;&#27979;&#22120;&#33719;&#24471;&#24320;&#25918;&#19990;&#30028;&#20107;&#23454;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FakeNewsGPT4&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#28155;&#29305;&#23450;&#20110;&#20266;&#36896;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36827;&#34892;&#25805;&#32437;&#25512;&#29702;&#65292;&#21516;&#26102;&#32487;&#25215;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#20316;&#20026;&#34917;&#20805;&#12290;FakeNewsGPT4&#20013;&#30340;&#30693;&#35782;&#22686;&#24378;&#28041;&#21450;&#33719;&#21462;&#20004;&#31181;&#20266;&#36896;&#29305;&#23450;&#30693;&#35782;&#65292;&#21363;&#35821;&#20041;&#30456;&#20851;&#21644;&#24037;&#20214;&#36861;&#36394;&#65292;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LVLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01988v1 Announce Type: new  Abstract: The massive generation of multimodal fake news exhibits substantial distribution discrepancies, prompting the need for generalized detectors. However, the insulated nature of training within specific domains restricts the capability of classical detectors to obtain open-world facts. In this paper, we propose FakeNewsGPT4, a novel framework that augments Large Vision-Language Models (LVLMs) with forgery-specific knowledge for manipulation reasoning while inheriting extensive world knowledge as complementary. Knowledge augmentation in FakeNewsGPT4 involves acquiring two types of forgery-specific knowledge, i.e., semantic correlation and artifact trace, and merging them into LVLMs. Specifically, we design a multi-level cross-modal reasoning module that establishes interactions across modalities for extracting semantic correlations. Concurrently, a dual-branch fine-grained verification module is presented to comprehend localized details to e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;&#35821;&#35328;&#23545;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340; Transformer &#27169;&#22411;&#65292;&#21457;&#29616;&#27491;&#30830;&#36873;&#25321;&#23376;&#35789;&#27169;&#22411;&#26159;&#32763;&#35793;&#24615;&#33021;&#30340;&#26368;&#22823;&#39537;&#21160;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.01985</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21464;&#21387;&#22120;&#65306;Is F\'eidir Linn&#65281;
&lt;/p&gt;
&lt;p&gt;
Transformers for Low-Resource Languages:Is F\'eidir Linn!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;&#35821;&#35328;&#23545;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340; Transformer &#27169;&#22411;&#65292;&#21457;&#29616;&#27491;&#30830;&#36873;&#25321;&#23376;&#35789;&#27169;&#22411;&#26159;&#32763;&#35793;&#24615;&#33021;&#30340;&#26368;&#22823;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#27169;&#22411;&#26159;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#31070;&#32463;&#32763;&#35793;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#35821;&#35328;&#23545;&#19978;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#65292;&#20351;&#29992;&#35813;&#32467;&#26500;&#36827;&#34892;&#23454;&#39564;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#20197;&#32763;&#35793;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#21442;&#25968;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#27491;&#30830;&#36873;&#25321;&#23376;&#35789;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#32763;&#35793;&#24615;&#33021;&#26368;&#22823;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#35780;&#20272;&#20102;&#20351;&#29992; unigram &#21644; BPE &#26041;&#27861;&#30340; SentencePiece &#27169;&#22411;&#12290;&#23545;&#27169;&#22411;&#26550;&#26500;&#30340;&#21464;&#21270;&#21253;&#25324;&#20462;&#25913;&#23618;&#25968;&#12289;&#27979;&#35797;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#21450;&#35780;&#20272;&#29992;&#20110;&#27880;&#24847;&#21147;&#30340;&#26368;&#20339;&#22836;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01985v1 Announce Type: cross  Abstract: The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20013;&#22830;&#24211;&#23572;&#24503;&#35821;&#31181;&#24320;&#21457;&#35821;&#35328;&#21644;&#35821;&#38899;&#25216;&#26415;&#65292;&#36890;&#36807;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#36716;&#24405;&#21019;&#24314;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#26426;&#22120;&#32763;&#35793;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01983</link><description>&lt;p&gt;
&#20013;&#22830;&#24211;&#23572;&#24503;&#35821;&#31181;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Language and Speech Technology for Central Kurdish Varieties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20013;&#22830;&#24211;&#23572;&#24503;&#35821;&#31181;&#24320;&#21457;&#35821;&#35328;&#21644;&#35821;&#38899;&#25216;&#26415;&#65292;&#36890;&#36807;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#36716;&#24405;&#21019;&#24314;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#26426;&#22120;&#32763;&#35793;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23572;&#24503;&#35821;&#26159;&#19968;&#31181;&#21360;&#27431;&#35821;&#35328;&#65292;&#25317;&#26377;&#36229;&#36807;3000&#19975;&#30340;&#20351;&#29992;&#32773;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26041;&#35328;&#36830;&#32493;&#20307;&#65292;&#20197;&#20854;&#22810;&#26679;&#24615;&#30340;&#35821;&#35328;&#21464;&#20307;&#32780;&#38395;&#21517;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#24211;&#23572;&#24503;&#35821;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#25216;&#26415;&#35270;&#20026;&#23439;&#35821;&#35328;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#26041;&#35328;&#21644;&#21464;&#20307;&#20043;&#38388;&#23384;&#22312;&#36164;&#28304;&#21644;&#24037;&#20855;&#30456;&#23545;&#21294;&#20047;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20026;&#20013;&#22830;&#24211;&#23572;&#24503;&#35821;&#31181;&#24320;&#21457;&#35821;&#35328;&#21644;&#35821;&#38899;&#25216;&#26415;&#36164;&#28304;&#36808;&#20986;&#19968;&#27493;&#65292;&#36890;&#36807;&#35760;&#24405;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#36716;&#24405;&#20316;&#20026;&#37326;&#22806;&#24037;&#20316;&#30340;&#26367;&#20195;&#26041;&#24335;&#26469;&#21019;&#24314;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#20013;&#22830;&#24211;&#23572;&#24503;&#35821;&#31181;&#19978;&#35780;&#20272;&#30340;&#26426;&#22120;&#32763;&#35793;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#35782;&#21035;&#24615;&#33021;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#12290;&#25968;&#25454;&#21644;&#27169;&#22411;&#24050;&#22312;https://github.com/sinaahmadi/CORDI&#20197;&#24320;&#25918;&#35768;&#21487;&#35777;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01983v1 Announce Type: new  Abstract: Kurdish, an Indo-European language spoken by over 30 million speakers, is considered a dialect continuum and known for its diversity in language varieties. Previous studies addressing language and speech technology for Kurdish handle it in a monolithic way as a macro-language, resulting in disparities for dialects and varieties for which there are few resources and tools available. In this paper, we take a step towards developing resources for language and speech technology for varieties of Central Kurdish, creating a corpus by transcribing movies and TV series as an alternative to fieldwork. Additionally, we report the performance of machine translation, automatic speech recognition, and language identification as downstream tasks evaluated on Central Kurdish varieties. Data and models are publicly available under an open license at https://github.com/sinaahmadi/CORDI.
&lt;/p&gt;</description></item><item><title>SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01976</link><description>&lt;p&gt;
SciAssess&#65306;&#22522;&#20934;&#27979;&#35797;LLM&#22312;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01976
&lt;/p&gt;
&lt;p&gt;
SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#31361;&#30772;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#32454;&#33268;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SciAssess&#65292;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#24615;&#12290;SciAssess&#19987;&#27880;&#20110;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#32972;&#26223;&#19979;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#22914;&#19968;&#33324;&#21270;&#23398;&#12289;&#26377;&#26426;&#26448;&#26009;&#21644;&#21512;&#37329;&#26448;&#26009;&#12290;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#25514;&#26045;&#30830;&#20445;&#20102;&#20854;&#22312;&#27491;&#30830;&#24615;&#12289;&#21311;&#21517;&#21270;&#21644;&#22797;&#21046;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MPIKGC&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#21892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25193;&#23637;&#23454;&#20307;&#25551;&#36848;&#12289;&#29702;&#35299;&#20851;&#31995;&#21644;&#25552;&#21462;&#32467;&#26500;&#65292;&#24357;&#34917;&#20102;&#32467;&#26500;&#19981;&#23436;&#25972;&#21644;&#25991;&#26412;&#36136;&#37327;&#38480;&#21046;&#24102;&#26469;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01972</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#35282;&#24230;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MPIKGC&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#21892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25193;&#23637;&#23454;&#20307;&#25551;&#36848;&#12289;&#29702;&#35299;&#20851;&#31995;&#21644;&#25552;&#21462;&#32467;&#26500;&#65292;&#24357;&#34917;&#20102;&#32467;&#26500;&#19981;&#23436;&#25972;&#21644;&#25991;&#26412;&#36136;&#37327;&#38480;&#21046;&#24102;&#26469;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#32570;&#22833;&#30340;&#38142;&#25509;&#36827;&#34892;&#39044;&#27979;&#26469;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;KGC&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21576;&#29616;&#20986;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25551;&#36848;&#30340;KGC&#30340;&#24615;&#33021;&#20173;&#28982;&#21463;&#25991;&#26412;&#36136;&#37327;&#21644;&#19981;&#23436;&#25972;&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#20307;&#25551;&#36848;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#20851;&#31995;&#21517;&#31216;&#65292;&#23548;&#33268;&#32467;&#26524;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPIKGC&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#21508;&#31181;&#35282;&#24230;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#34917;&#20607;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#30340;&#19981;&#36275;&#65292;&#20174;&#32780;&#25552;&#39640;KGC&#65292;&#20854;&#20013;&#28041;&#21450;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#12289;&#35299;&#37322;&#21644;&#24635;&#32467;&#33021;&#21147;&#26469;&#25193;&#23637;&#23454;&#20307;&#25551;&#36848;&#65292;&#29702;&#35299;&#20851;&#31995;&#21644;&#25552;&#21462;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01972v1 Announce Type: new  Abstract: Knowledge graph completion (KGC) is a widely used method to tackle incompleteness in knowledge graphs (KGs) by making predictions for missing links. Description-based KGC leverages pre-trained language models to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. We conduc
&lt;/p&gt;</description></item><item><title>AS-ES&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;CoT&#25968;&#25454;&#26469;&#23454;&#29616;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#36845;&#20195;&#29983;&#25104;&#65292;&#36229;&#36234;&#20102;&#30452;&#25509;seq2seq&#35757;&#32451;&#65292;&#22312;CoT&#20016;&#23500;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01969</link><description>&lt;p&gt;
AS-ES&#23398;&#20064;&#65306;&#23567;&#22411;&#27169;&#22411;&#20013;&#39640;&#25928;CoT&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AS-ES Learning: Towards Efficient CoT Learning in Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01969
&lt;/p&gt;
&lt;p&gt;
AS-ES&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;CoT&#25968;&#25454;&#26469;&#23454;&#29616;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#36845;&#20195;&#29983;&#25104;&#65292;&#36229;&#36234;&#20102;&#30452;&#25509;seq2seq&#35757;&#32451;&#65292;&#22312;CoT&#20016;&#23500;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#22312;LLM&#20013;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#26032;&#22411;&#33021;&#21147;&#65292;&#29305;&#21035;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#24341;&#20837;&#36825;&#31181;&#33021;&#21147;&#65292;&#20154;&#20204;&#23581;&#35797;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;CoT&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#21482;&#26159;&#31616;&#21333;&#22320;&#20174;LLMs&#29983;&#25104;&#26356;&#22810;&#25968;&#25454;&#24182;&#23558;&#20854;&#21152;&#20197;&#21033;&#29992;&#65292;&#32780;&#24573;&#35270;&#20102;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;CoT&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;AS-ES&#65288;&#25277;&#35937;&#29255;&#27573; - &#25552;&#21462;&#24615;&#29255;&#27573;&#65289;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CoT&#20013;&#30340;&#22266;&#26377;&#20449;&#24687;&#36827;&#34892;&#36845;&#20195;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31867;&#20284;MWP&#21644;PET&#25688;&#35201;&#31561;CoT&#20016;&#23500;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#30452;&#25509;&#30340;seq2seq&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#20462;&#25913;&#27169;&#22411;&#26412;&#36523;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#23398;&#20064;CoT&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#30340;&#21407;&#22240;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;AS-ES&#23398;&#20064;&#26377;&#25928;&#65292;&#20174;&#32780;&#28145;&#20837;&#30740;&#31350;&#20102;CoT&#23398;&#20064;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01969v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs, especially when it comes to logical reasoning. Attempts have been made to induce such ability in small models as well by distilling from the data with CoT generated by Large Language Models (LLMs). However, existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data. We here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET summarization, without data augmentation or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into 
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;VariErr&#65292;&#19987;&#27880;&#20110;NLI&#20219;&#21153;&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#21644;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#21306;&#20998;&#12290;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#22788;&#29702;&#20449;&#21495;&#38750;&#40657;&#30333;&#24773;&#20917;&#19979;&#30340;&#20808;&#21069;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.01931</link><description>&lt;p&gt;
VariErr NLI: &#23558;&#27880;&#37322;&#38169;&#35823;&#19982;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#21306;&#20998;&#24320;&#26469;
&lt;/p&gt;
&lt;p&gt;
VariErr NLI: Separating Annotation Error from Human Label Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01931
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;VariErr&#65292;&#19987;&#27880;&#20110;NLI&#20219;&#21153;&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#21644;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#21306;&#20998;&#12290;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#22788;&#29702;&#20449;&#21495;&#38750;&#40657;&#30333;&#24773;&#20917;&#19979;&#30340;&#20808;&#21069;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#26159;&#30001;&#20110;&#27880;&#37322;&#32773;&#20986;&#20110;&#26377;&#25928;&#21407;&#22240;&#23558;&#19981;&#21516;&#26631;&#31614;&#20998;&#37197;&#32473;&#21516;&#19968;&#39033;&#32780;&#20135;&#29983;&#30340;&#65292;&#32780;&#27880;&#37322;&#38169;&#35823;&#26159;&#25351;&#30001;&#20110;&#26080;&#25928;&#21407;&#22240;&#20998;&#37197;&#26631;&#31614;&#12290;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#26159;&#23396;&#31435;&#30740;&#31350;&#23427;&#20204;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#21069;&#27809;&#26377;&#19987;&#27880;&#20110;&#21306;&#20998;&#38169;&#35823;&#19982;&#20449;&#21495;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#21495;&#36229;&#36234;&#40657;&#30333;&#20043;&#22788;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#21644;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;VariErr&#65288;&#21464;&#24322;&#19982;&#38169;&#35823;&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#33521;&#35821;NLI&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#36718;&#27880;&#37322;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#27880;&#37322;&#32773;&#35299;&#37322;&#27599;&#20010;&#26631;&#31614;&#65292;&#28982;&#21518;&#21028;&#26029;&#26631;&#31614;&#35299;&#37322;&#23545;&#30340;&#26377;&#25928;&#24615;&#12290;VariErr&#21253;&#21547;&#23545;500&#20010;&#37325;&#26032;&#27880;&#37322;&#30340;NLI&#39033;&#30446;&#19978;&#30340;1,933&#20010;&#35299;&#37322;&#36827;&#34892;&#30340;7,574&#20010;&#26377;&#25928;&#24615;&#21028;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#33258;&#21160;&#38169;&#35823;&#26816;&#27979;&#65288;AED&#65289;&#26041;&#27861;&#21644;GPT&#22312;&#25581;&#31034;&#38169;&#35823;&#19982;&#20449;&#21495;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01931v1 Announce Type: new  Abstract: Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs. \name{} contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#30417;&#30563;&#24494;&#35843;&#12289;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#19977;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#65292;&#21457;&#29616;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#22312;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20855;&#26377;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.01929</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65306;&#25105;&#20204;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#30417;&#30563;&#24494;&#35843;&#12289;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#19977;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#65292;&#21457;&#29616;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#22312;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20855;&#26377;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12289;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#65288;SIT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#19977;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26367;&#20195;&#19988;&#23454;&#38469;&#26631;&#20934;&#26041;&#27861;&#12290;ICL&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#26368;&#36817;&#30001;&#20110;LLM&#30340;&#20986;&#29616;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#23545;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#29992;&#20110;&#22810;&#35821;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#35843;&#26597;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#37325;&#28857;&#20027;&#35201;&#37117;&#26159;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#19977;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#31995;&#32479;&#30340;&#27604;&#36739;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;6&#31181;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#12289;&#19977;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20197;&#21450;&#22810;&#31181;&#35821;&#35328;&#21644;&#39046;&#22495;&#35774;&#32622;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24615;&#33021;&#21482;&#26159;&#27604;&#36739;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35745;&#31639;&#25104;&#26412;&#12289;&#25512;&#29702;&#25104;&#26412;&#21644;&#36130;&#21153;&#25104;&#26412;&#30340;&#35270;&#35282;&#26469;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#34920;&#26126;&#65292;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#22312;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20855;&#26377;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01929v1 Announce Type: new  Abstract: Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on 6 high- and low-resource languages, three different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyse the approaches through the optics of their computational, inference and financial costs. Our observations show that supervised instruction tuning has the best trade-off between performance and resou
&lt;/p&gt;</description></item><item><title>IndicVoices&#33268;&#21147;&#20110;&#24314;&#31435;&#21253;&#23481;&#24615;&#21644;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24320;&#28304;&#25968;&#25454;&#25910;&#38598;&#34013;&#22270;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36215;&#27493;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.01926</link><description>&lt;p&gt;
IndicVoices: &#21162;&#21147;&#24314;&#31435;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#35328;&#30340;&#21253;&#23481;&#24615;&#22810;&#35821;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IndicVoices: Towards building an Inclusive Multilingual Speech Dataset for Indian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01926
&lt;/p&gt;
&lt;p&gt;
IndicVoices&#33268;&#21147;&#20110;&#24314;&#31435;&#21253;&#23481;&#24615;&#21644;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24320;&#28304;&#25968;&#25454;&#25910;&#38598;&#34013;&#22270;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#36215;&#27493;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;INDICVOICES&#65292;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;145&#20010;&#21360;&#24230;&#22320;&#21306;&#21644;22&#31181;&#35821;&#35328;&#30340;16237&#21517;&#21457;&#35328;&#32773;&#30340;&#33258;&#28982;&#21644;&#33258;&#21457;&#35821;&#38899;&#30340;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;7348&#23567;&#26102;&#65292;&#20854;&#20013;&#38405;&#35835;&#38899;&#39057;&#21344;9&#65285;&#65292;&#21363;&#20852;&#28436;&#35762;&#38899;&#39057;&#21344;74&#65285;&#65292;&#23545;&#35805;&#38899;&#39057;&#21344;17&#65285;&#12290;&#22312;&#36825;7348&#23567;&#26102;&#20013;&#65292;&#24050;&#32463;&#26377;1639&#23567;&#26102;&#34987;&#36716;&#24405;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#20013;&#20301;&#25968;&#20026;73&#23567;&#26102;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#25429;&#25417;&#21360;&#24230;&#25991;&#21270;&#12289;&#35821;&#35328;&#21644;&#20154;&#21475;&#22810;&#26679;&#24615;&#30340;&#32463;&#21382;&#65292;&#20197;&#21019;&#24314;&#19968;&#31181;&#29420;&#29305;&#19988;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#25910;&#38598;&#34013;&#22270;&#65292;&#20854;&#20013;&#21253;&#25324;&#26631;&#20934;&#21270;&#21327;&#35758;&#12289;&#38598;&#20013;&#24037;&#20855;&#12289;&#24341;&#20154;&#20837;&#32988;&#30340;&#38382;&#39064;&#12289;&#25552;&#31034;&#21644;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#21644;&#20027;&#39064;&#30340;&#23545;&#35805;&#24773;&#26223;&#30340;&#23384;&#20648;&#24211;&#65292;&#36136;&#37327;&#25511;&#21046;&#26426;&#21046;&#65292;&#20840;&#38754;&#30340;&#36716;&#24405;&#25351;&#21335;&#21644;&#36716;&#24405;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01926v1 Announce Type: new  Abstract: We present INDICVOICES, a dataset of natural and spontaneous speech containing a total of 7348 hours of read (9%), extempore (74%) and conversational (17%) audio from 16237 speakers covering 145 Indian districts and 22 languages. Of these 7348 hours, 1639 hours have already been transcribed, with a median of 73 hours per language. Through this paper, we share our journey of capturing the cultural, linguistic and demographic diversity of India to create a one-of-its-kind inclusive and representative dataset. More specifically, we share an open-source blueprint for data collection at scale comprising of standardised protocols, centralised tools, a repository of engaging questions, prompts and conversation scenarios spanning multiple domains and topics of interest, quality control mechanisms, comprehensive transcription guidelines and transcription tools. We hope that this open source blueprint will serve as a comprehensive starter kit for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.01924</link><description>&lt;p&gt;
&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#65311;&#20851;&#20110;&#20154;&#24037;&#29615;&#22659;&#22312;&#21307;&#23398;&#24320;&#25918;&#22495;&#38382;&#31572;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#38656;&#35201;&#22823;&#37327;&#19987;&#19994;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;&#36817;&#26399;&#30340;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#30693;&#35782;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#23545;&#25239;&#26550;&#26500;&#35268;&#27169;&#21270;&#65292;&#24182;&#20801;&#35768;&#22312;&#24120;&#35265;&#30340;&#20302;&#36164;&#28304;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26816;&#32034;&#28982;&#21518;&#38405;&#35835;&#30340;&#33539;&#24335;&#24050;&#21464;&#24471;&#26222;&#36941;&#65292;&#27169;&#22411;&#39044;&#27979;&#20381;&#36182;&#20110;&#26469;&#33258;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;PubMed&#12289;&#25945;&#31185;&#20070;&#21644;UMLS&#65289;&#30340;&#30456;&#20851;&#30693;&#35782;&#29255;&#27573;&#12290;&#21478;&#19968;&#26465;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#20294;&#30001;&#20110;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21464;&#24471;&#21487;&#33021;&#30340;&#36335;&#24452;&#26159;&#36890;&#36807;&#25552;&#31034;&#26500;&#24314;&#20154;&#24037;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#8220;&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#8221;&#25104;&#20026;&#20102;&#29616;&#20195;&#29256;&#30340;&#21704;&#22982;&#38647;&#29305;&#22256;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#29983;&#25104;&#28982;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;MedQA-USMLE&#12289;MedMCQA&#21644;MMLU&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#20174;&#23454;&#36341;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20551;&#35774;&#26368;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#38463;&#25289;&#20271;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#24191;&#27867;&#30740;&#31350;&#65292;&#36890;&#36807;&#20154;&#24037;&#35843;&#26597;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24378;&#21270;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#39064;&#21644;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01921</link><description>&lt;p&gt;
&#38463;&#25289;&#20271;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#65306;&#36890;&#36807;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#20998;&#26512;&#24378;&#21270;&#20154;&#24037;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with Wider Topic Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#38463;&#25289;&#20271;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#24191;&#27867;&#30740;&#31350;&#65292;&#36890;&#36807;&#20154;&#24037;&#35843;&#26597;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24378;&#21270;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#39064;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01921v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#19968;&#30452;&#26159;&#19968;&#20010;&#32321;&#33635;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38463;&#25289;&#20271;&#24773;&#24863;&#20998;&#26512;&#65288;ASA&#65289;&#30340;&#20219;&#21153;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#20173;&#28982;&#23569;&#35265;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;ASA&#25991;&#26412;&#20869;&#23481;&#30740;&#31350;&#30340;&#39318;&#27425;&#28145;&#20837;&#21644;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#20849;&#21516;&#20027;&#39064;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#26041;&#27861;&#12289;&#36884;&#24452;&#12289;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#12290;&#36825;&#39033;&#28145;&#20837;&#30740;&#31350;&#23545;2002&#24180;&#33267;2020&#24180;&#38388;&#21457;&#34920;&#30340;133&#31687;&#33521;&#25991;ASA&#35770;&#25991;&#36827;&#34892;&#20102;&#25163;&#24037;&#20998;&#26512;&#65292;&#36825;&#20123;&#35770;&#25991;&#26469;&#33258;&#22235;&#20010;&#23398;&#26415;&#25968;&#25454;&#24211;&#65288;SAGE&#12289;IEEE&#12289;Springer&#12289;WILEY&#65289;&#21644;&#35895;&#27468;&#23398;&#26415;&#12290;&#24191;&#27867;&#30740;&#31350;&#22312;2010&#24180;&#33267;2020&#24180;&#38388;&#20351;&#29992;&#29616;&#20195;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#20027;&#39064;&#24314;&#27169;&#21644;&#26102;&#38388;&#20998;&#26512;&#65292;&#23545;&#24320;&#25918;&#33719;&#21462;&#36164;&#28304;&#19978;&#30340;2297&#31687;ASA&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#24378;&#21270;&#65292;&#20197;&#24378;&#35843;&#20043;&#21069;&#30740;&#31350;&#25152;&#21457;&#29616;&#30340;&#20027;&#39064;&#21644;&#36235;&#21183;&#12290;&#20027;&#35201;&#21457;&#29616;&#26174;&#31034;&#20102;ASA&#20351;&#29992;&#30340;&#19981;&#21516;&#26041;&#27861;&#65306;&#26426;&#22120;&#23398;&#20064;&#12289;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01921v1 Announce Type: new  Abstract: Sentiment analysis (SA) has been, and is still, a thriving research area. However, the task of Arabic sentiment analysis (ASA) is still underrepresented in the body of research. This study offers the first in-depth and in-breadth analysis of existing ASA studies of textual content and identifies their common themes, domains of application, methods, approaches, technologies and algorithms used. The in-depth study manually analyses 133 ASA papers published in the English language between 2002 and 2020 from four academic databases (SAGE, IEEE, Springer, WILEY) and from Google Scholar. The in-breadth study uses modern, automatic machine learning techniques, such as topic modelling and temporal analysis, on Open Access resources, to reinforce themes and trends identified by the prior study, on 2297 ASA publications between 2010-2020. The main findings show the different approaches used for ASA: machine learning, lexicon-based and hybrid appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#20316;&#20986;&#20102;&#36129;&#29486;&#65292;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#21457;&#24067;&#20102;&#21253;&#25324;&#20159;&#32423;&#21442;&#25968; Albertina &#21644; Bertimbau &#22312;&#20869;&#30340;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#36827;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.01897</link><description>&lt;p&gt;
&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#30340;&#24320;&#25918;&#31070;&#32463;&#32534;&#30721;&#22120;&#29983;&#24577;&#31995;&#32479;&#19982;Albertina PT*&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#20316;&#20986;&#20102;&#36129;&#29486;&#65292;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#21457;&#24067;&#20102;&#21253;&#25324;&#20159;&#32423;&#21442;&#25968; Albertina &#21644; Bertimbau &#22312;&#20869;&#30340;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#36827;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#26412;&#25991;&#36129;&#29486;&#20102;&#20195;&#34920;&#22522;&#30784;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#20173;&#28982;&#38750;&#24120;&#31232;&#32570;&#30340;&#38024;&#23545;&#35813;&#35821;&#35328;&#29305;&#21035;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#30340;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#23436;&#20840;&#26159;&#24320;&#25918;&#30340;&#65292;&#21363;&#23427;&#20204;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#22312;&#19968;&#20010;&#24320;&#25918;&#35768;&#21487;&#19979;&#20813;&#36153;&#20998;&#21457;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#30446;&#30340;&#65292;&#21253;&#25324;&#30740;&#31350;&#21644;&#21830;&#19994;&#29992;&#36884;&#12290;&#19982;&#33521;&#35821;&#20197;&#22806;&#30340;&#22823;&#22810;&#25968;&#35821;&#35328;&#19968;&#26679;&#65292;&#33889;&#33796;&#29273;&#35821;&#22312;&#36825;&#20123;&#22522;&#30784;&#35821;&#35328;&#36164;&#28304;&#26041;&#38754;&#36164;&#28304;&#21294;&#20047;&#65292;&#36825;&#37324;&#26377;&#39318;&#23626;&#25317;&#26377; 9 &#20159;&#20010;&#21442;&#25968;&#30340; Albertina &#21644; 3.35 &#20159;&#20010;&#21442;&#25968;&#30340; Bertimbau&#12290;&#22312;&#20197;&#36825;&#23545;&#27169;&#22411;&#20026;&#39318;&#27425;&#38598;&#21512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#24335;&#33889;&#33796;&#29273;&#35821;&#32534;&#30721;&#22120;&#29983;&#24577;&#31995;&#32479;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25317;&#26377; 15 &#20159;&#21442;&#25968;&#30340;&#26356;&#22823;&#22411;&#12289;&#24615;&#33021;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#25317;&#26377; 1 &#20159;&#21442;&#25968;&#30340;&#26356;&#23567;&#22411;&#12289;&#25928;&#29575;&#39537;&#21160;&#30340;&#27169;&#22411;&#12290;&#22312;&#23454;&#29616;&#36825;&#19968;&#20027;&#35201;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#36824;&#24471;&#21040;&#20102;&#19968;&#20123;&#36827;&#19968;&#27493;&#30340;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01897v1 Announce Type: new  Abstract: To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are rele
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25991;&#26723;&#20013;&#30340;&#20016;&#23500;&#35821;&#27861;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.01886</link><description>&lt;p&gt;
FCDS: &#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;
&lt;/p&gt;
&lt;p&gt;
FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25991;&#26723;&#20013;&#30340;&#20016;&#23500;&#35821;&#27861;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26088;&#22312;&#35782;&#21035;&#21333;&#20010;&#25991;&#26723;&#20869;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;DocRE&#20013;&#65292;&#21033;&#29992;&#30701;&#35821;&#32467;&#26500;&#32858;&#21512;&#25972;&#20010;&#21477;&#23376;&#20449;&#24687;&#24182;&#36873;&#25321;&#30446;&#26631;&#23545;&#30340;&#25351;&#23548;&#24615;&#21477;&#23376;&#65292;&#21033;&#29992;&#20381;&#23384;&#21477;&#27861;&#22312;&#22270;&#32467;&#26500;&#20013;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#26681;&#25454;&#20381;&#23384;&#22270;&#36873;&#25321;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01886v1 Announce Type: cross  Abstract: Document-level Relation Extraction (DocRE) aims to identify relation labels between entities within a single document. It requires handling several sentences and reasoning over them. State-of-the-art DocRE methods use a graph structure to connect entities across the document to capture dependency syntax information. However, this is insufficient to fully exploit the rich syntax information in the document. In this work, we propose to fuse constituency and dependency syntax into DocRE. It uses constituency syntax to aggregate the whole sentence information and select the instructive sentences for the pairs of targets. It exploits the dependency syntax in a graph structure with constituency syntax enhancement and chooses the path between entity pairs based on the dependency graph. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed method. The code is publicly available at this url.
&lt;/p&gt;</description></item><item><title>TMMLU+&#26159;&#20256;&#32479;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#35268;&#27169;&#26159;&#21069;&#32773;&#30340;&#20845;&#20493;&#65292;&#21253;&#21547;66&#20010;&#22810;&#26679;&#21270;&#20027;&#39064;&#12290;&#30740;&#31350;&#26174;&#31034;&#20256;&#32479;&#20013;&#25991;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#31616;&#20307;&#20013;&#25991;&#27169;&#22411;&#65292;&#24182;&#19988;&#30446;&#21069;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#24179;&#22343;&#24471;&#20998;&#19978;&#23578;&#26410;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01858</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;&#20256;&#32479;&#20013;&#25991;&#22522;&#37329;&#27169;&#22411;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
An Improved Traditional Chinese Evaluation Suite for Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01858
&lt;/p&gt;
&lt;p&gt;
TMMLU+&#26159;&#20256;&#32479;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#35268;&#27169;&#26159;&#21069;&#32773;&#30340;&#20845;&#20493;&#65292;&#21253;&#21547;66&#20010;&#22810;&#26679;&#21270;&#20027;&#39064;&#12290;&#30740;&#31350;&#26174;&#31034;&#20256;&#32479;&#20013;&#25991;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#31616;&#20307;&#20013;&#25991;&#27169;&#22411;&#65292;&#24182;&#19988;&#30446;&#21069;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#24179;&#22343;&#24471;&#20998;&#19978;&#23578;&#26410;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TMMLU+&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#20256;&#32479;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290; TMMLU+&#26159;&#19968;&#20010;&#21253;&#21547;66&#20010;&#20174;&#22522;&#30784;&#21040;&#19987;&#19994;&#27700;&#24179;&#30340;&#36873;&#25321;&#39064;&#31572;&#39064;&#25968;&#25454;&#38598;&#12290;&#19982;&#20854;&#21069;&#36523;TMMLU&#30456;&#27604;&#65292;TMMLU+&#30340;&#35268;&#27169;&#22823;&#20845;&#20493;&#65292;&#20027;&#39064;&#20998;&#24067;&#26356;&#21152;&#24179;&#34913;&#12290;&#25105;&#20204;&#22312;TMMLU+&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#20197;&#21450;24&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;1.8B&#21040;72B&#30340;&#24320;&#28304;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#20013;&#25991;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#31616;&#20307;&#20013;&#25991;&#23545;&#24212;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#24179;&#22343;&#20998;&#25968;&#19978;&#20173;&#26410;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21450;&#30456;&#24212;&#30340;&#22522;&#20934;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01858v1 Announce Type: new  Abstract: We present TMMLU+, a comprehensive dataset designed for the Traditional Chinese massive multitask language understanding dataset. TMMLU+ is a multiple-choice question-answering dataset with 66 subjects from elementary to professional level. Compared to its predecessor, TMMLU, TMMLU+ is six times larger and boasts a more balanced subject distribution. We included benchmark results in TMMLU+ from closed-source models and 24 open-weight Chinese large language models of parameters ranging from 1.8B to 72B. Our findings reveal that Traditional Chinese models still trail behind their Simplified Chinese counterparts. Additionally, current large language models have yet to outperform human performance in average scores. We publicly release our dataset and the corresponding benchmark source code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20013;&#25991;Mixtral&#20026;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20013;&#25991;&#35821;&#35328;&#33021;&#21147;&#30340;Mixtral&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01851</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;LLM&#35821;&#35328;&#36866;&#24212;&#24615;&#65306;&#20197;&#20013;&#25991;Mixtral&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20013;&#25991;Mixtral&#20026;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20013;&#25991;&#35821;&#35328;&#33021;&#21147;&#30340;Mixtral&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixtral&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;(SMoE)&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20197;Mixtral-8x7B-v0.1&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20013;&#25991;-Mixtral&#21644;&#20013;&#25991;-Mixtral-Instruct&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#25552;&#39640;&#20102;&#20013;&#25991;&#35821;&#35328;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20013;&#25991;-Mixtral&#21644;&#20013;&#25991;-Mixtral-Instruct&#25104;&#21151;&#25552;&#21319;&#20102;&#20013;&#25991;&#29702;&#35299;&#21644;&#29983;&#25104;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#30340;&#33521;&#25991;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#26102;&#30340;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#25193;&#23637;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#30340;&#24517;&#35201;&#24615;&#20197;&#21450;&#21021;&#22987;&#21270;&#27169;&#22411;&#30340;&#36873;&#25321;&#65288;&#22522;&#30784;&#27169;&#22411;vs.&#25351;&#23548;&#27169;&#22411;&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#23454;&#35777;&#32467;&#26524;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#27599;&#20010;&#19987;&#23478;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#20197;&#26816;&#39564;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01851v1 Announce Type: cross  Abstract: Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning. Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on
&lt;/p&gt;</description></item><item><title>CET2&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#24314;&#27169;&#20027;&#39064;&#36716;&#25442;&#36873;&#25321;&#19982;&#23545;&#35805;&#19978;&#19979;&#25991;&#36830;&#36143;&#30340;&#30693;&#35782;&#65292;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#29983;&#25104;&#36830;&#36143;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#22797;&#12290;</title><link>https://arxiv.org/abs/2403.01848</link><description>&lt;p&gt;
CET2&#65306;&#24314;&#27169;&#20027;&#39064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36830;&#36143;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
CET2: Modelling Topic Transitions for Coherent and Engaging Knowledge-Grounded Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01848
&lt;/p&gt;
&lt;p&gt;
CET2&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#24314;&#27169;&#20027;&#39064;&#36716;&#25442;&#36873;&#25321;&#19982;&#23545;&#35805;&#19978;&#19979;&#25991;&#36830;&#36143;&#30340;&#30693;&#35782;&#65292;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#29983;&#25104;&#36830;&#36143;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#36873;&#25321;&#30340;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#36830;&#36143;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#20808;&#21069;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#23545;&#35805;&#19978;&#19979;&#25991;&#25110;&#36807;&#20998;&#24378;&#35843;&#25152;&#36873;&#30693;&#35782;&#20013;&#30340;&#26032;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#36873;&#25321;&#37325;&#22797;&#25110;&#19981;&#36830;&#36143;&#30340;&#30693;&#35782;&#65292;&#36827;&#32780;&#29983;&#25104;&#37325;&#22797;&#25110;&#19981;&#36830;&#36143;&#30340;&#22238;&#22797;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;Coherent and Engaging Topic Transition&#65288;CET2&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#20027;&#39064;&#36716;&#25442;&#20197;&#36873;&#25321;&#24688;&#24403;&#19982;&#23545;&#35805;&#19978;&#19979;&#25991;&#36830;&#36143;&#30340;&#30693;&#35782;&#65292;&#24182;&#20026;&#20027;&#39064;&#21457;&#23637;&#25552;&#20379;&#36275;&#22815;&#30340;&#30693;&#35782;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;CET2&#26694;&#26550;&#32771;&#34385;&#20102;&#30693;&#35782;&#36873;&#25321;&#30340;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#21040;&#19979;&#19968;&#20010;&#20027;&#39064;&#30340;&#26377;&#25928;&#36716;&#25442;&#36923;&#36753;&#20197;&#21450;&#31995;&#32479;&#24615;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01848v1 Announce Type: new  Abstract: Knowledge-grounded dialogue systems aim to generate coherent and engaging responses based on the dialogue contexts and selected external knowledge. Previous knowledge selection methods tend to rely too heavily on the dialogue contexts or over-emphasize the new information in the selected knowledge, resulting in the selection of repetitious or incongruous knowledge and further generating repetitive or incoherent responses, as the generation of the response depends on the chosen knowledge. To address these shortcomings, we introduce a Coherent and Engaging Topic Transition (CET2) framework to model topic transitions for selecting knowledge that is coherent to the context of the conversations while providing adequate knowledge diversity for topic development. Our CET2 framework considers multiple factors for knowledge selection, including valid transition logic from dialogue contexts to the following topics and systematic comparisons betwee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01841</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#39044;&#27979;&#19978;&#20248;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models Great on Tabular Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#31181;DNN&#30340;&#20248;&#21183;&#22312;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#65288;&#20363;&#22914;&#22238;&#24402;&#25110;&#20998;&#31867;&#20219;&#21153;&#65289;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TP-BERTa&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#23558;&#26631;&#37327;&#25968;&#20540;&#29305;&#24449;&#20540;&#36716;&#25442;&#20026;&#31163;&#25955;&#24230;&#39640;&#12289;&#39640;&#32500;&#24230;&#30340;&#26631;&#35760;&#65292;&#24182;&#19988;&#19968;&#31181;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#25972;&#21512;&#20102;&#29305;&#24449;&#21517;&#31216;&#21644;&#25968;&#20540;&#29305;&#24449;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;-Based Data-Centric AI &#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#26415;&#30028;&#25968;&#25454;&#36136;&#37327;&#21644;&#24037;&#19994;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#21512;&#27169;&#22411;&#32771;&#34385;&#21040;&#25968;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.01832</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#24357;&#21512;&#23398;&#26415;&#29702;&#24819;&#19982;&#24037;&#19994;&#23454;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;
&lt;/p&gt;
&lt;p&gt;
Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals and Industrial Pragmatism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;-Based Data-Centric AI &#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#26415;&#30028;&#25968;&#25454;&#36136;&#37327;&#21644;&#24037;&#19994;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#21512;&#27169;&#22411;&#32771;&#34385;&#21040;&#25968;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#23545;&#27604;&#35282;&#33394;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#21644;&#27169;&#22411;&#26080;&#20851;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#20391;&#37325;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#39318;&#35201;&#24615;&#65292;&#32780;&#27169;&#22411;&#26080;&#20851;&#20154;&#24037;&#26234;&#33021;&#21017;&#20248;&#20808;&#32771;&#34385;&#31639;&#27861;&#28789;&#27963;&#24615;&#65292;&#24448;&#24448;&#20197;&#29306;&#29298;&#25968;&#25454;&#36136;&#37327;&#32771;&#34385;&#20026;&#20195;&#20215;&#12290;&#36825;&#31181;&#21306;&#21035;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#26631;&#20934;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#24037;&#19994;&#24212;&#29992;&#30340;&#20005;&#26684;&#35201;&#27714;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;&#23398;&#26415;&#27169;&#22411;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#23427;&#20204;&#24102;&#26469;&#30340;&#25361;&#25112;&#20197;&#21450;&#24357;&#21512;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65306;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#27169;&#22411;&#32771;&#34385;&#34701;&#20837;&#25968;&#25454;&#20248;&#21270;&#36807;&#31243;&#26469;&#35843;&#21644;&#36825;&#20123;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01832v1 Announce Type: new  Abstract: This paper delves into the contrasting roles of data within academic and industrial spheres, highlighting the divergence between Data-Centric AI and Model-Agnostic AI approaches. We argue that while Data-Centric AI focuses on the primacy of high-quality data for model performance, Model-Agnostic AI prioritizes algorithmic flexibility, often at the expense of data quality considerations. This distinction reveals that academic standards for data quality frequently do not meet the rigorous demands of industrial applications, leading to potential pitfalls in deploying academic models in real-world settings. Through a comprehensive analysis, we address these disparities, presenting both the challenges they pose and strategies for bridging the gap. Furthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which aims to reconcile these differences by integrating model considerations into data optimization processes. This approach u
&lt;/p&gt;</description></item><item><title>NusaBERT&#36890;&#36807;&#23558;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#19982;IndoBERT&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#28041;&#21450;&#22810;&#31181;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013; state-of-the-art &#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20026;&#26410;&#34987;&#20805;&#20998;&#20195;&#34920;&#30340;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.01817</link><description>&lt;p&gt;
NusaBERT&#65306;&#25945;&#25480;IndoBERT&#25104;&#20026;&#19968;&#31181;&#22810;&#35821;&#31181;&#21644;&#22810;&#25991;&#21270;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01817
&lt;/p&gt;
&lt;p&gt;
NusaBERT&#36890;&#36807;&#23558;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#22810;&#26679;&#21270;&#35821;&#26009;&#24211;&#19982;IndoBERT&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#28041;&#21450;&#22810;&#31181;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013; state-of-the-art &#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20026;&#26410;&#34987;&#20805;&#20998;&#20195;&#34920;&#30340;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#35821;&#35328;&#24418;&#24577;&#26497;&#20854;&#22810;&#26679;&#65292;&#21253;&#25324;700&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#65292;&#20351;&#20854;&#25104;&#20026;&#19990;&#30028;&#19978;&#35821;&#35328;&#26368;&#20016;&#23500;&#30340;&#22269;&#23478;&#20043;&#19968;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#65292;&#21152;&#19978;&#24191;&#27867;&#23454;&#36341;&#30340;&#20195;&#30721;&#20999;&#25442;&#21644;&#20302;&#36164;&#28304;&#21306;&#22495;&#35821;&#35328;&#30340;&#23384;&#22312;&#65292;&#20026;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;NusaBERT&#65292;&#36890;&#36807;&#25193;&#23637;IndoBERT&#30340;&#35789;&#27719;&#37327;&#24182;&#21033;&#29992;&#21253;&#25324;&#21306;&#22495;&#35821;&#35328;&#21644;&#26041;&#35328;&#22312;&#20869;&#30340;&#22810;&#26679;&#21270;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;NusaBERT&#22312;&#28041;&#21450;&#21360;&#24230;&#23612;&#35199;&#20122;&#22810;&#31181;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#34987;&#20805;&#20998;&#20195;&#34920;&#30340;&#35821;&#35328;&#30340;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01817v1 Announce Type: new  Abstract: Indonesia's linguistic landscape is remarkably diverse, encompassing over 700 languages and dialects, making it one of the world's most linguistically rich nations. This diversity, coupled with the widespread practice of code-switching and the presence of low-resource regional languages, presents unique challenges for modern pre-trained language models. In response to these challenges, we developed NusaBERT, building upon IndoBERT by incorporating vocabulary expansion and leveraging a diverse multilingual corpus that includes regional languages and dialects. Through rigorous evaluation across a range of benchmarks, NusaBERT demonstrates state-of-the-art performance in tasks involving multiple languages of Indonesia, paving the way for future natural language understanding research for under-represented languages.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#24369;&#30417;&#30563;&#26631;&#27880;&#31243;&#24207;&#21644;&#22522;&#20110;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#22810;&#39046;&#22495;&#33258;&#21160;&#31616;&#31572;&#39064;&#35780;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.01811</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#31649;&#36947;&#22686;&#24378;&#22810;&#39046;&#22495;&#33258;&#21160;&#31616;&#31572;&#39064;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#24369;&#30417;&#30563;&#26631;&#27880;&#31243;&#24207;&#21644;&#22522;&#20110;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#22810;&#39046;&#22495;&#33258;&#21160;&#31616;&#31572;&#39064;&#35780;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#25512;&#29702;&#26469;&#33258;&#21160;&#35780;&#20998;&#31616;&#31572;&#39064;&#65292;&#24182;&#20351;&#35780;&#20998;&#20915;&#23450;&#32972;&#21518;&#30340;&#25512;&#29702;&#21487;&#35299;&#37322;&#26159;&#24403;&#21069;&#21464;&#21387;&#22120;&#27169;&#22411;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;ASAG&#20013;&#65292;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#22120;&#25506;&#27979;&#21512;&#29702;&#21270;&#32447;&#32034;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#26631;&#27880;&#22312;&#23398;&#29983;&#22238;&#31572;&#20013;&#30340;&#21512;&#29702;&#21270;&#32447;&#32034;&#65292;&#36825;&#20165;&#22312;&#23569;&#25968;ASAG&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#31181;&#29992;&#20110;ASAG&#25968;&#25454;&#38598;&#20013;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#24369;&#30417;&#30563;&#27880;&#37322;&#31243;&#24207;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#21512;&#29702;&#21270;&#32447;&#32034;&#30340;&#21487;&#35299;&#37322;ASAG&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#12290;&#19982;Short Answer Feedback&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21452;&#35821;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#38382;&#39064;&#35757;&#32451;&#35774;&#32622;&#20013;&#23558;&#22343;&#26041;&#26681;&#35823;&#24046;&#25552;&#39640;&#20102;0.24&#21040;0.3&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#35780;&#20998;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01811v1 Announce Type: new  Abstract: Grading short answer questions automatically with interpretable reasoning behind the grading decision is a challenging goal for current transformer approaches. Justification cue detection, in combination with logical reasoners, has shown a promising direction for neuro-symbolic architectures in ASAG. But, one of the main challenges is the requirement of annotated justification cues in the students' responses, which only exist for a few ASAG datasets. To overcome this challenge, we contribute (1) a weakly supervised annotation procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic model for explainable ASAG based on justification cues. Our approach improves upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short Answer Feedback dataset in a bilingual, multi-domain, and multi-question training setup. This result shows that our approach provides a promising direction for generating high-quality grades
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21160;&#24577;&#22522;&#20934;NPHardEval4V&#65292;&#21457;&#29616;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19981;&#21516;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#23545;&#20110;LLMs&#65292;MLLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#36739;&#24369;&#12290;</title><link>https://arxiv.org/abs/2403.01777</link><description>&lt;p&gt;
NPHardEval4V: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21160;&#24577;&#22522;&#20934;NPHardEval4V&#65292;&#21457;&#29616;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19981;&#21516;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#23545;&#20110;LLMs&#65292;MLLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#65292;NPHardEval4V&#65292;&#26088;&#22312;&#35299;&#20915;&#35780;&#20272;MLLM&#32431;&#31929;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20197;&#35299;&#24320;&#35832;&#22810;&#22240;&#32032;&#65288;&#22914;&#22270;&#20687;&#35782;&#21035;&#21644;&#25351;&#20196;&#36981;&#24490;&#65289;&#23545;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#19987;&#27880;&#20110;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#36739;&#20110;LLMs&#65292;MLLMs&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#30456;&#23545;&#36739;&#24369;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#26679;&#24335;&#65288;&#21253;&#25324;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#32467;&#21512;&#35270;&#35273;&#19982;&#25991;&#26412;&#25552;&#31034;&#65289;&#23545;MLLM&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01777v1 Announce Type: new  Abstract: Understanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall performance of the models, allowing us to focus solely on evaluating their reasoning abilities. Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles, including visual, text, and combined vision and text prompts, on the reasoning abilities of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. U
&lt;/p&gt;</description></item><item><title>WebCiteS&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#24341;&#25991;&#30340;&#26597;&#35810;&#28966;&#28857;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#21253;&#21547;7k&#20154;&#24037;&#27880;&#37322;&#25688;&#35201;&#21450;&#24341;&#25991;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#65292;&#20197;&#22788;&#29702;&#24402;&#22240;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01774</link><description>&lt;p&gt;
WebCiteS: &#22312;&#20013;&#22269;&#32593;&#39029;&#25628;&#32034;&#32467;&#26524;&#19978;&#36827;&#34892;&#24102;&#24341;&#25991;&#30340;&#26597;&#35810;&#28966;&#28857;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01774
&lt;/p&gt;
&lt;p&gt;
WebCiteS&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#24341;&#25991;&#30340;&#26597;&#35810;&#28966;&#28857;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#21253;&#21547;7k&#20154;&#24037;&#27880;&#37322;&#25688;&#35201;&#21450;&#24341;&#25991;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#65292;&#20197;&#22788;&#29702;&#24402;&#22240;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01774v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24402;&#22240;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#19968;&#20010;&#21487;&#34892;&#30340;&#26041;&#27861;&#26159;&#20351;LLMs&#33021;&#22815;&#24341;&#29992;&#25903;&#25345;&#20854;&#29983;&#25104;&#30340;&#22806;&#37096;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#20173;&#23384;&#22312;&#26126;&#26174;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#24102;&#24341;&#25991;&#30340;&#26597;&#35810;&#28966;&#28857;&#25688;&#35201;&#65288;AQFS&#65289;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;WebCiteS&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;7k&#20154;&#24037;&#27880;&#37322;&#25688;&#35201;&#21450;&#24341;&#25991;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;WebCiteS&#28304;&#33258;&#29616;&#23454;&#29992;&#25143;&#26597;&#35810;&#21644;&#32593;&#39029;&#25628;&#32034;&#32467;&#26524;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;&#20043;&#21069;&#20851;&#20110;&#24402;&#22240;&#35780;&#20272;&#30340;&#24037;&#20316;&#26410;&#33021;&#21306;&#20998;&#22522;&#20110;&#20107;&#23454;&#38169;&#35823;&#21644;&#24341;&#25991;&#38169;&#35823;&#12290;&#20182;&#20204;&#20134;&#26410;&#33021;&#33258;&#21160;&#39564;&#35777;&#37027;&#20123;&#37096;&#20998;&#20381;&#36182;&#22810;&#20010;&#26469;&#28304;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#35814;&#32454;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#20351;&#33258;&#21160;&#35780;&#20272;&#22120;&#33021;&#22815;&#23558;&#21477;&#23376;&#20998;&#35299;&#20026;&#23376;&#20027;&#24352;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01774v1 Announce Type: new  Abstract: Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grain
&lt;/p&gt;</description></item><item><title>KeNet&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26723;-&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25991;&#26723;&#26631;&#31614;&#20851;&#31995;&#24314;&#31435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01767</link><description>&lt;p&gt;
KeNet:&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26723;-&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label text classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01767
&lt;/p&gt;
&lt;p&gt;
KeNet&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#25991;&#26723;-&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25991;&#26723;&#26631;&#31614;&#20851;&#31995;&#24314;&#31435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01767v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;MLTC&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#22810;&#20010;&#26631;&#31614;&#20998;&#37197;&#32473;&#32473;&#23450;&#25991;&#26412;&#12290;MLTC&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#20027;&#39064;&#35782;&#21035;&#65292;&#25512;&#33616;&#31995;&#32479;&#65292;&#24773;&#24863;&#20998;&#26512;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23578;&#26410;&#35299;&#20915;&#26576;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#19968;&#20123;&#25991;&#26723;&#24456;&#31616;&#30701;&#20294;&#20855;&#26377;&#22823;&#37327;&#26631;&#31614;&#20197;&#21450;&#22914;&#20309;&#24314;&#31435;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;MLTC&#39046;&#22495;&#65292;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#25991;&#26723;-&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;KeNet&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#12289;&#26631;&#31614;&#23884;&#20837;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#8230;&#65288;&#25991;&#31456;&#25688;&#35201;&#25130;&#33267;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01767v1 Announce Type: new  Abstract: Multi-Label Text Classification (MLTC) is a fundamental task in the field of Natural Language Processing (NLP) that involves the assignment of multiple labels to a given text. MLTC has gained significant importance and has been widely applied in various domains such as topic recognition, recommendation systems, sentiment analysis, and information retrieval. However, traditional machine learning and Deep neural network have not yet addressed certain issues, such as the fact that some documents are brief but have a large number of labels and how to establish relationships between the labels. It is imperative to additionally acknowledge that the significance of knowledge is substantiated in the realm of MLTC. To address this issue, we provide a novel approach known as Knowledge-enhanced Doc-Label Attention Network (KeNet). Specifically, we design an Attention Network that incorporates external knowledge, label embedding, and a comprehensive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#36827;&#34892;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.01757</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#38598;&#25104;&#22914;&#20309;&#25552;&#21319;LLM&#22312;&#20248;&#21270;&#20013;&#30340;&#24615;&#33021;&#65306;&#20197;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#36827;&#34892;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#26126;&#26174;&#22320;&#23558;&#23427;&#20204;&#23450;&#20301;&#20026;&#35299;&#20915;&#22797;&#26434;&#20248;&#21270;&#25361;&#25112;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#34987;&#35748;&#21487;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#20248;&#21270;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#65292;&#22312;&#20165;&#20381;&#36182;&#20110;&#25968;&#23383;&#25991;&#26412;&#25552;&#31034;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#65292;&#38590;&#20197;&#25429;&#25417;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#26469;&#22686;&#24378;&#20248;&#21270;&#24615;&#33021;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#28145;&#20837;&#20102;&#35299;&#22788;&#29702;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#38598;&#25104;&#20801;&#35768;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;LLM&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#25193;&#23637;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01757v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using multimodal LLM capable of processing both textual and visual prompts for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a multimodal LLM-based optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through exten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20302;&#31209;&#27169;&#22359;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#33258;&#27880;&#24847;&#23618;&#20013;&#65292;&#24182;&#37319;&#29992;&#20004;&#31181;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#20132;&#26367;&#20248;&#21270;&#36825;&#20123;&#20302;&#31209;&#27169;&#22359;&#65292;&#30456;&#27604;&#29616;&#26377;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#20351;&#29992;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01754</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#26080;&#23548;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20302;&#31209;&#27169;&#22359;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#33258;&#27880;&#24847;&#23618;&#20013;&#65292;&#24182;&#37319;&#29992;&#20004;&#31181;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#20132;&#26367;&#20248;&#21270;&#36825;&#20123;&#20302;&#31209;&#27169;&#22359;&#65292;&#30456;&#27604;&#29616;&#26377;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#20351;&#29992;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LoRA&#31561;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#37096;&#20998;&#21442;&#25968;&#23454;&#29616;&#19982;&#27169;&#22411;&#35843;&#20248;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#35745;&#31639;&#26799;&#24230;&#24182;&#22312;&#25972;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#21033;&#29992;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36991;&#20813;&#35745;&#31639;&#26799;&#24230;&#65292;&#24182;&#23637;&#31034;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#22312;&#27169;&#22411;&#30340;&#27599;&#20010;&#33258;&#27880;&#24847;&#21147;&#23618;&#21069;&#32622;&#20302;&#31209;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#20004;&#31181;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#20132;&#26367;&#20248;&#21270;&#27599;&#23618;&#30340;&#36825;&#20123;&#20302;&#31209;&#27169;&#22359;&#12290;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01754v1 Announce Type: new  Abstract: Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings. In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based paramet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Aug-PE&#30340;&#22686;&#24378;PE&#31639;&#27861;&#65292;&#20197;&#20135;&#29983;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#65292;&#20026;&#35299;&#20915;&#31169;&#26377;&#25991;&#26412;&#25968;&#25454;&#20849;&#20139;&#19982;&#38544;&#31169;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21069;&#26223;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.01749</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;2&#65306;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 2: Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01749
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Aug-PE&#30340;&#22686;&#24378;PE&#31639;&#27861;&#65292;&#20197;&#20135;&#29983;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#65292;&#20026;&#35299;&#20915;&#31169;&#26377;&#25991;&#26412;&#25968;&#25454;&#20849;&#20139;&#19982;&#38544;&#31169;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21069;&#26223;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01749v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#30001;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#25991;&#26412;&#25968;&#25454;&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#20135;&#29983;&#30340;&#35768;&#22810;&#39640;&#36136;&#37327;&#25991;&#26412;&#25968;&#25454;&#26159;&#31169;&#23494;&#30340;&#65292;&#22240;&#27492;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#25110;&#20351;&#29992;&#12290;&#29983;&#25104;&#20855;&#26377;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#65288;&#21363;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65289;&#30340;&#31169;&#23494;&#25991;&#26412;&#25968;&#25454;&#30340;&#21512;&#25104;&#21103;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;DP&#24494;&#35843;&#65292;&#20197;&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#19987;&#26377;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#24182;&#19981;&#21487;&#34892;&#65292;&#32780;&#19988;&#23545;&#20110;&#24320;&#28304;LLM&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;Lin&#31561;&#20154;&#65288;2024&#65289;&#26368;&#36817;&#24341;&#20837;&#20102;&#31169;&#26377;&#36827;&#21270;&#65288;PE&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21482;&#36890;&#36807;API&#35775;&#38382;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;PE&#31639;&#27861;&#65292;&#21517;&#20026;Aug-PE&#65292;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#22797;&#26434;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01749v1 Announce Type: new  Abstract: Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01748</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#20026;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Decode Neural signal as Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#21160;&#24577;&#35299;&#30721;&#35821;&#35328;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#26041;&#21521;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#30456;&#23545;&#20110;&#38656;&#35201;&#30005;&#26497;&#26893;&#20837;&#25163;&#26415;&#30340;&#20405;&#20837;&#24615;&#20449;&#21495;&#65292;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#20449;&#21495;&#65288;&#22914;EEG&#12289;MEG&#65289;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#21644;&#26222;&#36866;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#65306;1&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#20808;&#21069;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;MEG&#20449;&#21495;&#36136;&#37327;&#26356;&#22909;&#30340;&#38382;&#39064;&#65307;2&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22312;&#29983;&#25104;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65307;3&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#22522;&#20110;&#8220;BART&#8221;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#22312;&#35821;&#38899;&#35299;&#30721;&#24418;&#24335;&#20013;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#20013;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20250;&#35805;&#25628;&#32034;&#20013;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#31572;&#26696;&#37325;&#20889;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31572;&#26696;&#37325;&#20889;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.01747</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#21253;&#21547;&#31572;&#26696;&#30340;&#26041;&#21521;&#65306;&#20250;&#35805;&#25628;&#32034;&#20013;&#22522;&#20110;&#23454;&#20307;&#30340;&#31572;&#26696;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20250;&#35805;&#25628;&#32034;&#20013;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#31572;&#26696;&#37325;&#20889;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31572;&#26696;&#37325;&#20889;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20449;&#24687;&#26816;&#32034;&#65288;CIS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30693;&#35782;&#33719;&#21462;&#21644;&#25506;&#32034;&#24615;&#25628;&#32034;&#33539;&#24335;&#12290;&#20256;&#32479;&#30340;&#32593;&#32476;&#25628;&#32034;&#30028;&#38754;&#21487;&#20197;&#36731;&#26494;&#25506;&#32034;&#23454;&#20307;&#65292;&#20294;&#22312;&#20250;&#35805;&#29615;&#22659;&#20013;&#21463;&#38480;&#20110;&#24102;&#23485;&#26377;&#38480;&#30340;&#30028;&#38754;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;CIS&#20013;&#37325;&#20889;&#31572;&#26696;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#31572;&#26696;&#32780;&#26080;&#38656;&#27714;&#21161;&#22806;&#37096;&#26381;&#21153;&#25110;&#26469;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#31361;&#20986;&#30340;&#23454;&#20307;--&#23545;&#20110;&#29702;&#35299;&#31572;&#26696;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#31361;&#20986;&#23454;&#20307;&#27880;&#37322;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21518;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#31572;&#26696;&#21253;&#21547;&#20102;&#31361;&#20986;&#23454;&#20307;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26088;&#22312;&#25913;&#21892;CIS&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#31572;&#26696;&#37325;&#20889;&#31574;&#30053;&#12290;&#20854;&#19968;&#36890;&#36807;&#20869;&#32852;&#23450;&#20041;&#31361;&#20986;&#23454;&#20307;&#26469;&#25193;&#23637;&#31572;&#26696;&#65292;&#20351;&#31572;&#26696;&#33258;&#21253;&#21547;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01747v1 Announce Type: cross  Abstract: Conversational information-seeking (CIS) is an emerging paradigm for knowledge acquisition and exploratory search. Traditional web search interfaces enable easy exploration of entities, but this is limited in conversational settings due to the limited-bandwidth interface. This paper explore ways to rewrite answers in CIS, so that users can understand them without having to resort to external services or sources. Specifically, we focus on salient entities -- entities that are central to understanding the answer. As our first contribution, we create a dataset of conversations annotated with entities for saliency. Our analysis of the collected data reveals that the majority of answers contain salient entities. As our second contribution, we propose two answer rewriting strategies aimed at improving the overall user experience in CIS. One approach expands answers with inline definitions of salient entities, making the answer self-contained
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.01699</link><description>&lt;p&gt;
Brilla AI: &#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;
&lt;/p&gt;
&lt;p&gt;
Brilla AI: AI Contestant for the National Science and Maths Quiz
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01699
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#22823;&#38470;&#32570;&#20047;&#36275;&#22815;&#30340;&#21512;&#26684;&#25945;&#24072;&#65292;&#36825;&#38459;&#30861;&#20102;&#25552;&#20379;&#36275;&#22815;&#30340;&#23398;&#20064;&#25903;&#25345;&#12290;&#20154;&#24037;&#26234;&#33021;&#26377;&#21487;&#33021;&#22686;&#24378;&#26377;&#38480;&#25968;&#37327;&#25945;&#24072;&#30340;&#21162;&#21147;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#23398;&#20064;&#25104;&#26524;&#12290;&#26412;&#25991;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;NSMQ AI Grand Challenge&#30340;&#39318;&#35201;&#25104;&#26524;&#65292;&#35813;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29616;&#23454;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#65306;&#8220;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65292;&#21442;&#21152;&#21152;&#32435;&#30340;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65288;NSMQ&#65289;&#65292;&#24182;&#33719;&#32988;&#8212;&#8212;&#22312;&#27604;&#36187;&#30340;&#25152;&#26377;&#36718;&#27425;&#21644;&#38454;&#27573;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20248;&#31168;&#30340;&#21442;&#36187;&#32773;&#8221;&#12290;NSMQ&#26159;&#21152;&#32435;&#30340;&#39640;&#20013;&#23398;&#29983;&#27599;&#24180;&#20030;&#34892;&#30340;&#29616;&#22330;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65292;3&#38431;2&#21517;&#23398;&#29983;&#36890;&#36807;&#22238;&#31572;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#29289;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#22312;5&#36718;&#27604;&#36187;&#20013;&#31454;&#20105;&#65292;&#36880;&#28176;&#26187;&#32423;&#33267;&#26368;&#32456;&#20896;&#20891;&#30340;&#38431;&#20237;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Brilla AI&#65292;&#19968;&#20010;&#21442;&#21152;NSMQ&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#36873;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01699v1 Announce Type: cross  Abstract: The African continent lacks enough qualified teachers which hampers the provision of adequate learning support. An AI could potentially augment the efforts of the limited number of teachers, leading to better learning outcomes. Towards that end, this work describes and evaluates the first key output for the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for such an AI: "Build an AI to compete live in Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing better than the best contestants in all rounds and stages of the competition". The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. In this work, we built Brilla AI, an AI contestant that we de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;&#25968;&#25454;&#38598;HEED&#21644;&#19968;&#20010;&#22522;&#20110;MoE&#30340;&#23454;&#20307;&#25552;&#21462;&#26694;&#26550;MoEEF&#65292;&#26377;&#25928;&#25972;&#21512;&#22810;&#20010;&#29305;&#24449;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01698</link><description>&lt;p&gt;
&#32593;&#39029;&#20013;&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Hypertext Entity Extraction in Webpage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;&#25968;&#25454;&#38598;HEED&#21644;&#19968;&#20010;&#22522;&#20110;MoE&#30340;&#23454;&#20307;&#25552;&#21462;&#26694;&#26550;MoEEF&#65292;&#26377;&#25928;&#25972;&#21512;&#22810;&#20010;&#29305;&#24449;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#23454;&#20307;&#25552;&#21462;&#26159;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#20170;&#22823;&#22810;&#25968;&#32593;&#39029;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#37117;&#26159;&#22312;&#21147;&#27714;&#20445;&#30041;&#25991;&#26412;&#20869;&#23481;&#21450;&#20854;&#32467;&#26500;&#20449;&#24687;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HEED&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#25910;&#38598;&#20102;&#25991;&#26412;&#21644;&#30456;&#24212;&#30340;&#26174;&#24335;&#36229;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#36827;&#34892;&#20102;&#39640;&#36136;&#37327;&#25163;&#21160;&#23454;&#20307;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;MoE&#30340;&#23454;&#20307;&#25552;&#21462;&#26694;&#26550;(MoEEF)&#65292;&#36890;&#36807;&#22810;&#19987;&#23478;&#28151;&#21512;&#26377;&#25928;&#22320;&#25972;&#21512;&#22810;&#20010;&#29305;&#24449;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01698v1 Announce Type: cross  Abstract: Webpage entity extraction is a fundamental natural language processing task in both research and applications. Nowadays, the majority of webpage entity extraction models are trained on structured datasets which strive to retain textual content and its structure information. However, existing datasets all overlook the rich hypertext features (e.g., font color, font size) which show their effectiveness in previous works. To this end, we first collect a \textbf{H}ypertext \textbf{E}ntity \textbf{E}xtraction \textbf{D}ataset (\textit{HEED}) from the e-commerce domains, scraping both the text and the corresponding explicit hypertext features with high-quality manual entity annotations. Furthermore, we present the \textbf{Mo}E-based \textbf{E}ntity \textbf{E}xtraction \textbf{F}ramework (\textit{MoEEF}), which efficiently integrates multiple features to enhance model performance by Mixture of Experts and outperforms strong baselines, includi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;LSTM&#21644;BERT&#27169;&#22411;&#65292;&#20197;&#21450;&#25968;&#25454;&#22686;&#24378;&#21644;&#28966;&#28857;&#25439;&#22833;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#21806;&#39046;&#22495;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#20135;&#21697;&#31867;&#21035;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.01638</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#20998;&#31867;&#36827;&#34892;&#22810;&#23618;&#20135;&#21697;&#31867;&#21035;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-level Product Category Prediction through Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;LSTM&#21644;BERT&#27169;&#22411;&#65292;&#20197;&#21450;&#25968;&#25454;&#22686;&#24378;&#21644;&#28966;&#28857;&#25439;&#22833;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#21806;&#39046;&#22495;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#20135;&#21697;&#31867;&#21035;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LSTM&#21644;BERT&#65292;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20197;&#39044;&#27979;&#38646;&#21806;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#31867;&#21035;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#28966;&#28857;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20351;&#29992;&#29282;&#22266;&#30340;&#24052;&#35199;&#38646;&#21806;&#25968;&#25454;&#38598;&#23558;&#20135;&#21697;&#20998;&#31867;&#21040;&#22810;&#20010;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;LSTM&#27169;&#22411;&#65292;&#20016;&#23500;&#20102;&#24052;&#35199;&#35789;&#23884;&#20837;&#65292;&#20197;&#21450;&#20197;&#20854;&#29702;&#35299;&#22797;&#26434;&#19978;&#19979;&#25991;&#32780;&#38395;&#21517;&#30340;BERT&#65292;&#34987;&#25913;&#32534;&#21644;&#20248;&#21270;&#29992;&#20110;&#27492;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01638v1 Announce Type: new  Abstract: This article investigates applying advanced machine learning models, specifically LSTM and BERT, for text classification to predict multiple categories in the retail sector. The study demonstrates how applying data augmentation techniques and the focal loss function can significantly enhance accuracy in classifying products into multiple categories using a robust Brazilian retail dataset. The LSTM model, enriched with Brazilian word embedding, and BERT, known for its effectiveness in understanding complex contexts, were adapted and optimized for this specific task. The results showed that the BERT model, with an F1 Macro Score of up to $99\%$ for segments, $96\%$ for categories and subcategories and $93\%$ for name products, outperformed LSTM in more detailed categories. However, LSTM also achieved high performance, especially after applying data augmentation and focal loss techniques. These results underscore the effectiveness of NLP te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25512;&#21160;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#20998;&#20139;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01616</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#38754;&#30340;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25512;&#21160;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#20998;&#20139;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#21644;&#20256;&#25773;&#29992;&#20110;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#22312;&#25512;&#21160;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#27700;&#24179;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01616v1 Announce Type: new  Abstract: This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SCHEMA&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36861;&#36394;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.01599</link><description>&lt;p&gt;
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SCHEMA&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36861;&#36394;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32473;&#20986;&#26681;&#25454;&#37096;&#20998;&#35270;&#35273;&#29366;&#24577;&#35266;&#23519;&#29983;&#25104;&#30446;&#26631;&#23548;&#21521;&#30340;&#21160;&#20316;&#27493;&#39588;&#24207;&#21015;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#32467;&#26500;&#21270;&#19988;&#21487;&#35268;&#21010;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#29366;&#24577;&#21464;&#21270;&#23545;&#20110;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#24456;&#37325;&#35201;&#65292;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#31435;&#26356;&#20026;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#22320;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36319;&#36394;&#31243;&#24207;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01599v1 Announce Type: cross  Abstract: We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;Transformer&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21644;&#23376;&#35789;&#27169;&#22411;&#31867;&#22411;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#24320;&#21457;&#20102;gaHealth&#65292;&#39318;&#20010;&#38024;&#23545;&#29233;&#23572;&#20848;&#35821;&#20581;&#24247;&#25968;&#25454;&#30340;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32763;&#35793;&#36136;&#37327;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.01580</link><description>&lt;p&gt;
&#21152;&#24378;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65306;&#35821;&#26009;&#24211;&#24320;&#21457;&#12289;&#20154;&#31867;&#35780;&#20272;&#21644;&#21487;&#35299;&#37322;&#30340; AI &#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;Transformer&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21644;&#23376;&#35789;&#27169;&#22411;&#31867;&#22411;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#24320;&#21457;&#20102;gaHealth&#65292;&#39318;&#20010;&#38024;&#23545;&#29233;&#23572;&#20848;&#35821;&#20581;&#24247;&#25968;&#25454;&#30340;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32763;&#35793;&#36136;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#20013;&#65292;Transformer&#26550;&#26500;&#23588;&#20854;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#33073;&#39062;&#32780;&#20986;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23427;&#22312;&#21253;&#25324;&#33521;&#35821;$\leftrightarrow$&#29233;&#23572;&#20848;&#35821;&#21644;&#33521;&#35821;$\leftrightarrow$&#39532;&#25289;&#22320;&#35821;&#35821;&#35328;&#23545;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#26368;&#20339;&#36229;&#21442;&#25968;&#21644;&#23376;&#35789;&#27169;&#22411;&#31867;&#22411;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;Transformer&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21487;&#33021;&#38459;&#30861;MT&#30340;&#21457;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;gaHealth&#65292;&#36825;&#26159;&#29233;&#23572;&#20848;&#35821;&#20581;&#24247;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#21452;&#35821;&#35821;&#26009;&#24211;&#12290;&#20351;&#29992;&#36825;&#19968;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#22312;&#19982;LoResMT2021&#20849;&#20139;&#20219;&#21153;&#27169;&#22411;&#30456;&#27604;&#26102;&#65292;&#22312;BLEU&#35780;&#20998;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#38543;&#21518;&#30340;&#20154;&#31867;&#35780;&#20272;&#20351;&#29992;&#22810;&#32500;&#24230;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01580v1 Announce Type: cross  Abstract: In the current machine translation (MT) landscape, the Transformer architecture stands out as the gold standard, especially for high-resource language pairs. This research delves into its efficacy for low-resource language pairs including both the English$\leftrightarrow$Irish and English$\leftrightarrow$Marathi language pairs. Notably, the study identifies the optimal hyperparameters and subword model type to significantly improve the translation quality of Transformer models for low-resource language pairs.   The scarcity of parallel datasets for low-resource languages can hinder MT development. To address this, gaHealth was developed, the first bilingual corpus of health data for the Irish language. Focusing on the health domain, models developed using this in-domain dataset exhibited very significant improvements in BLEU score when compared with models from the LoResMT2021 Shared Task. A subsequent human evaluation using the multid
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01570</link><description>&lt;p&gt;
SERVAL&#65306;&#22402;&#30452;&#27169;&#22411;&#21644;LLM&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#23454;&#29616;&#38646;-shot&#32423;&#21035;&#30340;&#21307;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20986;&#23545;&#36890;&#29992;&#21644;&#24120;&#35782;&#38382;&#39064;&#21331;&#36234;&#30340;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22402;&#30452;&#30693;&#35782;&#26041;&#38754;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22402;&#30452;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#19987;&#23478;&#21442;&#19982;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#22686;&#24378;&#27169;&#22411;&#22402;&#30452;&#33021;&#21147;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23545;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#36827;&#34892;&#26080;&#30417;&#30563;&#24320;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SERVAL&#21033;&#29992;LLMs&#30340;&#38646;-shot&#36755;&#20986;&#20316;&#20026;&#27880;&#37322;&#65292;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#26469;&#20174;&#22836;&#24320;&#22987;&#25945;&#25480;&#19968;&#20010;&#24378;&#22823;&#30340;&#22402;&#30452;&#27169;&#22411;&#12290;&#21453;&#36807;&#26469;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#22402;&#30452;&#27169;&#22411;&#24341;&#23548;LLM&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#38646;-shot&#33021;&#21147;&#65292;&#36880;&#27493;&#25913;&#36827;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.01528</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01528
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;BL&#65289;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#28304;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#20016;&#23500;&#22810;&#38754;&#25551;&#36848;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#35745;&#31639;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#34920;&#36798;&#30340;&#24494;&#22937;&#21465;&#36848;&#19982;&#36890;&#36807;&#21508;&#31181;&#20998;&#23376;&#24314;&#27169;&#25216;&#26415;&#25551;&#36848;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#32454;&#33410;&#34701;&#21512;&#65292;&#25171;&#24320;&#20102;&#20840;&#38754;&#34920;&#24449;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#30340;&#26032;&#36884;&#24452;&#12290;&#36890;&#36807;&#23558;&#22260;&#32469;&#29983;&#29289;&#20998;&#23376;&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#25968;&#25454;&#32435;&#20837;&#24314;&#27169;&#20013;&#65292;BL&#26088;&#22312;&#25429;&#25417;&#21253;&#21547;&#35821;&#35328;&#20256;&#36798;&#30340;&#31526;&#21495;&#29305;&#24615;&#20197;&#21450;&#25968;&#37327;&#21270;&#32467;&#26500;&#29305;&#24449;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01528v1 Announce Type: cross  Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this r
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#23558;&#21442;&#25968;&#36716;&#21464;&#20026;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#25552;&#20379;&#19968;&#31181;&#20855;&#26377;&#20869;&#23384;&#26435;&#37325;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#24418;&#24335;&#65292;&#26356;&#31526;&#21512;&#31070;&#32463;&#31185;&#23398;&#20013;&#35760;&#24518;&#27010;&#24565;&#65292;&#19988;&#22312;&#25552;&#21319;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#26102;&#23588;&#20854;&#26377;&#36259;&#12290;</title><link>https://arxiv.org/abs/2403.01518</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21160;&#24577;&#35780;&#20272;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01518
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#23558;&#21442;&#25968;&#36716;&#21464;&#20026;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#25552;&#20379;&#19968;&#31181;&#20855;&#26377;&#20869;&#23384;&#26435;&#37325;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#24418;&#24335;&#65292;&#26356;&#31526;&#21512;&#31070;&#32463;&#31185;&#23398;&#20013;&#35760;&#24518;&#27010;&#24565;&#65292;&#19988;&#22312;&#25552;&#21319;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#26102;&#23588;&#20854;&#26377;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#20063;&#21363;&#31216;&#20026;&#21160;&#24577;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33324;&#35748;&#20026;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24378;&#35843;&#22312;&#32447;&#35843;&#25972;&#23558;&#21442;&#25968;&#36716;&#21464;&#20026;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#20869;&#23384;&#26435;&#37325;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#24418;&#24335;&#65292;&#26356;&#31526;&#21512;&#31070;&#32463;&#31185;&#23398;&#20013;&#35760;&#24518;&#27010;&#24565;&#30340;&#24605;&#36335;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36866;&#24212;&#36895;&#24230;&#65288;&#20197;&#26679;&#26412;&#25928;&#29575;&#34913;&#37327;&#65289;&#12289;&#23545;&#25972;&#20307;&#20998;&#24067;&#24615;&#28418;&#31227;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#25191;&#34892;&#26799;&#24230;&#35745;&#31639;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;&#20309;&#26102;&#22312;&#32447;&#35843;&#25972;&#23588;&#20026;&#26377;&#36259;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;&#32447;&#35843;&#25972;&#20351;&#24471;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#20869;&#23384;&#22312;&#27010;&#24565;&#19978;&#30340;&#21306;&#20998;&#27169;&#31946;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01518v1 Announce Type: new  Abstract: We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#27599;&#20010;&#23618;&#30340;&#26411;&#31471;&#25506;&#27979;&#20854;&#38544;&#34255;&#29366;&#24577;&#65292;&#20351;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#35782;&#21035;&#20219;&#21153;&#65292;&#26412;&#25991;&#20855;&#20307;&#30740;&#31350;&#20102;Llama2&#30340;&#33258;&#19979;&#32780;&#19978;&#35789;&#27719;&#35821;&#20041;&#28436;&#21464;&#65292;&#21457;&#29616;&#36739;&#20302;&#23618;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#65292;&#32780;&#36739;&#39640;&#23618;&#36127;&#36131;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.01509</link><description>&lt;p&gt;
&#22855;&#24187;&#35821;&#20041;&#19982;&#23547;&#25214;&#20043;&#22320;&#65306;&#25506;&#35752;&#29983;&#25104;&#22411;LLM&#30340;&#21738;&#20123;&#23618;&#27425;&#21453;&#26144;&#35789;&#27719;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01509
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#27599;&#20010;&#23618;&#30340;&#26411;&#31471;&#25506;&#27979;&#20854;&#38544;&#34255;&#29366;&#24577;&#65292;&#20351;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#35782;&#21035;&#20219;&#21153;&#65292;&#26412;&#25991;&#20855;&#20307;&#30740;&#31350;&#20102;Llama2&#30340;&#33258;&#19979;&#32780;&#19978;&#35789;&#27719;&#35821;&#20041;&#28436;&#21464;&#65292;&#21457;&#29616;&#36739;&#20302;&#23618;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#65292;&#32780;&#36739;&#39640;&#23618;&#36127;&#36131;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#36861;&#27714;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#27861;&#23478;&#26063;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#20041;&#28436;&#21464;&#38543;&#30528;&#28145;&#24230;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20687;&#23427;&#20204;&#30340;&#21069;&#36744;&#65292;&#27604;&#22914;BERT&#31867;&#26550;&#26500;&#12290;&#26412;&#25991;&#20855;&#20307;&#30740;&#31350;&#20102;&#19968;&#27454;&#27969;&#34892;LLM&#65292;&#21363;Llama2&#30340;&#33258;&#19979;&#32780;&#19978;&#35789;&#27719;&#35821;&#20041;&#28436;&#21464;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23618;&#30340;&#26411;&#31471;&#25506;&#27979;&#20854;&#38544;&#34255;&#29366;&#24577;&#65292;&#20351;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#35782;&#21035;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36739;&#20302;&#23618;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#65292;&#32780;&#36739;&#39640;&#23618;&#65292;&#20854;&#35821;&#20041;&#24402;&#32435;&#36739;&#24369;&#65292;&#36127;&#36131;&#39044;&#27979;&#12290;&#36825;&#19982;&#20855;&#26377;&#21028;&#21035;&#30446;&#26631;&#30340;&#27169;&#22411;&#24418;&#25104;&#23545;&#27604;&#65292;&#27604;&#22914;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#37027;&#37324;&#36739;&#39640;&#23618;&#33719;&#24471;&#26356;&#22909;&#30340;&#35789;&#27719;&#35821;&#20041;&#12290;&#32467;&#35770;&#36827;&#19968;&#27493;&#24471;&#21040;&#24615;&#33021;&#21333;&#35843;&#22686;&#21152;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01509v1 Announce Type: new  Abstract: Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#25552;&#31034;&#26469;&#36827;&#34892;&#30693;&#35782;&#27880;&#20837;&#30340;&#31616;&#21333;&#20294;&#36890;&#29992;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01481</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28748;&#36755;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Infusing Knowledge into Large Language Models with Contextual Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#25552;&#31034;&#26469;&#36827;&#34892;&#30693;&#35782;&#27880;&#20837;&#30340;&#31616;&#21333;&#20294;&#36890;&#29992;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#26159;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;NLP&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#23545;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#12290;&#36825;&#31181;&#22686;&#24378;&#30340;LLM&#36890;&#24120;&#20381;&#36182;&#20110;&#26469;&#33258;&#29616;&#26377;&#30693;&#35782;&#22270;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#25110;&#30693;&#35782;&#25552;&#31034;&#65292;&#20294;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#21453;&#65292;&#30452;&#25509;&#20174;&#30456;&#20851;&#25991;&#26723;&#20013;&#27880;&#20837;&#30693;&#35782;&#26356;&#20855;&#19968;&#33324;&#24615;&#65292;&#20943;&#36731;&#20102;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;&#20110;&#36890;&#24120;&#19981;&#22312;&#20219;&#20309;&#30693;&#35782;&#22270;&#20013;&#25214;&#21040;&#30340;&#23454;&#20307;&#20063;&#24456;&#26377;&#29992;&#12290;&#37492;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#24494;&#35843;&#21518;&#30340;LLM&#36827;&#34892;&#25506;&#27979;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01481v1 Announce Type: new  Abstract: Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch. These augmented LLMs typically depend on additional pre-training or knowledge prompts from an existing knowledge graph, which is impractical in many applications. In contrast, knowledge infusion directly from relevant documents is more generalisable and alleviates the need for structured knowledge graphs while also being useful for entities that are usually not found in any knowledge graph. With this motivation, we propose a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs.
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"</title><link>https://arxiv.org/abs/2403.01479</link><description>&lt;p&gt;
Align-to-Distill: &#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01479
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;Transformer&#26550;&#26500;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#23450;&#35201;&#20174;&#21738;&#20123;&#25945;&#24072;&#23618;&#20013;&#33976;&#39311;&#30693;&#35782;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#26469;&#35299;&#20915;&#29305;&#24449;&#26144;&#23556;&#38382;&#39064;&#12290;A2D&#20013;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#27169;&#22359;&#25191;&#34892;&#23398;&#29983;&#21644;&#25945;&#24072;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#30340;&#23494;&#38598;&#36880;&#22836;&#27604;&#36739;&#65292;&#23558;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De-&gt;Dsb and WMT-2014 En-&gt;De, respe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.01472</link><description>&lt;p&gt;
WARDEN&#65306;&#22810;&#26041;&#21521;&#32972;&#38376;&#27700;&#21360;&#29992;&#20110;Embedding-as-a-Service&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WARDEN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#25991;&#26412;&#20013;&#21152;&#20837;&#22810;&#20010;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#65292;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#65292;&#20197;&#24212;&#23545;EaaS&#20013;&#32972;&#38376;&#27700;&#21360;&#34987;&#31227;&#38500;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Embedding as a Service&#65288;EaaS&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#65307;&#28982;&#32780;&#65292;&#36890;&#36807;&#21521;&#25991;&#26412;&#23884;&#20837;&#28155;&#21152;&#32972;&#38376;&#27700;&#21360;&#65292;&#24182;&#38543;&#21518;&#39564;&#35777;&#25915;&#20987;&#27169;&#22411;&#30340;&#21457;&#24067;&#21518;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#26368;&#36817;&#29992;&#20110;EaaS&#30340;&#27700;&#21360;&#31574;&#30053;EmbMarker&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CSE&#65288;Cluster&#12289;Selection&#12289;Elimination&#65289;&#25915;&#20987;&#65292;&#23427;&#33021;&#22815;&#31227;&#38500;&#32972;&#38376;&#27700;&#21360;&#21516;&#26102;&#20445;&#25345;&#23884;&#20837;&#30340;&#39640;&#25928;&#24615;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#27700;&#21360;&#26041;&#27861;&#26159;&#21487;&#20197;&#34987;&#31361;&#30772;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#23041;&#32961;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#21487;&#33021;&#30340;&#27700;&#21360;&#26041;&#21521;&#20351;&#27700;&#21360;&#30340;&#31227;&#38500;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;WARDEN&#26174;&#33879;&#22686;&#21152;&#20102;&#27700;&#21360;&#28040;&#38500;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01472v1 Announce Type: cross  Abstract: Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the ste
&lt;/p&gt;</description></item><item><title>KorMedMCQA&#26159;&#39318;&#20010;&#20174;&#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#20013;&#34893;&#29983;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;HuggingFace&#19978;&#20844;&#24320;&#20102;&#25968;&#25454;&#65292;&#20026;&#38889;&#22269;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01469</link><description>&lt;p&gt;
KorMedMCQA: &#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01469
&lt;/p&gt;
&lt;p&gt;
KorMedMCQA&#26159;&#39318;&#20010;&#20174;&#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#20013;&#34893;&#29983;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;HuggingFace&#19978;&#20844;&#24320;&#20102;&#25968;&#25454;&#65292;&#20026;&#38889;&#22269;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;KorMedMCQA&#65292;&#36825;&#26159;&#39318;&#20010;&#28304;&#33258;&#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#30340;&#38889;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#65288;MCQA&#65289;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20174;2012&#24180;&#21040;2023&#24180;&#30340;&#32771;&#35797;&#20869;&#23481;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#21644;&#33647;&#21058;&#24072;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#19968;&#37096;&#20998;&#38382;&#39064;&#65292;&#28085;&#30422;&#22810;&#31181;&#23398;&#31185;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#32447;&#23454;&#39564;&#65292;&#21253;&#25324;&#19987;&#26377;/&#24320;&#28304;&#12289;&#22810;&#35821;&#35328;/&#38889;&#35821;&#38468;&#21152;&#39044;&#35757;&#32451;&#21644;&#20020;&#24202;&#32972;&#26223;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;HuggingFace&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;LM-Harness&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#33050;&#26412;&#65292;&#36992;&#35831;&#22312;&#38889;&#22269;&#21307;&#30103;&#29615;&#22659;&#20013;&#36827;&#34892;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01469v1 Announce Type: new  Abstract: We introduce KorMedMCQA, the first Korean multiple-choice question answering (MCQA) benchmark derived from Korean healthcare professional licensing examinations, covering from the year 2012 to year 2023. This dataset consists of a selection of questions from the license examinations for doctors, nurses, and pharmacists, featuring a diverse array of subjects. We conduct baseline experiments on various large language models, including proprietary/open-source, multilingual/Korean-additional pretrained, and clinical context pretrained models, highlighting the potential for further enhancements. We make our data publicly available on HuggingFace and provide a evaluation script via LM-Harness, inviting further exploration and advancement in Korean healthcare environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26816;&#32034;&#31995;&#32479;&#30340;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#38543;&#26426;&#21270;&#31574;&#30053;&#35757;&#32451;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#20855;&#26377;&#39640;&#35821;&#20041;&#37325;&#21472;&#30340;&#26080;&#20851;&#25991;&#26412;&#25688;&#24405;&#26041;&#38754;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01461</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#30340;&#21487;&#22238;&#31572;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Answerability in Retrieval-Augmented Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26816;&#32034;&#31995;&#32479;&#30340;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#38543;&#26426;&#21270;&#31574;&#30053;&#35757;&#32451;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#20855;&#26377;&#39640;&#35821;&#20041;&#37325;&#21472;&#30340;&#26080;&#20851;&#25991;&#26412;&#25688;&#24405;&#26041;&#38754;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#26816;&#32034;&#31995;&#32479;&#30340;&#24615;&#33021;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#34892;&#20026;&#65292;&#25552;&#20379;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#26080;&#20851;&#24615;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#35782;&#21035;&#26080;&#20851;&#25991;&#26412;&#25688;&#24405;&#30340;&#31034;&#20363;&#12290;&#20808;&#21069;&#30340;&#23581;&#35797;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#38382;&#39064;&#65292;&#20294;&#20381;&#36182;&#20110;&#23558;&#38382;&#39064;&#19982;&#38543;&#26426;&#25991;&#26412;&#25688;&#24405;&#37197;&#23545;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#36825;&#31181;&#38543;&#26426;&#21270;&#31574;&#30053;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20854;&#22312;&#27867;&#21270;&#21040;&#20855;&#26377;&#39640;&#35821;&#20041;&#37325;&#21472;&#30340;&#26080;&#20851;&#25991;&#26412;&#25688;&#24405;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#20174;98%&#19979;&#38477;&#21040;1%&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#27492;&#31867;&#25688;&#24405;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;SQuAD 2.0&#25968;&#25454;&#38598;&#30340;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#23545;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;(~100%)&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01461v1 Announce Type: new  Abstract: The performance of Open-Domain Question Answering (ODQA) retrieval systems can exhibit sub-optimal behavior, providing text excerpts with varying degrees of irrelevance. Unfortunately, many existing ODQA datasets lack examples specifically targeting the identification of irrelevant text excerpts. Previous attempts to address this gap have relied on a simplistic approach of pairing questions with random text excerpts. This paper aims to investigate the effectiveness of models trained using this randomized strategy, uncovering an important limitation in their ability to generalize to irrelevant text excerpts with high semantic overlap. As a result, we observed a substantial decrease in predictive accuracy, from 98% to 1%. To address this limitation, we discovered an efficient approach for training models to recognize such excerpts. By leveraging unanswerable pairs from the SQuAD 2.0 dataset, our models achieve a nearly perfect (~100%) accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#22686;&#24378;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65288;NS-LCR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26696;&#20363;&#32423;&#21035;&#21644;&#27861;&#24459;&#32423;&#21035;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#23558;&#35268;&#21017;&#20197;&#31070;&#32463;&#31526;&#21495;&#26041;&#24335;&#38598;&#25104;&#21040;&#26816;&#32034;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#20379;&#36923;&#36753;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.01457</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#21017;&#20316;&#20026;&#35299;&#37322;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Logic Rules as Explanations for Legal Case Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#22686;&#24378;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65288;NS-LCR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26696;&#20363;&#32423;&#21035;&#21644;&#27861;&#24459;&#32423;&#21035;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#23558;&#35268;&#21017;&#20197;&#31070;&#32463;&#31526;&#21495;&#26041;&#24335;&#38598;&#25104;&#21040;&#26816;&#32034;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#20379;&#36923;&#36753;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#35299;&#37322;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#20219;&#21153;&#23545;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#29992;&#25143;&#65288;&#22914;&#24459;&#24072;&#25110;&#27861;&#23448;&#65289;&#20855;&#26377;&#39640;&#24230;&#19987;&#19994;&#21270;&#65292;&#38656;&#35201;&#31995;&#32479;&#22312;&#20570;&#20986;&#27861;&#24459;&#20915;&#31574;&#20043;&#21069;&#25552;&#20379;&#36923;&#36753;&#12289;&#24544;&#23454;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#24037;&#20316;&#26088;&#22312;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20174;&#27861;&#24459;&#26696;&#20363;&#20013;&#36873;&#25321;&#22522;&#26412;&#21407;&#29702;&#65288;&#20851;&#38190;&#21477;&#65289;&#20316;&#20026;&#35299;&#37322;&#65292;&#26410;&#33021;&#25552;&#20379;&#24544;&#23454;&#21644;&#36923;&#36753;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#22686;&#24378;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65288;NS-LCR&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#26696;&#20363;&#32423;&#21035;&#21644;&#27861;&#24459;&#32423;&#21035;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#26126;&#30830;&#22320;&#23545;&#27861;&#24459;&#26696;&#20363;&#30340;&#21305;&#37197;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#35268;&#21017;&#20197;&#31070;&#32463;&#31526;&#21495;&#26041;&#24335;&#38598;&#25104;&#21040;&#26816;&#32034;&#36807;&#31243;&#20013;&#12290;&#30001;&#20110;&#36923;&#36753;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01457v1 Announce Type: cross  Abstract: In this paper, we address the issue of using logic rules to explain the results from legal case retrieval. The task is critical to legal case retrieval because the users (e.g., lawyers or judges) are highly specialized and require the system to provide logical, faithful, and interpretable explanations before making legal decisions. Recently, research efforts have been made to learn explainable legal case retrieval models. However, these methods usually select rationales (key sentences) from the legal cases as explanations, failing to provide faithful and logically correct explanations. In this paper, we propose Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that explicitly conducts reasoning on the matching of legal cases through learning case-level and law-level logic rules. The learned rules are then integrated into the retrieval process in a neuro-symbolic manner. Benefiting from the logic and interpretable natu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#35268;&#21017;&#25511;&#21046;&#22635;&#31354;&#27979;&#35797;&#39064;&#30446;&#20013;&#31354;&#30333;&#21644;&#24178;&#25200;&#39033;&#30340;&#38590;&#24230;&#27700;&#24179;&#65292;&#26377;&#25928;&#35780;&#20272;MC&#22635;&#31354;&#27979;&#35797;&#30340;&#38590;&#24230;&#27700;&#24179;</title><link>https://arxiv.org/abs/2403.01456</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;PLM&#30340;&#20195;&#29702;&#27169;&#22411;&#25511;&#21046;IRT&#35780;&#20272;&#20013;&#30340;&#22635;&#31354;&#27979;&#35797;&#39064;&#30446;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01456
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#35268;&#21017;&#25511;&#21046;&#22635;&#31354;&#27979;&#35797;&#39064;&#30446;&#20013;&#31354;&#30333;&#21644;&#24178;&#25200;&#39033;&#30340;&#38590;&#24230;&#27700;&#24179;&#65292;&#26377;&#25928;&#35780;&#20272;MC&#22635;&#31354;&#27979;&#35797;&#30340;&#38590;&#24230;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#38590;&#24230;&#22312;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#29983;&#25104;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#65288;MC&#65289;&#22635;&#31354;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#35780;&#20272;&#65292;&#36991;&#20813;&#38656;&#35201;&#20154;&#31867;&#27979;&#35797;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#36890;&#36807;&#25490;&#21517;&#35268;&#21017;&#25511;&#21046;&#31354;&#30333;&#21644;&#24178;&#25200;&#39033;&#30340;&#38590;&#24230;&#27700;&#24179;&#65292;&#20197;&#20943;&#23569;&#26080;&#25928;&#24178;&#25200;&#39033;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#21644;&#35780;&#20272;MC&#22635;&#31354;&#27979;&#35797;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01456v1 Announce Type: cross  Abstract: Item difficulty plays a crucial role in adaptive testing. However, few works have focused on generating questions of varying difficulty levels, especially for multiple-choice (MC) cloze tests. We propose training pre-trained language models (PLMs) as surrogate models to enable item response theory (IRT) assessment, avoiding the need for human test subjects. We also propose two strategies to control the difficulty levels of both the gaps and the distractors using ranking rules to reduce invalid distractors. Experimentation on a benchmark dataset demonstrates that our proposed framework and methods can effectively control and evaluate the difficulty levels of MC cloze tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01432</link><description>&lt;p&gt;
&#24494;&#35843;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29992;&#20110;&#19981;&#22826;&#27969;&#34892;&#30693;&#35782;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35760;&#24518;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#24403;&#22788;&#29702;&#19981;&#22826;&#27969;&#34892;&#25110;&#20302;&#39057;&#27010;&#24565;&#21644;&#23454;&#20307;&#26102;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#20363;&#22914;&#22312;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#23545;&#23450;&#21046;LLMs&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FT&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#21463;&#27426;&#36814;&#21644;&#26368;&#19981;&#21463;&#27426;&#36814;&#30340;&#32676;&#20307;&#20013;&#65292;&#32780;RAG&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#21478;&#22806;&#65292;&#26816;&#32034;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#36827;&#27493;&#21152;&#24378;&#20102;RAG&#21644;FT&#26041;&#27861;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01432v1 Announce Type: new  Abstract: Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#22312;&#32447;&#35270;&#39057;&#30340;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;OVEL&#65292;&#19987;&#27880;&#20110;&#22312;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#20013;&#24314;&#31435;&#19982;&#30693;&#35782;&#24211;&#20043;&#38388;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#36830;&#25509;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#20132;&#20184;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;LIVE&#12290;</title><link>https://arxiv.org/abs/2403.01411</link><description>&lt;p&gt;
OVEL: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22312;&#32447;&#35270;&#39057;&#23454;&#20307;&#38142;&#25509;&#30340;&#20869;&#23384;&#31649;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
OVEL: Large Language Model as Memory Manager for Online Video Entity Linking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#22312;&#32447;&#35270;&#39057;&#30340;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;OVEL&#65292;&#19987;&#27880;&#20110;&#22312;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#20013;&#24314;&#31435;&#19982;&#30693;&#35782;&#24211;&#20043;&#38388;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#36830;&#25509;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#20132;&#20184;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;LIVE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#65288;MEL&#65289;&#22312;&#30740;&#31350;&#30028;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#35270;&#39057;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MEL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#38142;&#25509;&#25991;&#26412;&#21644;&#35270;&#35273;&#25552;&#21450;&#65292;&#25110;&#26159;&#31163;&#32447;&#35270;&#39057;&#25552;&#21450;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#65292;&#32780;&#23545;&#20110;&#38142;&#25509;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#25552;&#21450;&#25237;&#20837;&#30340;&#21162;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Online Video Entity Linking OVEL &#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#24314;&#31435;&#22312;&#32447;&#35270;&#39057;&#20013;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#21450;&#26102;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;OVEL&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23454;&#26102;&#20132;&#20184;&#22330;&#26223;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;LIVE&#30340;&#23454;&#26102;&#20132;&#20184;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#32771;&#34385;&#20102;&#26102;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01411v1 Announce Type: new  Abstract: In recent years, multi-modal entity linking (MEL) has garnered increasing attention in the research community due to its significance in numerous multi-modal applications. Video, as a popular means of information transmission, has become prevalent in people's daily lives. However, most existing MEL methods primarily focus on linking textual and visual mentions or offline videos's mentions to entities in multi-modal knowledge bases, with limited efforts devoted to linking mentions within online video content. In this paper, we propose a task called Online Video Entity Linking OVEL, aiming to establish connections between mentions in online videos and a knowledge base with high accuracy and timeliness. To facilitate the research works of OVEL, we specifically concentrate on live delivery scenarios and construct a live delivery entity linking dataset called LIVE. Besides, we propose an evaluation metric that considers timelessness, robustne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;&#22810;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21253;&#25324;&#32763;&#35793;-&#27979;&#35797;&#26041;&#27861;&#65292;&#35270;&#35273;&#32534;&#31243;&#26041;&#27861;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01404</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20013;&#30340;&#32570;&#22833;&#21450;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What Is Missing in Multilingual Visual Reasoning and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;&#22810;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21253;&#25324;&#32763;&#35793;-&#27979;&#35797;&#26041;&#27861;&#65292;&#35270;&#35273;&#32534;&#31243;&#26041;&#27861;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#27169;&#22411;&#20170;&#22825;&#22312;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#29992;&#25143;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#19968;&#20010;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#65292;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20687;GPT-4V&#36825;&#26679;&#30340;&#19987;&#26377;&#31995;&#32479;&#29616;&#22312;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#19982;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#24046;&#36317;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;GPT-4V&#22312;&#33521;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#20043;&#38388;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24320;&#21457;&#20844;&#24179;&#30340;&#31995;&#32479;&#20855;&#26377;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#22833;&#36133;&#36827;&#34892;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20351;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#22810;&#35821;&#35328;&#24615;&#65292;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21253;&#25324;&#19968;&#31181;&#32763;&#35793;-&#27979;&#35797;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#24615;&#65292;&#19968;&#31181;&#35270;&#35273;&#32534;&#31243;&#26041;&#27861;&#26469;&#20998;&#35299;&#22797;&#26434;&#25512;&#29702;&#65292;&#20197;&#21450;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01404v1 Announce Type: new  Abstract: NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KGQA&#25968;&#25454;&#38598; CR-LT-KGQA&#65292;&#35201;&#27714;&#36827;&#34892;&#24120;&#35782;&#25512;&#29702;&#65292;&#24182;&#20851;&#27880;&#38271;&#23614;&#23454;&#20307;&#65292;&#20026;&#37027;&#20123;&#24448;&#24448;&#20135;&#29983;&#22916;&#24819;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22522;&#20110;&#20107;&#23454;&#21644;&#21487;&#24402;&#23646;&#24120;&#35782;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01395</link><description>&lt;p&gt;
CR-LT-KGQA&#65306;&#19968;&#20010;&#38656;&#35201;&#24120;&#35782;&#25512;&#29702;&#21644;&#38271;&#23614;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01395
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KGQA&#25968;&#25454;&#38598; CR-LT-KGQA&#65292;&#35201;&#27714;&#36827;&#34892;&#24120;&#35782;&#25512;&#29702;&#65292;&#24182;&#20851;&#27880;&#38271;&#23614;&#23454;&#20307;&#65292;&#20026;&#37027;&#20123;&#24448;&#24448;&#20135;&#29983;&#22916;&#24819;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22522;&#20110;&#20107;&#23454;&#21644;&#21487;&#24402;&#23646;&#24120;&#35782;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01395v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20026;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38382;&#39064;&#25552;&#20379;&#20107;&#23454;&#31572;&#26696;&#30340;&#25104;&#29087;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;KGQA&#25968;&#25454;&#38598;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#23616;&#38480;&#65306;&#65288;1&#65289;&#27809;&#26377;&#29616;&#26377;&#30340;KGQA&#25968;&#25454;&#38598;&#38656;&#35201;&#24120;&#35782;&#25512;&#29702;&#25165;&#33021;&#24471;&#20986;&#31572;&#26696;&#65307;&#65288;2&#65289;&#29616;&#26377;&#30340;KGQA&#25968;&#25454;&#38598;&#37325;&#28857;&#20851;&#27880;&#27969;&#34892;&#23454;&#20307;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#30452;&#25509;&#22238;&#31572;&#32780;&#26080;&#38656;&#20135;&#29983;&#22916;&#24819;&#21644;&#26080;&#38656;&#21033;&#29992;KG&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#19968;&#20010;&#25903;&#25345;&#24120;&#35782;&#25512;&#29702;&#24182;&#19987;&#27880;&#20110;&#38271;&#23614;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#38750;&#20027;&#27969;&#21644;&#26368;&#36817;&#23454;&#20307;&#65289;&#30340;&#26032;&#39062;KGQA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;LLMs&#32463;&#24120;&#20135;&#29983;&#22916;&#24819;&#65292;&#22240;&#27492;&#38656;&#35201;&#20511;&#21161;KG&#36827;&#34892;&#22522;&#20110;&#20107;&#23454;&#19988;&#21487;&#24402;&#23646;&#30340;&#24120;&#35782;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24120;&#35782;&#25512;&#29702;&#65288;CR&#65289;&#21644;&#38271;&#23614;&#65288;LT&#65289;KGQA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;--&#38382;&#39064;&#22238;&#31572;&#21644;&#22768;&#26126;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01395v1 Announce Type: new  Abstract: Knowledge graph question answering (KGQA) is a well-established field that seeks to provide factual answers to natural language (NL) questions by leveraging knowledge graphs (KGs). However, existing KGQA datasets suffer from two significant limitations: (1) no existing KGQA dataset requires commonsense reasoning to arrive at an answer and (2) existing KGQA datasets focus on popular entities for which large language models (LLMs) can directly answer without hallucinating and without leveraging the KG. In this work, we seek a novel KGQA dataset that supports commonsense reasoning and focuses on long-tail entities (e.g., non-mainstream and recent entities) where LLMs frequently hallucinate, and thus create the need for novel methodologies that leverage the KG for factual and attributable commonsense inference. We create a novel Commonsense Reasoning (CR) and Long-Tail (LT) KGQA dataset with two subtasks -- question answering and claim verif
&lt;/p&gt;</description></item><item><title>LLM-based KGQA methods struggle with hallucination on commonsense reasoning questions, hindering their applicability in real-world applications.</title><link>https://arxiv.org/abs/2403.01390</link><description>&lt;p&gt;
&#27491;&#24403;&#19988;&#20805;&#20998;&#65306;&#21487;&#39564;&#35777;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01390
&lt;/p&gt;
&lt;p&gt;
LLM-based KGQA methods struggle with hallucination on commonsense reasoning questions, hindering their applicability in real-world applications.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#65288;KGQA&#65289;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#30693;&#35782;&#22270;&#20013;&#23384;&#20648;&#30340;&#20851;&#31995;&#20449;&#24687;&#26469;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#21450;&#20854;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;KGQA&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;Silvio Berlusconi&#30340;&#31532;&#19968;&#20219;&#22971;&#23376;&#20986;&#29983;&#22312;&#21738;&#24231;&#22478;&#24066;&#65311;&#8221;&#65292;&#32780;&#24573;&#30053;&#20102;&#28041;&#21450;&#24120;&#35782;&#25512;&#29702;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#29616;&#23454;&#19990;&#30028;&#29992;&#25143;&#21487;&#33021;&#26356;&#32463;&#24120;&#25552;&#20986;&#30340;&#65292;&#20363;&#22914;&#8220;&#25105;&#38656;&#35201;&#21333;&#29420;&#30340;&#31614;&#35777;&#25165;&#33021;&#30475;&#21040;&#23041;&#20262;&#22810;&#22827;&#30340;&#32500;&#32435;&#26031;&#24182;&#21442;&#21152;&#20170;&#24180;&#22799;&#22825;&#30340;&#22885;&#36816;&#20250;&#21527;&#65311;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;KGQA&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#26102;&#38590;&#20197;&#20135;&#29983;&#30495;&#23454;&#30340;&#31572;&#26696;&#65292;&#23588;&#20854;&#26159;&#23545;&#38024;&#23545;&#38271;&#23614;&#23454;&#20307;&#30340;&#26597;&#35810;&#65288;&#20363;&#22914;&#38750;&#20027;&#27969;&#21644;&#26368;&#36817;&#30340;&#23454;&#20307;&#65289;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#21487;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01390v1 Announce Type: new  Abstract: Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., "In which city was Silvio Berlusconi's first wife born?", leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., "Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?" unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especial
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01384</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compressibility of Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#36793;&#32536;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#20943;&#23569;&#25968;&#25454;&#31227;&#21160;&#65292;&#20174;&#32780;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#37327;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#38271;&#23614;&#30693;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01382</link><description>&lt;p&gt;
&#38271;&#23614;&#30693;&#35782;&#30340;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automatic Question-Answer Generation for Long-Tail Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#38271;&#23614;&#30693;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#22238;&#31572;&#19982;&#24120;&#35265;&#30693;&#35782;&#30456;&#20851;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#23398;&#20064;&#32597;&#35265;&#30340;&#38271;&#23614;&#30693;&#35782;&#65288;&#23614;&#37096;&#23454;&#20307;&#65289;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#19987;&#38376;&#29992;&#20110;&#23614;&#37096;&#23454;&#20307;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22312;&#25105;&#20204;&#26032;&#29983;&#25104;&#30340;&#38271;&#23614;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#26377;&#26080;&#22806;&#37096;&#36164;&#28304;&#65288;&#21253;&#25324;&#32500;&#22522;&#30334;&#31185;&#21644;&#32500;&#22522;&#25968;&#25454;&#30693;&#35782;&#22270;&#35889;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01382v1 Announce Type: new  Abstract: Pretrained Large Language Models (LLMs) have gained significant attention for addressing open-domain Question Answering (QA). While they exhibit high accuracy in answering questions related to common knowledge, LLMs encounter difficulties in learning about uncommon long-tail knowledge (tail entities). Since manually constructing QA datasets demands substantial human resources, the types of existing QA datasets are limited, leaving us with a scarcity of datasets to study the performance of LLMs on tail entities. In this paper, we propose an automatic approach to generate specialized QA datasets for tail entities and present the associated research challenges. We conduct extensive experiments by employing pretrained LLMs on our newly generated long-tail QA datasets, comparing their performance with and without external resources including Wikipedia and Wikidata knowledge graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;</title><link>https://arxiv.org/abs/2403.01373</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#65306;&#19968;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#30456;&#20851;&#25361;&#25112;&#26041;&#38754;&#30340;&#26174;&#33879;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#24187;&#35273;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#24187;&#35273;&#65292;&#31216;&#20026;&#25968;&#23383;&#24187;&#35273;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#35782;&#21035;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#25968;&#23383;&#24187;&#35273;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#22312;&#20027;&#27969;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#20013;&#30340;&#26126;&#26174;&#26222;&#36941;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#30456;&#20851;&#35270;&#35282;&#28145;&#20837;&#20998;&#26512;&#20102;&#25968;&#23383;&#24187;&#35273;&#65292;&#32771;&#23519;&#20102;&#20869;&#22312;&#21644;&#22806;&#22312;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26159;&#25968;&#23383;&#24187;&#35273;&#30340;&#19968;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#20316;&#20026;&#20943;&#36731;&#27492;&#31867;&#24187;&#35273;&#30340;&#25163;&#27573;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;8%&#30340;&#24179;&#22343;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01373v1 Announce Type: new  Abstract: Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination, which achieves an average improvement of 8\%
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#30721;&#20999;&#25442;&#30340;&#20132;&#26367;&#36328;&#35821;&#35328;PTM&#65292;&#39318;&#27425;&#23558;&#20195;&#30721;&#20999;&#25442;&#26041;&#27861;&#24212;&#29992;&#20110;&#36328;&#35821;&#35328;&#35821;&#20041;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.01364</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#20999;&#25442;&#25913;&#36827;&#35821;&#20041;&#26816;&#32034;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Cross-lingual Representation for Semantic Retrieval with Code-switching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#30721;&#20999;&#25442;&#30340;&#20132;&#26367;&#36328;&#35821;&#35328;PTM&#65292;&#39318;&#27425;&#23558;&#20195;&#30721;&#20999;&#25442;&#26041;&#27861;&#24212;&#29992;&#20110;&#36328;&#35821;&#35328;&#35821;&#20041;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01364v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#35821;&#20041;&#26816;&#32034;&#65288;SR&#65289;&#24050;&#25104;&#20026;&#20219;&#21153;&#23548;&#21521;&#38382;&#31572;&#65288;QA&#65289;&#23545;&#35805;&#22330;&#26223;&#20013;FAQ&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#25110;&#26576;&#20123;&#29305;&#23450;&#19994;&#21153;&#29615;&#22659;&#30340;&#36328;&#35821;&#35328;&#26234;&#33021;&#23458;&#25143;&#26381;&#21153;&#31995;&#32479;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#30452;&#25509;&#21033;&#29992;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#29992;&#20110;&#22810;&#35821;&#35328;&#30693;&#35782;&#26816;&#32034;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#30740;&#31350;&#20063;&#21033;&#29992;&#25345;&#32493;&#39044;&#35757;&#32451;&#22312;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;PTMs&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#27169;&#24335;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#24573;&#30053;&#20102;&#21521;PTMs&#21578;&#30693;&#19982;SR&#30456;&#20851;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#21363;&#22312;&#19981;&#25552;&#20379;&#19982;SR&#30456;&#20851;&#30340;&#20219;&#20309;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20182;&#20204;&#30340;PTMs&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20195;&#30721;&#20999;&#25442;&#30340;&#20132;&#26367;&#36328;&#35821;&#35328;PTM&#29992;&#20110;SR&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20026;&#36328;&#35821;&#35328;SR&#20351;&#29992;&#20195;&#30721;&#20999;&#25442;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#26032;&#39062;&#30340;&#20195;&#30721;&#20999;&#25442;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01364v1 Announce Type: new  Abstract: Semantic Retrieval (SR) has become an indispensable part of the FAQ system in the task-oriented question-answering (QA) dialogue scenario. The demands for a cross-lingual smart-customer-service system for an e-commerce platform or some particular business conditions have been increasing recently. Most previous studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual knowledge retrieval directly, while some others also leverage the continual pre-training before fine-tuning PTMs on the downstream tasks. However, no matter which schema is used, the previous work ignores to inform PTMs of some features of the downstream task, i.e. train their PTMs without providing any signals related to SR. To this end, in this work, we propose an Alternative Cross-lingual PTM for SR via code-switching. We are the first to utilize the code-switching approach for cross-lingual SR. Besides, we introduce the novel code-switched continual pre-t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#30693;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35821;&#35328;&#25551;&#36848;&#20026;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT-4&#22312;&#19968;&#27425;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;LM4OPT&#8221;&#26694;&#26550;&#36827;&#34892;Llama-2-7b&#30340;&#28176;&#36827;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.01342</link><description>&lt;p&gt;
LM4OPT&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#23450;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#30693;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35821;&#35328;&#25551;&#36848;&#20026;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT-4&#22312;&#19968;&#27425;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;LM4OPT&#8221;&#26694;&#26550;&#36827;&#34892;Llama-2-7b&#30340;&#28176;&#36827;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#23558;&#35821;&#35328;&#25551;&#36848;&#32763;&#35793;&#25104;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#30340;&#25968;&#23398;&#20844;&#24335;&#26159;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#65292;&#35201;&#27714;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22797;&#26434;&#30340;&#29702;&#35299;&#21644;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#30693;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;GPT-3.5&#12289;GPT-4&#21644;Llama-2-7b&#65292;&#22312;&#38646;&#27425;&#21644;&#19968;&#27425;&#35774;&#32622;&#20013;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#20986;GPT-4&#22312;&#19968;&#27425;&#22330;&#26223;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;&#20854;&#20013;&#24515;&#37096;&#20998;&#26159;&#24341;&#20837;&#20102;&#8220;LM4OPT&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22122;&#22768;&#23884;&#20837;&#21644;&#19987;&#38376;&#25968;&#25454;&#38598;&#36827;&#34892;Llama-2-7b&#28176;&#36827;&#24494;&#35843;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#31361;&#20986;&#20102;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;Llama-2-7b&#65289;&#22312;&#22788;&#29702;&#20887;&#38271;&#21644;&#22797;&#26434;&#36755;&#20837;&#19978;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#19982;&#26356;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21033;&#29992;&#20102;NL4Opt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01342v1 Announce Type: new  Abstract: In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task. Our findings show GPT-4's superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt
&lt;/p&gt;</description></item><item><title>VNLP&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#21253;&#21547;&#22810;&#31181;NLP&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#12290;</title><link>https://arxiv.org/abs/2403.01309</link><description>&lt;p&gt;
VNLP&#65306;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
VNLP: Turkish NLP Package
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01309
&lt;/p&gt;
&lt;p&gt;
VNLP&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#21253;&#21547;&#22810;&#31181;NLP&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VNLP&#65306;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#23436;&#25972;&#12289;&#24320;&#28304;&#12289;&#25991;&#26723;&#23436;&#22791;&#12289;&#36731;&#37327;&#32423;&#12289;&#21487;&#25237;&#20837;&#29983;&#20135;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#21508;&#31181;&#24037;&#20855;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#22914;&#21477;&#23376;&#20998;&#21106;&#21644;&#25991;&#26412;&#35268;&#33539;&#21270;&#65292;&#21040;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#21644;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#12290;&#20854;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26159;&#32534;&#30721;&#22120;&#21448;&#26159;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26032;&#39062;&#26550;&#26500;&#12290;VNLP&#27169;&#22411;&#35299;&#20915;&#30340;NLP&#20219;&#21153;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24418;&#24577;&#20998;&#26512;&#21644;&#28040;&#27495;&#20197;&#21450;&#35789;&#24615;&#26631;&#27880;&#12290;&#27492;&#22806;&#65292;&#23427;&#37197;&#22791;&#20102;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#21644;&#30456;&#24212;&#30340;SentencePiece Unigram&#26631;&#35760;&#22120;&#12290;VNLP&#20855;&#26377;&#24320;&#28304;&#30340;GitHub&#23384;&#20648;&#24211;&#12289;ReadtheDocs&#25991;&#26723;&#12289;&#26041;&#20415;&#23433;&#35013;&#30340;PyPi&#21253;&#12289;Python&#21644;&#36887;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01309v1 Announce Type: cross  Abstract: In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models. Its token classification models are based on "Context Model", a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis \&amp; Disambiguation and Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and comma
&lt;/p&gt;</description></item><item><title>VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01308</link><description>&lt;p&gt;
VBART: &#22303;&#32819;&#20854;LLM
&lt;/p&gt;
&lt;p&gt;
VBART: The Turkish LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01308
&lt;/p&gt;
&lt;p&gt;
VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VBART&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#22823;&#35821;&#26009;&#24211;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;VBART&#26159;&#22522;&#20110;BART&#21644;mBART&#27169;&#22411;&#30340;&#22909;&#24605;&#36335;&#26500;&#24314;&#30340;&#32039;&#20945;&#22411;LLMs&#65292;&#20998;&#20026;Large&#21644;XLarge&#20004;&#20010;&#23610;&#23544;&#12290;&#24494;&#35843;&#21518;&#30340;VBART&#27169;&#22411;&#22312;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;&#12289;&#26631;&#39064;&#29983;&#25104;&#12289;&#25991;&#26412;&#25913;&#20889;&#12289;&#38382;&#31572;&#21644;&#38382;&#39064;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#23427;&#20204;&#20801;&#35768;&#20026;&#26410;&#26469;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25317;&#26377;&#20026;&#22303;&#32819;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLM&#27604;&#22810;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#20026;&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#35821;&#20998;&#35789;&#22120;&#27604;OpenAI&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#26356;&#39640;&#25928;7&#20493;&#12290;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#29616;&#26377;&#39044;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#25968;&#23398;&#21453;&#39304;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#27880;&#37322;&#20154;&#24037;&#32534;&#20889;&#30340;&#21644;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01304</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Automatically Generated Feedback via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#25968;&#23398;&#21453;&#39304;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#27880;&#37322;&#20154;&#24037;&#32534;&#20889;&#30340;&#21644;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#21644;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#20855;&#26377;&#25913;&#21892;&#35768;&#22810;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#27491;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01304v1 Announce Type: new  Abstract: Automatically generating feedback via large language models (LLMs) in intelligent tutoring systems and online learning platforms has the potential to improve the learning outcomes of many students. However, both feedback generation and evaluation are challenging: feedback content has to be valid especially in subjects like math, which requires models to understand the problem, the solution, and where the student's error lies. Feedback also has to be pedagogically valid to reflect effective tutoring strategies, such as explaining possible misconceptions and encouraging the student, among other desirable features. In this work, we address both problems of automatically generating and evaluating feedback while considering both correctness and alignment. First, we propose a rubric for evaluating math feedback and show that GPT-4 is able to effectively use it to annotate human-written and LLM-generated feedback. Second, we propose a framework
&lt;/p&gt;</description></item><item><title>&#23545; NLP &#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#20998;&#35789;&#22120;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#36138;&#23146;&#25512;&#29702;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#35789;&#22120; SaGe &#22312;&#24418;&#24577;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26368;&#20248;&#12290;</title><link>https://arxiv.org/abs/2403.01289</link><description>&lt;p&gt;
&#36138;&#23146;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#23545;&#20998;&#35789;&#25512;&#29702;&#26041;&#27861;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Greed is All You Need: An Evaluation of Tokenizer Inference Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01289
&lt;/p&gt;
&lt;p&gt;
&#23545; NLP &#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#20998;&#35789;&#22120;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#36138;&#23146;&#25512;&#29702;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#35789;&#22120; SaGe &#22312;&#24418;&#24577;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982; BPE &#21644; WordPiece &#36825;&#26679;&#30340;&#23376;&#35789;&#20998;&#35789;&#22120;&#36890;&#24120;&#29992;&#20110;&#26500;&#24314; NLP &#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#20294;&#26159;&#23558;&#25991;&#26412;&#35299;&#30721;&#20026;&#36825;&#20123;&#35789;&#27719;&#34920;&#20013;&#30340;&#19968;&#31995;&#21015;&#26631;&#35760;&#30340;&#26041;&#27861;&#36890;&#24120;&#26410;&#25351;&#23450;&#65292;&#25110;&#32773;&#19981;&#36866;&#21512;&#23427;&#20204;&#26500;&#24314;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#31639;&#27861;&#21644;&#19977;&#31181;&#35789;&#27719;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19971;&#31181;&#20998;&#35789;&#22120;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#33521;&#35821;&#19978;&#20026;&#27492;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#22312;&#35780;&#20272;&#22871;&#20214;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#24418;&#24577;&#23398;&#12289;&#35748;&#30693;&#21644;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#26368;&#24120;&#29992;&#30340;&#20998;&#35789;&#22120;&#65292;&#36138;&#23146;&#25512;&#29702;&#34920;&#29616;&#20986;&#20154;&#24847;&#26009;&#22320;&#33391;&#22909;&#65307;&#26368;&#36817;&#24341;&#20837;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#35789;&#22120; SaGe &#22312;&#24418;&#24577;&#23545;&#40784;&#26041;&#38754;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01289v1 Announce Type: new  Abstract: While subword tokenizers such as BPE and WordPiece are typically used to build vocabularies for NLP models, the method of decoding text into a sequence of tokens from these vocabularies is often left unspecified, or ill-suited to the method in which they were constructed. We provide a controlled analysis of seven tokenizer inference methods across four different algorithms and three vocabulary sizes, performed on a novel intrinsic evaluation suite we curated for English, combining measures rooted in morphology, cognition, and information theory. We show that for the most commonly used tokenizers, greedy inference performs surprisingly well; and that SaGe, a recently-introduced contextually-informed tokenizer, outperforms all others on morphological alignment.
&lt;/p&gt;</description></item><item><title>NoMAD-Attention&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;CPU&#19978;&#20351;&#29992;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#21462;&#20195;MAD&#25805;&#20316;&#65292;&#20197;&#23454;&#29616;LLM&#25512;&#26029;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.01273</link><description>&lt;p&gt;
NoMAD-Attention: &#36890;&#36807;&#26080;MAD&#25805;&#20316;&#23454;&#29616;CPU&#19978;&#39640;&#25928;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01273
&lt;/p&gt;
&lt;p&gt;
NoMAD-Attention&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;CPU&#19978;&#20351;&#29992;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#21462;&#20195;MAD&#25805;&#20316;&#65292;&#20197;&#23454;&#29616;LLM&#25512;&#26029;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#19978;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#23384;&#22312;&#22823;&#37327;&#26114;&#36149;&#30340;MAD&#30697;&#38453;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#29616;&#20195;CPU&#20013;&#30340;&#21333;&#25351;&#20196;&#22810;&#25968;&#25454;&#65288;SIMD&#65289;&#23492;&#23384;&#22120;&#26159;&#19968;&#31181;&#29645;&#36149;&#30340;&#23453;&#30707;&#65292;&#23427;&#20801;&#35768;&#22312;&#25209;&#22788;&#29702;&#20013;&#36827;&#34892;&#36229;&#20302;&#24310;&#36831;&#26597;&#25214;&#12290;&#25105;&#20204;&#21033;&#29992;CPU&#30340;&#36825;&#19968;&#29420;&#29305;&#33021;&#21147;&#25552;&#20986;&#20102;NoMAD-Attention&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;MAD&#25805;&#20316;&#26367;&#25442;&#20026;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#12290;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#30340;&#31639;&#27861;&#35774;&#35745;&#65292;NoMAD-Attention&#23454;&#29616;&#20102;&#36890;&#36807;&#37325;&#22797;&#24555;&#36895;&#35775;&#38382;SIMD&#23492;&#23384;&#22120;&#26469;&#35745;&#31639;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#22823;&#23567;&#38750;&#24120;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;NoMAD-Attention&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LLM&#65292;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;NoMAD-Attention&#24456;&#22909;&#22320;&#20445;&#25345;&#20102;&#21407;&#22987;LLM&#30340;&#36136;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;4&#20301;&#37327;&#21270;&#30340;LLaMA-7B-bas&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01273v1 Announce Type: cross  Abstract: Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning. Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#39318;&#20010;&#35814;&#32454;&#30340;&#36866;&#29992;&#20110;&#20219;&#20309;&#35821;&#35328;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.01270</link><description>&lt;p&gt;
&#20511;&#21161;&#24773;&#24863;&#20998;&#26512;&#30340;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#36328;&#35821;&#35328;&#26694;&#26550;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A comprehensive cross-language framework for harmful content detection with the aid of sentiment analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#39318;&#20010;&#35814;&#32454;&#30340;&#36866;&#29992;&#20110;&#20219;&#20309;&#35821;&#35328;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#19990;&#30028;&#20013;&#65292;&#31038;&#20132;&#23186;&#20307;&#22312;&#20419;&#36827;&#27807;&#36890;&#21644;&#20869;&#23481;&#20849;&#20139;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#24613;&#21095;&#22686;&#38271;&#23548;&#33268;&#20102;&#22312;&#32500;&#25252;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21033;&#29992;&#21311;&#21517;&#24615;&#20351;&#29992;&#26377;&#23475;&#35821;&#35328;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#20307;&#39564;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#24182;&#24341;&#21457;&#20005;&#37325;&#30340;&#31038;&#20250;&#38382;&#39064;&#12290;&#37492;&#20110;&#25163;&#21160;&#23457;&#26680;&#30340;&#23616;&#38480;&#24615;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38556;&#30861;&#65292;&#21253;&#25324;&#32570;&#20047;&#26377;&#23475;&#35821;&#35328;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#36328;&#35821;&#35328;&#19981;&#36275;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#20197;&#21450;&#26368;&#37325;&#35201;&#30340;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#39318;&#27425;&#24341;&#20837;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#20309;&#35821;&#35328;&#30340;&#35814;&#32454;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01270v1 Announce Type: new  Abstract: In today's digital world, social media plays a significant role in facilitating communication and content sharing. However, the exponential rise in user-generated content has led to challenges in maintaining a respectful online environment. In some cases, users have taken advantage of anonymity in order to use harmful language, which can negatively affect the user experience and pose serious social problems. Recognizing the limitations of manual moderation, automatic detection systems have been developed to tackle this problem. Nevertheless, several obstacles persist, including the absence of a universal definition for harmful language, inadequate datasets across languages, the need for detailed annotation guideline, and most importantly, a comprehensive framework. This study aims to address these challenges by introducing, for the first time, a detailed framework adaptable to any language. This framework encompasses various aspects of h
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01267</link><description>&lt;p&gt;
&#35299;&#21078;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dissecting Language Models: Machine Unlearning via Selective Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01267
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#36873;&#25321;&#24615;&#20462;&#21098;&#26041;&#27861;&#65292;&#26681;&#25454;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;&#33021;&#21147;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#31227;&#38500;&#31070;&#32463;&#20803;&#65292;&#32780;&#38750;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21024;&#38500;&#33021;&#22815;&#23454;&#29616;&#29305;&#23450;&#34892;&#20026;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#20013;&#30340;&#21069;&#39304;&#31070;&#32463;&#20803;&#21644;&#27880;&#24847;&#21147;&#31070;&#32463;&#20803;&#26159;&#19987;&#38376;&#21270;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#26576;&#20123;&#31070;&#32463;&#20803;&#27604;&#20854;&#20182;&#31070;&#32463;&#20803;&#26356;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01267v1 Announce Type: cross  Abstract: Understanding and shaping the behaviour of Large Language Models (LLMs) is increasingly important as applications become more powerful and more frequently adopted. This paper introduces a machine unlearning method specifically designed for LLMs. We introduce a selective pruning method for LLMs that removes neurons based on their relative importance on a targeted capability compared to overall network performance. This approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. Our findings reveal that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25506;&#26597;&#37319;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#33609;&#31295;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65292;&#26469;&#21152;&#36895;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#31639;&#27861;&#65292;&#23454;&#29616;&#39640;&#36798;5.6&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.01251</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#26597;&#37319;&#26679;&#21152;&#36895;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Accelerating Greedy Coordinate Gradient via Probe Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01251
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25506;&#26597;&#37319;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#33609;&#31295;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65292;&#26469;&#21152;&#36895;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#31639;&#27861;&#65292;&#23454;&#29616;&#39640;&#36798;5.6&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#20013;&#24515;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#65288;GCG&#65289;&#22312;&#26500;&#24314;&#21253;&#21547;&#23545;&#25239;&#21518;&#32512;&#30340;&#25552;&#31034;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20197;&#30772;&#22351;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;LLMs&#65292;&#20294;GCG&#30340;&#20248;&#21270;&#32791;&#26102;&#36739;&#38271;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;GCG&#30340;&#26102;&#38388;&#25104;&#26412;&#24182;&#23454;&#29616;&#23545;LLMs&#23433;&#20840;&#24615;&#26356;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25506;&#26597;&#37319;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;GCG&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26426;&#21046;&#65292;&#21160;&#24577;&#30830;&#23450;&#36739;&#23567;&#33609;&#31295;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#30446;&#26631;&#27169;&#22411;&#30340;&#25552;&#31034;&#20505;&#36873;&#39044;&#27979;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#24403;&#30446;&#26631;&#27169;&#22411;&#19982;&#33609;&#31295;&#27169;&#22411;&#30456;&#20284;&#26102;&#65292;&#25105;&#20204;&#22823;&#37327;&#20381;&#36182;&#20110;&#33609;&#31295;&#27169;&#22411;&#26469;&#36807;&#28388;&#22823;&#37327;&#28508;&#22312;&#25552;&#31034;&#20505;&#36873;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#25506;&#26597;&#37319;&#26679;&#20351;&#29992;Llam&#23454;&#29616;&#39640;&#36798;5.6&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01251v1 Announce Type: new  Abstract: Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llam
&lt;/p&gt;</description></item><item><title>SceneCraft&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#21487;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#20195;&#30721;&#65292;&#23454;&#29616;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#36890;&#36807;&#20808;&#24314;&#27169;&#31354;&#38388;&#20851;&#31995;&#20877;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#24182;&#20511;&#21161;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22330;&#26223;&#20248;&#21270;&#21644;&#24211;&#23398;&#20064;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01248</link><description>&lt;p&gt;
SceneCraft&#65306;&#19968;&#20010;&#29992;&#20110;&#23558;&#25991;&#26412;&#25551;&#36848;&#21512;&#25104;&#20026;Blender&#20195;&#30721;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01248
&lt;/p&gt;
&lt;p&gt;
SceneCraft&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#21487;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#20195;&#30721;&#65292;&#23454;&#29616;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#36890;&#36807;&#20808;&#24314;&#27169;&#31354;&#38388;&#20851;&#31995;&#20877;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#24182;&#20511;&#21161;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22330;&#26223;&#20248;&#21270;&#21644;&#24211;&#23398;&#20064;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SceneCraft&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#21487;&#25191;&#34892;&#30340;Python&#33050;&#26412;&#65292;&#29992;&#20110;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#35813;&#36807;&#31243;&#38656;&#35201;&#22797;&#26434;&#30340;&#31354;&#38388;&#35268;&#21010;&#21644;&#24067;&#23616;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#32423;&#25277;&#35937;&#12289;&#25112;&#30053;&#35268;&#21010;&#21644;&#24211;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SceneCraft&#39318;&#20808;&#23558;&#22330;&#26223;&#22270;&#24314;&#27169;&#20026;&#34013;&#22270;&#65292;&#35814;&#32454;&#25551;&#36848;&#22330;&#26223;&#20013;&#21508;&#36164;&#20135;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;SceneCraft&#26681;&#25454;&#36825;&#20010;&#22270;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#23558;&#20851;&#31995;&#36716;&#21270;&#20026;&#36164;&#20135;&#24067;&#23616;&#30340;&#25968;&#20540;&#32422;&#26463;&#12290;&#25509;&#19979;&#26469;&#65292;SceneCraft&#21033;&#29992;&#20687;GPT-V&#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#24863;&#30693;&#20248;&#21183;&#26469;&#20998;&#26512;&#28210;&#26579;&#22270;&#20687;&#24182;&#36845;&#20195;&#22320;&#20248;&#21270;&#22330;&#26223;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20043;&#19978;&#65292;SceneCraft&#20855;&#22791;&#19968;&#20010;&#24211;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#24120;&#35265;&#30340;&#33050;&#26412;&#20989;&#25968;&#32534;&#35793;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#24211;&#65292;&#20419;&#36827;&#25345;&#32493;&#30340;&#33258;&#25105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01248v1 Announce Type: cross  Abstract: This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01244</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#29983;&#25104;&#30340;&#22797;&#36848;&#26469;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#22797;&#36848;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#36825;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#29983;&#25104;&#22797;&#36848;&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#22797;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01242</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#33258;&#21160;&#21270;&#65306;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#33258;&#21160;&#21270;&#31995;&#32479;&#22312;&#25511;&#21046;&#30005;&#36335;&#21644;&#35774;&#22791;&#26102;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#21629;&#20196;&#36827;&#34892;&#25511;&#21046;&#65292;&#38480;&#21046;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#22686;&#24378;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#29992;&#25143;&#25351;&#20196;&#34920;&#31034;&#20026;&#24847;&#22270;&#65292;&#20801;&#35768;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#21629;&#20196;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#25511;&#21046;&#30005;&#36335;&#12290;&#36890;&#36807;&#35757;&#32451;&#22312;&#26631;&#35760;&#30340;&#29992;&#25143;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#23545;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#30452;&#35266;&#21644;&#21487;&#36866;&#24212;&#30340;&#25511;&#21046;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#24847;&#22270;&#30340;&#30005;&#21160;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#29992;&#20110;&#24847;&#22270;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01242v1 Announce Type: cross  Abstract: Electric automation systems offer convenience and efficiency in controlling electrical circuits and devices. Traditionally, these systems rely on predefined commands for control, limiting flexibility and adaptability. In this paper, we propose a novel approach to augment automation by introducing intent-based user instruction classification using machine learning techniques. Our system represents user instructions as intents, allowing for dynamic control of electrical circuits without relying on predefined commands. Through a machine learning model trained on a labeled dataset of user instructions, our system classifies intents from user input, enabling a more intuitive and adaptable control scheme. We present the design and implementation of our intent-based electric automation system, detailing the development of the machine learning model for intent classification. Experimental results demonstrate the effectiveness of our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.01241</link><description>&lt;p&gt;
IntactKV: &#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#23436;&#25972;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#19968;&#38590;&#39064;&#65292;&#20154;&#20204;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19968;&#20010;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#24322;&#24120;&#28857;&#31867;&#22411;&#12290;&#36825;&#20123;&#24322;&#24120;&#28857;&#34987;&#21457;&#29616;&#23558;&#22823;&#37096;&#20998;&#27880;&#24847;&#21147;&#20998;&#37197;&#32473;&#36755;&#20837;&#30340;&#21021;&#22987;&#26631;&#35760;&#65292;&#34987;&#31216;&#20026;&#20851;&#38190;&#26631;&#35760;&#65292;&#36825;&#23545;&#20110;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IntactKV&#65292;&#20174;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20013;&#26080;&#25439;&#22320;&#29983;&#25104;&#20851;&#38190;&#26631;&#35760;&#30340;KV&#32531;&#23384;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#29616;&#26377;&#30340;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;IntactKV&#21487;&#20197;&#34987;&#26657;&#20934;&#20026;&#39069;&#22806;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#23398;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;IntactKV&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;IntactKV&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#25439;&#30340;&#20165;&#26435;&#37325;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only
&lt;/p&gt;</description></item><item><title>NLP&#20013;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#23384;&#22312;&#30528;&#36235;&#21183;&#21644;&#24046;&#36317;&#65292;&#26410;&#26469;&#26041;&#21521;&#38656;&#32771;&#34385;&#24773;&#24863;&#20219;&#21153;&#23450;&#20041;&#12289;&#24773;&#24863;&#26694;&#26550;&#12289;&#24773;&#24863;&#20027;&#35266;&#24615;&#19982;NLP&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.01222</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;&#65306;&#36235;&#21183;&#12289;&#24046;&#36317;&#21644;&#26410;&#26469;&#26041;&#21521;&#36335;&#32447;
&lt;/p&gt;
&lt;p&gt;
Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01222
&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#23384;&#22312;&#30528;&#36235;&#21183;&#21644;&#24046;&#36317;&#65292;&#26410;&#26469;&#26041;&#21521;&#38656;&#32771;&#34385;&#24773;&#24863;&#20219;&#21153;&#23450;&#20041;&#12289;&#24773;&#24863;&#26694;&#26550;&#12289;&#24773;&#24863;&#20027;&#35266;&#24615;&#19982;NLP&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#26159;&#27807;&#36890;&#30340;&#19968;&#20010;&#20013;&#24515;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#24773;&#24863;&#20998;&#26512;&#65288;EA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33539;&#22260;&#12289;&#26041;&#21521;&#25110;&#26041;&#27861;&#65292;&#23578;&#26080;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#30340;154&#31687;&#30456;&#20851;NLP&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#24443;&#24213;&#23457;&#26597;&#12290;&#22522;&#20110;&#27492;&#23457;&#26597;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#65288;1&#65289;NLP&#20013;&#22914;&#20309;&#23450;&#20041;EA&#20219;&#21153;&#65311;&#65288;2&#65289;&#20160;&#20040;&#26159;&#26368;&#31361;&#20986;&#30340;&#24773;&#24863;&#26694;&#26550;&#65292;&#20197;&#21450;&#21738;&#20123;&#24773;&#24863;&#34987;&#24314;&#27169;&#65311;&#65288;3&#65289;&#22312;&#20154;&#21475;&#32479;&#35745;&#21644;&#25991;&#21270;&#22240;&#32032;&#26041;&#38754;&#26159;&#21542;&#32771;&#34385;&#20102;&#24773;&#32490;&#30340;&#20027;&#35266;&#24615;&#65311;&#20197;&#21450;&#65288;4&#65289;EA&#30340;&#20027;&#35201;NLP&#24212;&#29992;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#24635;&#32467;&#20102;EA&#21644;&#20219;&#21153;&#12289;&#20351;&#29992;&#30340;&#24773;&#24863;&#26694;&#26550;&#12289;&#29616;&#26377;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#36235;&#21183;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#20010;&#31354;&#30333;&#65306;&#65288;1&#65289;&#32570;&#20047;&#20154;&#21475;&#32479;&#35745;&#21644;&#25991;&#21270;&#22240;&#32032;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#19981;&#21516;&#25991;&#21270;&#22914;&#20309;&#24863;&#30693;&#24773;&#32490;&#30340;&#24046;&#24322;&#65292;&#32780;&#26159;&#20551;&#35774;&#23427;&#20204;&#26159;&#26222;&#36941;&#32463;&#21382;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01222v1 Announce Type: new  Abstract: Emotions are a central aspect of communication. Consequently, emotion analysis (EA) is a rapidly growing field in natural language processing (NLP). However, there is no consensus on scope, direction, or methods. In this paper, we conduct a thorough review of 154 relevant NLP publications from the last decade. Based on this review, we address four different questions: (1) How are EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and which emotions are modeled? (3) Is the subjectivity of emotions considered in terms of demographics and cultural factors? and (4) What are the primary NLP applications for EA? We take stock of trends in EA and tasks, emotion frameworks used, existing datasets, methods, and applications. We then discuss four lacunae: (1) the absence of demographic and cultural aspects does not account for the variation in how emotions are perceived, but instead assumes they are universally experienced
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01216</link><description>&lt;p&gt;
API&#23601;&#22815;&#20102;&#65306;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#27861;&#35775;&#38382;&#23545;&#25968;&#26102;&#22914;&#20309;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36825;&#19968;&#26222;&#36941;&#25361;&#25112;&#12290;&#25972;&#20307;&#39044;&#27979;&#65288;CP&#65289;&#20197;&#20854;&#19982;&#27169;&#22411;&#26080;&#20851;&#21644;&#26080;&#38656;&#20998;&#24067;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#65292;&#26159;&#21508;&#31181;LLMs&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#24819;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#23545;&#25968;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#20165;&#25903;&#25345;API&#30340;LLMs&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#23545;&#25968;&#21487;&#33021;&#23384;&#22312;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;CP&#26041;&#27861;&#65292;&#65288;1&#65289;&#19987;&#20026;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;API-only LLMs&#37327;&#36523;&#23450;&#21046;; (2) &#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;; &#20197;&#21450;(3)&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#31895;&#31890;&#24230;&#65288;&#20363;&#22914;&#65292;&#26679;&#26412;&#39057;&#29575;&#65289;&#21644;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#26469;&#21046;&#23450;&#19981;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#26657;&#20934;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#36807;&#28388;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#65292;&#22686;&#24378;&#27169;&#24577;&#19981;&#21464;&#24615;&#20849;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01203</link><description>&lt;p&gt;
&#20266;&#26631;&#31614;&#26657;&#20934;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#26657;&#20934;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#36807;&#28388;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#65292;&#22686;&#24378;&#27169;&#24577;&#19981;&#21464;&#24615;&#20849;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(MMEA)&#26088;&#22312;&#35782;&#21035;&#20004;&#20010;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#31561;&#20215;&#23454;&#20307;&#65292;&#20197;&#36827;&#34892;&#25972;&#21512;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25913;&#36827;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21364;&#24573;&#35270;&#20102;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20266;&#26631;&#31614;&#26657;&#20934;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;(PCMEA)&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#29983;&#25104;&#20840;&#38754;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#21508;&#31181;&#23884;&#20837;&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#21462;&#35270;&#35273;&#12289;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#36807;&#28388;&#27169;&#24577;&#29305;&#23450;&#22122;&#38899;&#24182;&#22686;&#24378;&#27169;&#24577;&#19981;&#21464;&#24615;&#20849;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20266;&#26631;&#31614;&#26657;&#20934;&#19982;&#22522;&#20110;&#21160;&#37327;&#30340;&#23545;&#27604;&#34701;&#21512;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01203v1 Announce Type: cross  Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities between two multi-modal knowledge graphs for integration. Unfortunately, prior arts have attempted to improve the interaction and fusion of multi-modal information, which have overlooked the influence of modal-specific noise and the usage of labeled and unlabeled data in semi-supervised settings. In this work, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment (PCMEA) in a semi-supervised way. Specifically, in order to generate holistic entity representations, we first devise various embedding modules and attention mechanisms to extract visual, structural, relational, and attribute features. Different from the prior direct fusion methods, we next propose to exploit mutual information maximization to filter the modal-specific noise and to augment modal-invariant commonality. Then, we combine pseudo-label calibration with momentum-based contrastive
&lt;/p&gt;</description></item><item><title>DMoERM&#39318;&#27425;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#22870;&#21169;&#24314;&#27169;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#21452;&#23618;MoE RM&#65288;DMoERM&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#22806;&#23618;MoE&#21644;&#23494;&#38598;&#30340;&#20869;&#23618;MoE&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#22810;&#20219;&#21153;&#24178;&#25200;&#21644;&#25968;&#25454;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01197</link><description>&lt;p&gt;
DMoERM: &#26377;&#25928;&#22870;&#21169;&#24314;&#27169;&#30340;&#28151;&#21512;&#19987;&#23478;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01197
&lt;/p&gt;
&lt;p&gt;
DMoERM&#39318;&#27425;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#22870;&#21169;&#24314;&#27169;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#21452;&#23618;MoE RM&#65288;DMoERM&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#22806;&#23618;MoE&#21644;&#23494;&#38598;&#30340;&#20869;&#23618;MoE&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#22810;&#20219;&#21153;&#24178;&#25200;&#21644;&#25968;&#25454;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#30340;&#24615;&#33021;&#26159;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23545;&#40784;&#24494;&#35843;&#36807;&#31243;&#20013;&#25928;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290; RM&#35757;&#32451;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#20351;&#29992;&#21508;&#31181;&#31867;&#21035;&#25968;&#25454;&#35757;&#32451;&#30456;&#21516;&#30340;RM&#21487;&#33021;&#23548;&#33268;&#20854;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#22810;&#20219;&#21153;&#24178;&#25200;&#30340;&#24433;&#21709;&#65307;2&#65289;&#20154;&#31867;&#27880;&#37322;&#30340;&#19968;&#33268;&#24615;&#29575;&#36890;&#24120;&#20165;&#20026;60%&#33267;75%&#65292;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#22122;&#22768;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;RM&#39046;&#22495;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23618;MoE RM&#65288;DMoERM&#65289;&#12290; &#22806;&#23618;MoE&#26159;&#19968;&#31181;&#31232;&#30095;&#27169;&#22411;&#12290; &#23558;&#36755;&#20837;&#20998;&#31867;&#25104;&#20219;&#21153;&#31867;&#21035;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#36335;&#30001;&#21040;&#30456;&#24212;&#30340;&#20869;&#23618;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20013;&#12290; &#20869;&#23618;MoE&#26159;&#19968;&#31181;&#23494;&#38598;&#27169;&#22411;&#12290; &#25105;&#20204;&#23558;&#29305;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#33021;&#21147;&#32500;&#24230;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#20010;&#32500;&#24230;&#19978;&#23545;LoRA&#19987;&#23478;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01197v1 Announce Type: new  Abstract: The performance of the reward model (RM) is a critical factor in improving the effectiveness of the large language model (LLM) during alignment fine-tuning. There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only $60\%$ to $75\%$, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one. T
&lt;/p&gt;</description></item><item><title>&#22312;LoResMT 2021&#20013;&#65292;&#38024;&#23545;Covid&#25968;&#25454;&#20174;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#30340;&#32763;&#35793;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#21644;&#25193;&#23637;&#39046;&#22495;&#20869;Covid&#25968;&#25454;&#38598;&#35757;&#32451;Transformer&#26550;&#26500;&#65292;&#25104;&#21151;&#25913;&#21892;&#20102;&#32763;&#35793;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01196</link><description>&lt;p&gt;
Covid&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;LoResMT 2021&#20013;&#33521;&#29233;&#23572;&#20848;&#35821;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01196
&lt;/p&gt;
&lt;p&gt;
&#22312;LoResMT 2021&#20013;&#65292;&#38024;&#23545;Covid&#25968;&#25454;&#20174;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#30340;&#32763;&#35793;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#21644;&#25193;&#23637;&#39046;&#22495;&#20869;Covid&#25968;&#25454;&#38598;&#35757;&#32451;Transformer&#26550;&#26500;&#65292;&#25104;&#21151;&#25913;&#21892;&#20102;&#32763;&#35793;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20174;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#32763;&#35793;Covid&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;&#65292;&#24320;&#21457;&#20102;LoResMT 2021&#20849;&#20139;&#20219;&#21153;&#30340;&#32763;&#35793;&#27169;&#22411;&#12290;&#21033;&#29992;&#26469;&#33258;&#32763;&#35793;&#24635;&#21496;&#25351;&#23548;&#22788;&#30340;Covid&#36866;&#37197;&#36890;&#29992;55k&#35821;&#26009;&#24211;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#23558;&#24494;&#35843;&#12289;&#28151;&#21512;&#24494;&#35843;&#21644;&#32452;&#21512;&#25968;&#25454;&#38598;&#26041;&#27861;&#19982;&#22312;&#25193;&#23637;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#24320;&#21457;&#20102;&#19968;&#20221;&#20581;&#24247;&#21644;&#25945;&#32946;&#39046;&#22495;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;Covid&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#20351;&#29992;&#25193;&#23637;&#30340;&#39046;&#22495;&#20869;Covid&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#25193;&#23637;8k&#39046;&#22495;&#20869;&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#38656;&#20877;&#22686;&#21152;5k&#34892;&#65292;&#23601;&#23558;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;27&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01196v1 Announce Type: cross  Abstract: Translation models for the specific domain of translating Covid data from English to Irish were developed for the LoResMT 2021 shared task. Domain adaptation techniques, using a Covid-adapted generic 55k corpus from the Directorate General of Translation, were applied. Fine-tuning, mixed fine-tuning and combined dataset approaches were compared with models trained on an extended in-domain dataset. As part of this study, an English-Irish dataset of Covid related data, from the Health and Education domains, was developed. The highest-performing model used a Transformer architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01193</link><description>&lt;p&gt;
RAGged Edges: Retrieval-Augmented Chatbots&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273; - &#29983;&#25104;&#30475;&#20284;&#27491;&#30830;&#20294;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20851;&#38190;&#65292;&#23601;&#20687;&#26368;&#36817;&#30340;&#27861;&#38498;&#26696;&#20363;&#20013;&#30475;&#21040;&#30340;&#37027;&#26679;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#19981;&#23384;&#22312;&#30340;&#27861;&#24459;&#35009;&#20915;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#25552;&#31034;&#38598;&#25104;&#26469;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25269;&#21046;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26088;&#22312;&#35825;&#23548;&#24187;&#35273;&#30340;&#25552;&#31034;&#26469;&#23545;RAG&#19982;&#26631;&#20934;LLMs&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;RAG&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24403;&#25552;&#31034;&#30452;&#25509;&#19982;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#30456;&#30683;&#30462;&#26102;&#65292;RAG&#20173;&#28982;&#20250;&#34987;&#35823;&#23548;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24187;&#35273;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RAG&#37096;&#32626;&#30340;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
&lt;/p&gt;</description></item><item><title>UD&#31867;&#22411;&#28436;&#31639;&#26159;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#36890;&#29992;&#20381;&#23384;&#21477;&#27861;&#26694;&#26550;&#30340;&#35821;&#20041;&#31867;&#22411;&#21644;&#36923;&#36753;&#24418;&#24335;&#30340;&#32452;&#21512;&#12289;&#21407;&#21017;&#24615;&#21644;&#19982;&#35821;&#35328;&#26080;&#20851;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#26631;&#31614;&#25512;&#23548;&#20855;&#26377;&#21508;&#31181;&#21477;&#27861;&#32467;&#26500;&#30340;&#21477;&#23376;&#30340;&#27491;&#30830;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.01187</link><description>&lt;p&gt;
&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#30340;&#32452;&#21512;&#24335;&#31867;&#22411;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
A Compositional Typed Semantics for Universal Dependencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01187
&lt;/p&gt;
&lt;p&gt;
UD&#31867;&#22411;&#28436;&#31639;&#26159;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#36890;&#29992;&#20381;&#23384;&#21477;&#27861;&#26694;&#26550;&#30340;&#35821;&#20041;&#31867;&#22411;&#21644;&#36923;&#36753;&#24418;&#24335;&#30340;&#32452;&#21512;&#12289;&#21407;&#21017;&#24615;&#21644;&#19982;&#35821;&#35328;&#26080;&#20851;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#26631;&#31614;&#25512;&#23548;&#20855;&#26377;&#21508;&#31181;&#21477;&#27861;&#32467;&#26500;&#30340;&#21477;&#23376;&#30340;&#27491;&#30830;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#21487;&#33021;&#20351;&#29992;&#19981;&#21516;&#30340;&#21477;&#23376;&#32467;&#26500;&#26469;&#32534;&#30721;&#31867;&#20284;&#30340;&#21547;&#20041;&#12290;&#36825;&#20351;&#24471;&#25552;&#20379;&#19968;&#20010;&#21487;&#20197;&#19968;&#27425;&#24615;&#20174;&#22810;&#31181;&#35821;&#35328;&#21477;&#23376;&#20013;&#25512;&#23548;&#21547;&#20041;&#30340;&#21333;&#19968;&#24418;&#24335;&#35268;&#21017;&#38598;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#35821;&#35328;&#36890;&#29992;&#30340;&#24847;&#20041;&#21644;&#21477;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#26500;&#24314;&#36328;&#35821;&#35328;&#24179;&#34892;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UD&#31867;&#22411;&#28436;&#31639;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#36890;&#29992;&#20381;&#23384;&#21477;&#27861;&#26694;&#26550;&#30340;&#35821;&#20041;&#31867;&#22411;&#21644;&#36923;&#36753;&#24418;&#24335;&#30340;&#32452;&#21512;&#12289;&#21407;&#21017;&#24615;&#21644;&#19982;&#35821;&#35328;&#26080;&#20851;&#31995;&#32479;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;UD&#31867;&#22411;&#28436;&#31639;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#37117;&#28041;&#21450;&#32473;&#20104;&#20381;&#23384;&#20851;&#31995;&#19982;&#21333;&#35789;&#31867;&#20284;&#30340;&#25351;&#31216;&#12290;&#36825;&#20351;&#24471;UD-TC&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#26631;&#31614;&#25512;&#23548;&#20855;&#26377;&#21508;&#31181;&#21477;&#27861;&#32467;&#26500;&#30340;&#21477;&#23376;&#30340;&#27491;&#30830;&#21547;&#20041;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#29616;&#26377;&#35821;&#26009;&#24211;&#19978;&#21576;&#29616;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01187v1 Announce Type: new  Abstract: Languages may encode similar meanings using different sentence structures. This makes it a challenge to provide a single set of formal rules that can derive meanings from sentences in many languages at once. To overcome the challenge, we can take advantage of language-general connections between meaning and syntax, and build on cross-linguistically parallel syntactic structures. We introduce UD Type Calculus, a compositional, principled, and language-independent system of semantic types and logical forms for lexical items which builds on a widely-used language-general dependency syntax framework. We explain the essential features of UD Type Calculus, which all involve giving dependency relations denotations just like those of words. These allow UD-TC to derive correct meanings for sentences with a wide range of syntactic structures by making use of dependency labels. Finally, we present evaluation results on a large existing corpus of se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36923;&#36753;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLLF&#65289;&#22312;LLMs&#20013;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#65292;&#20197;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#24615;&#33021;&#39564;&#35777;&#20102;&#36825;&#31181;&#24179;&#34913;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.01185</link><description>&lt;p&gt;
&#22312;LLM&#20013;&#20351;&#29992;Soft RLLF&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#65292;&#20197;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01185
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36923;&#36753;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLLF&#65289;&#22312;LLMs&#20013;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#65292;&#20197;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#24615;&#33021;&#39564;&#35777;&#20102;&#36825;&#31181;&#24179;&#34913;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#20013;&#65292;&#35843;&#25972;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#24320;&#21457;&#32780;&#19981;&#26159;&#25506;&#32034;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#31181;&#26377;&#38480;&#30340;&#25506;&#32034;&#21487;&#33021;&#38480;&#21046;&#23427;&#20204;&#22312;&#22797;&#26434;&#12289;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#65292;&#37027;&#37324;&#20934;&#30830;&#30340;&#21542;&#23450;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#36923;&#36753;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLLF&#65289;&#22312;LLMs&#20013;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26377;&#25928;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#36866;&#24403;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#36890;&#36807;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;&#33021;&#21147;&#26469;&#24378;&#35843;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;RLLF&#22686;&#24378;&#30340;LLMs&#30340;&#24615;&#33021;&#19982;&#26410;&#20351;&#29992;RLLF&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#34913;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27861;&#24459;AI&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01185v1 Announce Type: cross  Abstract: Finetuning approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical reasoning abilities are crucial. To address this issue, we leverage Reinforcement Learning from Logical Feedback (RLLF) to create an effective balance between exploration and exploitation in LLMs. Our approach employs an appropriate benchmark dataset for training and evaluation, highlighting the importance of exploration in enhancing negation understanding capabilities. We compare the performance of our RLLF-enhanced LLMs with baseline models trained without RLLF, demonstrating the value of this balanced approach. Furthermore, we showcase the potential of our method in legal AI applications by employing transfer learning and evaluatin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01166</link><description>&lt;p&gt;
DINER&#65306;&#20351;&#29992;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#26469;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#27169;&#22411;&#23481;&#26131;&#20174;&#27880;&#37322;&#20559;&#35265;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#23545;&#25239;&#24615;&#25968;&#25454;&#36716;&#25442;&#19978;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;&#22312;&#21435;&#20559;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#65292;&#20027;&#35201;&#21487;&#20998;&#20026;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#21435;&#20559;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21333;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#19978;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#21464;&#37327;&#65288;&#30446;&#26631;&#26041;&#38754;&#21644;&#35780;&#35770;&#65289;&#30340;ABSA&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;ABSA&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#35265;&#22522;&#20110;&#19981;&#21516;&#30340;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#24471;&#21040;&#22788;&#29702;&#12290;&#23545;&#20110;&#35780;&#35770;&#20998;&#25903;&#65292;&#20559;&#35265;&#34987;&#24314;&#27169;&#20026;&#26469;&#33258;&#19978;&#19979;&#25991;&#30340;&#38388;&#25509;&#28151;&#26434;&#65292;&#20854;&#20013;&#23454;&#26045;&#21453;&#21521;&#35843;&#25972;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01166v1 Announce Type: cross  Abstract: Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01165</link><description>&lt;p&gt;
STAR: &#20351;&#29992;&#21160;&#24577;&#20027;&#21160;&#23398;&#20064;&#32422;&#26463;LoRA&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#25552;&#31034;&#26041;&#27861;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20173;&#38656;&#30417;&#30563;&#35757;&#32451;&#12290;&#38024;&#23545;LLMs&#30340;&#21442;&#25968;&#20247;&#22810;&#21644;&#20869;&#23384;&#28040;&#32791;&#22823;&#38382;&#39064;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#26041;&#27861;&#21644;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26088;&#22312;&#35299;&#20915;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#28040;&#32791;&#30340;&#38382;&#39064;&#65292;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#19968;&#31181;&#26126;&#26174;&#30340;&#26041;&#24335;&#26159;&#23558;PEFT&#26041;&#27861;&#19982;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#32452;&#21512;&#24182;&#38750;&#31616;&#21333;&#65292;&#24182;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#38024;&#23454;&#39564;&#65292;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21487;&#33021;&#30001;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#35299;&#37322;&#65306;&#19981;&#30830;&#23450;&#24615;&#24046;&#36317;&#21644;&#27169;&#22411;&#26657;&#20934;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01165v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the u
&lt;/p&gt;</description></item><item><title>BootTOD&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#26694;&#26550;&#23398;&#20064;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#65292;&#28040;&#38500;&#20102;&#23545;&#27604;&#23545;&#30340;&#35201;&#27714;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#22238;&#22797;&#30446;&#26631;&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01163</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#22810;&#26679;&#21270;&#22238;&#22797;&#26469;&#24341;&#23548;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#30340;BootstrapTOD
&lt;/p&gt;
&lt;p&gt;
BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01163
&lt;/p&gt;
&lt;p&gt;
BootTOD&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#26694;&#26550;&#23398;&#20064;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#65292;&#28040;&#38500;&#20102;&#23545;&#27604;&#23545;&#30340;&#35201;&#27714;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#22238;&#22797;&#30446;&#26631;&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#65292;&#30001;&#20110;&#36890;&#29992;&#25991;&#26412;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20043;&#38388;&#22266;&#26377;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#21069;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#39044;&#35757;&#32451;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#27604;&#26694;&#26550;&#65292;&#38754;&#20020;&#30528;&#35832;&#22914;&#36873;&#25321;&#30495;&#27491;&#20363;&#21644;&#38590;&#36127;&#20363;&#65292;&#20197;&#21450;&#32570;&#20047;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BootTOD&#30340;&#26032;&#22411;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#26694;&#26550;&#23398;&#20064;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#34920;&#31034;&#12290;&#19982;&#23545;&#27604;&#23545;&#24212;&#29289;&#30456;&#21453;&#65292;BootTOD&#36890;&#36807;&#23545;&#40784;&#19978;&#19979;&#25991;&#21644;&#19978;&#19979;&#25991;+&#22238;&#22797;&#34920;&#31034;&#26469;&#28040;&#38500;&#23545;&#27604;&#23545;&#30340;&#35201;&#27714;&#12290;BootTOD&#36824;&#20351;&#29992;&#22810;&#20010;&#36866;&#24403;&#30340;&#22238;&#22797;&#30446;&#26631;&#26469;&#24314;&#27169;&#20154;&#31867;&#23545;&#35805;&#22266;&#26377;&#30340;&#19968;&#23545;&#22810;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BootTOD&#22312;&#22810;&#26679;&#30340;&#19979;&#28216;&#23545;&#35805;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;TOD&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01163v1 Announce Type: new  Abstract: Pre-trained language models have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#20197;&#23454;&#29616;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#23454;&#38469;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.01152</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#32508;&#36848;&#65306;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#20197;&#23454;&#29616;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#23454;&#38469;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26368;&#36817;&#30446;&#20987;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#36825;&#20123;LLMs&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#37325;&#22823;&#39118;&#38505;&#65292;&#27604;&#22914;&#21487;&#33021;&#22823;&#35268;&#27169;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#23459;&#20256;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#35875;&#35328;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#36825;&#26159;&#19968;&#20010;&#24212;&#23545;LLM&#28389;&#29992;&#25361;&#25112;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#35814;&#32454;&#30340;&#20998;&#31867;&#27861;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#39046;&#22495;&#29616;&#26377;&#30340;&#21162;&#21147;&#65292;&#30528;&#30524;&#20110;&#19977;&#20010;&#20027;&#35201;&#25903;&#26609;&#65306;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;&#12290;&#36825;&#20123;&#25903;&#26609;&#20351;&#20154;&#20204;&#33021;&#22815;&#23454;&#38469;&#29702;&#35299;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#35782;&#21035;AI&#29983;&#25104;&#20869;&#23481;&#65288;&#26816;&#27979;&#65289;&#12289;&#30830;&#23450;&#28041;&#21450;&#30340;&#20855;&#20307;AI&#27169;&#22411;&#65288;&#24402;&#22240;&#65289;&#20197;&#21450;&#23545;&#25991;&#26412;&#30340;&#22522;&#26412;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#65288;&#29305;&#24449;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01152v1 Announce Type: cross  Abstract: We have witnessed lately a rapid proliferation of advanced Large Language Models (LLMs) capable of generating high-quality text. While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale. This paper offers a review of AI-generated text forensic systems, an emerging field addressing the challenges of LLM misuses. We present an overview of the existing efforts in AI-generated text forensics by introducing a detailed taxonomy, focusing on three primary pillars: detection, attribution, and characterization. These pillars enable a practical understanding of AI-generated text, from identifying AI-generated content (detection), determining the specific AI model involved (attribution), and grouping the underlying intents of the text (characterization). Furtherm
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01131</link><description>&lt;p&gt;
LLaMoCo&#65306;&#29992;&#20110;&#20248;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01131
&lt;/p&gt;
&lt;p&gt;
LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#26041;&#27861;&#21253;&#25324;&#20174;LLMs&#36845;&#20195;&#22320;&#23547;&#25214;&#19979;&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#30452;&#25509;&#25552;&#31034;LLMs&#20197;&#33719;&#21462;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#20302;&#12289;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#24230;&#39640;&#20197;&#21450;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMoCo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#28165;&#26224;&#25551;&#36848;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#26377;&#25928;&#20248;&#21270;&#20195;&#30721;&#30340;&#20840;&#38754;&#25351;&#20196;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#20043;&#21069;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28909;&#36523;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26399;&#38388;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;LLaMoCo&#31934;&#35843;&#30340;CodeGen&#65288;350M&#65289;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
&lt;/p&gt;</description></item><item><title>MulCogBench&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#35748;&#30693;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#20013;&#25991;&#21644;&#33521;&#25991;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#20284;&#24615;&#32534;&#30721;&#20998;&#26512;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#35748;&#30693;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#30456;&#20284;&#24615;&#27169;&#24335;&#21463;&#25968;&#25454;&#27169;&#24577;&#21644;&#35843;&#33410;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.01116</link><description>&lt;p&gt;
MulCogBench&#65306;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#21644;&#33521;&#25991;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24335;&#35748;&#30693;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01116
&lt;/p&gt;
&lt;p&gt;
MulCogBench&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#35748;&#30693;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#20013;&#25991;&#21644;&#33521;&#25991;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#20284;&#24615;&#32534;&#30721;&#20998;&#26512;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#35748;&#30693;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#30456;&#20284;&#24615;&#27169;&#24335;&#21463;&#25968;&#25454;&#27169;&#24577;&#21644;&#35843;&#33410;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01116v1 &#23459;&#24067;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#31867;&#35748;&#20026;&#26159;&#29420;&#26377;&#30340;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23427;&#20204;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#34920;&#31034;&#21644;&#22788;&#29702;&#35821;&#35328;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MulCogBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#27597;&#35821;&#20026;&#20013;&#25991;&#21644;&#33521;&#25991;&#30340;&#21442;&#19982;&#32773;&#37027;&#37324;&#25910;&#38598;&#30340;&#22810;&#27169;&#24335;&#35748;&#30693;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#21508;&#31181;&#35748;&#30693;&#25968;&#25454;&#65292;&#21253;&#25324;&#20027;&#35266;&#35821;&#20041;&#35780;&#20998;&#65292;&#30524;&#21160;&#36861;&#36394;&#65292;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24040;&#30913;&#33041;&#30005;&#22270;&#65288;MEG&#65289;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#19982;&#35748;&#30693;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30456;&#20284;&#24615;&#32534;&#30721;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#26681;&#25454;&#20854;&#19982;&#25991;&#26412;&#23884;&#20837;&#30340;&#27169;&#24335;&#30456;&#20284;&#24615;&#23545;&#35748;&#30693;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#35748;&#30693;&#25968;&#25454;&#20849;&#20139;&#26174;&#33879;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#30456;&#20284;&#24615;&#27169;&#24335;&#21463;&#25968;&#25454;&#27169;&#24577;&#30340;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01116v1 Announce Type: new  Abstract: Pre-trained computational language models have recently made remarkable progress in harnessing the language abilities which were considered unique to humans. Their success has raised interest in whether these models represent and process language like humans. To answer this question, this paper proposes MulCogBench, a multi-modal cognitive benchmark dataset collected from native Chinese and English participants. It encompasses a variety of cognitive data, including subjective semantic ratings, eye-tracking, functional magnetic resonance imaging (fMRI), and magnetoencephalography (MEG). To assess the relationship between language models and cognitive data, we conducted a similarity-encoding analysis which decodes cognitive data based on its pattern similarity with textual embeddings. Results show that language models share significant similarities with human cognitive data and the similarity patterns are modulated by the data modality and
&lt;/p&gt;</description></item><item><title>CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01106</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#35299;&#37322;&#25552;&#28860;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Distilling Text Style Transfer With Self-Explanation From LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01106
&lt;/p&gt;
&lt;p&gt;
CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;TST&#65289;&#26088;&#22312;&#25913;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#20854;&#26680;&#24515;&#20869;&#23481;&#12290;&#37492;&#20110;TST&#30340;&#26377;&#38480;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoTeX&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#26469;&#20419;&#36827;TST&#30340;&#26694;&#26550;&#12290;CoTeX&#23558;LLMs&#30340;&#22797;&#26434;&#37325;&#20889;&#21644;&#25512;&#29702;&#33021;&#21147;&#25552;&#28860;&#25104;&#26356;&#31616;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;TST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;CoTeX&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23558;CoTeX&#19982;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#12289;&#30417;&#30563;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25216;&#26415;&#20197;&#21450;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;CoTeX&#36890;&#36807;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#20854;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01081</link><description>&lt;p&gt;
LAB&#65306;&#38024;&#23545;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
LAB: Large-Scale Alignment for ChatBots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01081
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;LAB&#65288;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;LAB&#26174;&#33879;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LAB&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;&#20351;&#29992;&#20256;&#32479;&#20154;&#31867;&#27880;&#37322;&#25110;GPT-4&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#21644;&#25351;&#20196;&#36981;&#24490;&#34892;&#20026;&#65292;&#26631;&#24535;&#30528;&#22312;&#39640;&#25928;&#35757;&#32451;&#21508;&#31181;&#24212;&#29992;&#30340;LLM&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#20840;&#38754;&#26631;&#20934;&#20026;&#20219;&#21153;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#24182;&#22312;&#35770;&#25991;&#24341;&#35328;&#20889;&#20316;&#12289;Python&#20195;&#30721;&#32534;&#20889;&#21644;Reddit&#24086;&#23376;&#25776;&#20889;&#31561;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01069</link><description>&lt;p&gt;
LLMCRIT:&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
LLMCRIT: Teaching Large Language Models to Use Criteria
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#20840;&#38754;&#26631;&#20934;&#20026;&#20219;&#21153;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#24182;&#22312;&#35770;&#25991;&#24341;&#35328;&#20889;&#20316;&#12289;Python&#20195;&#30721;&#32534;&#20889;&#21644;Reddit&#24086;&#23376;&#25776;&#20889;&#31561;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#36981;&#24490;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#30452;&#25509;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#26631;&#20934;&#25552;&#20379;&#21453;&#39304;&#21487;&#20197;&#24110;&#21161;&#20154;&#31867;&#25110;&#27169;&#22411;&#26356;&#22909;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#21482;&#32771;&#34385;&#26377;&#38480;&#30340;&#26631;&#20934;&#25110;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#20351;&#29992;&#20840;&#38754;&#30340;&#26631;&#20934;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;-&#29615;&#36335;&#26694;&#26550;&#65292;&#20174;&#25910;&#38598;&#30340;&#19981;&#21516;&#20889;&#20316;&#20219;&#21153;&#25351;&#21335;&#20013;&#21322;&#33258;&#21160;&#22320;&#25552;&#21462;&#26631;&#20934;&#65292;&#24182;&#20026;&#27599;&#20010;&#26631;&#20934;&#26500;&#24314;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#26469;&#33258;&#29616;&#23454;&#22330;&#26223;&#30340;&#19977;&#20010;&#20219;&#21153;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#65306;&#35770;&#25991;&#24341;&#35328;&#20889;&#20316;&#12289;Python&#20195;&#30721;&#32534;&#20889;&#21644;Reddit&#24086;&#23376;&#25776;&#20889;&#65292;&#24182;&#35780;&#20272;&#25105;&#20204;&#30340;&#21453;&#39304;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01069v1 Announce Type: new  Abstract: Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback gener
&lt;/p&gt;</description></item><item><title>FaiMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29305;&#24449;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#22810;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.01063</link><description>&lt;p&gt;
FaiMA&#65306;&#38024;&#23545;&#22810;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#29305;&#24449;&#24863;&#30693;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01063
&lt;/p&gt;
&lt;p&gt;
FaiMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29305;&#24449;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#22810;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#25429;&#25417;&#36328;&#22810;&#26679;&#39046;&#22495;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#26041;&#27861;&#38480;&#21046;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#65292;&#29421;&#31364;&#22320;&#20851;&#27880;&#21333;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20294;&#29616;&#23454;&#24773;&#24863;&#33258;&#28982;&#22320;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;ABSA&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#26377;&#25928;&#19982;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#65288;&#21253;&#25324;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#21644;&#35821;&#35328;&#23398;&#65289;&#38598;&#25104;&#65292;&#22240;&#20026;&#20462;&#25913;&#23427;&#20204;&#30340;&#20869;&#37096;&#26550;&#26500;&#24182;&#19981;&#23481;&#26131;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#38024;&#23545;&#22810;&#39046;&#22495;ABSA&#30340;&#29305;&#24449;&#24863;&#30693;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;FaiMA&#65289;&#12290;FaiMA&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20316;&#20026;&#19968;&#31181;&#29305;&#24449;&#24863;&#30693;&#26426;&#21046;&#65292;&#20419;&#36827;&#22810;&#39046;&#22495;ABSA&#20219;&#21153;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22810;&#22836;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#20316;&#20026;&#19968;&#20010;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01063v1 Announce Type: new  Abstract: Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture fine-grained sentiment across diverse domains. While existing research narrowly focuses on single-domain applications constrained by methodological limitations and data scarcity, the reality is that sentiment naturally traverses multiple domains. Although large language models (LLMs) offer a promising solution for ABSA, it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy. To alleviate this problem, we propose a novel framework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The core insight of FaiMA is to utilize in-context learning (ICL) as a feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA tasks. Specifically, we employ a multi-head graph attention network as a text encoder optimized by heuristic rul
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30701;&#31687;&#23567;&#35828;&#25688;&#35201;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24544;&#23454;&#24615;&#21644;&#35299;&#37322;&#28508;&#21488;&#35789;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#20294;&#22312;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#26102;&#34920;&#29616;&#20986;&#24605;&#32771;&#28145;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.01061</link><description>&lt;p&gt;
&#38405;&#35835;&#28508;&#21488;&#35789;&#65306;&#22312;&#30701;&#31687;&#23567;&#35828;&#25688;&#35201;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20316;&#32773;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01061
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30701;&#31687;&#23567;&#35828;&#25688;&#35201;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24544;&#23454;&#24615;&#21644;&#35299;&#37322;&#28508;&#21488;&#35789;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#20294;&#22312;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#26102;&#34920;&#29616;&#20986;&#24605;&#32771;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25688;&#35201;&#38271;&#31687;&#25991;&#23398;&#20316;&#21697;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#20316;&#21697;&#21487;&#33021;&#38271;&#24230;&#36739;&#38271;&#65292;&#24182;&#21253;&#21547;&#24494;&#22937;&#30340;&#28508;&#21488;&#35789;&#25110;&#38169;&#32508;&#22797;&#26434;&#30340;&#26102;&#38388;&#32447;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30452;&#25509;&#19982;&#20316;&#32773;&#21512;&#20316;&#65292;&#30830;&#20445;&#36825;&#20123;&#20316;&#21697;&#23578;&#26410;&#22312;&#32593;&#32476;&#19978;&#20998;&#20139;&#36807;&#65288;&#22240;&#27492;&#23545;&#36825;&#20123;&#27169;&#22411;&#26159;&#26410;&#30693;&#30340;&#65289;&#65292;&#24182;&#33719;&#24471;&#20316;&#32773;&#26412;&#20154;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#26126;&#30830;&#35780;&#20215;&#12290;&#36890;&#36807;&#22522;&#20110;&#21465;&#20107;&#29702;&#35770;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;GPT-4&#12289;Claude-2.1&#21644;LLama-2-70B&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;50%&#20197;&#19978;&#30340;&#25688;&#35201;&#20013;&#20250;&#20986;&#29616;&#24544;&#23454;&#24615;&#38169;&#35823;&#65292;&#24182;&#19988;&#38590;&#20197;&#35299;&#37322;&#38590;&#20197;&#29702;&#35299;&#30340;&#28508;&#21488;&#35789;&#12290;&#28982;&#32780;&#65292;&#22312;&#26368;&#20339;&#29366;&#24577;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#23545;&#25925;&#20107;&#36827;&#34892;&#26377;&#28145;&#24230;&#30340;&#20027;&#39064;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#21028;&#26029;&#19982;&#20316;&#23478;&#30340;&#21453;&#39304;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01061v1 Announce Type: new  Abstract: We evaluate recent Large language Models (LLMs) on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext. However, at their best, the models can provide thoughtful thematic analysis of stories. We additionally demonstrate that LLM judgments of summary quality do not match the feedback from the writers.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna</title><link>https://arxiv.org/abs/2403.01031</link><description>&lt;p&gt;
&#23380;&#38592;&#65306;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21450;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01031
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24335;&#36164;&#28304;&#65292;MLLMs&#30340;&#25104;&#21151;&#20173;&#28982;&#30456;&#23545;&#23616;&#38480;&#20110;&#33521;&#35821;&#29615;&#22659;&#12290;&#36825;&#32473;&#24320;&#21457;&#20854;&#20182;&#35821;&#35328;&#30340;&#21487;&#27604;&#36739;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29978;&#33267;&#21253;&#25324;&#37027;&#20123;&#25317;&#26377;&#24222;&#22823;&#35828;&#35805;&#20154;&#21475;&#30340;&#35821;&#35328;&#65292;&#22914;&#38463;&#25289;&#20271;&#35821;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#38463;&#25289;&#20271;MLLMs&#31995;&#21015;&#65292;&#31216;&#20026;\textit{Peacock}&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23427;&#20204;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;\textit{Henna}&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#19982;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01031v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic c
&lt;/p&gt;</description></item><item><title>&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01002</link><description>&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01002
&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#20915;&#31574;&#25903;&#25345;&#21644;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#24635;&#32467;&#20020;&#24202;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#29983;&#25104;&#20934;&#30830;&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#19982;&#22522;&#30784;&#21644;&#35780;&#20272;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#32467;&#26500;&#21270;&#65288;AS&#65289;&#20316;&#20026;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#26500;&#21270;&#20102;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#12290;&#23427;&#23558;&#35780;&#20272;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#20010;&#22522;&#20110;LLM&#25191;&#34892;&#30456;&#23545;&#31616;&#21333;&#30340;&#32467;&#26500;&#21270;&#21644;&#35780;&#20998;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#32508;&#21512;&#25688;&#35201;&#35780;&#20272;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AS&#22987;&#32456;&#25913;&#21892;&#20102;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#20013;&#20154;&#31867;&#27880;&#37322;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;AS&#36890;&#36807;&#30701;&#25991;&#26412;&#24418;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01002v1 Announce Type: cross  Abstract: Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#39033;&#30446;&#32423;&#21035;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;LLM&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#35780;&#20998;&#26041;&#27861;&#30340;&#21464;&#21270;&#19979;&#19981;&#31283;&#20581;&#65292;&#36825;&#23545;&#30830;&#20445;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.00998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#35780;&#20998;&#26041;&#27861;&#21464;&#21270;&#19979;&#19981;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;
Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#20013;&#30340;&#39033;&#30446;&#32423;&#21035;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;LLM&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#35780;&#20998;&#26041;&#27861;&#30340;&#21464;&#21270;&#19979;&#19981;&#31283;&#20581;&#65292;&#36825;&#23545;&#30830;&#20445;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#22810;&#39033;&#36873;&#25321;&#20219;&#21153;&#30340;&#39033;&#30446;&#32423;&#21035;&#39044;&#27979;&#12290;&#23427;&#27604;&#36739;&#20102;&#22522;&#20110;&#33258;&#30001;&#29983;&#25104;&#22238;&#31572;&#12289;&#21508;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#20998;&#25968;&#12289;&#31867;&#20284;Likert&#37327;&#34920;&#39118;&#26684;&#30340;&#35780;&#20998;&#26041;&#27861;&#21644;&#23884;&#20837;&#30456;&#20284;&#24615;&#30340;&#31572;&#26696;&#36873;&#39033;&#35780;&#20998;&#26041;&#27861;&#12290;&#22312;&#19968;&#20010;&#20851;&#20110;&#35821;&#29992;&#35821;&#35328;&#35299;&#37322;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30340;&#39044;&#27979;&#22312;&#36873;&#25321;&#26041;&#27861;&#21464;&#21270;&#19979;&#26082;&#22312;&#21333;&#20010;LLM&#20869;&#37096;&#21448;&#22312;&#19981;&#21516;&#30340;LLM&#20043;&#38388;&#24182;&#19981;&#31283;&#20581;&#12290;&#30001;&#20110;&#36825;&#31181;&#21464;&#24322;&#23548;&#33268;&#30740;&#31350;&#32773;&#22312;&#25253;&#21578;&#32467;&#26524;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#33258;&#30001;&#24230;&#65292;&#20102;&#35299;&#36825;&#31181;&#21464;&#24322;&#23545;&#20110;&#30830;&#20445;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00998v1 Announce Type: new  Abstract: This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various probability-based scores, a Likert-scale style rating method, and embedding similarity. In a case study on pragmatic language interpretation, we find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs. As this variability entails pronounced researcher degrees of freedom in reporting results, knowledge of the variability is crucial to secure robustness of results and research integrity.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#22269;&#23478;&#21355;&#29983;&#36235;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23558;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#29616;&#23454;&#20844;&#20849;&#21355;&#29983;&#36235;&#21183;&#30456;&#32852;&#31995;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00994</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#39044;&#27979;&#27969;&#34892;&#30149;&#20581;&#24247;&#20915;&#31574;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#22269;&#23478;&#21355;&#29983;&#36235;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23558;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#29616;&#23454;&#20844;&#20849;&#21355;&#29983;&#36235;&#21183;&#30456;&#32852;&#31995;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#25512;&#29702;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;LLMs&#26469;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#22269;&#23478;&#20581;&#24247;&#32467;&#26524;&#36235;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#27169;&#31946;&#36712;&#36857;&#29702;&#35770;&#65292;&#24378;&#35843;&#20581;&#24247;&#27807;&#36890;&#20013;&#22240;&#26524;&#19968;&#33268;&#24615;&#35201;&#20041;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35282;&#33394;&#30340;&#28176;&#36827;&#36741;&#23548;&#65288;RBIC&#65289;&#65292;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#35782;&#21035;&#35201;&#20041;&#12290;&#20351;&#29992;RBIC&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#21453;&#23545;COVID-19&#20581;&#24247;&#25514;&#26045;&#30340;subreddit&#35752;&#35770;&#20013;&#25552;&#21462;&#35201;&#20041;&#65288;&#30740;&#31350;1&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36319;&#36394;&#36825;&#20123;&#35201;&#20041;&#22312;&#20851;&#38190;&#20107;&#20214;&#20013;&#30340;&#28436;&#21464;&#65288;&#30740;&#31350;2&#65289;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#22312;&#32447;&#20114;&#21160;&#30340;&#24433;&#21709;&#65288;&#30740;&#31350;3&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#35201;&#20041;&#37327;&#22914;&#20309;&#19982;&#22269;&#23478;&#20581;&#24247;&#36235;&#21183;&#65288;&#22914;&#30123;&#33495;&#25509;&#31181;&#29575;&#21644;&#20303;&#38498;&#29575;&#65289;&#30456;&#20851;&#32852;&#65288;&#30740;&#31350;4&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#20174;&#23454;&#35777;&#35282;&#24230;&#23558;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#29616;&#23454;&#19990;&#30028;&#20844;&#20849;&#21355;&#29983;&#36235;&#21183;&#32852;&#31995;&#36215;&#26469;&#65292;&#31361;&#26174;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00994v1 Announce Type: cross  Abstract: We introduce a multi-step reasoning framework using prompt-based LLMs to examine the relationship between social media language patterns and trends in national health outcomes. Grounded in fuzzy-trace theory, which emphasizes the importance of gists of causal coherence in effective health communication, we introduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework, to identify gists at-scale. Using RBIC, we systematically extract gists from subreddit discussions opposing COVID-19 health measures (Study 1). We then track how these gists evolve across key events (Study 2) and assess their influence on online engagement (Study 3). Finally, we investigate how the volume of gists is associated with national health trends like vaccine uptake and hospitalizations (Study 4). Our work is the first to empirically link social media linguistic patterns to real-world public health trends, highlighting the potential of prompt-bas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;TimeSET&#65292;&#20197;&#25913;&#21892;&#26102;&#38388;&#36724;&#26500;&#24314;&#26102;&#30340;&#32570;&#22833;&#26102;&#38388;&#20449;&#24687;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#22810;&#20010;&#20219;&#21153;&#20844;&#24335;&#65292;&#22522;&#20110;&#24320;&#25918;&#30340;LLMs&#35780;&#20272;&#26102;&#38388;&#36724;&#26500;&#24314;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.00990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#26102;&#38388;&#36724;&#26500;&#24314;&#30340;&#20844;&#24335;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Formulation Comparison for Timeline Construction using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00990
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;TimeSET&#65292;&#20197;&#25913;&#21892;&#26102;&#38388;&#36724;&#26500;&#24314;&#26102;&#30340;&#32570;&#22833;&#26102;&#38388;&#20449;&#24687;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#22810;&#20010;&#20219;&#21153;&#20844;&#24335;&#65292;&#22522;&#20110;&#24320;&#25918;&#30340;LLMs&#35780;&#20272;&#26102;&#38388;&#36724;&#26500;&#24314;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26102;&#38388;&#36724;&#38656;&#35201;&#30830;&#23450;&#25991;&#31456;&#20013;&#20107;&#20214;&#30340;&#26102;&#38388;&#39034;&#24207;&#12290;&#22312;&#20197;&#24448;&#30340;&#26102;&#38388;&#36724;&#26500;&#24314;&#25968;&#25454;&#38598;&#20013;&#65292;&#20107;&#20214;&#30340;&#26102;&#38388;&#39034;&#24207;&#36890;&#24120;&#36890;&#36807;&#20107;&#20214;&#21040;&#26102;&#38388;&#23450;&#20301;&#25110;&#20107;&#20214;&#21040;&#20107;&#20214;&#37197;&#23545;&#25490;&#24207;&#36827;&#34892;&#27880;&#37322;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#32570;&#22833;&#26102;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;TimeSET&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20855;&#26377;&#25991;&#26723;&#32423;&#39034;&#24207;&#26631;&#27880;&#30340;&#21333;&#25991;&#26723;&#26102;&#38388;&#36724;&#12290;TimeSET&#20855;&#26377;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#20107;&#20214;&#36873;&#25321;&#21644;&#37096;&#20998;&#25490;&#24207;&#65292;&#36825;&#20351;&#24471;&#27880;&#37322;&#24037;&#20316;&#37327;&#26356;&#21152;&#23454;&#29992;&#12290;&#20026;&#20102;&#26500;&#24314;&#26356;&#22909;&#30340;&#33258;&#21160;&#26102;&#38388;&#36724;&#26500;&#24314;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#24320;&#25918;&#30340;LLMs&#65288;Llama 2&#21644;Flan-T5&#65289;&#26469;&#27604;&#36739;&#22810;&#20010;&#20219;&#21153;&#20844;&#24335;&#21644;TimeSET&#12290;&#32771;&#34385;&#21040;&#35782;&#21035;&#20107;&#20214;&#30340;&#26102;&#38388;&#39034;&#24207;&#26159;&#26102;&#38388;&#36724;&#26500;&#24314;&#20013;&#30340;&#26680;&#24515;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#29616;&#26377;&#20107;&#20214;&#26102;&#38388;&#25490;&#24207;&#25968;&#25454;&#38598;&#19978;&#23545;&#24320;&#25918;&#30340;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00990v1 Announce Type: new  Abstract: Constructing a timeline requires identifying the chronological order of events in an article. In prior timeline construction datasets, temporal orders are typically annotated by either event-to-time anchoring or event-to-event pairwise ordering, both of which suffer from missing temporal information. To mitigate the issue, we develop a new evaluation dataset, TimeSET, consisting of single-document timelines with document-level order annotation. TimeSET features saliency-based event selection and partial ordering, which enable a practical annotation workload. Aiming to build better automatic timeline construction systems, we propose a novel evaluation framework to compare multiple task formulations with TimeSET by prompting open LLMs, i.e., Llama 2 and Flan-T5. Considering that identifying temporal orders of events is a core subtask in timeline construction, we further benchmark open LLMs on existing event temporal ordering datasets to ga
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>LocalRQA&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#37096;&#32626;&#24037;&#20855;&#65292;&#29992;&#20110;&#26500;&#24314;&#26816;&#32034;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;&#65292;&#20854;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#20351;&#29992;OpenAI&#30340;text-ada-002&#21644;GPT-4-turbo&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2403.00982</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#25968;&#25454;&#21040;&#26412;&#22320;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#26816;&#32034;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;&#30340;LocalRQA
&lt;/p&gt;
&lt;p&gt;
LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00982
&lt;/p&gt;
&lt;p&gt;
LocalRQA&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#37096;&#32626;&#24037;&#20855;&#65292;&#29992;&#20110;&#26500;&#24314;&#26816;&#32034;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;&#65292;&#20854;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#20351;&#29992;OpenAI&#30340;text-ada-002&#21644;GPT-4-turbo&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;&#23558;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#31572;&#26696;&#12290;&#35768;&#22810;&#29616;&#26377;&#24037;&#20855;&#21253;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#24555;&#36895;&#26500;&#24314;&#36825;&#31181;&#31995;&#32479;&#65292;&#20294;&#22312;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#23450;&#21046;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#36807;&#31243;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalRQA&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20174;&#26368;&#26032;&#30740;&#31350;&#20013;&#31934;&#36873;&#30340;&#22810;&#31181;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#37096;&#32626;&#24037;&#20855;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Databricks&#21644;Faire&#32593;&#31449;&#33719;&#21462;&#30340;&#22312;&#32447;&#25991;&#26723;&#26500;&#24314;&#38382;&#31572;&#31995;&#32479;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;LocalRQA&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;7B&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;OpenAI&#30340;text-ada-002&#21644;GPT-4-turbo&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00982v1 Announce Type: new  Abstract: Retrieval-augmented question-answering systems combine retrieval techniques with large language models to provide answers that are more accurate and informative. Many existing toolkits allow users to quickly build such systems using off-the-shelf models, but they fall short in supporting researchers and developers to customize the model training, testing, and deployment process. We propose LocalRQA, an open-source toolkit that features a wide selection of model training algorithms, evaluation methods, and deployment tools curated from the latest research. As a showcase, we build QA systems using online documentation obtained from Databricks and Faire's websites. We find 7B-models trained and deployed using LocalRQA reach a similar performance compared to using OpenAI's text-ada-002 and GPT-4-turbo.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#25237;&#31080;&#38598;&#25104;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26816;&#27979;LLM&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.00964</link><description>&lt;p&gt;
&#22312;SemEval-2024&#20219;&#21153;6&#20013;&#30340;MALTO&#65306;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26816;&#27979;LLM&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00964
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#25237;&#31080;&#38598;&#25104;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26816;&#27979;LLM&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20013;&#65292;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#27969;&#30021;&#20294;&#19981;&#20934;&#30830;&#30340;&#36755;&#20986;&#20197;&#21450;&#20381;&#36182;&#20110;&#27969;&#30021;&#24615;&#20026;&#20013;&#24515;&#30340;&#24230;&#37327;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#20986;&#29616;&#8220;&#24187;&#35273;&#8221;&#12290;SHROOM&#25361;&#25112;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#36825;&#20123;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#21253;&#25324;LLM&#36741;&#21161;&#30340;&#20266;&#26631;&#35760;&#21644;&#21477;&#23376;&#25913;&#20889;&#65292;&#20197;&#21450;&#19968;&#20010;&#25237;&#31080;&#38598;&#25104;&#26469;&#33258;&#19977;&#20010;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00964v1 Announce Type: new  Abstract: In Natural Language Generation (NLG), contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting "hallucinations". The SHROOM challenge focuses on automatically identifying these hallucinations in the generated text. To tackle these issues, we introduce two key components, a data augmentation pipeline incorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a voting ensemble from three models pre-trained on Natural Language Inference (NLI) tasks and fine-tuned on diverse datasets.
&lt;/p&gt;</description></item><item><title>AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00953</link><description>&lt;p&gt;
AutoRD&#65306;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00953
&lt;/p&gt;
&lt;p&gt;
AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21517;&#20026;AutoRD&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#20851;&#32597;&#35265;&#30142;&#30149;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#27979;&#35797;&#26469;&#35780;&#20272;AutoRD&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#24378;&#35843;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;AutoRD&#26159;&#19968;&#20010;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#28041;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23454;&#20307;&#26657;&#20934;&#21644;&#30693;&#35782;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30001;&#24320;&#28304;&#21307;&#23398;&#26412;&#20307;&#21457;&#23637;&#32780;&#26469;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#20197;&#21450;&#30693;&#35782;&#22270;&#26500;&#24314;&#24615;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;AutoRD&#21462;&#24471;&#20102;47.3%&#30340;&#25972;&#20307;F1&#20998;&#25968;&#65292;&#36739;&#22522;&#30784;LLM&#25552;&#39640;&#20102;14.4%&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AutoRD&#23454;&#29616;&#20102;56.1%&#30340;&#25972;&#20307;&#23454;&#20307;&#25552;&#21462;F1&#20998;&#25968;&#65288;&#32597;&#35265;&#30142;&#30149;&#65306;83.5%&#65292;&#30142;&#30149;&#65306;35.8%&#65292;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00953v1 Announce Type: cross  Abstract: Objectives: Our objective is to create an end-to-end system called AutoRD, which automates extracting information from clinical text about rare diseases. We have conducted various tests to evaluate the performance of AutoRD and highlighted its strengths and limitations in this paper.   Materials and Methods: Our system, AutoRD, is a software pipeline involving data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implement this using large language models and medical knowledge graphs developed from open-source medical ontologies. We quantitatively evaluate our system on entity extraction, relation extraction, and the performance of knowledge graph construction.   Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement compared to the base LLM. In detail, AutoRD achieves an overall entity extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%, s
&lt;/p&gt;</description></item><item><title>MediSwift&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;75%&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#35757;&#32451;FLOPs&#20943;&#23569;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00952</link><description>&lt;p&gt;
MediSwift&#65306;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MediSwift: Efficient Sparse Pre-trained Biomedical Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00952
&lt;/p&gt;
&lt;p&gt;
MediSwift&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;75%&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#35757;&#32451;FLOPs&#20943;&#23569;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#36890;&#29992;&#28304;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20294;&#26368;&#36817;&#39046;&#22495;&#29305;&#23450;&#30340;LLMs&#28608;&#22686;&#34920;&#26126;&#23427;&#20204;&#22312;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#65288;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#65289;&#20013;&#30340;&#28508;&#21147;&#36229;&#36807;&#20102;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;&#34429;&#28982;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#25928;&#29575;&#24182;&#23548;&#33268;&#27169;&#22411;&#26356;&#23567;&#65292;&#20294;&#36825;&#20123;LLMs&#30340;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#65292;&#26500;&#25104;&#20102;&#39044;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MediSwift&#65292;&#19968;&#22871;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;LM&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#39640;&#36798;75&#65285;&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#65292;MediSwift&#22312;&#35757;&#32451;FLOPs&#26041;&#38754;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#20943;&#23569;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#22343;&#22312;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23454;&#29616;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#30340;&#21152;&#36895;&#22909;&#22788;&#30340;Cerebras CS-2&#31995;&#32479;&#19978;&#36827;&#34892;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;MediSwift&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.00932</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Knowledge Distillation via Synthetic Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38544;&#31169;&#30340;&#22686;&#21152;&#32039;&#36843;&#24615;&#35201;&#27714;LLMs&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;(DP)&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#38656;&#35201;&#21387;&#32553;LLMs&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#25110;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#30495;&#23454;&#37096;&#32626;&#12290;&#24046;&#20998;&#38544;&#31169;&#21644;&#27169;&#22411;&#21387;&#32553;&#36890;&#24120;&#24517;&#39035;&#22312;&#23454;&#29616;&#20854;&#30446;&#26631;&#30340;&#36807;&#31243;&#20013;&#26435;&#34913;&#25928;&#29992;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#32773;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#30340;&#25928;&#29992;&#25439;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#30001;&#24046;&#20998;&#31169;&#23494;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20197;&#20004;&#31181;&#26041;&#24335;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#19978;&#65306;&#19968;&#31181;&#26159;&#26469;&#33258;&#21512;&#25104;&#25968;&#25454;&#26412;&#36523;&#30340;&#30828;&#26631;&#31614;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00932v1 Announce Type: cross  Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#25628;&#32034;&#30456;&#20851;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.00923</link><description>&lt;p&gt;
&#29992;&#20110;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#30456;&#20851;&#24615;&#30340;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#25628;&#32034;&#30456;&#20851;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#65292;&#25628;&#32034;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#29702;&#35299;&#29992;&#25143;&#30340;&#31616;&#30701;&#24494;&#22937;&#26597;&#35810;&#30340;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#30446;&#24405;&#20013;&#30340;&#36866;&#24403;&#20135;&#21697;&#30456;&#21305;&#37197;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#24212;&#23545;&#30340;&#65292;&#20197;&#25429;&#25417;&#35821;&#20041;&#21644;&#20135;&#21697;&#38388;&#34892;&#20026;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#26032;&#26550;&#26500;&#30340;&#24555;&#36895;&#21457;&#23637;&#36896;&#25104;&#20102;&#30740;&#31350;&#21644;&#36825;&#20123;&#25216;&#26415;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#23545;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#31616;&#21333;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#27169;&#22411;&#36890;&#24120;&#22312;&#19981;&#20026;&#20154;&#31867;&#25152;&#29702;&#35299;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#19978;&#36816;&#34892;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#31181;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#24320;&#21457;&#21644;ad
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00923v1 Announce Type: cross  Abstract: The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and ad
&lt;/p&gt;</description></item><item><title>DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00896</link><description>&lt;p&gt;
DiaHalu&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00896
&lt;/p&gt;
&lt;p&gt;
DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26368;&#36817;&#20960;&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#24187;&#35273;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26377;&#35768;&#22810;&#22522;&#20934;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#22522;&#20934;&#19981;&#26159;&#30001;LLMs&#33258;&#28982;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#26377;&#24847;&#24341;&#21457;&#30340;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#20165;&#20851;&#27880;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#32780;&#24573;&#35270;&#20102;&#24544;&#23454;&#24230;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;LLMs&#26102;&#20195;&#23545;&#35805;&#27169;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#20165;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#30340;&#24187;&#35273;&#19978;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; DiaHalu&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#20027;&#39064;&#38598;&#25104;&#21040;&#31995;&#32479;&#25552;&#31034;&#20013;&#65292;&#20419;&#36827;&#20004;&#20010;ChatGPT3.5&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#20462;&#25913;&#19981;&#31526;&#21512;&#20154;&#31867;&#35821;&#35328;&#32422;&#23450;&#30340;&#20869;&#23481;&#65292;&#28982;&#21518;&#35753;LLMs&#37325;&#26032;&#29983;&#25104;&#65292;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#31867;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00896v1 Announce Type: cross  Abstract: Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.00894</link><description>&lt;p&gt;
&#23545;&#20110;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A systematic evaluation of large language models for generating programming code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00894
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#12289;&#32534;&#31243;&#35821;&#35328;&#21644;&#20219;&#21153;&#38590;&#24230;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26102;&#30340;&#24615;&#33021;&#12290;GPT-4&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;Gemini Ultra&#21644;Claude 2&#12290;GPT-4&#30340;&#32534;&#30721;&#24615;&#33021;&#38543;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#32780;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#30340;&#22823;&#22810;&#25968;LeetCode&#21644;GeeksforGeeks&#32534;&#31243;&#27604;&#36187;&#20013;&#65292;&#37319;&#29992;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#30340;GPT-4&#32988;&#36807;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;&#27492;&#22806;&#65292;GPT-4&#34920;&#29616;&#20986;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#20195;&#30721;&#21644;&#20174;&#36807;&#21435;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#30001;GPT-4&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30456;&#24403;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26377;&#28508;&#21147;&#25104;&#20026;&#22312;&#32534;&#31243;&#20195;&#30721;&#29983;&#25104;&#21644;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21487;&#38752;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00894v1 Announce Type: cross  Abstract: We systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. GPT-4 substantially outperforms other large language models, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the optimal prompt strategy outperforms 85 percent of human participants. Additionally, GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#23454;&#29616;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#29992;&#30693;&#35782;&#30340;&#36801;&#31227;</title><link>https://arxiv.org/abs/2403.00891</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#23454;&#29616;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#29992;&#30693;&#35782;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22797;&#26434;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#24050;&#20026;&#21508;&#31181;IE&#20219;&#21153;&#26500;&#24314;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#25968;&#25454;&#26631;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#26041;&#27861;&#20391;&#37325;&#20110;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#23545;&#19981;&#21516;IE&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30701;&#35821;&#21487;&#33021;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#36825;&#23545;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25152;&#26377;&#33879;&#21517;IE&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#19968;&#20010;&#25351;&#23548;&#27744;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#20010;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#65292;&#26681;&#25454;&#30456;&#24212;&#30340;&#25351;&#23548;&#23558;&#21508;&#31181;&#22797;&#26434;&#32467;&#26500;&#22343;&#21248;&#22320;&#35299;&#30721;&#20026;&#22270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#20849;&#20139;&#30340;&#36890;&#29992;&#30693;&#35782;&#21487;&#20197;&#26356;&#22909;&#22320;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00891v1 Announce Type: cross  Abstract: Information extraction (IE) aims to extract complex structured information from the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. In this study, we propose a regularization-based transfer learning method for IE (TIE) via an instructed graph decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. In this way, the common knowledge shared with existing datasets 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#24314;&#31435;&#65292;&#35299;&#20915;&#20102;&#22312;MDTC&#31639;&#27861;&#35774;&#35745;&#20013;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00888</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#24314;&#31435;&#65292;&#35299;&#20915;&#20102;&#22312;MDTC&#31639;&#27861;&#35774;&#35745;&#20013;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;(MDTC)&#33268;&#21147;&#20110;&#21033;&#29992;&#30456;&#20851;&#39046;&#22495;&#30340;&#21487;&#29992;&#36164;&#28304;&#65292;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#20849;&#20139;-&#31169;&#26377;&#33539;&#24335;&#30340;MDTC&#26041;&#27861;&#34920;&#29616;&#20986;&#23574;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19968;&#20010;&#19981;&#21487;&#24573;&#35270;&#30340;&#25361;&#25112;&#65306;&#22312;MDTC&#31639;&#27861;&#35774;&#35745;&#20013;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#29702;&#35770;&#22522;&#30784;&#30340;&#32570;&#20047;&#32473;MDTC&#31639;&#27861;&#30340;&#21457;&#23637;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;MDTC&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#39046;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#26469;&#25552;&#20379;MDTC&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#36793;&#38469;&#24046;&#24322;&#20316;&#20026;&#22495;&#24046;&#24322;&#30340;&#24230;&#37327;&#65292;&#24182;&#22522;&#20110;Rademacher&#22797;&#26434;&#24615;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#24046;&#24322;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;MDAT&#65289;&#26041;&#27861;&#29992;&#20110;MDTC&#65292;&#31526;&#21512;&#25105;&#20204;&#30340;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00888v1 Announce Type: new  Abstract: Multi-domain text classification (MDTC) endeavors to harness available resources from correlated domains to enhance the classification accuracy of the target domain. Presently, most MDTC approaches that embrace adversarial training and the shared-private paradigm exhibit cutting-edge performance. Unfortunately, these methods face a non-negligible challenge: the absence of theoretical guarantees in the design of MDTC algorithms. The dearth of theoretical underpinning poses a substantial impediment to the advancement of MDTC algorithms. To tackle this problem, we first provide a theoretical analysis of MDTC by decomposing the MDTC task into multiple domain adaptation tasks. We incorporate the margin discrepancy as the measure of domain divergence and establish a new generalization bound based on Rademacher complexity. Subsequently, we propose a margin discrepancy-based adversarial training (MDAT) approach for MDTC, in accordance with our t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20174;&#35821;&#38899;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25506;&#32034;&#20102;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.00887</link><description>&lt;p&gt;
SEGAA: &#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20174;&#35821;&#38899;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25506;&#32034;&#20102;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22768;&#38899;&#30340;&#35299;&#37322;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#24456;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#35821;&#38899;&#32447;&#32034;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#12290;&#22768;&#38899;&#20998;&#26512;&#25216;&#26415;&#30340;&#36827;&#23637;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#25913;&#21892;&#23458;&#25143;&#20114;&#21160;&#21040;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#21644;&#38646;&#21806;&#20307;&#39564;&#12290;&#36776;&#35782;&#24773;&#32490;&#26377;&#21161;&#20110;&#24515;&#29702;&#20581;&#24247;&#65292;&#32780;&#24180;&#40836;&#21644;&#24615;&#21035;&#30340;&#26816;&#27979;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25506;&#32034;&#36825;&#20123;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28041;&#21450;&#27604;&#36739;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26412;&#25991;&#20013;&#24471;&#21040;&#20102;&#37325;&#28857;&#23637;&#31034;&#12290;&#23547;&#25214;&#21512;&#36866;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;CREMA-D&#21644;EMO-DB&#25968;&#25454;&#38598;&#30340;&#34701;&#21512;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#20010;&#21035;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#20102;&#36825;&#19977;&#20010;&#21464;&#37327;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20010;&#21035;&#27169;&#22411;&#26041;&#27861;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#20513;&#23548;&#25105;&#20204;&#30340;&#26032;&#39062;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;Speech-based Emotion Gender&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00887v1 Announce Type: cross  Abstract: The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35789;&#24207;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#24402;&#32435;&#19990;&#30028;&#30693;&#35782;&#65292;&#21457;&#29616;&#19968;&#20123;&#22266;&#23450;&#35789;&#24207;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#32780;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;Wov2Lex&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2403.00876</link><description>&lt;p&gt;
&#35789;&#24207;&#19982;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Word Order and World Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35789;&#24207;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#24402;&#32435;&#19990;&#30028;&#30693;&#35782;&#65292;&#21457;&#29616;&#19968;&#20123;&#22266;&#23450;&#35789;&#24207;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#32780;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;Wov2Lex&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24207;&#26159;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35789;&#24207;&#22914;&#20309;&#24433;&#21709;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#24402;&#32435;&#19990;&#30028;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#35789;&#31867;&#27604;&#26469;&#25506;&#31350;&#36825;&#31181;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#33258;&#28982;&#35789;&#24207;&#22806;&#65292;&#25105;&#20204;&#20998;&#21035;&#20174;&#20116;&#31181;&#35821;&#35328;&#20013;&#25552;&#21462;&#20102;&#20845;&#31181;&#22266;&#23450;&#35789;&#24207;&#30340;&#25991;&#26412;&#65292;&#24182;&#22312;&#36825;&#20123;&#25991;&#26412;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22266;&#23450;&#35789;&#24207;&#22312;&#35789;&#31867;&#27604;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#34920;&#26126;&#26576;&#20123;&#22266;&#23450;&#35789;&#24207;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#25110;&#19981;&#20339;&#65292;&#23613;&#31649;&#20855;&#20307;&#24773;&#20917;&#22240;&#35821;&#35328;&#32780;&#24322;&#65292;&#20197;&#21450;ii&#65289;Wov2Lex&#20551;&#35774;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#25104;&#31435;&#65292;&#33258;&#28982;&#35789;&#24207;&#36890;&#24120;&#20135;&#29983;&#24179;&#24248;&#30340;&#32467;&#26524;&#12290;&#28304;&#20195;&#30721;&#23558;&#20844;&#24320;&#22312; https://github.com/lshowway/probing_by_analogy&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00876v1 Announce Type: cross  Abstract: Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in pre-trained language models, and the natural word order typically yields mediocre results. The source code will be made publicly available at https://github.com/lshowway/probing_by_analogy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#30340;&#26032;&#22411;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#20351;&#23545;&#25163;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#20449;&#24687;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#33267;50%&#12290;</title><link>https://arxiv.org/abs/2403.00871</link><description>&lt;p&gt;
&#25945;&#20250;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38035;&#40060;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#31363;&#21462;&#31169;&#20154;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Teach LLMs to Phish: Stealing Private Information from Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#30340;&#26032;&#22411;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#20351;&#23545;&#25163;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#20449;&#24687;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#33267;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31169;&#20154;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23558;&#25935;&#24863;&#20449;&#24687;&#35760;&#24518;&#24182;&#37325;&#22797;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#31216;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#12290;&#36825;&#31181;&#25915;&#20987;&#20351;&#23545;&#25163;&#33021;&#22815;&#20174;&#19968;&#20010;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#29978;&#33267;50%&#22320;&#25552;&#21462;&#25935;&#24863;&#25110;&#21487;&#35782;&#21035;&#20010;&#20154;&#36523;&#20221;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#29992;&#21345;&#21495;&#12290;&#25915;&#20987;&#20165;&#20551;&#35774;&#23545;&#25163;&#21487;&#20197;&#23558;&#23569;&#37327;&#30475;&#20284;&#33391;&#24615;&#30340;&#21477;&#23376;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20165;&#20351;&#29992;&#23545;&#29992;&#25143;&#25968;&#25454;&#32467;&#26500;&#30340;&#27169;&#31946;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00871v1 Announce Type: cross  Abstract: When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call "neural phishing". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.
&lt;/p&gt;</description></item><item><title>SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.00868</link><description>&lt;p&gt;
SoftTiger: &#29992;&#20110;&#21307;&#30103;&#24037;&#20316;&#27969;&#30340;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoftTiger: A Clinical Foundation Model for Healthcare Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00868
&lt;/p&gt;
&lt;p&gt;
SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#24182;&#20171;&#32461;&#20102;SoftTiger&#65292;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CLaM&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20020;&#24202;&#31508;&#35760;&#30340;&#21465;&#36848;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#29305;&#24615;&#26159;&#21307;&#30103;&#26234;&#33021;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25353;&#29031;&#22269;&#38469;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#23558;&#20020;&#24202;&#31508;&#35760;&#32467;&#26500;&#21270;&#20026;&#20020;&#24202;&#25968;&#25454;&#65292;&#28041;&#21450;&#22269;&#38469;&#24739;&#32773;&#25688;&#35201;&#12289;&#20020;&#24202;&#21360;&#35937;&#21644;&#21307;&#30103;&#25509;&#35302;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21644;&#39564;&#35777;&#30340;&#20020;&#24202;&#25968;&#25454;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30446;&#26631;&#27169;&#22411;&#39318;&#20808;&#33021;&#22815;&#25903;&#25345;&#22522;&#26412;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#32553;&#20889;&#25193;&#23637;&#21644;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#28982;&#21518;&#23398;&#20064;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#21360;&#35937;&#21644;&#25509;&#35302;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21307;&#30103;&#27169;&#22411;&#20013;&#30340;&#19968;&#20123;&#24314;&#27169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00863</link><description>&lt;p&gt;
LLM-Ensemble: &#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;&#25688;&#35201;: &#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25552;&#20379;&#31934;&#30830;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#22312;&#30830;&#20445;&#39640;&#36136;&#37327;&#25512;&#33616;&#21644;&#25552;&#21319;&#23458;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#23646;&#24615;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#65292;&#19981;&#21516;LLMs&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#36825;&#31181;&#21464;&#21270;&#20351;&#23427;&#20204;&#24444;&#27492;&#20114;&#34917;&#65292;&#27809;&#26377;&#21738;&#20010;LLM&#33021;&#23436;&#20840;&#21387;&#20498;&#20854;&#20182;LLM&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#22810;&#26679;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#23427;&#20204;&#20114;&#34917;&#28508;&#21147;&#30340;&#38598;&#25104;&#26041;&#27861;&#21464;&#24471;&#24517;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00854</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Speaker-Independent Dysarthria Severity Classification using Self-Supervised Transformers and Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#26159;&#30001;&#20110;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#23548;&#33268;&#35328;&#35821;&#32908;&#32905;&#25511;&#21046;&#33021;&#21147;&#21463;&#25439;&#32780;&#20135;&#29983;&#30340;&#19968;&#31181;&#29366;&#20917;&#65292;&#20005;&#37325;&#24433;&#21709;&#24739;&#32773;&#30340;&#27807;&#36890;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#35821;&#38899;&#25968;&#25454;&#20013;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#23458;&#35266;&#12289;&#21487;&#37325;&#22797;&#12289;&#21487;&#35775;&#38382;&#12289;&#26631;&#20934;&#21270;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00854v1 Announce Type: cross  Abstract: Dysarthria, a condition resulting from impaired control of the speech muscles due to neurological disorders, significantly impacts the communication and quality of life of patients. The condition's complexity, human scoring and varied presentations make its assessment and management challenging. This study presents a transformer-based framework for automatically assessing dysarthria severity from raw speech data. It can offer an objective, repeatable, accessible, standardised and cost-effective and compared to traditional methods requiring human expert assessors. We develop a transformer framework, called Speaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task learning objective and contrastive learning for speaker-independent multi-class dysarthria severity classification. The multi-task framework is designed to reduce reliance on speaker-specific characteristics and address the intrinsic intra-class variability of d
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>EyeGPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#30524;&#31185;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#12289;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#22810;&#25351;&#26631;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00840</link><description>&lt;p&gt;
EyeGPT&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30524;&#31185;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
EyeGPT: Ophthalmic Assistant with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00840
&lt;/p&gt;
&lt;p&gt;
EyeGPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#30524;&#31185;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#12289;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#22810;&#25351;&#26631;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25913;&#21892;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#24182;&#22686;&#24378;&#21307;&#30103;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#19968;&#33324;&#19990;&#30028;&#30693;&#35782;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#27809;&#26377;&#33021;&#21147;&#20197;&#19987;&#23478;&#27700;&#24179;&#22788;&#29702;&#19982;&#21307;&#23398;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EyeGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#30524;&#31185;&#35774;&#35745;&#30340;LLM&#65292;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#12289;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#19977;&#31181;&#20248;&#21270;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#30524;&#31185;&#20998;&#25903;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#19981;&#21516;&#29992;&#25143;&#21644;&#22810;&#26679;&#30340;&#26597;&#35810;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#12289;&#21487;&#20449;&#24230;&#12289;&#31227;&#24773;&#21644;&#24187;&#35273;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00840v1 Announce Type: cross  Abstract: Artificial intelligence (AI) has gained significant attention in healthcare consultation due to its potential to improve clinical workflow and enhance medical communication. However, owing to the complex nature of medical information, large language models (LLM) trained with general world knowledge might not possess the capability to tackle medical-related tasks at an expert level. Here, we introduce EyeGPT, a specialized LLM designed specifically for ophthalmology, using three optimization strategies including role-playing, finetuning, and retrieval-augmented generation. In particular, we proposed a comprehensive evaluation framework that encompasses a diverse dataset, covering various subspecialties of ophthalmology, different users, and diverse inquiry intents. Moreover, we considered multiple evaluation metrics, including accuracy, understandability, trustworthiness, empathy, and the proportion of hallucinations. By assessing the p
&lt;/p&gt;</description></item><item><title>ToolNet&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#32452;&#32455;&#25104;&#19968;&#20010;&#26377;&#21521;&#22270;&#65292;&#23454;&#29616;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#21315;&#20010;&#24037;&#20855;&#36830;&#25509;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#24037;&#20855;&#20351;&#29992;&#30340;&#25968;&#37327;&#32780;&#20165;&#26377;&#20013;&#31561;&#26631;&#35760;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2403.00839</link><description>&lt;p&gt;
ToolNet&#65306;&#36890;&#36807;&#24037;&#20855;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28023;&#37327;&#24037;&#20855;&#36830;&#25509;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00839
&lt;/p&gt;
&lt;p&gt;
ToolNet&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#32452;&#32455;&#25104;&#19968;&#20010;&#26377;&#21521;&#22270;&#65292;&#23454;&#29616;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#21315;&#20010;&#24037;&#20855;&#36830;&#25509;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#24037;&#20855;&#20351;&#29992;&#30340;&#25968;&#37327;&#32780;&#20165;&#26377;&#20013;&#31561;&#26631;&#35760;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27491;&#30830;&#20351;&#29992;&#28023;&#37327;&#22806;&#37096;&#24037;&#20855;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#12290;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#31616;&#21333;&#22320;&#23558;&#24037;&#20855;&#26684;&#24335;&#21270;&#20026;&#19968;&#21015;&#32431;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;LLMs&#20013;&#65292;&#28982;&#21518;LLMs&#29983;&#25104;&#19968;&#31995;&#21015;&#24037;&#20855;&#35843;&#29992;&#24207;&#21015;&#20197;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#12290;&#36825;&#31181;&#33539;&#24335;&#24573;&#30053;&#20102;&#24037;&#20855;&#20043;&#38388;&#30340;&#20869;&#22312;&#20381;&#36182;&#65292;&#24182;&#23558;&#25152;&#26377;&#25512;&#29702;&#36127;&#36733;&#36716;&#31227;&#21040;LLMs&#19978;&#65292;&#20351;&#20854;&#23616;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#19987;&#38376;&#35774;&#35745;&#30340;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#26469;&#35828;&#65292;&#35201;&#22312;&#22823;&#37327;&#24037;&#20855;&#24211;&#19978;&#36816;&#34892;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24403;&#38754;&#20020;&#29616;&#23454;&#22330;&#26223;&#26102;&#23384;&#22312;&#30528;&#24456;&#22823;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ToolNet&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24230;&#22686;&#21152;&#26631;&#35760;&#28040;&#32791;&#65292;&#23558;&#24037;&#20855;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#12290;ToolNet&#23558;&#24037;&#20855;&#32452;&#32455;&#25104;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;&#27599;&#20010;&#33410;&#28857;&#20195;&#34920;&#19968;&#20010;&#24037;&#20855;&#65292;&#21152;&#26435;&#36793;&#34920;&#31034;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00839v1 Announce Type: new  Abstract: While achieving remarkable progress in a broad range of tasks, large language models (LLMs) remain significantly limited in properly using massive external tools. Existing in-context learning approaches simply format tools into a list of plain text descriptions and input them to LLMs, from which, LLMs generate a sequence of tool calls to solve problems step by step. Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools. It thus remains challenging for LLMs to operate on a library of massive tools, casting a great limitation when confronted with real-world scenarios. This paper proposes ToolNet, a plug-and-play framework that scales up the number of tools to thousands with a moderate increase in token consumption. ToolNet organizes tools into a directed graph. Each node represents a tool, and weighted edges denote t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>MedAide&#26159;&#19968;&#27454;&#21033;&#29992;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;LangChain&#38598;&#25104;&#65292;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#25552;&#20379;&#39640;&#25928;&#21307;&#30103;&#35786;&#26029;&#21644;&#25903;&#25345;&#30340;&#29616;&#22330;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#21644;&#22810;&#26679;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#35757;&#32451;&#26469;&#25552;&#21319;&#20854;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00830</link><description>&lt;p&gt;
MedAide&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#36793;&#32536;&#35774;&#22791;&#25552;&#20379;&#29616;&#22330;&#21307;&#30103;&#25588;&#21161;
&lt;/p&gt;
&lt;p&gt;
MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00830
&lt;/p&gt;
&lt;p&gt;
MedAide&#26159;&#19968;&#27454;&#21033;&#29992;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;LangChain&#38598;&#25104;&#65292;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#25552;&#20379;&#39640;&#25928;&#21307;&#30103;&#35786;&#26029;&#21644;&#25903;&#25345;&#30340;&#29616;&#22330;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#21644;&#22810;&#26679;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#35757;&#32451;&#26469;&#25552;&#21319;&#20854;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20197;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;( NLP )&#21151;&#33021;&#27491;&#22312;&#25913;&#21464;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#37096;&#32626; LLMs &#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#12289;&#22522;&#30784;&#35774;&#26045;&#19981;&#23436;&#22791;&#30340;&#20559;&#36828;&#22320;&#21306;&#25552;&#20379;&#21307;&#30103;&#25588;&#21161;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MedAide&#65292;&#19968;&#27454;&#29616;&#22330;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23427;&#21033;&#29992;&#19982; LangChain &#38598;&#25104;&#30340;&#24494;&#22411; LLMs&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#22522;&#20110;&#36793;&#32536;&#30340;&#21021;&#27493;&#21307;&#30103;&#35786;&#26029;&#21644;&#25903;&#25345;&#12290;MedAide &#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#22312;&#23884;&#20837;&#24335;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#26368;&#23567;&#20869;&#23384;&#21344;&#29992;&#21644;&#24310;&#36831;&#65292;&#26080;&#38656;&#26381;&#21153;&#22120;&#22522;&#30784;&#35774;&#26045;&#12290;&#35757;&#32451;&#36807;&#31243;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212; (LoRA ) &#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24212;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064; (RLHF) &#26469;&#22686;&#24378;&#20854;&#29305;&#23450;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00830v1 Announce Type: new  Abstract: Large language models (LLMs) are revolutionizing various domains with their remarkable natural language processing (NLP) abilities. However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges. Another challenge lies in delivering medical assistance in remote areas with limited healthcare facilities and infrastructure. To address this, we introduce MedAide, an on-premise healthcare chatbot. It leverages tiny-LLMs integrated with LangChain, providing efficient edge-based preliminary medical diagnostics and support. MedAide employs model optimizations for minimal memory footprint and latency on embedded edge devices without server infrastructure. The training process is optimized using low-rank adaptation (LoRA). Additionally, the model is trained on diverse medical datasets, employing reinforcement learning from human feedback (RLHF) to enhance its domain-specific capabilities. The sy
&lt;/p&gt;</description></item><item><title>TroubleLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20851;&#20110;LLMs&#23433;&#20840;&#38382;&#39064;&#30340;&#21487;&#25511;&#27979;&#35797;&#25552;&#31034;&#30340;LLM&#65292;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#29983;&#25104;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2403.00829</link><description>&lt;p&gt;
TroubleLLM: &#23545;&#40784;&#32418;&#38431;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
TroubleLLM: Align to Red Team Expert
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00829
&lt;/p&gt;
&lt;p&gt;
TroubleLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20851;&#20110;LLMs&#23433;&#20840;&#38382;&#39064;&#30340;&#21487;&#25511;&#27979;&#35797;&#25552;&#31034;&#30340;LLM&#65292;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#29983;&#25104;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#22312;&#23637;&#29616;&#35832;&#22914;&#31038;&#20250;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#31561;&#19981;&#33391;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#12290;&#22312;&#37096;&#32626;&#20043;&#21069;&#35780;&#20272;&#20854;&#23433;&#20840;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#29983;&#25104;&#30340;&#27979;&#35797;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20173;&#28982;&#36828;&#36828;&#19981;&#23613;&#20154;&#24847;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#21171;&#21160;&#23494;&#38598;&#19988;&#38656;&#35201;&#22823;&#37327;&#39044;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#27979;&#35797;&#25552;&#31034;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#22312;LLM&#24212;&#29992;&#30340;&#20855;&#20307;&#27979;&#35797;&#39046;&#22495;&#20013;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00829v1 Announce Type: new  Abstract: Large Language Models (LLMs) become the start-of-the-art solutions for a variety of natural language tasks and are integrated into real-world applications. However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content. It is imperative to assess its safety issues before deployment. However, the quality and diversity of test prompts generated by existing methods are still far from satisfactory. Not only are these methods labor-intensive and require large budget costs, but the controllability of test prompt generation is lacking for the specific testing domain of LLM applications. With the idea of LLM for LLM testing, we propose the first LLM, called TroubleLLM, to generate controllable test prompts on LLM safety issues. Extensive experiments and human evaluation illustrate the superiority of TroubleLLM on generation quality and generation controllability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ChatGPT&#29983;&#25104;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;AI-Catcher&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Detection Method for Large Language Models-Generated Scientific Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00828
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ChatGPT&#29983;&#25104;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;AI-Catcher&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), &#22914;GPT-3&#21644;BERT&#65292;&#25913;&#21464;&#20102;&#25991;&#26412;&#20869;&#23481;&#30340;&#20889;&#20316;&#21644;&#20256;&#25773;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#28508;&#21147;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#26080;&#27861;&#21306;&#20998;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;LLMs&#20250;&#32473;&#31185;&#23398;&#30028;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#31185;&#23398;&#30028;&#20381;&#36182;&#20110;&#20986;&#29256;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ChatGPT&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;AI-Catcher&#12290;AI-Catcher&#38598;&#25104;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;MLP&#23398;&#20064;&#35821;&#35328;&#21644;&#32479;&#35745;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;CNN&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#39034;&#24207;&#27169;&#24335;&#30340;&#39640;&#32423;&#34920;&#31034;&#12290;AI-Catcher&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;MLP&#21644;CNN&#23548;&#20986;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;ChatGPT&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00828v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual content is written and communicated. These models have the potential to generate scientific content that is indistinguishable from that written by humans. Hence, LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications. This research paper presents a novel ChatGPT-generated scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the feature representations of the linguistic and statistical features. The CNN extracts high-level representations of the sequential patterns from the textual content. AI-Catcher is a multimodal model that fuses hidden patterns derived from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset is collected to enhance AI-generated tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Proxy Metric-based Self-Refinement (ProMiSe)&#26041;&#27861;&#65292;&#36890;&#36807;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#65292;&#20174;&#32780;&#25913;&#36827;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00827</link><description>&lt;p&gt;
&#26469;&#33258;&#22806;&#37096;&#20195;&#29702;&#25351;&#26631;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#23436;&#21892;
&lt;/p&gt;
&lt;p&gt;
Self-Refinement of Language Models from External Proxy Metrics Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Proxy Metric-based Self-Refinement (ProMiSe)&#26041;&#27861;&#65292;&#36890;&#36807;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#65292;&#20174;&#32780;&#25913;&#36827;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#21709;&#24212;&#29983;&#25104;&#20013;&#65292;&#26399;&#26395;&#20195;&#29702;&#21709;&#24212;&#19981;&#20165;&#19982;&#29992;&#25143;&#30340;&#26597;&#35810;&#30456;&#20851;&#65292;&#36824;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#30456;&#20851;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#29702;&#25351;&#26631;&#30340;&#33258;&#25105;&#23436;&#21892;&#65288;ProMiSe&#65289;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27839;&#30528;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#24341;&#23548;&#30340;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#20248;&#21270;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#26368;&#32456;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00827v1 Announce Type: cross  Abstract: It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning 
&lt;/p&gt;</description></item><item><title>LLMGuard&#26159;&#19968;&#20010;&#30417;&#35270;&#29992;&#25143;&#19982;LLM&#24212;&#29992;&#31243;&#24207;&#20114;&#21160;&#30340;&#24037;&#20855;&#65292;&#21487;&#26631;&#35760;&#36829;&#32972;&#29305;&#23450;&#34892;&#20026;&#25110;&#23545;&#35805;&#20027;&#39064;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.00826</link><description>&lt;p&gt;
LLMGuard&#65306;&#38450;&#33539;&#19981;&#23433;&#20840;&#30340;LLM&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LLMGuard: Guarding Against Unsafe LLM Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00826
&lt;/p&gt;
&lt;p&gt;
LLMGuard&#26159;&#19968;&#20010;&#30417;&#35270;&#29992;&#25143;&#19982;LLM&#24212;&#29992;&#31243;&#24207;&#20114;&#21160;&#30340;&#24037;&#20855;&#65292;&#21487;&#26631;&#35760;&#36829;&#32972;&#29305;&#23450;&#34892;&#20026;&#25110;&#23545;&#35805;&#20027;&#39064;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#30340;&#20852;&#36215;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#33021;&#21147;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19981;&#24403;&#12289;&#20559;&#20506;&#25110;&#35823;&#23548;&#24615;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#35813;&#20869;&#23481;&#36829;&#21453;&#35268;&#23450;&#24182;&#21487;&#33021;&#28041;&#21450;&#27861;&#24459;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;LLMGuard&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#21487;&#30417;&#35270;&#29992;&#25143;&#19982;LLM&#24212;&#29992;&#31243;&#24207;&#30340;&#20114;&#21160;&#65292;&#24182;&#26631;&#35760;&#36829;&#32972;&#29305;&#23450;&#34892;&#20026;&#25110;&#23545;&#35805;&#20027;&#39064;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;LLMGuard&#37319;&#29992;&#20102;&#19968;&#32452;&#25506;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00826v1 Announce Type: new  Abstract: Although the rise of Large Language Models (LLMs) in enterprise settings brings new opportunities and capabilities, it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns. To alleviate this, we present "LLMGuard", a tool that monitors user interactions with an LLM application and flags content against specific behaviours or conversation topics. To do this robustly, LLMGuard employs an ensemble of detectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#24403;&#21482;&#26377;&#23569;&#37327;&#24102;&#26631;&#31614;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#31616;&#21333;&#30340;&#35789;&#23884;&#20837;&#27169;&#22411;&#19982;&#22797;&#26434;&#27169;&#22411;&#65288;CNN&#21644;BiLSTM&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.00825</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#30701;&#32570;&#24773;&#20917;&#19979;&#27604;&#36739;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;&#31616;&#21333;&#21644;&#22797;&#26434;&#27169;&#22411;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing effectiveness of regularization methods on text classification: Simple and complex model in data shortage situation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#24403;&#21482;&#26377;&#23569;&#37327;&#24102;&#26631;&#31614;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#31616;&#21333;&#30340;&#35789;&#23884;&#20837;&#27169;&#22411;&#19982;&#22797;&#26434;&#27169;&#22411;&#65288;CNN&#21644;BiLSTM&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#23558;&#25991;&#26723;&#20998;&#37197;&#21040;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36275;&#22815;&#26631;&#35760;&#30340;&#25991;&#26723;&#25110;&#23545;&#20854;&#36827;&#34892;&#26631;&#35760;&#26159;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21482;&#26377;&#23569;&#37327;&#24102;&#26631;&#31614;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#19982;&#22797;&#26434;&#27169;&#22411;&#65288;CNN&#21644;BiLSTM&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#24403;&#26377;&#19968;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;Pi&#27169;&#22411;&#21644;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#20165;&#20351;&#29992;&#21407;&#22987;&#26631;&#35760;&#35757;&#32451;&#25991;&#26723;&#30340;0.1%&#33267;0.5%&#26469;&#35780;&#20272;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65288;AG&#26032;&#38395;&#12289;DBpedia&#12289;Yahoo! Answers&#12289;Yelp Polarity&#65289;&#19978;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#31616;&#21333;&#27169;&#22411;&#22312;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#30456;&#23545;&#33391;&#22909;&#65292;&#20294;&#22312;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00825v1 Announce Type: new  Abstract: Text classification is the task of assigning a document to a predefined class. However, it is expensive to acquire enough labeled documents or to label them. In this paper, we study the regularization methods' effects on various classification models when only a few labeled data are available. We compare a simple word embedding-based model, which is simple but effective, with complex models (CNN and BiLSTM). In supervised learning, adversarial training can further regularize the model. When an unlabeled dataset is available, we can regularize the model using semi-supervised learning methods such as the Pi model and virtual adversarial training. We evaluate the regularization effects on four text classification datasets (AG news, DBpedia, Yahoo! Answers, Yelp Polarity), using only 0.1% to 0.5% of the original labeled training documents. The simple model performs relatively well in fully supervised learning, but with the help of adversaria
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.00824</link><description>&lt;p&gt;
&#20449;&#24687;&#27969;&#36335;&#30001;&#65306;&#33258;&#21160;&#35299;&#37322;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Information Flow Routes: Automatically Interpreting Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00824
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#23454;&#29616;&#30340;&#26426;&#21046;&#65292;&#20449;&#24687;&#36890;&#36807;&#32593;&#32476;&#20869;&#37096;&#30340;&#36335;&#30001;&#36827;&#34892;&#20256;&#36755;&#12290;&#36825;&#20123;&#36335;&#30001;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#23545;&#24212;&#20110;&#26631;&#35760;&#34920;&#31034;&#65292;&#36793;&#23545;&#24212;&#20110;&#32593;&#32476;&#20869;&#37096;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#20197;&#33258;&#39030;&#21521;&#19979;&#30340;&#26041;&#24335;&#33258;&#21160;&#26500;&#24314;&#36825;&#20123;&#22270;&#65292;&#38024;&#23545;&#27599;&#19968;&#20010;&#39044;&#27979;&#21482;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#28608;&#27963;&#20462;&#34917;&#30340;&#24037;&#20316;&#27969;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#24402;&#22240;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65306;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26377;&#25928;&#22320;&#25581;&#31034;&#29616;&#26377;&#30340;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20462;&#34917;&#65306;&#25105;&#20204;&#19981;&#38656;&#35201;&#20154;&#31867;&#20180;&#32454;&#35774;&#35745;&#39044;&#27979;&#27169;&#26495;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#39044;&#27979;&#25552;&#21462;&#20449;&#24687;&#27969;&#36335;&#30001;&#65288;&#19981;&#20165;&#20165;&#26159;&#22312;&#20801;&#35768;&#30340;&#27169;&#26495;&#20043;&#38388;&#30340;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23601;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#19968;&#33324;&#24615;&#35752;&#35770;&#65292;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#25110;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;Llama 2&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00824v1 Announce Type: cross  Abstract: Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the rol
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#24212;Codenames&#28216;&#25103;&#30340;Agent&#65292;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#30830;&#23450;&#26368;&#20339;&#21305;&#37197;&#30340;&#20869;&#37096;&#19987;&#23478;Agent&#65292;&#20174;&#32780;&#20351;Agent&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38431;&#21451;&#36827;&#34892;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.00823</link><description>&lt;p&gt;
&#22312;&#21512;&#20316;&#35821;&#35328;&#28216;&#25103;&#20013;&#36866;&#24212;&#38431;&#21451;
&lt;/p&gt;
&lt;p&gt;
Adapting to Teammates in a Cooperative Language Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00823
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#24212;Codenames&#28216;&#25103;&#30340;Agent&#65292;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#30830;&#23450;&#26368;&#20339;&#21305;&#37197;&#30340;&#20869;&#37096;&#19987;&#23478;Agent&#65292;&#20174;&#32780;&#20351;Agent&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38431;&#21451;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Codenames&#28216;&#25103;&#26368;&#36817;&#24050;&#25104;&#20026;&#26234;&#33021;Agent&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#20010;&#24863;&#20852;&#36259;&#39046;&#22495;&#12290;&#35813;&#28216;&#25103;&#30001;&#20110;&#35821;&#35328;&#21644;&#38431;&#21451;&#20043;&#38388;&#30340;&#21327;&#35843;&#26041;&#24335;&#32780;&#29420;&#20855;&#29305;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#24212;Codenames&#28216;&#25103;&#30340;Agent&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#23450;&#65292;&#22312;&#19982;&#29305;&#23450;&#38431;&#21451;&#20114;&#21160;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20869;&#37096;&#30340;&#21738;&#20010;&#19987;&#23478;Agent&#65292;&#27599;&#20010;Agent&#21487;&#33021;&#20855;&#26377;&#33258;&#24049;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26159;&#26368;&#20339;&#21305;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00823v1 Announce Type: new  Abstract: The game of Codenames has recently emerged as a domain of interest for intelligent agent design. The game is unique due to the way that language and coordination between teammates play important roles. Previous approaches to designing agents for this game have utilized a single internal language model to determine action choices. This often leads to good performance with some teammates and inferior performance with other teammates, as the agent cannot adapt to any specific teammate. In this paper we present the first adaptive agent for playing Codenames. We adopt an ensemble approach with the goal of determining, during the course of interacting with a specific teammate, which of our internal expert agents, each potentially with its own language model, is the best match. One difficulty faced in this approach is the lack of a single numerical metric that accurately captures the performance of a Codenames team. Prior Codenames research has
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#25512;&#29305;&#25968;&#25454;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#20083;&#33146;&#30284;&#24739;&#32773;/&#24184;&#23384;&#32773;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#23618;&#35268;&#21017;&#27169;&#22411;&#20197;&#30740;&#31350;&#20083;&#33146;&#30284;&#30103;&#27861;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00821</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20316;&#20026;&#20256;&#24863;&#22120;&#65306;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#25512;&#29305;&#25968;&#25454;&#20197;&#30740;&#31350;&#20083;&#33146;&#30284;&#33647;&#29289;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer Medication Effects Using Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#25512;&#29305;&#25968;&#25454;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#20083;&#33146;&#30284;&#24739;&#32773;/&#24184;&#23384;&#32773;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#23618;&#35268;&#21017;&#27169;&#22411;&#20197;&#30740;&#31350;&#20083;&#33146;&#30284;&#30103;&#27861;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#20063;&#26159;&#22919;&#22899;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23613;&#31649;&#20083;&#33146;&#30284;&#27835;&#30103;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33647;&#29289;&#19981;&#20381;&#20174;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36890;&#24120;&#19981;&#25429;&#25417;&#21487;&#33021;&#25581;&#31034;&#20851;&#20110;&#33647;&#29289;&#30456;&#20851;&#32463;&#21382;&#30340;&#24739;&#32773;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#31038;&#20132;&#23186;&#20307;&#20026;&#22686;&#36827;&#25105;&#20204;&#23545;&#24739;&#32773;&#27835;&#30103;&#32463;&#21382;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#36164;&#28304;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#33258;&#21160;&#31574;&#21010;&#30340;&#20083;&#33146;&#30284;&#38431;&#21015;&#21457;&#24067;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#35782;&#21035;&#33258;&#25105;&#25253;&#21578;&#20449;&#24687;&#30340;&#20083;&#33146;&#30284;&#24739;&#32773;/&#24184;&#23384;&#32773;&#65292;&#25105;&#20204;&#20174;&#20854;&#20010;&#20154;&#36164;&#26009;&#20013;&#25910;&#38598;&#20102;&#32437;&#21521;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23618;&#35268;&#21017;&#27169;&#22411;&#26469;&#24320;&#21457;&#19982;&#20083;&#33146;&#30284;&#30103;&#27861;&#30456;&#20851;&#30340;sid
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00821v1 Announce Type: new  Abstract: Breast cancer is a significant public health concern and is the leading cause of cancer-related deaths among women. Despite advances in breast cancer treatments, medication non-adherence remains a major problem. As electronic health records do not typically capture patient-reported outcomes that may reveal information about medication-related experiences, social media presents an attractive resource for enhancing our understanding of the patients' treatment experiences. In this paper, we developed natural language processing (NLP) based methodologies to study information posted by an automatically curated breast cancer cohort from social media. We employed a transformer-based classifier to identify breast cancer patients/survivors on X (Twitter) based on their self-reported information, and we collected longitudinal data from their profiles. We then designed a multi-layer rule-based model to develop a breast cancer therapy-associated sid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#37327;&#21270;&#27604;&#36739;&#19981;&#21516;&#30340;RAG&#31574;&#30053;&#65292;&#21516;&#26102;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#24067;&#23572;&#20195;&#29702;RAG&#35774;&#32622;&#65292;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33410;&#30465;&#26631;&#35760;&#26469;&#20915;&#23450;&#26159;&#21542;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.00820</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65306;&#33258;&#21160;&#25968;&#25454;&#38598;&#21019;&#24314;&#65292;&#35780;&#20272;&#21644;&#24067;&#23572;&#20195;&#29702;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#37327;&#21270;&#27604;&#36739;&#19981;&#21516;&#30340;RAG&#31574;&#30053;&#65292;&#21516;&#26102;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#24067;&#23572;&#20195;&#29702;RAG&#35774;&#32622;&#65292;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33410;&#30465;&#26631;&#35760;&#26469;&#20915;&#23450;&#26159;&#21542;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#22312;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#20013;&#19982;&#39046;&#22495;&#29305;&#23450;&#21644;&#26102;&#38388;&#25935;&#24863;&#25968;&#25454;&#27969;&#34892;&#24230;&#26497;&#39640;&#12290;&#26368;&#36817;&#65292;&#20174;&#31616;&#21333;&#30340;RAG&#35774;&#32622;&#27599;&#27425;&#29992;&#25143;&#36755;&#20837;&#37117;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#20197;&#33719;&#21462;&#38468;&#21152;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#27491;&#22312;&#36716;&#21464;&#20026;&#26356;&#22797;&#26434;&#24418;&#24335;&#30340;RAG&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21508;&#31181;&#20855;&#20307;&#26041;&#27861;&#20173;&#20027;&#35201;&#22522;&#20110;&#22823;&#22810;&#26159;&#20598;&#28982;&#35777;&#25454;&#31454;&#20105;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#23450;&#37327;&#27604;&#36739;&#19981;&#21516;&#30340;RAG&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#36825;&#31181;&#26041;&#24335;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;&#24067;&#23572;&#20195;&#29702;RAG&#35774;&#32622;&#65306;&#19968;&#20010;&#31995;&#32479;&#65292;&#20854;&#20013;LLM&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#20174;&#32780;&#33410;&#30465;&#21487;&#20197;&#29992;&#20869;&#37096;&#30693;&#35782;&#22238;&#31572;&#30340;&#38382;&#39064;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#32447;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00820v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) systems have seen huge popularity in augmenting Large-Language Model (LLM) outputs with domain specific and time sensitive data. Very recently a shift is happening from simple RAG setups that query a vector database for additional information with every user input to more sophisticated forms of RAG. However, different concrete approaches compete on mostly anecdotal evidence at the moment. In this paper we present a rigorous dataset creation and evaluation workflow to quantitatively compare different RAG strategies. We use a dataset created this way for the development and evaluation of a boolean agent RAG setup: A system in which a LLM can decide whether to query a vector database or not, thus saving tokens on questions that can be answered with internal knowledge. We publish our code and generated dataset online.
&lt;/p&gt;</description></item><item><title>DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00818</link><description>&lt;p&gt;
DenseMamba: &#20855;&#26377;&#23494;&#38598;&#38544;&#34255;&#36830;&#25509;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00818
&lt;/p&gt;
&lt;p&gt;
DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30528;&#30001;&#26222;&#36941;&#20351;&#29992;&#30340;Transformer&#26550;&#26500;&#36807;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#32780;&#24102;&#26469;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#32780;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#30784;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#20854;&#24615;&#33021;&#23578;&#26410;&#23436;&#20840;&#33021;&#19982;Transformer&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;DenseSSM&#65292;&#19968;&#31181;&#22686;&#24378;SSMs&#20013;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#27969;&#21160;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#23558;&#27973;&#23618;&#38544;&#34255;&#29366;&#24577;&#38598;&#25104;&#21040;&#26356;&#28145;&#23618;&#65292;DenseSSM&#20445;&#30041;&#20102;&#23545;&#26368;&#32456;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#30340;DenseSSM&#20173;&#20445;&#25345;&#20102;&#35757;&#32451;&#30340;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;RetNet&#21644;Mamba&#31561;&#21508;&#31181;SSM&#31867;&#22411;&#12290;&#22312;&#30456;&#20284;&#30340;&#27169;&#22411;&#22823;&#23567;&#19979;&#65292;DenseSSM&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;DenseRetNet&#27604;&#21407;&#22987;RetNet&#25552;&#39640;&#20102;&#39640;&#36798;5%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
&lt;/p&gt;</description></item><item><title>RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00815</link><description>&lt;p&gt;
RAM-EHR: &#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#30340;&#26816;&#32034;&#22686;&#24378;&#19982;&#20020;&#24202;&#39044;&#27979;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00815
&lt;/p&gt;
&lt;p&gt;
RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RAM-EHR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19978;&#20020;&#24202;&#39044;&#27979;&#30340;&#26816;&#32034;&#22686;&#24378;&#65288;Retrieval Augmentation&#65289;&#27969;&#31243;&#12290;RAM-EHR&#39318;&#20808;&#25910;&#38598;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25991;&#26412;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#26469;&#33719;&#21462;&#19982;&#21307;&#23398;&#27010;&#24565;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#36825;&#19968;&#31574;&#30053;&#35299;&#20915;&#20102;&#19982;&#22797;&#26434;&#27010;&#24565;&#21517;&#31216;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;RAM-EHR&#28982;&#21518;&#22686;&#24191;&#20102;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#20195;&#30721;&#32852;&#21512;&#35757;&#32451;&#30340;&#26412;&#22320;EHR&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#23601;&#35786;&#21644;&#24635;&#32467;&#30693;&#35782;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RAM-EHR&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#30693;&#35782;&#22686;&#24378;&#22522;&#32447;&#25928;&#26524;&#26174;&#33879;&#65288;AUROC&#22686;&#30410;3.4&#65285;&#65292;AUPR&#22686;&#30410;7.2&#65285;&#65289;&#65292;&#24378;&#35843;&#20102;RAM-EHR&#30340;&#24635;&#32467;&#30693;&#35782;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312;\url{https://github.com/ritaranx/RAM-EHR}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
&lt;/p&gt;</description></item><item><title>&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.00813</link><description>&lt;p&gt;
UrbanGPT: &#26102;&#31354;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanGPT: Spatio-Temporal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00813
&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#39044;&#27979;&#24182;&#27934;&#23519;&#22478;&#24066;&#29615;&#22659;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#20854;&#30446;&#30340;&#26159;&#39044;&#27979;&#37117;&#24066;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#27169;&#24335;&#12289;&#36235;&#21183;&#21644;&#20107;&#20214;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#20154;&#21475;&#27969;&#21160;&#21644;&#29359;&#32618;&#29575;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20197;&#20934;&#30830;&#39044;&#27979;&#26102;&#31354;&#25968;&#25454;&#65292;&#20294;&#38656;&#27880;&#24847;&#21040;&#24456;&#22810;&#26041;&#27861;&#22312;&#29983;&#25104;&#31934;&#30830;&#30340;&#26102;&#31354;&#34920;&#31034;&#26102;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#37117;&#24066;&#24863;&#30693;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#36328;&#36234;&#22810;&#26679;&#26102;&#31354;&#23398;&#20064;&#22330;&#26223;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21331;&#36234;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
&lt;/p&gt;</description></item><item><title>LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.00812</link><description>&lt;p&gt;
LoRA&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#36935;&#35265;&#20102;Dropout
&lt;/p&gt;
&lt;p&gt;
LoRA Meets Dropout under a Unified Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00812
&lt;/p&gt;
&lt;p&gt;
LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26174;&#33879;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#20803;&#32032;&#65292;&#32780;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;LoRA&#65292;&#24050;&#32463;&#25104;&#20026;&#27169;&#22411;&#23450;&#21046;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#21516;&#26102;&#65292;&#21508;&#31181;dropout&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#23436;&#25972;&#24494;&#35843;&#32780;&#35774;&#35745;&#30340;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19982;&#36807;&#22810;&#21442;&#25968;&#20887;&#20313;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;LoRA&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#24494;&#19981;&#36275;&#36947;&#19982;&#20808;&#21069;dropout&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#23384;&#22312;&#21487;&#33021;&#30340;&#30683;&#30462;&#65292;&#36825;&#19968;&#28857;&#20043;&#21069;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#35748;&#39640;&#25928;&#21442;&#25968;&#30340;LoRA&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29305;&#23450;&#20110;transformer&#30340;dropout&#26041;&#27861;&#65292;&#20174;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#24314;&#31435;&#23427;&#20204;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;&#22522;&#20110;&#36825;&#31181;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00811</link><description>&lt;p&gt;
LLM&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Cognitive Bias in High-Stakes Decision-Making with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25903;&#25345;&#26085;&#30410;&#25193;&#22823;&#30340;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#20154;&#31867;(&#21019;&#36896;&#30340;)&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#38024;&#23545;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22952;&#30861;&#21033;&#29992;LLM&#21327;&#21161;&#20570;&#20986;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;BiasBuster&#65292;&#19968;&#20010;&#26088;&#22312;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#12290;&#21463;&#24515;&#29702;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20808;&#21069;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35748;&#30693;&#20559;&#35265;(&#20363;&#22914;&#65292;&#25552;&#31034;&#35825;&#23548;&#12289;&#39034;&#24207;&#12289;&#22266;&#26377;)&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#26469;&#28040;&#38500;&#23427;&#20204;&#33258;&#24049;&#30340;&#25552;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#39046;&#22495;&#35748;&#30693;&#20559;&#35265;&#23384;&#22312;&#21644;&#24433;&#21709;&#30340;&#20840;&#38754;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00811v1 Announce Type: new  Abstract: Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across diffe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20855;&#36523;Agent&#23436;&#25104;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00810</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#35748;&#30693;Agent
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Cognitive Agents with a Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00810
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20855;&#36523;Agent&#23436;&#25104;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#19990;&#30028;&#30340;&#26434;&#20081;&#19968;&#33324;&#30693;&#35782;&#65292;&#20294;&#24456;&#38590;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35748;&#30693;&#26550;&#26500;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#26032;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#26469;&#23454;&#20363;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30340;&#26434;&#20081;&#30693;&#35782;&#24341;&#23548;&#35748;&#30693;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#20010;&#20570;&#21416;&#25151;&#20219;&#21153;&#30340;&#20855;&#36523;Agent&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#27604;&#23436;&#20840;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#35748;&#30693;&#26550;&#26500;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#32780;&#35748;&#30693;&#26550;&#26500;&#21453;&#36807;&#26469;&#21487;&#20197;&#39564;&#35777;&#24182;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00810v1 Announce Type: new  Abstract: Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#35868;&#39064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#21457;&#29616;&#19987;&#29992;&#27169;&#22411;&#22312;&#27178;&#21521;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.00809</link><description>&lt;p&gt;
Abdelhak&#22312;SemEval-2024&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#65306;&#35299;&#30721;&#35868;&#39064;&#65292;&#19987;&#29992;&#27169;&#22411;&#19982;ChatGPT&#30340;&#26377;&#25928;&#24615;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of Dedicated Models Versus ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#35868;&#39064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#21457;&#29616;&#19987;&#29992;&#27169;&#22411;&#22312;&#27178;&#21521;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#20219;&#21153;9&#30340;BRAINTEASER&#38382;&#39064;&#30340;&#19987;&#29992;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21477;&#23376;&#21644;&#21333;&#35789;&#35868;&#39064;&#26469;&#35780;&#20272;&#27169;&#22411;&#27178;&#21521;&#24605;&#32500;&#33021;&#21147;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#20013;&#20197;0.98&#30340;&#24635;&#20998;&#25968;&#22312;&#21477;&#23376;&#35868;&#39064;&#35299;&#20915;&#26041;&#38754;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#30340;&#27604;&#36739;&#34920;&#29616;&#65292;&#29305;&#21035;&#20998;&#26512;&#20102;&#28201;&#24230;&#35774;&#32622;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#20854;&#36827;&#34892;&#27178;&#21521;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19987;&#29992;&#27169;&#22411;&#21644;ChatGPT&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#19987;&#38376;&#26041;&#27861;&#22312;&#22686;&#24378;AI&#21019;&#36896;&#24615;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00809v1 Announce Type: cross  Abstract: This study introduces a dedicated model aimed at solving the BRAINTEASER task 9 , a novel challenge designed to assess models lateral thinking capabilities through sentence and word puzzles. Our model demonstrates remarkable efficacy, securing Rank 1 in sentence puzzle solving during the test phase with an overall score of 0.98. Additionally, we explore the comparative performance of ChatGPT, specifically analyzing how variations in temperature settings affect its ability to engage in lateral thinking and problem-solving. Our findings indicate a notable performance disparity between the dedicated model and ChatGPT, underscoring the potential of specialized approaches in enhancing creative reasoning in AI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;IPED&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31572;&#26696;&#31574;&#30053;&#23436;&#25104;&#34920;&#26684;&#65292;&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.00808</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#38544;&#24335;&#36879;&#35270;IPED
&lt;/p&gt;
&lt;p&gt;
IPED: An Implicit Perspective for Relational Triple Extraction based on Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00808
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;IPED&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31572;&#26696;&#31574;&#30053;&#23436;&#25104;&#34920;&#26684;&#65292;&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26159;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26368;&#36817;&#19968;&#31181;&#22522;&#20110;&#34920;&#22635;&#20805;&#30340;&#21069;&#26223;&#26694;&#26550;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290; &#20294;&#26159;&#65292;&#22266;&#26377;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#20887;&#20313;&#20449;&#24687;&#21644;&#19981;&#23436;&#25972;&#19977;&#20803;&#32452;&#35782;&#21035;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24335;&#35282;&#24230;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;IPED&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; &#25105;&#20204;&#30340;&#26080;&#20998;&#31867;&#22120;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#38544;&#24335;&#31574;&#30053;&#65292;&#20351;&#29992;&#22359;&#35206;&#30422;&#23436;&#25104;&#34920;&#26684;&#65292;&#36991;&#20813;&#20102;&#26174;&#24335;&#26631;&#35760;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290; &#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#32467;&#26500;&#65292;&#22359;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#19982;&#25105;&#20204;&#30340;&#38544;&#24335;&#36879;&#35270;&#21512;&#20316;&#65292;&#24182;&#26377;&#25928;&#22320;&#35268;&#36991;&#20102;&#20887;&#20313;&#20449;&#24687;&#24178;&#25200;&#12290; &#20004;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;I
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00808v1 Announce Type: cross  Abstract: Relational triple extraction is a fundamental task in the field of information extraction, and a promising framework based on table filling has recently gained attention as a potential baseline for entity relation extraction. However, inherent shortcomings such as redundant information and incomplete triple recognition remain problematic. To address these challenges, we propose an Implicit Perspective for relational triple Extraction based on Diffusion model (IPED), an innovative approach for extracting relational triples. Our classifier-free solution adopts an implicit strategy using block coverage to complete the tables, avoiding the limitations of explicit tagging methods. Additionally, we introduce a generative model structure, the block-denoising diffusion model, to collaborate with our implicit perspective and effectively circumvent redundant information disruptions. Experimental results on two popular datasets demonstrate that I
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#25552;&#21319;&#20113;&#31471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#65292;&#23588;&#20854;&#22312;&#23454;&#29616;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#26377;&#26174;&#33879;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.00807</link><description>&lt;p&gt;
&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00807
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#25552;&#21319;&#20113;&#31471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#65292;&#23588;&#20854;&#22312;&#23454;&#29616;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#26377;&#26174;&#33879;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#31867;&#21033;&#29992;Transformer&#32593;&#32476;&#26500;&#24314;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#35782;&#21035;&#12289;&#24635;&#32467;&#12289;&#32763;&#35793;&#12289;&#39044;&#27979;&#21644;&#29983;&#25104;&#35821;&#35328;&#12290;LLMs&#25215;&#35834;&#25913;&#21464;&#31038;&#20250;&#65292;&#28982;&#32780;&#35757;&#32451;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#35821;&#20041;&#21521;&#37327;&#25628;&#32034;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25628;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#19982;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#26041;&#27861;&#19981;&#21516;&#65292;&#35821;&#20041;&#25628;&#32034;&#21033;&#29992;&#21333;&#35789;&#30340;&#21547;&#20041;&#21644;&#19978;&#19979;&#25991;&#26469;&#29702;&#35299;&#26597;&#35810;&#32972;&#21518;&#30340;&#24847;&#22270;&#65292;&#24182;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;Elasticsearch&#26159;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#20041;&#25628;&#32034;&#65292;&#26159;&#19968;&#20010;&#19987;&#20026;&#32034;&#24341;&#21644;&#25628;&#32034;&#22823;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#21487;&#25193;&#23637;&#21644;&#31283;&#20581;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35821;&#20041;&#25628;&#32034;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00807v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are a class of generative AI models built using the Transformer network, capable of leveraging vast datasets to identify, summarize, translate, predict, and generate language. LLMs promise to revolutionize society, yet training these foundational models poses immense challenges. Semantic vector search within large language models is a potent technique that can significantly enhance search result accuracy and relevance. Unlike traditional keyword-based search methods, semantic search utilizes the meaning and context of words to grasp the intent behind queries and deliver more precise outcomes. Elasticsearch emerges as one of the most popular tools for implementing semantic search an exceptionally scalable and robust search engine designed for indexing and searching extensive datasets. In this article, we delve into the fundamentals of semantic search and explore how to harness Elasticsearch and Transformer m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#20132;&#20114;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#38656;&#27714;&#65292;&#25345;&#32493;&#25913;&#21892;&#20135;&#21697;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00806</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Enhanced User Interaction in Operating Systems through Machine Learning Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00806
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#20132;&#20114;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#38656;&#27714;&#65292;&#25345;&#32493;&#25913;&#21892;&#20135;&#21697;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#26159;&#21542;&#33021;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#30340;&#20132;&#20114;&#34892;&#20026;&#65292;&#20174;&#32780;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#34394;&#25311;&#25512;&#33616;A/B&#27979;&#35797;&#22330;&#26223;&#65292;&#24110;&#21161;&#25512;&#33616;&#30740;&#31350;&#30340;&#24212;&#29992;&#26159;&#19968;&#20010;&#36843;&#20999;&#12289;&#37325;&#35201;&#24182;&#20855;&#26377;&#32463;&#27982;&#20215;&#20540;&#30340;&#38382;&#39064;&#12290;&#20132;&#20114;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#33021;&#20026;&#20135;&#21697;&#21644;&#26381;&#21153;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#36825;&#31181;&#20010;&#24615;&#21270;&#26381;&#21153;&#21487;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#24544;&#35802;&#24230;&#12290;&#27492;&#22806;&#65292;&#20132;&#20114;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#33391;&#22909;&#30340;&#29992;&#25143;&#30028;&#38754;&#21644;&#20132;&#20114;&#20307;&#39564;&#26469;&#29702;&#35299;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#30475;&#27861;&#21644;&#38656;&#27714;&#65292;&#28982;&#21518;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#21644;&#20248;&#21270;&#20135;&#21697;&#12290;&#36825;&#31181;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#21487;&#20197;&#25345;&#32493;&#25913;&#21892;&#20135;&#21697;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00806v1 Announce Type: cross  Abstract: With the large language model showing human-like logical reasoning and understanding ability, whether agents based on the large language model can simulate the interaction behavior of real users, so as to build a reliable virtual recommendation A/B test scene to help the application of recommendation research is an urgent, important and economic value problem. The combination of interaction design and machine learning can provide a more efficient and personalized user experience for products and services. This personalized service can meet the specific needs of users and improve user satisfaction and loyalty. Second, the interactive system can understand the user's views and needs for the product by providing a good user interface and interactive experience, and then use machine learning algorithms to improve and optimize the product. This iterative optimization process can continuously improve the quality and performance of the produc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00804</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#25581;&#31034;&#23458;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Uncovering Customer Issues through Topological Natural Language Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#23458;&#25143;&#26381;&#21153;&#35831;&#27714;&#12290;&#23613;&#31649;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#27880;&#37322;&#31995;&#32479;&#26469;&#24635;&#32467;&#23458;&#25143;&#32852;&#31995;&#30340;&#20027;&#39064;&#65292;&#20294;&#28145;&#20837;&#25506;&#35752;&#27599;&#20010;&#20855;&#20307;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26469;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26631;&#35760;&#27599;&#20010;&#23458;&#25143;&#23545;&#35805;&#35760;&#24405;&#30340;&#20027;&#35201;&#38382;&#39064;&#21477;&#65292;&#24182;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#23884;&#20837;&#21521;&#37327;&#36827;&#34892;&#30333;&#21270;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#26080;&#21521;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#27599;&#20010;&#23545;&#35805;&#35760;&#24405;&#30340;&#25299;&#25169;&#29305;&#24615;&#26469;&#23450;&#20041;&#28909;&#38376;&#21644;&#26032;&#20852;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00804v1 Announce Type: cross  Abstract: E-commerce companies deal with a high volume of customer service requests daily. While a simple annotation system is often used to summarize the topics of customer contacts, thoroughly exploring each specific issue can be challenging. This presents a critical concern, especially during an emerging outbreak where companies must quickly identify and address specific issues. To tackle this challenge, we propose a novel machine learning algorithm that leverages natural language techniques and topological data analysis to monitor emerging and trending customer issues. Our approach involves an end-to-end deep learning framework that simultaneously tags the primary question sentence of each customer's transcript and generates sentence embedding vectors. We then whiten the embedding vectors and use them to construct an undirected graph. From there, we define trending and emerging issues based on the topological properties of each transcript. W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00801</link><description>&lt;p&gt;
&#33258;&#20027;&#26816;&#32034;&#65306;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Retrieval: Building an Information Retrieval System with One Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00801
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#25913;&#21464;&#20102;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#22312;&#20154;&#31867;&#33719;&#21462;&#20449;&#24687;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#29616;&#26377;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20855;&#26377;&#23396;&#31435;&#30340;&#26550;&#26500;&#21644;&#26377;&#38480;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#30452;&#25509;&#21521;&#20154;&#31867;&#25552;&#20379;&#20449;&#24687;&#36716;&#21464;&#20026;&#38388;&#25509;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38656;&#30340;&#33021;&#21147;&#21040;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#33258;&#20027;&#26816;&#32034;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32034;&#24341;&#26550;&#26500;&#23558;&#35201;&#26816;&#32034;&#30340;&#35821;&#26009;&#20869;&#21270;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#25972;&#20010;&#26816;&#32034;&#36807;&#31243;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26723;&#29983;&#25104;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31471;&#21040;&#31471;&#25191;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00801v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that S
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;Brain&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#26126;&#30830;&#25552;&#21462;&#20986;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.00800</link><description>&lt;p&gt;
&#20511;&#37492;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#30340;&#33041;&#21551;&#21457;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00800
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;Brain&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#26126;&#30830;&#25552;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25913;&#36827;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#22312;&#24320;&#28304;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#39069;&#21494;&#27169;&#22411;&#29983;&#25104;&#35745;&#21010;&#65292;&#28982;&#21518;&#20351;&#29992;&#39030;&#21494;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#24182;&#25191;&#34892;&#20197;&#33719;&#24471;&#31572;&#26696;&#65292;&#26469;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27492;&#26041;&#27861;&#19982;&#22522;&#20110;Code LLaMA 7B&#30340;&#27169;&#22411;&#30456;&#27604;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#26126;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://github.com/cyzhh/Brain&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00800v1 Announce Type: cross  Abstract: Although large language models demonstrate emergent abilities in solving math word problems, there is a challenging task in complex multi-step mathematical reasoning tasks. To improve model performance on mathematical reasoning tasks, previous work has conducted supervised fine-tuning on open-source models by improving the quality and quantity of data. In this paper, we propose a novel approach, named Brain, to imitate human thought processes to enhance mathematical reasoning abilities, using the Frontal Lobe Model to generate plans, and then employing the Parietal Lobe Model to generate code and execute to obtain answers. First, we achieve SOTA performance in comparison with Code LLaMA 7B based models through this method. Secondly, we find that plans can be explicitly extracted from natural language, code, or formal language. Our code and data are publicly available at https://github.com/cyzhh/Brain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30830;&#23450;&#26368;&#20248;&#36335;&#24452;&#38598;&#65292;&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#26469;&#32047;&#31215;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00799</link><description>&lt;p&gt;
LLM&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#25968;&#25454;&#33021;&#21147;&#36793;&#30028;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00799
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#26368;&#20248;&#36335;&#24452;&#38598;&#65292;&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#26469;&#32047;&#31215;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#23637;&#31034;&#23545;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22686;&#24378;&#24320;&#28304;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#36890;&#29992;&#30340;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#20197;&#24110;&#21161;&#20248;&#21270;&#21644;&#25299;&#23637;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#25512;&#29702;&#36335;&#24452;&#30340;&#26368;&#20248;&#36335;&#24452;&#38598;&#30830;&#23450;&#25512;&#29702;&#36335;&#24452;&#22686;&#24378;&#30340;&#33021;&#21147;&#36793;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#39564;&#35777;&#27169;&#22411;&#19981;&#21516;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#28151;&#21512;&#26469;&#32047;&#31215;&#22686;&#24378;&#65292;&#32780;&#25105;&#20204;&#30340;&#27169;&#22411;MMOS&#22312;&#26356;&#20302;&#30340;&#26500;&#24314;&#25104;&#26412;&#19979;&#23454;&#29616;&#20102;&#31995;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;SOTA&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;GSM-HARD&#24182;&#19981;&#30495;&#27491;&#22256;&#38590;&#65292;&#24403;&#20170;&#30340;LLMs&#19981;&#20877;&#32570;&#20047;&#25968;&#20540;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#29992;&#20110;&#31283;&#20581;&#24615;&#27979;&#35797;&#21644;&#25945;&#32946;&#24212;&#29992;&#30340;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00799v1 Announce Type: cross  Abstract: Large language models (LLMs) are displaying emergent abilities for math reasoning tasks,and there is a growing attention on enhancing the ability of open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability.Firstly, we determine the ability boundary of reasoning paths augmentation by identifying these paths' minimal optimal set.Secondly, we validate that different abilities of the model can be cumulatively enhanced by Mix of Minimal Optimal Sets of corresponding types of data, while our models MMOS achieve SOTA performance on series base models under much lower construction costs.Besides, we point out GSM-HARD is not really hard and today's LLMs no longer lack numerical robustness.Also, we provide an Auto Problem Generator for robustness testing and educational applications.Our code and data are publicly available
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2403.00795</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31639;&#27861;&#65306;&#19968;&#39033;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Executing Natural Language-Described Algorithms with Large Language Models: An Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00795
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#36861;&#27714;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#30340;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#36825;&#19968;&#30446;&#26631;&#30340;&#36947;&#36335;&#24050;&#32463;&#34987;&#38416;&#26126;&#12290;&#26412;&#25991;&#26088;&#22312;&#26816;&#39564;&#29616;&#26377;LLMs&#29702;&#35299;&#21644;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#12298;&#31639;&#27861;&#23548;&#35770;&#12299;&#20013;&#36873;&#21462;&#20102;&#19968;&#20010;&#31639;&#27861;&#27979;&#35797;&#38598;&#65292;&#35813;&#20070;&#26159;&#19968;&#26412;&#21253;&#21547;&#35768;&#22810;&#20195;&#34920;&#24615;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#30693;&#21517;&#25945;&#26448;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25191;&#34892;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;30&#20010;&#31639;&#27861;&#65292;&#20849;&#29983;&#25104;&#20102;300&#20010;&#38543;&#26426;&#25277;&#26679;&#23454;&#20363;&#65292;&#24182;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#29305;&#21035;&#26159;GPT-4&#31561;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#21482;&#35201;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00795v1 Announce Type: cross  Abstract: Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our f
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00794</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#24189;&#40664;&#65306;&#21033;&#29992;&#19981;&#39118;&#36259;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#24189;&#40664;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00794
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#20114;&#21160;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#36827;&#23637;&#65292;&#24189;&#40664;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#24189;&#40664;&#25991;&#26412;&#19982;&#31867;&#20284;&#38750;&#24189;&#40664;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;&#24189;&#40664;&#26816;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#24403;&#21069;LLMs&#22312;&#8220;&#21462;&#28040;&#39118;&#36259;&#8221;&#31505;&#35805;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#21028;&#26029;&#21644;&#24189;&#40664;&#26816;&#27979;&#30340;&#19979;&#28216;&#20219;&#21153;&#34913;&#37327;&#32780;&#24471;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#28151;&#21512;&#32534;&#30721;&#30340;&#33521;&#35821;-&#21360;&#22320;&#35821;&#24189;&#40664;&#25968;&#25454;&#38598;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#21457;&#29616;GPT-4&#30340;&#21512;&#25104;&#25968;&#25454;&#34987;&#21452;&#35821;&#27880;&#37322;&#21592;&#39640;&#24230;&#35780;&#20215;&#65292;&#24182;&#20026;&#24189;&#40664;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00791</link><description>&lt;p&gt;
$\textit{L+M-24}$&#65306;&#22312;ACL 2024&#24180;&#20026;&#35821;&#35328;+&#20998;&#23376;&#26500;&#24314;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;-&#20998;&#23376;&#27169;&#22411;&#24050;&#25104;&#20026;&#20998;&#23376;&#21457;&#29616;&#21644;&#29702;&#35299;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#23376;-&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26377;&#20197;&#19979;&#20960;&#31181;&#31867;&#22411;&#65306;1) &#23567;&#35268;&#27169;&#19988;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#25235;&#21462;&#65292;2) &#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#19988;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#29486;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#26469;&#26500;&#24314;&#65292;3) &#36890;&#36807;&#23558;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#27169;&#26495;&#32780;&#26500;&#24314;&#12290;&#22312;&#26412;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#21019;&#24314;&#30340;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;$\textit{L+M-24}$&#26088;&#22312;&#38598;&#20013;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#39033;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00791v1 Announce Type: cross  Abstract: Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the $\textit{L+M-24}$ dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, $\textit{L+M-24}$ is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#25216;&#26415;&#25552;&#20379;&#26356;&#26131;&#35835;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00788</link><description>&lt;p&gt;
PRECISE&#26694;&#26550;&#65306;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#20197;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#23454;&#29616;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
PRECISE Framework: GPT-based Text For Improved Readability, Reliability, and Understandability of Radiology Reports For Patient-Centered Care
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#25216;&#26415;&#25552;&#20379;&#26356;&#26131;&#35835;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;OpenAI&#30340;GPT-4&#26469;&#22686;&#24378;&#24739;&#32773;&#21442;&#19982;&#24230;&#65292;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26356;&#26131;&#35835;&#30340;&#20845;&#24180;&#32423;&#38405;&#35835;&#27700;&#24179;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#35813;&#26694;&#26550;&#22312;500&#20221;&#25253;&#21578;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#22312;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#32479;&#35745;&#20998;&#26512;&#35777;&#23454;&#20102;PRECISE&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20915;&#31574;&#20013;&#24515;&#30340;&#25252;&#29702;&#20132;&#20184;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00788v1 Announce Type: cross  Abstract: This study introduces and evaluates the PRECISE framework, utilizing OpenAI's GPT-4 to enhance patient engagement by providing clearer and more accessible chest X-ray reports at a sixth-grade reading level. The framework was tested on 500 reports, demonstrating significant improvements in readability, reliability, and understandability. Statistical analyses confirmed the effectiveness of the PRECISE approach, highlighting its potential to foster patient-centric care delivery in healthcare decision-making.
&lt;/p&gt;</description></item><item><title>BERT&#30340;&#24341;&#20837;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#20102;&#31361;&#30772;&#65292;&#30740;&#31350;&#32773;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2403.00784</link><description>&lt;p&gt;
&#21033;&#29992;BERT&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65306;&#35843;&#30740;&#12289;&#24212;&#29992;&#12289;&#36164;&#28304;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00784
&lt;/p&gt;
&lt;p&gt;
BERT&#30340;&#24341;&#20837;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#20102;&#31361;&#30772;&#65292;&#30740;&#31350;&#32773;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#38382;&#39064;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#26368;&#21021;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#23427;&#20204;&#39034;&#24207;&#25110;&#21333;&#21521;&#24615;&#36136;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#38590;&#20197;&#25429;&#25417;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#20174;&#21464;&#21387;&#22120;&#65288;BERT&#65289;&#20013;&#24341;&#20837;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#25552;&#20379;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29702;&#35299;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23558;BERT&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#12290;&#22240;&#27492;&#65292;&#19968;&#39033;&#20851;&#27880;&#23558;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#22914;BERT&#24212;&#29992;&#20110;IR&#30340;&#26222;&#36941;&#26041;&#27861;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#35843;&#26597;&#23545;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#26377;&#29992;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#35843;&#26597;&#37325;&#26032;&#23457;&#35270;&#20102;&#21508;&#31181;&#22522;&#20110;BERT&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00784v1 Announce Type: cross  Abstract: Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wid
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ploutos&#65292;&#19968;&#20010;&#26032;&#22411;&#37329;&#34701;LLM&#26694;&#26550;&#65292;&#36890;&#36807;PloutosGen&#21644;PloutosGPT&#28789;&#27963;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.00782</link><description>&lt;p&gt;
Ploutos&#65306;&#22522;&#20110;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ploutos: Towards interpretable stock movement prediction with financial large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ploutos&#65292;&#19968;&#20010;&#26032;&#22411;&#37329;&#34701;LLM&#26694;&#26550;&#65292;&#36890;&#36807;PloutosGen&#21644;PloutosGPT&#28789;&#27963;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24320;&#36767;&#20102;&#35768;&#22810;&#39046;&#22495;&#30340;&#26032;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#37329;&#34701;&#25237;&#36164;&#39046;&#22495;&#20013;&#65292;LLMs &#30340;&#23436;&#25972;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#21033;&#29992;&#12290;&#23545;&#20110;&#37327;&#21270;&#37329;&#34701;&#30340;&#20856;&#22411;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22312;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;&#20013;&#28789;&#27963;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;&#20256;&#32479;&#26041;&#27861;&#32570;&#20047;&#28165;&#26224;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#39044;&#27979;&#29702;&#30001;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Ploutos&#65292;&#19968;&#20010;&#30001; PloutosGen &#21644; PloutosGPT &#32452;&#25104;&#30340;&#26032;&#22411;&#37329;&#34701;LLM&#26694;&#26550;&#12290;PloutosGen &#21253;&#21547;&#22810;&#20010;&#20027;&#35201;&#19987;&#23478;&#65292;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#30340;&#27169;&#24577;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#25968;&#20540;&#65292;&#24182;&#20174;&#19981;&#21516;&#35282;&#24230;&#25552;&#20379;&#37327;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518; PloutosGPT &#32467;&#21512;&#23427;&#20204;&#30340;&#35265;&#35299;&#21644;&#39044;&#27979;&#65292;&#29983;&#25104;&#21487;&#35299;&#37322;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00782v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have opened new pathways for many domains. However, the full potential of LLMs in financial investments remains largely untapped. There are two main challenges for typical deep learning-based methods for quantitative finance. First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and interpretability, which impedes their application in scenarios where the justification for predictions is essential. To solve the above challenges, we propose Ploutos, a novel financial LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen contains multiple primary experts that can analyze different modal data, such as text and numbers, and provide quantitative strategies from different perspectives. Then PloutosGPT combines their insights and predictions and generates interpretable rationales. To g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#20102;&#21306;&#22495;&#36890;&#36135;&#33192;&#32960;&#30340;&#19978;&#21319;&#21644;&#19979;&#38477;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#31038;&#20132;&#32593;&#32476;&#35752;&#35770;&#23545;&#36890;&#36135;&#33192;&#32960;&#39044;&#26399;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.00774</link><description>&lt;p&gt;
&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#36827;&#34892;&#21306;&#22495;&#36890;&#36135;&#33192;&#32960;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regional inflation analysis using social network data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#20102;&#21306;&#22495;&#36890;&#36135;&#33192;&#32960;&#30340;&#19978;&#21319;&#21644;&#19979;&#38477;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#31038;&#20132;&#32593;&#32476;&#35752;&#35770;&#23545;&#36890;&#36135;&#33192;&#32960;&#39044;&#26399;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36135;&#33192;&#32960;&#26159;&#24433;&#21709;&#20219;&#20309;&#22269;&#23478;&#21644;&#22320;&#21306;&#20154;&#21475;&#30340;&#26368;&#37325;&#35201;&#30340;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#20043;&#19968;&#12290;&#36890;&#36135;&#33192;&#32960;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#36890;&#36135;&#33192;&#32960;&#39044;&#26399;&#12290;&#35768;&#22810;&#22830;&#34892;&#22312;&#23454;&#26045;&#20197;&#36890;&#36135;&#33192;&#32960;&#30446;&#26631;&#20026;&#26680;&#24515;&#30340;&#36135;&#24065;&#25919;&#31574;&#26102;&#32771;&#34385;&#21040;&#36825;&#19968;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;Vkontakte&#31038;&#20132;&#32593;&#32476;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#28041;&#21450;&#36890;&#36135;&#33192;&#32960;&#19978;&#21319;&#21644;&#19979;&#38477;&#36235;&#21183;&#30340;&#20869;&#23481;&#65288;&#20197;&#37122;&#26408;&#26031;&#20811;&#22320;&#21306;&#20026;&#20363;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00774v1 Announce Type: cross  Abstract: Inflation is one of the most important macroeconomic indicators that have a great impact on the population of any country and region. Inflation is influenced by range of factors, one of which is inflation expectations. Many central banks take this factor into consideration while implementing monetary policy within the inflation targeting regime. Nowadays, a lot of people are active users of the Internet, especially social networks. There is a hypothesis that people search, read, and discuss mainly only those issues that are of particular interest to them. It is logical to assume that the dynamics of prices may also be in the focus of user discussions. So, such discussions could be regarded as an alternative source of more rapid information about inflation expectations. This study is based on unstructured data from Vkontakte social network to analyze upward and downward inflationary trends (on the example of the Omsk region). The sample
&lt;/p&gt;</description></item><item><title>ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00510</link><description>&lt;p&gt;
ROME: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35760;&#24518;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00510
&lt;/p&gt;
&lt;p&gt;
ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#29992;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#22797;&#21046;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#38271;&#24230;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#35760;&#24518;&#21270;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#35268;&#27169;&#24040;&#22823;&#19988;&#20854;&#39044;&#22788;&#29702;&#32791;&#26102;&#12290;&#20026;&#20102;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25506;&#32034;&#35760;&#24518;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#39318;&#20808;&#23558;&#36873;&#23450;&#30340;&#26679;&#26412;&#20998;&#20026;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#32452;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35265;&#35299;&#27604;&#36739;&#36825;&#20004;&#32452;&#20013;&#30340;&#28436;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21253;&#25324;&#35789;&#38271;&#12289;&#35789;&#24615;&#12289;&#35789;&#39057;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#22312;&#20869;&#30340;&#22240;&#32032;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ArXiv&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;ArXivQA&#36890;&#36807;&#31185;&#23398;&#22270;&#29983;&#25104;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00231</link><description>&lt;p&gt;
Multimodal ArXiv: &#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ArXiv&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;ArXivQA&#36890;&#36807;&#31185;&#23398;&#22270;&#29983;&#25104;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65292;&#20197;GPT-4V&#20026;&#20363;&#65292;&#22312;&#28041;&#21450;&#33258;&#28982;&#22330;&#26223;&#20013;&#30340;&#20855;&#20307;&#22270;&#20687;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#39046;&#22495;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#65292;&#23427;&#20204;&#22312;&#35299;&#37322;&#25277;&#35937;&#22270;&#24418;&#65288;&#20363;&#22914;&#20960;&#20309;&#24418;&#29366;&#21644;&#31185;&#23398;&#22270;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Multimodal ArXiv&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#20197;&#22686;&#24378;LVLMs&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;ArXivCap&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#28085;&#30422;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#30340;572K&#20221;ArXiv&#35770;&#25991;&#30340;6.4M&#24352;&#22270;&#20687;&#21644;3.9M&#20010;&#26631;&#39064;&#30340;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#12290;&#20511;&#37492;ArXivCap&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ArXivQA&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;GPT-4V&#29983;&#25104;&#30340;&#22522;&#20110;&#31185;&#23398;&#22270;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;ArXivQA&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;LVLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#27169;&#24577;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;10.4%&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;ArXivCap&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#20174;&#35270;&#35273;&#21040;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas
&lt;/p&gt;</description></item><item><title>&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.19406</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#22320;&#29702;&#34920;&#31034;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Scaling Laws of Geographical Representation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19406
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#34987;&#35777;&#26126;&#22312;&#20854;&#38544;&#34255;&#34920;&#31034;&#20013;&#23884;&#20837;&#20102;&#22320;&#29702;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#22320;&#29702;&#30693;&#35782;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#22635;&#34917;&#29616;&#26377;&#21644;&#26368;&#36817;&#25991;&#29486;&#20043;&#38388;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#24494;&#23567;&#27169;&#22411;&#65292;&#22320;&#29702;&#30693;&#35782;&#20063;&#26159;&#21487;&#35266;&#27979;&#30340;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19406v1 Announce Type: cross  Abstract: Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.
&lt;/p&gt;</description></item><item><title>WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.19282</link><description>&lt;p&gt;
WanJuan-CC&#65306;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19282
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; WanJuan-CC&#65292;&#36825;&#26159;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26469;&#28304;&#20110;Common Crawl&#25968;&#25454;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#20026;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;Common Crawl&#25968;&#25454;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#36807;&#28388;&#12289;&#27169;&#31946;&#21435;&#37325;&#12289;&#20869;&#23481;&#23433;&#20840;&#36807;&#28388;&#21644;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#12290;&#20174;&#22823;&#32422;680&#20159;&#20010;&#21407;&#22987;&#33521;&#25991;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;22&#19975;&#20159;&#26631;&#35760;&#30340;&#23433;&#20840;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#36873;&#20986;&#20102;10&#19975;&#20159;&#26631;&#35760;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20316;&#20026;WanJuan-CC&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#28304;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;3000&#20159;&#26631;&#35760;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19982;&#25968;&#25454;&#36136;&#37327;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#12290;&#20026;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;WanJuan-CC&#35757;&#32451;&#20102;10&#20159;&#21442;&#25968;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CDQA&#65292;&#19968;&#20010;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#21160;&#24577;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#31934;&#32454;&#26679;&#26412;&#20998;&#31867;&#23454;&#29616;&#20102;&#23545;LLMs&#33021;&#21147;&#26356;&#32454;&#33268;&#30340;&#35266;&#23519;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CDQA&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19248</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#23545;&#26368;&#26032;&#25361;&#25112;&#65281;&#19968;&#20010;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CDQA&#65292;&#19968;&#20010;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#21160;&#24577;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#31934;&#32454;&#26679;&#26412;&#20998;&#31867;&#23454;&#29616;&#20102;&#23545;LLMs&#33021;&#21147;&#26356;&#32454;&#33268;&#30340;&#35266;&#23519;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CDQA&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19248v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#22914;&#20309;&#26356;&#22909;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26159;&#24403;&#21069;LLMs&#30740;&#31350;&#30340;&#28966;&#28857;&#21644;&#28909;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#30001;&#20110;&#22823;&#35268;&#27169;&#36845;&#20195;&#26356;&#26032;LLMs&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#22238;&#31572;&#26368;&#26032;&#30340;&#21160;&#24577;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#20013;&#25991;LLMs&#22238;&#31572;&#21160;&#24577;&#38382;&#39064;&#30340;&#33021;&#21147;&#25552;&#21319;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; CDQA&#65292;&#19968;&#20010;&#21253;&#21547;&#19982;&#20013;&#22269;&#20114;&#32852;&#32593;&#19978;&#26368;&#26032;&#26032;&#38395;&#30456;&#20851;&#30340;&#38382;&#31572;&#23545;&#30340;&#20013;&#25991;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20154;&#31867;&#21644;&#27169;&#22411;&#32467;&#21512;&#30340;&#27969;&#31243;&#33719;&#24471;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#31572;&#26696;&#21464;&#21270;&#39057;&#29575;&#31934;&#32454;&#20998;&#31867;&#26679;&#26412;&#65292;&#20197;&#20415;&#26356;&#32454;&#33268;&#22320;&#35266;&#23519;LLMs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#22312;CDQA&#19978;&#35780;&#20272;&#21644;&#20998;&#26512;&#20102;&#20027;&#27969;&#21644;&#20808;&#36827;&#30340;&#20013;&#25991;LLMs&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#23453;&#36149;&#30340;&#35265;&#35299;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CDQA&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19248v1 Announce Type: new  Abstract: How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further stud
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.19116</link><description>&lt;p&gt;
&#22914;&#20309;&#29702;&#35299;&#8220;&#25903;&#25345;&#8221;&#65311;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#65288;WPG&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#25512;&#26029;&#32454;&#31890;&#24230;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#65292;&#20165;&#21033;&#29992;&#31895;&#31890;&#24230;&#30340;&#21477;&#23376;-&#22270;&#20687;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;WPG&#30340;&#30740;&#31350;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#38544;&#24335;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#28145;&#23618;&#22810;&#27169;&#24577;&#35821;&#20041;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#65288;IECI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#23545;&#24314;&#27169;&#38544;&#24335;&#20851;&#31995;&#21644;&#31361;&#20986;&#26174;&#24615;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#21033;&#29992;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25216;&#26415;&#26469;&#24212;&#23545;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#26631;&#27880;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#24335;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;IECI&#65292;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;IECI&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19116v1 Announce Type: cross  Abstract: Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting findi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;</title><link>https://arxiv.org/abs/2402.18284</link><description>&lt;p&gt;
&#20247;&#21253;&#26159;&#21542;&#35753;&#24744;&#30772;&#20135;&#20102;&#65311;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18284
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24191;&#27867;&#20351;&#29992;&#20984;&#26174;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#35757;&#32451;&#27969;&#31243;&#20381;&#36182;&#20110;&#20154;&#24037;&#25490;&#24207;&#65292;&#36825;&#26159;&#19968;&#20010;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#24212;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#27010;&#29575;&#25277;&#26679;&#24320;&#22987;&#65292;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#20026;&#27599;&#20010;&#36755;&#20837;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;TextRank&#21644;ISODATA&#31639;&#27861;&#65292;&#22522;&#20110;&#35821;&#20041;&#23545;&#36825;&#20123;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#21644;&#32858;&#31867;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#23398;&#20064;&#25490;&#21517;&#24182;&#20248;&#21270;&#25105;&#20204;&#30340;&#29983;&#25104;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#20351;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;BLEU&#12289;GLEU&#21644;METEOR&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25163;&#21160;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#23548;&#24494;&#35843;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#21457;&#29616;&#23581;&#35797;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#23398;&#20064;&#39069;&#22806;&#19990;&#30028;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#37325;&#28857;&#22312;&#20110;&#20445;&#25345;&#20869;&#37096;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18243</link><description>&lt;p&gt;
&#23398;&#20064;&#36824;&#26159;&#33258;&#25105;&#35843;&#25972;&#65311;&#37325;&#26032;&#24605;&#32771;&#25351;&#23548;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Learning or Self-aligning? Rethinking Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#23548;&#24494;&#35843;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#21457;&#29616;&#23581;&#35797;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#23398;&#20064;&#39069;&#22806;&#19990;&#30028;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#37325;&#28857;&#22312;&#20110;&#20445;&#25345;&#20869;&#37096;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#24494;&#35843;&#65288;IFT&#65289;&#26159;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38454;&#27573;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;IFT&#22312;&#34892;&#20026;&#35268;&#33539;&#20256;&#36882;&#21644;&#39069;&#22806;&#19990;&#30028;&#30693;&#35782;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;IFT&#28508;&#22312;&#26426;&#21046;&#30340;&#29702;&#35299;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#30693;&#35782;&#24178;&#39044;&#26694;&#26550;&#65292;&#20197;&#35299;&#32806;IFT&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#19981;&#21516;&#22240;&#32032;&#30340;&#20010;&#20307;&#20998;&#26512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#65292;&#36890;&#36807;IFT&#35797;&#22270;&#23398;&#20064;&#39069;&#22806;&#30340;&#19990;&#30028;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#26126;&#26174;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;IFT&#20043;&#21069;&#21644;&#20043;&#21518;&#20445;&#25345;&#20869;&#37096;&#30693;&#35782;&#19968;&#33268;&#24615;&#26159;&#23454;&#29616;&#25104;&#21151;IFT&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;IFT&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#20026;&#26368;&#26032;&#21644;&#28508;&#22312;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18243v1 Announce Type: new  Abstract: Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.17897</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#20307;&#35770;&#20013;&#26032;&#27010;&#24565;&#25918;&#32622;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Language Model based Framework for New Concept Placement in Ontologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#26412;&#20307;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65306;&#36793;&#25628;&#32034;&#65292;&#21363;&#25214;&#21040;&#35201;&#25554;&#20837;&#30340;&#20505;&#36873;&#20301;&#32622;&#38598;&#65288;&#21363;&#27010;&#24565;&#20043;&#38388;&#30340;&#21253;&#21547;&#20851;&#31995;&#65289;&#65292;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#65292;&#21033;&#29992;&#26412;&#20307;&#32467;&#26500;&#29983;&#25104;&#21644;&#22686;&#24378;&#36793;&#20505;&#36873;&#65292;&#20197;&#21450;&#36793;&#36873;&#25321;&#65292;&#26368;&#32456;&#30830;&#23450;&#35201;&#25918;&#32622;&#30340;&#36793;&#12290;&#22312;&#25152;&#26377;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#20854;&#20013;&#24212;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#22914;BERT&#29992;&#20110;&#36793;&#25628;&#32034;&#65292;&#37319;&#29992;&#22522;&#20110;BERT&#24494;&#35843;&#30340;&#22810;&#26631;&#31614;&#36793;&#20132;&#21449;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;GPT&#31995;&#21015;&#12289;FLAN-T5 &#21644; Llama 2 &#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#36793;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#21019;&#24314;&#30340;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17897v1 Announce Type: new  Abstract: We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our fram
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#26368;&#20339;&#23398;&#20064;&#30340;&#29702;&#35770;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#21387;&#32553;&#27604;&#29575;&#26469;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#65292;&#26681;&#25454;&#23398;&#20064;&#23450;&#24459;&#25581;&#31034;&#20102;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#30340;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17759</link><description>&lt;p&gt;
&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#26368;&#20339;&#23398;&#20064;&#30340;&#29702;&#35770;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#21387;&#32553;&#27604;&#29575;&#26469;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#65292;&#26681;&#25454;&#23398;&#20064;&#23450;&#24459;&#25581;&#31034;&#20102;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#30340;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23398;&#20064;&#30340;&#19968;&#33324;&#21407;&#21017;&#65292;&#26088;&#22312;&#20943;&#23569;&#23454;&#29616;&#20248;&#36234;&#24615;&#33021;&#25152;&#38656;&#30340;&#35757;&#32451;&#27493;&#39588;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;LMs&#30340;&#26368;&#20339;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22312;&#8220;LM&#35757;&#32451;&#20316;&#20026;&#26080;&#25439;&#21387;&#32553;&#8221;&#35270;&#22270;&#20013;&#26368;&#22823;&#21270;&#25968;&#25454;&#21387;&#32553;&#27604;&#29575;&#26469;&#20248;&#21270;LM&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#23450;&#29702;&#65292;&#21517;&#20026;&#23398;&#20064;&#23450;&#24459;&#65292;&#25581;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#30446;&#26631;&#19979;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#20998;&#31867;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#23450;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;LMs&#30340;&#26368;&#20339;&#23398;&#20064;&#20027;&#35201;&#28304;&#20110;&#25913;&#21892;LMs&#30340;&#32553;&#25918;&#23450;&#24459;&#30340;&#31995;&#25968;&#65292;&#20026;&#35774;&#35745;&#23454;&#38469;&#23398;&#20064;&#21152;&#36895;&#26041;&#27861;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#21644;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://aka.ms/LearningLaw&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17759v1 Announce Type: new  Abstract: This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.17512</link><description>&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#26041;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36890;&#36807;&#23450;&#20041;&#28508;&#22312;&#21521;&#37327;&#30340;&#27880;&#24847;&#21147;&#26469;&#23558;&#20854;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#26631;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#30340;&#8220;Latte Transformer&#8221;&#27169;&#22411;&#21487;&#29992;&#20110;&#21452;&#21521;&#21644;&#21333;&#21521;&#20219;&#21153;&#65292;&#22240;&#26524;&#29256;&#26412;&#20801;&#35768;&#19968;&#31181;&#22312;&#25512;&#29702;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#36882;&#24402;&#23454;&#29616;&#12290;&#26631;&#20934;transformer&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#65292;&#32780;Latte Transformer&#35745;&#31639;&#19979;&#19968;&#20010;&#26631;&#35760;&#25152;&#38656;&#30340;&#26102;&#38388;&#26159;&#24658;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#35777;&#34920;&#29616;&#21487;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#27880;&#24847;&#21147;&#23454;&#38469;&#21487;&#34892;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#33268;&#24615;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;GPT3.5&#31561;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.17411</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#20174;&#40657;&#30418;&#35282;&#24230;&#25506;&#32034;LLMs&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#33268;&#24615;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;GPT3.5&#31561;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22914;&#20170;&#65292;&#21830;&#19994;&#21644;&#24320;&#28304;&#23398;&#26415;LLM&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#27969;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;LLM&#19968;&#33268;&#24615;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;LLM&#30740;&#31350;&#21644;&#37096;&#32626;&#30340;&#21508;&#20010;&#38454;&#27573;&#20013;&#65292;&#20854;&#20869;&#37096;&#21442;&#25968;&#21644;&#33021;&#21147;&#24212;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#19968;&#38382;&#39064;&#23384;&#22312;&#20110;&#24037;&#19994;&#21644;&#23398;&#26415;&#39046;&#22495;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#32791;&#26102;&#19988;&#21171;&#21147;&#23494;&#38598;&#65292;&#36824;&#26377;&#39069;&#22806;&#30340;&#20108;&#27425;&#37096;&#32626;&#25104;&#26412;&#65292;&#23548;&#33268;&#32463;&#27982;&#21644;&#26102;&#38388;&#25439;&#22833;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#19968;&#33268;&#24615;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#20010;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20027;&#35201;&#23454;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;LightGBM&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;&#65288;&#22914;ROUGE&#12289;BLEU&#12289;METEOR&#65289;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;&#26368;&#32456;&#32467;&#26524;&#36229;&#36807;&#20102;&#20154;&#24037;&#35780;&#20272;&#20197;&#21450;GPT3.5&#21644;&#20854;&#20182;&#27169;&#22411;&#22312;&#20027;&#35201;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17411v1 Announce Type: new  Abstract: Nowadays both commercial and open-source academic LLM have become the mainstream models of NLP. However, there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive, and there is also an additional cost of secondary deployment, resulting in economic and time losses. To fill this gap, we build an LLM consistency task dataset and design several baselines. Additionally, we choose models of diverse scales for the main experiments. Specifically, in the LightGBM experiment, we used traditional NLG metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training. The final result exceeds the manual evaluation and GPT3.5 as well as other models in the main experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;LLMs&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#33021;&#21147;&#65292;&#20294;&#19982;&#23436;&#20840;&#36164;&#28304;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#20173;&#22788;&#20110;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.17256</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#65306;&#30740;&#31350;LLMs&#22312;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#19978;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;LLMs&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#33021;&#21147;&#65292;&#20294;&#19982;&#23436;&#20840;&#36164;&#28304;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#20173;&#22788;&#20110;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#26088;&#22312;&#26816;&#26597;&#29992;&#25143;&#30340;&#26597;&#35810;&#26159;&#21542;&#36229;&#20986;&#31995;&#32479;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#65292;&#36825;&#23545;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#30340;&#27491;&#24120;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#24494;&#35843;&#21306;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;OOD&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#23545;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#27010;&#36848;&#20102;LLM&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#33021;&#21147;&#65292;&#20294;&#19982;&#23436;&#20840;&#36164;&#28304;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#20173;&#22788;&#20110;&#21155;&#21183;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#38468;&#21152;&#20998;&#26512;&#23454;&#39564;&#65292;&#26412;&#25991;&#26356;&#28145;&#20837;&#22320;&#35752;&#35770;&#21644;&#24635;&#32467;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17256v1 Announce Type: new  Abstract: Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16379</link><description>&lt;p&gt;
&#29992;&#31995;&#32479;&#33258;&#26657;&#27491;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving LLM-based Machine Translation with Systematic Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16379
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#20180;&#32454;&#35780;&#20272;&#21457;&#29616;&#65292;LLMs&#29983;&#25104;&#30340;&#32763;&#35793;&#20173;&#28982;&#21253;&#21547;&#22810;&#20010;&#38169;&#35823;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23558;&#36825;&#31181;&#38169;&#35823;&#20449;&#24687;&#21453;&#39304;&#21040;LLMs&#20013;&#21487;&#20197;&#23454;&#29616;&#33258;&#26657;&#27491;&#65292;&#24182;&#25913;&#21892;&#32763;&#35793;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#20195;&#34920;&#20102;&#22312;&#36825;&#19968;&#26041;&#21521;&#19978;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#25105;&#20204;&#30340;&#33258;&#26657;&#27491;&#26694;&#26550;&#25104;&#21151;&#22320;&#24110;&#21161;LLMs&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36824;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#36824;&#26159;&#22260;&#32469;&#20854;&#20182;&#35821;&#35328;&#65307;2&#65289;TER&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
&lt;/p&gt;</description></item><item><title>FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16107</link><description>&lt;p&gt;
FuseChat&#65306;&#23545;&#35805;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FuseChat: Knowledge Fusion of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16107
&lt;/p&gt;
&lt;p&gt;
FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#29420;&#29305;&#33021;&#21147;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#20135;&#29983;&#24040;&#22823;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#31454;&#20105;&#33021;&#21147;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#26159;&#23558;&#29616;&#26377;&#30340;LLMs&#32452;&#21512;&#25104;&#26356;&#24378;&#22823;&#30340;LLM&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#26679;&#21270;&#26550;&#26500;&#65292;&#30452;&#25509;&#21442;&#25968;&#34701;&#21512;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#65292;FuseLLM&#24341;&#20837;&#20102;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25345;&#32493;&#35757;&#32451;&#23558;&#22810;&#20010;&#32467;&#26500;&#22810;&#26679;&#30340;LLM&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;LLM&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;FuseLLM&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;LLM&#30340;&#34701;&#21512;&#65292;&#29983;&#25104;&#20102;FuseChat&#12290;FuseChat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#21644;&#35268;&#27169;&#19981;&#21516;&#30340;&#28304;LLMs&#36827;&#34892;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;&#25506;&#31350;&#20219;&#21153;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36880;&#23618;&#32534;&#30721;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;LLMs&#26356;&#20542;&#21521;&#20110;&#22312;&#19978;&#23618;&#32534;&#30721;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.16061</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;&#65311;&#19968;&#39033;&#36880;&#23618;&#25506;&#31350;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;&#25506;&#31350;&#20219;&#21153;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36880;&#23618;&#32534;&#30721;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;LLMs&#26356;&#20542;&#21521;&#20110;&#22312;&#19978;&#23618;&#32534;&#30721;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#20107;&#23454;&#21644;&#22788;&#29702;&#19978;&#19979;&#25991;&#30693;&#35782;&#26041;&#38754;&#30340;&#24341;&#20154;&#27880;&#30446;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#36880;&#23618;&#32534;&#30721;&#30693;&#35782;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#36825;&#25361;&#25112;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#31350;&#20219;&#21153;&#26469;&#39318;&#27425;&#30740;&#31350;LLMs&#36880;&#23618;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#26500;&#24314;&#25506;&#31350;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#19982;&#21508;&#31181;&#20107;&#23454;&#30456;&#23545;&#24212;&#30340;&#22810;&#26679;&#19988;&#36830;&#36143;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;$\mathcal V$-usable&#20449;&#24687;&#20316;&#20026;&#39564;&#35777;&#25351;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#36328;&#19981;&#21516;&#23618;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#26377;&#20914;&#31361;&#21644;&#26032;&#33719;&#24471;&#30693;&#35782;&#26041;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#65306;&#65288;1&#65289;&#26356;&#20542;&#21521;&#20110;&#22312;&#19978;&#23618;&#32534;&#30721;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65307;&#65288;2&#65289;&#20027;&#35201;&#22312;&#19982;&#30693;&#35782;&#30456;&#20851;&#30340;&#23454;&#20307;&#26631;&#35760;&#20869;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16061v1 Announce Type: new  Abstract: Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.15708</link><description>&lt;p&gt;
&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#26597;&#35810;&#35821;&#20041;&#30340;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Augmentation by Decoding Semantics from Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#29992;&#20110;&#32454;&#21270;&#35821;&#20041;&#19981;&#20934;&#30830;&#26597;&#35810;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#26597;&#35810;&#25193;&#23637;&#20381;&#36182;&#20110;&#20174;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#12289;&#28508;&#22312;&#30456;&#20851;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22914;&#26524;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#36136;&#37327;&#36739;&#20302;&#65292;&#21017;&#26597;&#35810;&#25193;&#23637;&#30340;&#26377;&#25928;&#24615;&#20063;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Brain-Aug&#65292;&#36890;&#36807;&#23558;&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#26597;&#35810;&#20013;&#26469;&#22686;&#24378;&#26597;&#35810;&#12290;Brain-Aug&#20351;&#29992;&#20102;&#22312;&#33041;&#20449;&#21495;&#20449;&#24687;&#26500;&#24314;&#30340;&#25552;&#31034;&#21644;&#38754;&#21521;&#25490;&#21517;&#30340;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#21407;&#22987;&#26597;&#35810;&#30340;&#24310;&#32493;&#37096;&#20998;&#12290;&#23545;fMRI&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Brain-Aug&#29983;&#25104;&#30340;&#26597;&#35810;&#22312;&#35821;&#20041;&#19978;&#26356;&#20934;&#30830;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#12290;&#33041;&#20449;&#21495;&#24102;&#26469;&#30340;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#27169;&#31946;&#26597;&#35810;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15302</link><description>&lt;p&gt;
&#26377;&#20851;LLMs&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#65288;&#19981;&#36947;&#24503;&#65289;&#31243;&#24230;&#26377;&#22810;&#39640;&#65311;&#25581;&#31034;&#23433;&#20840;&#38450;&#25252;&#26639;&#23545;&#26377;&#23475;&#26597;&#35810;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#27450;&#39575;&#65292;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65292;&#21253;&#25324;&#8220;&#36234;&#29425;&#8221;&#25216;&#26415;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#65306;LLMs&#22312;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#20197;&#20266;&#20195;&#30721;&#12289;&#31243;&#24207;&#25110;&#36719;&#20214;&#29255;&#27573;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#26102;&#65292;&#26377;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26222;&#36890;&#25991;&#26412;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TechHazardQA&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24212;&#20197;&#25991;&#26412;&#21644;&#20197;&#25351;&#20196;&#20026;&#20013;&#24515;&#26684;&#24335;&#65288;&#20363;&#22914;&#20266;&#20195;&#30721;&#65289;&#22238;&#31572;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#26088;&#22312;&#35782;&#21035;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#26597;&#35810;&#20102;&#19968;&#31995;&#21015;LLMs-- Llama-2-13b&#65292;Llama-2-7b&#65292;Mistral-V2&#21644;Mistral 8X7B--&#24182;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#25991;&#26412;&#21644;&#25351;&#20196;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12151</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Causal Language Models Perform Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12151
&lt;/p&gt;
&lt;p&gt;
Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;LLM&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#39069;&#22806;&#35757;&#32451;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#24050;&#32463;&#26174;&#31034;&#20986;&#24456;&#22823;&#25913;&#36827;&#65292;&#28982;&#32780;&#65292;&#23548;&#33268;&#26377;&#25928;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#26426;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#32780;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22914;&#20309;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#65292;&#24182;&#22312;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11924</link><description>&lt;p&gt;
MRKE&#65306;&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23545;LLMs&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;MHQA&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30495;&#27491;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24453;&#25506;&#35752;&#12290;&#30446;&#21069;&#30340;LLM QA&#35780;&#20272;&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65292;&#35780;&#20272;&#25968;&#25454;&#21487;&#33021;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#32473;LLMs&#65307;&#20197;&#21450;2&#65289;&#24573;&#35270;&#25512;&#29702;&#38142;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#26159;&#22522;&#20110;&#32534;&#36753;&#29616;&#25104;HotpotQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#12289;&#21069;&#25152;&#26410;&#26377;&#30340;&#30693;&#35782;&#30340;&#31532;&#19968;&#20010;QA&#22522;&#20934;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#20197;&#23376;&#38382;&#39064;&#21644;&#20013;&#38388;&#31572;&#26696;&#30340;&#24418;&#24335;&#23545;&#24212;&#20110;&#22810;&#36339;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#65292;1&#65289;LLMs&#22312;&#21407;&#22987;HotpotQA&#21644;&#25105;&#20204;&#32534;&#36753;&#30340;&#25968;&#25454;&#20043;&#38388;&#26174;&#31034;&#24615;&#33021;&#24046;&#36317;&#65292;&#35748;&#20026;&#24403;&#21069;&#30340;MHQA&#22522;&#20934;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#38590;&#20197;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11924v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11094</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35789;&#23884;&#20837;&#65306;LLMs&#26159;&#21542;&#25552;&#20379;&#26032;&#30340;&#19996;&#35199;&#65311;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings Revisited: Do LLMs Offer Something New?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35789;&#23884;&#20837;&#23545;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20852;&#36215;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#26032;&#30340;&#21333;&#35789;/&#21477;&#23376;/&#25991;&#26723;&#23884;&#20837;&#27169;&#22411;&#12290;&#23613;&#31649;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;&#24615;&#33021;&#30340;&#25552;&#21319;&#20165;&#20165;&#26159;&#22240;&#20026;&#35268;&#27169;&#36824;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#24213;&#23618;&#23884;&#20837;&#19982;&#21477;&#23376;-BERT&#65288;SBERT&#65289;&#25110;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;USE&#65289;&#20043;&#31867;&#30340;&#20256;&#32479;&#32534;&#30721;&#27169;&#22411;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#19982;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#23427;&#20204;&#30340;&#28508;&#22312;&#21521;&#37327;&#35821;&#20041;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;LLMs&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20063;&#39640;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#19968;&#20123;LLMs&#20542;&#21521;&#20110;&#20135;&#29983;&#35789;&#23884;&#20837;si&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.09282</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;NLP&#20219;&#21153;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#30340;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#20026;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32454;&#24605;&#36830;&#24819;&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#20174;GPT-4&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25913;&#36827;&#36739;&#23567;&#27169;&#22411;BERT&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65306;&#39318;&#20808;&#20351;&#29992;GPT-4&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#33976;&#39311;&#21644;&#21407;&#22987;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#30340;&#32452;&#21512;&#23545;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;F1&#20998;&#25968;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09282v1 Announce Type: new Abstract: The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The 
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08939</link><description>&lt;p&gt;
&#35770;&#25454;&#39034;&#24207;&#22312;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#36215;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Premise Order Matters in Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08939
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#33030;&#24369;&#24615;&#65306;&#23613;&#31649;&#36825;&#31181;&#39034;&#24207;&#19981;&#20250;&#25913;&#21464;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;LLMs&#23545;&#20110;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#33030;&#24369;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#35770;&#25454;&#39034;&#24207;&#19982;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#26102;&#65292;LLMs&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#35770;&#25454;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#65288;&#32780;&#19981;&#26159;&#38543;&#26426;&#39034;&#24207;&#65289;&#20250;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#23545;&#28436;&#32462;&#25512;&#29702;&#20013;&#35770;&#25454;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#35770;&#25454;&#39034;&#24207;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#36229;&#36807;30&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#22522;&#20110;GSM8K&#30340;&#22522;&#20934;&#27979;&#35797;R-GSM&#26469;&#30740;&#31350;&#39034;&#24207;&#25928;&#24212;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07787</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#35780;&#20272;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#34920;&#36798;&#20197;&#29702;&#35299;&#24773;&#24863;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#21152;&#24378;ABSA&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20381;&#36182;&#21644;&#32452;&#25104;&#26641;&#19978;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#12290;&#38543;&#30528;ABSA&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#21019;&#26032;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#34987;&#34701;&#20837;&#20854;&#20013;&#65288;&#20363;&#22914;&#28508;&#22312;&#22270;&#65289;&#65292;&#20294;&#36825;&#20063;&#24341;&#20837;&#20102;&#22797;&#26434;&#24615;&#21644;&#28151;&#28102;&#12290;&#30446;&#21069;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22810;&#26679;&#24615;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#38598;&#25104;&#21040;ABSA&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#65288;EMGF&#65289;&#32593;&#32476;&#65292;&#23427;&#25972;&#21512;&#20102;&#26469;&#33258;&#21477;&#27861;&#20381;&#36182;&#21644;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#20449;&#24687;&#12290;EMGF&#37197;&#22791;&#20102;&#22810;&#38170;&#28857;&#19977;&#20803;&#23398;&#20064;&#21644;&#27491;&#20132;&#25237;&#24433;&#65292;&#39640;&#25928;&#22320;&#21033;&#29992;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#32508;&#21512;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25506;&#35752;&#20102;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2402.03223</link><description>&lt;p&gt;
&#33521;&#25991;&#25552;&#31034;&#27604;&#30446;&#26631;&#35821;&#35328;&#25552;&#31034;&#26356;&#36866;&#29992;&#20110;&#22522;&#20110;NLI&#30340;&#38646;-shot&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25506;&#35752;&#20102;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24773;&#32490;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#35748;&#30693;&#25512;&#35770;&#36807;&#31243;&#26469;&#35299;&#37322;&#25991;&#23383;&#21050;&#28608;&#12290;&#27492;&#22806;&#65292;&#24773;&#32490;&#31867;&#21035;&#38598;&#21512;&#39640;&#24230;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#20363;&#22914;&#65292;&#25991;&#23398;&#20998;&#26512;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#23457;&#32654;&#24773;&#24863;&#65288;&#20363;&#22914;&#65292;&#21457;&#29616;&#26576;&#29289;&#32654;&#20029;&#65289;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21017;&#21487;&#20197;&#20174;&#32454;&#31890;&#24230;&#30340;&#38598;&#21512;&#20013;&#33719;&#21462;&#22909;&#22788;&#65288;&#20363;&#22914;&#65292;&#23558;&#24868;&#24594;&#19982;&#28902;&#24700;&#20998;&#24320;&#65289;&#65292;&#19982;&#22522;&#26412;&#24773;&#32490;&#31867;&#21035;&#30456;&#23545;&#24212;&#12290;&#36825;&#20351;&#24471;&#35813;&#20219;&#21153;&#25104;&#20026;&#20102;&#38646;-shot&#20998;&#31867;&#30340;&#19968;&#20010;&#26377;&#36259;&#39046;&#22495;&#65292;&#22312;&#36825;&#31181;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#24320;&#21457;&#26102;&#19981;&#30693;&#36947;&#26631;&#31614;&#38598;&#21512;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#20998;&#26512;&#36164;&#28304;&#37117;&#26159;&#33521;&#25991;&#30340;&#65292;&#22240;&#27492;&#65292;&#24773;&#32490;&#20998;&#26512;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#26159;&#29992;&#33521;&#25991;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#37027;&#20123;&#28041;&#21450;&#20351;&#29992;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#36825;&#32473;&#25105;&#20204;&#30041;&#19979;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25506;&#35752;&#65306;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#65292;&#25105;&#20204;&#24212;&#35813;&#29992;&#21738;&#31181;&#35821;&#35328;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This i
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02564</link><description>&lt;p&gt;
&#19968;&#20010;&#30495;&#27491;&#32852;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20998;&#21106;&#21644;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Truly Joint Neural Architecture for Segmentation and Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22810;&#35821;&#35328;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#21487;&#20197;&#35299;&#26512;&#22810;&#31181;&#35821;&#35328;&#65292;&#20294;&#23545;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#20854;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#30001;&#20110;&#36755;&#20837;&#26631;&#35760;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21644;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#20316;&#20026;&#26641;&#20013;&#33410;&#28857;&#30340;&#35821;&#35328;&#21333;&#20301;&#20107;&#20808;&#26159;&#26410;&#30693;&#30340;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#36981;&#24490;&#32852;&#21512;&#24418;&#24577;-&#21477;&#27861;&#20551;&#35774;&#65292;&#21363;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#24212;&#35813;&#22312;&#35299;&#26512;&#36807;&#31243;&#20013;&#19968;&#24182;&#35299;&#20915;&#65292;&#32780;&#19981;&#26159;&#20808;&#36827;&#34892;&#20998;&#21106;&#20877;&#36827;&#34892;&#35299;&#26512;&#30340;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#37319;&#29992;&#20005;&#26684;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#20445;&#30041;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#25552;&#20379;&#32473;&#19968;&#20010;&#22522;&#20110;&#24359;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#24076;&#20271;&#26469;&#35821;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#35821;&#35328;&#24418;&#24577;&#20016;&#23500;&#19988;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and
&lt;/p&gt;</description></item><item><title>LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02446</link><description>&lt;p&gt;
LQER: &#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#37325;&#24314;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
LQER: Low-Rank Quantization Error Reconstruction for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02446
&lt;/p&gt;
&lt;p&gt;
LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#20943;&#23569;&#65288;LQER&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#37327;&#21270;&#21644;&#20302;&#31209;&#36924;&#36817;&#26469;&#24674;&#22797;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;LQER&#21033;&#29992;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#23558;&#37327;&#21270;&#35823;&#24046;&#30340;&#22855;&#24322;&#20540;&#20998;&#24067;&#25512;&#21521;&#26399;&#26395;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;LLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#36817;&#20046;&#26080;&#25439;&#30340;W4A8&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#12289;&#32593;&#26684;&#25628;&#32034;&#25110;&#22522;&#20110;&#26799;&#24230;&#30340;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LQER&#30340;&#35745;&#31639;&#27169;&#24335;&#28040;&#38500;&#20102;&#20174;&#19981;&#35268;&#21017;&#20869;&#23384;&#20301;&#32622;&#25910;&#38598;&#39640;&#31934;&#24230;&#26435;&#37325;&#25152;&#38656;&#30340;&#19987;&#29992;Scatter&#21644;Gather&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;W4A8 LLMs&#22312;&#20845;&#20010;&#28909;&#38376;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#30828;&#20214;&#36164;&#28304;&#27604;&#39046;&#20808;&#30340;&#26368;&#26032;&#26041;&#27861;&#23569;1.36&#20493;&#12290;&#19968;&#26086;&#35770;&#25991;&#34987;&#25509;&#21463;&#65292;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.06915</link><description>&lt;p&gt;
DocFinQA&#65306;&#19968;&#20010;&#38271;&#25991;&#26412;&#36130;&#21153;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DocFinQA: A Long-Context Financial Reasoning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06915
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#39046;&#22495;&#21457;&#25381;&#20316;&#29992;&#65292;&#38656;&#35201;&#30740;&#31350;&#29616;&#23454;&#20219;&#21153;&#21644;&#25968;&#25454;&#12290;&#37329;&#34701;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#19982;&#38271;&#36798;&#25968;&#30334;&#39029;&#30340;&#25991;&#26723;&#36827;&#34892;&#20132;&#20114;&#65292;&#20294;&#22823;&#22810;&#25968;&#37329;&#34701;&#30740;&#31350;&#25968;&#25454;&#38598;&#20165;&#22788;&#29702;&#36825;&#20123;&#25991;&#26723;&#30340;&#31616;&#30701;&#25688;&#24405;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;FinQA&#25968;&#25454;&#38598;&#20013;&#30340;7,437&#20010;&#38382;&#39064;&#20013;&#22686;&#21152;&#23436;&#25972;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#23558;FinQA&#20013;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;&#19981;&#21040;700&#20010;&#35789;&#25193;&#23637;&#21040;DocFinQA&#20013;&#30340;123k&#20010;&#35789;&#12290;&#25105;&#20204;&#22312;&#26816;&#32034;&#24335;QA&#31649;&#36947;&#21644;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;DocFinQA&#20063;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;DocFinQA&#20013;&#26368;&#38271;&#25991;&#26723;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#36825;&#20123;&#25991;&#26723;&#19978;&#29305;&#21035;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06915v2 Announce Type: replace-cross  Abstract: For large language models (LLMs) to be effective in the financial domain -- where each decision can have a significant impact -- it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#30446;&#26631;&#39118;&#26684;&#30340;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#22522;&#20110;&#25351;&#20196;&#30340;&#27169;&#22411;&#38590;&#20197;&#37325;&#26032;&#29616;&#20986;&#20316;&#32773;&#29305;&#23450;&#39118;&#26684;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.17242</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#20070;&#20889;&#39118;&#26684;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Text in Arbitrary Writing Styles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#30446;&#26631;&#39118;&#26684;&#30340;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#22522;&#20110;&#25351;&#20196;&#30340;&#27169;&#22411;&#38590;&#20197;&#37325;&#26032;&#29616;&#20986;&#20316;&#32773;&#29305;&#23450;&#39118;&#26684;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#22312;&#39118;&#26684;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20219;&#21153;&#19978;&#65292;&#20363;&#22914;&#27169;&#20223;&#22810;&#20135;&#25991;&#23398;&#20316;&#32773;&#30340;&#39118;&#26684;&#65292;&#29983;&#25104;&#27491;&#24335;&#25110;&#38750;&#27491;&#24335;&#25991;&#26412;&#65292;&#24182;&#20943;&#36731;&#29983;&#25104;&#25991;&#26412;&#30340;&#26377;&#23475;&#24615;&#12290;&#36825;&#20123;&#39118;&#26684;&#30340;&#20016;&#23500;&#23637;&#31034;&#21487;&#29992;&#65292;&#24182;&#19988;&#22240;&#27492;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#27169;&#20223;&#23427;&#20204;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#25552;&#31034;&#36824;&#26159;&#21306;&#20998;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#35832;&#22914;&#20889;&#20316;&#21161;&#25163;&#20043;&#31867;&#30340;&#24212;&#29992;&#20013;&#65292;&#26399;&#26395;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21487;&#33021;&#24456;&#23567;&#30340;&#20889;&#20316;&#26679;&#26412;&#20197;&#26576;&#20301;&#20316;&#32773;&#29305;&#23450;&#30340;&#39118;&#26684;&#29983;&#25104;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#29305;&#23450;&#26041;&#35328;&#30340;&#20154;&#21487;&#33021;&#26356;&#21916;&#27426;&#20445;&#30041;&#30456;&#21516;&#26041;&#35328;&#30340;&#20889;&#20316;&#24314;&#35758;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#20197;&#23545;&#27604;&#35757;&#32451;&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#30446;&#26631;&#39118;&#26684;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36825;&#20123;&#34920;&#31034;&#25429;&#25417;&#21040;&#20102;&#25991;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17242v2 Announce Type: replace  Abstract: Prior work in style-controlled text generation has focused on tasks such as emulating the style of prolific literary authors, producing formal or informal text, and mitigating toxicity of generated text. Plentiful demonstrations of these styles are available, and as a result modern language models are often able to emulate them, either via prompting or discriminative control. However, in applications such as writing assistants, it is desirable for language models to produce text in an author-specific style on the basis of a potentially small writing sample. For example, someone writing in a particular dialect may prefer writing suggestions that retain the same dialect. We find that instruction-tuned language models can struggle to reproduce author-specific style demonstrated in a prompt. Instead, we propose to guide a language model to generate text in a target style using contrastively-trained representations that capture stylometri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23402;&#29983;&#32467;&#26500;&#23545;GPT-3&#23884;&#20837;&#36827;&#34892;&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25216;&#26415;&#24086;&#23376;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.15068</link><description>&lt;p&gt;
&#20351;&#29992;&#23402;&#29983;&#32467;&#26500;&#23545;GPT-3&#23884;&#20837;&#36827;&#34892;&#32454;&#21270;&#65292;&#29992;&#20110;&#25216;&#26415;&#24086;&#23376;&#30340;&#37325;&#22797;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#23402;&#29983;&#32467;&#26500;&#23545;GPT-3&#23884;&#20837;&#36827;&#34892;&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25216;&#26415;&#24086;&#23376;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#22312;&#32447;&#31038;&#21306;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#19968;&#20010;&#22320;&#26041;&#25214;&#21040;&#27491;&#30830;&#31572;&#26696;&#12290;&#19968;&#20010;&#38382;&#39064;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21644;&#25514;&#36766;&#34987;&#25552;&#20986;&#65292;&#23548;&#33268;&#25216;&#26415;&#35770;&#22363;&#19978;&#23384;&#22312;&#37325;&#22797;&#24086;&#23376;&#12290;&#22914;&#20309;&#21457;&#29616;&#21644;&#38142;&#25509;&#37325;&#22797;&#24086;&#23376;&#24341;&#36215;&#24320;&#21457;&#32773;&#31038;&#21306;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;Stack Overflow&#37319;&#29992;&#22522;&#20110;&#25237;&#31080;&#30340;&#26426;&#21046;&#26469;&#26631;&#35760;&#21644;&#20851;&#38381;&#37325;&#22797;&#24086;&#23376;&#12290;&#28982;&#32780;&#65292;&#21450;&#26102;&#22788;&#29702;&#36825;&#20123;&#19981;&#26029;&#20986;&#29616;&#30340;&#37325;&#22797;&#24086;&#23376;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;&#25216;&#26415;&#35770;&#22363;&#24086;&#23376;&#20013;&#30340;&#37325;&#22797;&#24086;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#24086;&#23376;&#30340;&#35821;&#20041;&#65292;&#35201;&#20040;&#32570;&#20047;&#30417;&#30563;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15068v2 Announce Type: replace-cross  Abstract: One goal of technical online communities is to help developers find the right answer in one place. A single question can be asked in different ways with different wordings, leading to the existence of duplicate posts on technical forums. The question of how to discover and link duplicate posts has garnered the attention of both developer communities and researchers. For example, Stack Overflow adopts a voting-based mechanism to mark and close duplicate posts. However, addressing these constantly emerging duplicate posts in a timely manner continues to pose challenges. Therefore, various approaches have been proposed to detect duplicate posts on technical forum posts automatically. The existing methods suffer from limitations either due to their reliance on handcrafted similarity metrics which can not sufficiently capture the semantics of posts, or their lack of supervision to improve the performance. Additionally, the efficienc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#26816;&#26597;&#25253;&#21578;&#38169;&#35823;&#30340;&#21161;&#25163;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#38169;&#35823;&#24182;&#23545;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#31616;&#21333;&#32423;&#21035;&#19978;&#22312;X&#20809;&#21644;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.13103</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#38169;&#35823;&#26816;&#26597;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring Multimodal Large Language Models for Radiology Report Error-checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#26816;&#26597;&#25253;&#21578;&#38169;&#35823;&#30340;&#21161;&#25163;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#38169;&#35823;&#24182;&#23545;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#31616;&#21333;&#32423;&#21035;&#19978;&#22312;X&#20809;&#21644;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#26816;&#26597;&#25253;&#21578;&#38169;&#35823;&#30340;&#21161;&#25163;&#20043;&#19968;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#19990;&#30028;&#30340;&#25918;&#23556;&#23398;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;X&#20809;&#21644;CT&#25195;&#25551;&#65289;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#19968;&#37096;&#20998;&#21407;&#22987;&#25253;&#21578;&#34987;&#20462;&#25913;&#65292;&#20197;&#21253;&#21547;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#30340;&#21512;&#25104;&#38169;&#35823;&#65306;"&#25554;&#20837;"&#65292;"&#21024;&#38500;"&#21644;"&#26367;&#25442;"&#12290;&#35780;&#20272;&#21253;&#21547;&#20004;&#20010;&#38590;&#24230;&#32423;&#21035;&#65306;&#31616;&#21333;&#32423;&#21035;&#29992;&#20110;&#20108;&#36827;&#21046;&#38169;&#35823;&#26816;&#26597;&#65292;&#22797;&#26434;&#32423;&#21035;&#29992;&#20110;&#35782;&#21035;&#38169;&#35823;&#31867;&#22411;&#12290;&#22312;&#31616;&#21333;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;MIMIC-CXR&#21644;IU X&#20809;&#25968;&#25454;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;47.4%&#21644;25.4%&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#22312;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;CT&#25195;&#25551;&#20013;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#65292;&#22240;&#20026;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;19.46%&#12290;&#27169;&#22411;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#20013;&#30340;&#20934;&#30830;&#24615;&#20063;&#27604;&#39046;&#22495;&#19987;&#23478;&#39640;&#20986;1.67%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#27979;&#35797;&#38598;&#30340;&#23376;&#38598;&#65288;N=21&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13103v2 Announce Type: replace  Abstract: This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports. We created an evaluation dataset from real-world radiology datasets (including X-rays and CT scans). A subset of original reports was modified to contain synthetic errors by introducing three types of mistakes: "insert", "remove", and "substitute". The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types. At the SIMPLE level, our fine-tuned model significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU X-ray data, respectively. This performance boost is also observed in unseen modality, CT scans, as the model performed 19.46% better than the baseline model. The model also surpassed the domain expert's accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets (N=21) of the tes
&lt;/p&gt;</description></item><item><title>&#24314;&#31435;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;NoMIRACL&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#34913;&#37327;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#65306;&#24187;&#35273;&#29575;&#21644;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11361</link><description>&lt;p&gt;
NoMIRACL: &#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#40065;&#26834;&#22810;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11361
&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;NoMIRACL&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#34913;&#37327;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#65306;&#24187;&#35273;&#29575;&#21644;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11361v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#26469;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#19982;&#29616;&#23454;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#20943;&#23569;&#20107;&#23454;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#19981;&#21516;&#35821;&#35328;&#26063;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#35780;&#20272;LLM&#23545;&#22806;&#37096;&#26816;&#32034;&#30693;&#35782;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;NoMIRACL&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;RAG&#20013;LLM&#23545;18&#31181;&#22312;&#31867;&#22411;&#19978;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#12290;NoMIRACL&#21253;&#25324;&#19968;&#20010;&#38750;&#30456;&#20851;&#23376;&#38598;&#21644;&#19968;&#20010;&#30456;&#20851;&#23376;&#38598;&#12290;&#38750;&#30456;&#20851;&#23376;&#38598;&#20013;&#30340;&#26597;&#35810;&#21253;&#21547;&#34987;&#21028;&#26029;&#20026;&#19981;&#30456;&#20851;&#30340;&#27573;&#33853;&#65292;&#32780;&#30456;&#20851;&#23376;&#38598;&#20013;&#30340;&#26597;&#35810;&#33267;&#23569;&#21253;&#21547;&#19968;&#20010;&#34987;&#21028;&#26029;&#20026;&#30456;&#20851;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#26469;&#34913;&#37327;LLM&#30340;&#40065;&#26834;&#24615;&#65306;&#65288;i&#65289;&#24187;&#35273;&#29575;&#65292;&#34913;&#37327;&#27169;&#22411;&#20542;&#21521;&#20110;&#22312;&#38750;&#30456;&#20851;&#23376;&#38598;&#30340;&#27573;&#33853;&#20013;&#20135;&#29983;&#24187;&#35273;&#31572;&#26696;&#30340;&#31243;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38169;&#35823;&#29575;&#65292;&#34913;&#37327;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11361v2 Announce Type: replace  Abstract: Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccurac
&lt;/p&gt;</description></item><item><title>MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.02436</link><description>&lt;p&gt;
MUFFIN: &#29992;&#20110;&#25913;&#21892;&#25351;&#31034;&#36981;&#24490;&#30340;&#22810;&#26041;&#38754;&#25351;&#21335;&#30340;&#31574;&#21010;
&lt;/p&gt;
&lt;p&gt;
MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02436
&lt;/p&gt;
&lt;p&gt;
MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#65292;&#21152;&#24378;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#31574;&#21010;&#24191;&#27867;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;MUFFIN&#65292;&#20855;&#20307;&#22320;&#36890;&#36807;&#29992;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20197;&#20016;&#23500;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02436v2 Announce Type: replace-cross  Abstract: In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#26426;&#21046;&#21160;&#24577;&#33258;&#21160;&#36873;&#25321;&#20197;&#36328;&#27169;&#24577;&#21644;&#20869;&#27169;&#24577;&#30456;&#20284;&#24615;&#20026;&#22522;&#30784;&#30340;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.01714</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#26426;&#21046;&#21160;&#24577;&#33258;&#21160;&#36873;&#25321;&#20197;&#36328;&#27169;&#24577;&#21644;&#20869;&#27169;&#24577;&#30456;&#20284;&#24615;&#20026;&#22522;&#30784;&#30340;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#36215;&#20102;&#23545;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#26041;&#27861;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#33021;&#22815;&#22686;&#24378;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CoT&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#24310;&#20280;&#21040;LLMs&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;CoT&#28436;&#31034;&#31034;&#20363;&#23545;&#20110;LLMs&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#65292;&#36825;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#31034;&#20363;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#26426;&#21046;&#26469;&#21160;&#24577;&#33258;&#21160;&#22320;&#36873;&#25321;&#22522;&#20110;&#36328;&#27169;&#24577;&#21644;&#20869;&#27169;&#24577;&#30456;&#20284;&#24615;&#30340;&#28436;&#31034;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#23558;&#28436;&#31034;&#31034;&#20363;&#20998;&#31867;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#23567;&#32452;&#65292;&#28982;&#21518;&#20998;&#21035;&#20174;&#19981;&#21516;&#32452;&#20013;&#26816;&#32034;&#31034;&#20363;&#65292;&#20197;&#20419;&#36827;&#28436;&#31034;&#31034;&#20363;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01714v2 Announce Type: replace  Abstract: The advancement of Large Language Models (LLMs) has brought substantial attention to the Chain of Thought (CoT) approach, primarily due to its ability to enhance the capability of LLMs on complex reasoning tasks. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks. However, the selection of optimal CoT demonstration examples in multi-modal reasoning remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal and intra-modal similarities. Furthermore, we employ a Stratified Sampling method of categorising demonstration examples into groups based on their types and then retrieving examples from different groups respectively to promote the diversity of demonstration examples.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;Reddit&#23398;&#26415;&#31038;&#21306;&#20013;&#19982;&#21387;&#21147;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#21457;&#29616;&#35789;&#34955;&#26159;&#26368;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2312.01050</link><description>&lt;p&gt;
Reddit&#23398;&#26415;&#31038;&#21306;&#20013;&#19982;&#21387;&#21147;&#26377;&#20851;&#24086;&#23376;&#30340;&#26816;&#27979;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;Reddit&#23398;&#26415;&#31038;&#21306;&#20013;&#19982;&#21387;&#21147;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#21457;&#29616;&#35789;&#34955;&#26159;&#26368;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#30417;&#27979;&#21387;&#21147;&#27700;&#24179;&#24182;&#21450;&#26089;&#35782;&#21035;&#24515;&#29702;&#30142;&#30149;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#12290;&#25991;&#26412;&#20013;&#30340;&#33258;&#21160;&#21387;&#21147;&#26816;&#27979;&#33021;&#22815;&#31215;&#26497;&#24110;&#21161;&#31649;&#29702;&#21387;&#21147;&#65292;&#20445;&#25252;&#24515;&#29702;&#20581;&#24247;&#12290;&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21453;&#26144;&#20102;&#19981;&#21516;&#31038;&#21306;&#20869;&#30340;&#24515;&#29702;&#20581;&#24247;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#20391;&#37325;&#20110;&#26816;&#27979;&#21644;&#20998;&#26512;Reddit&#23398;&#26415;&#31038;&#21306;&#20013;&#19982;&#21387;&#21147;&#26377;&#20851;&#30340;&#24086;&#23376;&#12290;&#30001;&#20110;&#22312;&#32447;&#25945;&#32946;&#21644;&#36828;&#31243;&#24037;&#20316;&#65292;&#36825;&#20123;&#31038;&#21306;&#24050;&#32463;&#25104;&#20026;&#23398;&#26415;&#35752;&#35770;&#21644;&#25903;&#25345;&#30340;&#20013;&#24515;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23558;&#25991;&#26412;&#20998;&#31867;&#20026;&#26377;&#21387;&#21147;&#25110;&#26080;&#21387;&#21147;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;Dreaddit&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Reddit&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#20998;&#26512;&#26469;&#33258;&#21508;&#31181;&#23398;&#26415;&#23376;&#31038;&#21306;&#30340;&#24086;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#21387;&#21147;&#26816;&#27979;&#26368;&#26377;&#25928;&#30340;&#20010;&#21035;&#29305;&#24449;&#26159;&#35789;&#34955;&#65288;Bag of Words&#65289;&#65292;&#37197;&#21512;&#36923;&#36753;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01050v2 Announce Type: replace  Abstract: Nowadays, the significance of monitoring stress levels and recognizing early signs of mental illness cannot be overstated. Automatic stress detection in text can proactively help manage stress and protect mental well-being. In today's digital era, social media platforms reflect the psychological well-being and stress levels within various communities. This study focuses on detecting and analyzing stress-related posts in Reddit academic communities. Due to online education and remote work, these communities have become central for academic discussions and support. We classify text as stressed or not using natural language processing and machine learning classifiers, with Dreaddit as our training dataset, which contains labeled data from Reddit. Next, we collect and analyze posts from various academic subreddits. We identified that the most effective individual feature for stress detection is the Bag of Words, paired with the Logistic 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20840;&#29699;&#35268;&#27169;&#30340;Prompt Hacking&#31454;&#36187;&#65292;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#30340;&#31995;&#32479;&#28431;&#27934;&#65292;&#39564;&#35777;&#20102;&#24403;&#21069;LLMs&#21487;&#20197;&#34987;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25805;&#32437;&#12290;</title><link>https://arxiv.org/abs/2311.16119</link><description>&lt;p&gt;
&#24573;&#30053;&#36825;&#20010;&#26631;&#39064;&#24182;HackAPrompt&#65306;&#36890;&#36807;&#20840;&#29699;&#35268;&#27169;&#30340;Prompt Hacking&#31454;&#36187;&#25581;&#31034;LLMs&#30340;&#31995;&#32479;&#24615;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20840;&#29699;&#35268;&#27169;&#30340;Prompt Hacking&#31454;&#36187;&#65292;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#30340;&#31995;&#32479;&#28431;&#27934;&#65292;&#39564;&#35777;&#20102;&#24403;&#21069;LLMs&#21487;&#20197;&#34987;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#30452;&#25509;&#19982;&#29992;&#25143;&#20114;&#21160;&#30340;&#24773;&#22659;&#20013;&#65292;&#20363;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20889;&#20316;&#21161;&#25163;&#12290;&#36825;&#20123;&#37096;&#32626;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#21644;&#36234;&#29425;&#65288;&#32479;&#31216;&#20026;Prompt Hacking&#65289;&#30340;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#34987;&#25805;&#32437;&#20197;&#24573;&#30053;&#20854;&#21407;&#22987;&#25351;&#20196;&#24182;&#36981;&#24490;&#21487;&#33021;&#24694;&#24847;&#30340;&#25351;&#20196;&#12290;&#34429;&#28982;&#24191;&#20026;&#20154;&#30693;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#20294;&#20851;&#20110;Prompt Hacking&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#21644;&#23450;&#37327;&#30740;&#31350;&#30340;&#36164;&#26009;&#21294;&#20047;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21457;&#36215;&#20102;&#19968;&#22330;&#20840;&#29699;Prompt Hacking&#31454;&#36187;&#65292;&#20801;&#35768;&#33258;&#30001;&#24418;&#24335;&#30340;&#20154;&#31867;&#36755;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#25628;&#38598;&#20102;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#21457;&#36215;&#30340;&#36229;&#36807;60&#19975;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#25551;&#36848;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#24403;&#21069;LLMs&#30830;&#23454;&#21487;&#20197;&#36890;&#36807;Prompt Hacking&#34987;&#25805;&#32437;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#31867;&#22411;&#30340;&#20840;&#38754;&#20998;&#31867;&#26412;&#20307;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16119v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;BeInfo&#26041;&#27861;&#30340;&#34892;&#20026;&#24494;&#35843;&#65292;&#21487;&#20197;&#25552;&#39640;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#31995;&#32479;&#23545;&#30693;&#35782;&#26469;&#28304;&#30340;&#20934;&#30830;&#24615;&#21644;&#24544;&#23454;&#24230;</title><link>https://arxiv.org/abs/2311.09800</link><description>&lt;p&gt;
$\textit{Dial BeInfo for Faithfulness}$: &#36890;&#36807;&#34892;&#20026;&#24494;&#35843;&#25552;&#39640;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09800
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;BeInfo&#26041;&#27861;&#30340;&#34892;&#20026;&#24494;&#35843;&#65292;&#21487;&#20197;&#25552;&#39640;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#31995;&#32479;&#23545;&#30693;&#35782;&#26469;&#28304;&#30340;&#20934;&#30830;&#24615;&#21644;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24615;&#26159;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#20013;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35201;&#27714;&#65306;&#31995;&#32479;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#26597;&#35810;&#20570;&#20986;&#21709;&#24212;&#65292;&#20351;&#24471;&#36825;&#20123;&#22238;&#22797;&#26377;&#24847;&#20041;&#19988;&#19982;&#31995;&#32479;&#25552;&#20379;&#30340;&#30693;&#35782;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24187;&#35273;&#29616;&#35937;&#65292;&#21363;&#23427;&#20204;&#29983;&#25104;&#30340;&#22238;&#22797;&#19981;&#21463;&#25903;&#25345;&#25110;&#19982;&#30693;&#35782;&#26469;&#28304;&#30456;&#30683;&#30462;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#24182;&#25552;&#39640;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BeInfo&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34892;&#20026;&#35843;&#25972;&#26469;&#24110;&#21161;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#12290;&#36890;&#36807;&#20381;&#36182;&#20110;&#19977;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;BeInfo&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20013;&#21464;&#24471;&#26356;&#21152;&#24544;&#23454;&#20110;&#30693;&#35782;&#26469;&#28304;&#65292;&#26080;&#35770;&#26159;&#22312;&#24212;&#29992;&#38646;-shot&#26041;&#24335;&#26102;&#30475;&#21040;&#30340;&#39046;&#22495;&#36824;&#26159;&#26410;&#30475;&#21040;&#30340;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;BeInfo&#24494;&#35843;&#30340;3B&#21442;&#25968;&#27169;&#22411;&#65288;&#22914;Flan-T5&#65289;&#34920;&#29616;&#20986;&#33394;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09800v2 Announce Type: replace  Abstract: Factuality is a crucial requirement in information seeking dialogue: the system should respond to the user's queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models suffer from hallucinations, that is, they generate responses not supported by or contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue. Relying on three standard datasets, we show that models tuned with BeInfo} become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo demonstrate strong perf
&lt;/p&gt;</description></item><item><title>MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;</title><link>https://arxiv.org/abs/2311.09033</link><description>&lt;p&gt;
MELA&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELA: Multilingual Evaluation of Linguistic Acceptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09033
&lt;/p&gt;
&lt;p&gt;
MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#24212;&#29992;&#39537;&#21160;&#30340;&#20219;&#21153;&#65292;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#65292;&#23548;&#33268;LLMs&#30340;&#32431;&#35821;&#35328;&#35780;&#20272;&#20005;&#37325;&#19981;&#36275;&#12290;&#38024;&#23545;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multilingual Evaluation of Linguistic Acceptability&#65288;MELA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#26469;&#33258;&#22810;&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;10&#31181;&#35821;&#35328;&#12289;&#20849;48K&#20010;&#26679;&#26412;&#30340;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22810;&#35821;&#35328;&#22522;&#20934;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24120;&#29992;LLMs&#21644;&#30417;&#30563;&#27169;&#22411;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;XLM-R&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#39564;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24494;&#35843;&#21518;&#30340;XLM-R&#30340;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#35782;&#21035;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36801;&#31227;&#22256;&#38590;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21463;&#30410;&#33391;&#22810;&#65292;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#65292;&#32780;GPT-4&#30340;&#24615;&#33021;&#19982;&#20043;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09033v2 Announce Type: replace-cross  Abstract: Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with f
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#31616;&#21270;&#25688;&#35201;&#21487;&#20197;&#24110;&#21161;&#20844;&#20247;&#26356;&#22909;&#22320;&#29702;&#35299;&#21496;&#27861;&#24847;&#35265;&#65292;&#23588;&#20854;&#23545;&#20110;&#21463;&#25945;&#32946;&#31243;&#24230;&#36739;&#20302;&#30340;&#20154;&#32676;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2311.06534</link><description>&lt;p&gt;
&#29992;&#27861;&#24459;&#25688;&#35201;&#22120;&#25552;&#21319;&#20844;&#20247;&#23545;&#27861;&#38498;&#21028;&#20915;&#30340;&#29702;&#35299;&#65306;&#32763;&#35793;&#27861;&#24459;&#26415;&#35821;
&lt;/p&gt;
&lt;p&gt;
Translating Legalese: Enhancing Public Understanding of Court Opinions with Legal Summarizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06534
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#31616;&#21270;&#25688;&#35201;&#21487;&#20197;&#24110;&#21161;&#20844;&#20247;&#26356;&#22909;&#22320;&#29702;&#35299;&#21496;&#27861;&#24847;&#35265;&#65292;&#23588;&#20854;&#23545;&#20110;&#21463;&#25945;&#32946;&#31243;&#24230;&#36739;&#20302;&#30340;&#20154;&#32676;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21496;&#27861;&#24847;&#35265;&#34987;&#20889;&#20316;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#24314;&#31435;&#20844;&#20247;&#23545;&#27861;&#38498;&#21028;&#20915;&#30340;&#20449;&#20219;&#65292;&#28982;&#32780;&#23545;&#38750;&#19987;&#19994;&#20154;&#22763;&#26469;&#35828;&#29702;&#35299;&#36215;&#26469;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#29983;&#25104;&#21496;&#27861;&#24847;&#35265;&#31616;&#21270;&#25688;&#35201;&#30340;&#27969;&#31243;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#25776;&#20889;&#30340;&#25688;&#35201;&#30456;&#27604;&#65292;&#36825;&#20123;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#31616;&#21270;&#25688;&#35201;&#26356;&#26131;&#20026;&#20844;&#20247;&#25509;&#35302;&#21644;&#38750;&#19987;&#23478;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#39033;&#35843;&#26597;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#25688;&#35201;&#26377;&#21161;&#20110;&#35843;&#26597;&#23545;&#35937;&#29702;&#35299;&#35009;&#20915;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#34987;&#35748;&#20026;&#36136;&#37327;&#26356;&#39640;&#65292;&#23588;&#20854;&#23545;&#20110;&#21463;&#25945;&#32946;&#31243;&#24230;&#36739;&#20302;&#30340;&#35843;&#26597;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06534v2 Announce Type: replace  Abstract: Judicial opinions are written to be persuasive and could build public trust in court decisions, yet they can be difficult for non-experts to understand. We present a pipeline for using an AI assistant to generate simplified summaries of judicial opinions. Compared to existing expert-written summaries, these AI-generated simple summaries are more accessible to the public and more easily understood by non-experts. We show in a survey experiment that the AI summaries help respondents understand the key features of a ruling, and have higher perceived quality, especially for respondents with less formal education.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;&#65292;&#21253;&#25324;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#30340;&#38750;&#25925;&#24847;&#22330;&#26223;&#21644;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#24694;&#24847;&#25915;&#20987;LLMs&#30340;&#25925;&#24847;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2310.06474</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Multilingual Jailbreak Challenges in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;&#65292;&#21253;&#25324;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#30340;&#38750;&#25925;&#24847;&#22330;&#26223;&#21644;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#24694;&#24847;&#25915;&#20987;LLMs&#30340;&#25925;&#24847;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#36234;&#29425;&#8221;&#38382;&#39064;&#65292;&#21363;&#24694;&#24847;&#25351;&#20196;&#21487;&#33021;&#25805;&#32437;LLMs&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#12290;&#23613;&#31649;&#24050;&#32463;&#21046;&#23450;&#20102;&#20960;&#31181;&#39044;&#38450;&#25514;&#26045;&#26469;&#20943;&#36731;&#19982;LLMs&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20294;&#36825;&#20123;&#25514;&#26045;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#20013;&#23384;&#22312;&#30340;&#22810;&#35821;&#35328;&#36234;&#29425;&#25361;&#25112;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#28508;&#22312;&#30340;&#39118;&#38505;&#22330;&#26223;&#65306;&#38750;&#25925;&#24847;&#21644;&#25925;&#24847;&#12290;&#38750;&#25925;&#24847;&#22330;&#26223;&#28041;&#21450;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#25552;&#31034;&#26597;&#35810;LLMs&#24182;&#26080;&#24847;&#20013;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#65292;&#32780;&#25925;&#24847;&#22330;&#26223;&#28041;&#21450;&#24694;&#24847;&#29992;&#25143;&#23558;&#24694;&#24847;&#25351;&#20196;&#19982;&#22810;&#35821;&#35328;&#25552;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#25925;&#24847;&#25915;&#20987;LLMs&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38750;&#25925;&#24847;&#22330;&#26223;&#20013;&#65292;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#27604;&#29575;&#22686;&#21152;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06474v2 Announce Type: replace  Abstract: While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#26102;&#31354;&#34920;&#24449;&#65292;&#21253;&#25324;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#20197;&#21450;&#20010;&#20307;&#30340;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;</title><link>https://arxiv.org/abs/2310.02207</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#31354;&#38388;&#21644;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Space and Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02207
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#26102;&#31354;&#34920;&#24449;&#65292;&#21253;&#25324;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#20197;&#21450;&#20010;&#20307;&#30340;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#21040;&#24213;&#26159;&#20165;&#20165;&#23398;&#20064;&#20102;&#24222;&#22823;&#30340;&#34920;&#38754;&#32479;&#35745;&#20449;&#24687;&#36824;&#26159;&#23398;&#21040;&#20102;&#26356;&#36830;&#36143;&#12289;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#24449;&#30340;&#20105;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;Llama-2&#31995;&#21015;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#19977;&#20010;&#31354;&#38388;&#25968;&#25454;&#38598;&#65288;&#19990;&#30028;&#12289;&#32654;&#22269;&#12289;&#32445;&#32422;&#30340;&#22320;&#28857;&#65289;&#21644;&#19977;&#20010;&#26102;&#38388;&#25968;&#25454;&#38598;&#65288;&#21382;&#21490;&#20154;&#29289;&#12289;&#33402;&#26415;&#21697;&#12289;&#26032;&#38395;&#22836;&#26465;&#65289;&#30340;&#23398;&#20064;&#34920;&#24449;&#25214;&#21040;&#20102;&#25903;&#25345;&#21518;&#32773;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#12290;&#36825;&#20123;&#34920;&#24449;&#23545;&#25552;&#31034;&#21464;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#65288;&#20363;&#22914;&#22478;&#24066;&#21644;&#22320;&#26631;&#65289;&#20043;&#38388;&#26159;&#32479;&#19968;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21487;&#38752;&#22320;&#32534;&#30721;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#30340;&#20010;&#20307;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;&#34429;&#28982;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#29616;&#20195;LLM&#23398;&#20064;&#21040;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20016;&#23500;&#26102;&#31354;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02207v3 Announce Type: replace-cross  Abstract: The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#26377;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#35328;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#35268;&#24459;&#12290;</title><link>https://arxiv.org/abs/2303.07196</link><description>&lt;p&gt;
&#29616;&#26377;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#20840;&#38754;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Empirical Evaluation of Existing Word Embedding Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.07196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#26377;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#35328;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#21270;&#30340;&#35789;&#34920;&#31034;&#24110;&#21161;&#26080;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#25429;&#25417;&#35821;&#35328;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#35268;&#24459;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#29616;&#26377;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#29305;&#28857;&#65292;&#24182;&#38024;&#23545;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#31867;&#20026;&#20004;&#22823;&#32452; - &#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#30697;&#38453;&#20998;&#35299;&#26469;&#29983;&#25104;&#35789;&#34920;&#31034;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#35821;&#35328;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#35268;&#24459;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#35821;&#35328;&#30340;&#22797;&#26434;&#35268;&#24459;&#65292;&#24182;&#22312;&#29983;&#25104;&#30340;&#35789;&#34920;&#31034;&#20013;&#20445;&#30041;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.07196v2 Announce Type: replace  Abstract: Vector-based word representations help countless Natural Language Processing (NLP) tasks capture the language's semantic and syntactic regularities. In this paper, we present the characteristics of existing word embedding approaches and analyze them with regard to many classification tasks. We categorize the methods into two main groups - Traditional approaches mostly use matrix factorization to produce word representations, and they are not able to capture the semantic and syntactic regularities of the language very well. On the other hand, Neural-network-based approaches can capture sophisticated regularities of the language and preserve the word relationships in the generated word representations. We report experimental results on multiple classification tasks and highlight the scenarios where one approach performs better than the rest.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#22810;&#26679;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2212.03827</link><description>&lt;p&gt;
&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Discovering Latent Knowledge in Language Models Without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#22810;&#26679;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#25216;&#26415;&#21487;&#33021;&#19982;&#30495;&#30456;&#19981;&#19968;&#33268;&#65306;&#22914;&#26524;&#25105;&#20204;&#29992;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#37325;&#29616;&#20154;&#31867;&#30340;&#38169;&#35823;&#65307;&#22914;&#26524;&#25105;&#20204;&#35757;&#32451;&#23427;&#20204;&#29983;&#25104;&#20154;&#31867;&#35780;&#20215;&#39640;&#30340;&#25991;&#26412;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#36755;&#20986;&#20154;&#31867;&#35780;&#20272;&#32773;&#26080;&#27861;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#26469;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19988;&#26159;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#21482;&#32473;&#23450;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#25214;&#21040;&#28385;&#36275;&#36923;&#36753;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#26041;&#21521;&#26469;&#24037;&#20316;&#65292;&#20363;&#22914;&#19968;&#20010;&#38472;&#36848;&#21450;&#20854;&#21542;&#23450;&#20855;&#26377;&#30456;&#21453;&#30340;&#30495;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#30417;&#30563;&#21644;&#27169;&#22411;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#34920;&#22810;&#26679;&#30693;&#35782;&#65306;&#22312;6&#20010;&#27169;&#22411;&#21644;10&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03827v2 Announce Type: replace-cross  Abstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;SMiLE&#65289;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#27169;&#24335;&#20316;&#20026;&#20808;&#39564;&#32422;&#26463;&#26469;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2210.04870</link><description>&lt;p&gt;
SMiLE&#65306;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.04870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;SMiLE&#65289;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#27169;&#24335;&#20316;&#20026;&#20808;&#39564;&#32422;&#26463;&#26469;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#25512;&#26029;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#20043;&#38388;&#32570;&#22833;&#38142;&#25509;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#19977;&#20803;&#32452;&#20013;&#30340;&#20851;&#31995;&#27169;&#24335;&#22312;&#35299;&#20915;&#27492;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#23454;&#20307;&#37051;&#22495;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#21040;&#23427;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#20851;&#27880;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#23454;&#20307;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35748;&#20026;&#30693;&#35782;&#22270;&#35889;&#30340;&#27169;&#24335;&#21253;&#21547;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#20445;&#25345;&#23454;&#20307;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;SMiLE&#65289;&#26469;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.04870v3 Announce Type: replace-cross  Abstract: Link prediction is the task of inferring missing links between entities in knowledge graphs. Embedding-based methods have shown effectiveness in addressing this problem by modeling relational patterns in triples. However, the link prediction task often requires contextual information in entity neighborhoods, while most existing embedding-based methods fail to capture it. Additionally, little attention is paid to the diversity of entity representations in different contexts, which often leads to false prediction results. In this situation, we consider that the schema of knowledge graph contains the specific contextual information, and it is beneficial for preserving the consistency of entities across contexts. In this paper, we propose a novel Schema-augmented Multi-level contrastive LEarning framework (SMiLE) to conduct knowledge graph link prediction. Specifically, we first exploit network schema as the prior constraint to sam
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#26159;&#35299;&#20915;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.15422</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#26159;&#35299;&#20915;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35821;&#35328;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#36817;&#20284;&#20154;&#31867;&#32423;&#26234;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22823;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#26356;&#26032;&#65292;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20648;&#22791;&#21487;&#33021;&#24456;&#24555;&#29992;&#23613;&#12290;&#36825;&#20010;&#25361;&#25112;&#20652;&#29983;&#20102;&#22823;&#37327;&#20851;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#21033;&#29992;&#22823;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#25991;&#32508;&#21512;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#22823;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#35814;&#23613;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#30456;&#20851;&#30740;&#31350;&#20998;&#20026;&#22270;&#20687;&#22686;&#24378;&#12289;&#25991;&#26412;&#22686;&#24378;&#21644;&#37197;&#23545;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#22823;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#30456;&#20851;&#30340;&#21508;&#31181;&#25968;&#25454;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data au
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#65292;&#26088;&#22312;&#20811;&#26381;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#24847;&#20041;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.06509</link><description>&lt;p&gt;
AntEval: &#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions. (arXiv:2401.06509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#65292;&#26088;&#22312;&#20811;&#26381;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#24847;&#20041;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#24050;&#25104;&#21151;&#22320;&#27169;&#20223;&#20102;&#21508;&#31181;&#24773;&#22659;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#22797;&#26434;&#30340;&#12289;&#22810;&#35282;&#33394;&#31038;&#20132;&#20114;&#21160;&#22312;&#25193;&#23637;&#29615;&#22659;&#20013;&#30340;&#39046;&#22495;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#38544;&#31169;&#38382;&#39064;&#20351;&#25429;&#25417;&#21644;&#21033;&#29992;&#22797;&#26434;&#30340;&#29616;&#23454;&#29983;&#27963;&#20114;&#21160;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#32570;&#20047;&#23450;&#37327;&#35780;&#20272;&#26041;&#27861;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26234;&#33021;&#20307;&#20114;&#21160;&#30340;&#36861;&#27714;&#65292;&#23548;&#33268;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#26377;&#38480;&#65292;&#34920;&#29616;&#20026;&#32932;&#27973;&#30340;&#38386;&#32842;&#32780;&#27809;&#26377;&#28165;&#26224;&#30340;&#24847;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65288;TRPG&#65289;&#30340;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21033;&#20110;&#22797;&#26434;&#12289;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#20114;&#21160;&#30340;&#29615;&#22659;&#65292;&#24378;&#35843;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#12290;&#36825;&#20010;&#34394;&#25311;&#29615;&#22659;&#20943;&#36731;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#28608;&#21169;&#26234;&#33021;&#20307;&#20316;&#20026;&#28216;&#25103;&#30446;&#26631;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11984</link><description>&lt;p&gt;
&#20174;&#25554;&#20540;&#21040;&#22806;&#25512;&#65306;&#31639;&#26415;Transformer&#30340;&#23436;&#25972;&#38271;&#24230;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25552;&#20986;&#20197;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#31639;&#27861;&#20219;&#21153;&#20013;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#26041;&#38754;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#27880;&#24847;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#26368;&#20339;&#38271;&#24230;&#27867;&#21270;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#30446;&#26631;&#25351;&#21521;&#20559;&#32622;&#26469;&#27867;&#21270;&#21040;&#38271;&#38271;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Attention Bias Calibration&#65288;ABC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26657;&#20934;&#38454;&#27573;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#20559;&#32622;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#26426;&#21046;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;ABC&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#20123;&#31639;&#26415;&#20219;&#21153;&#19978;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#23436;&#32654;&#38271;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11053</link><description>&lt;p&gt;
Denevil: &#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#26469;&#35299;&#35835;&#21644;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. (arXiv:2310.11053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31361;&#30772;&#65292;&#28982;&#32780;&#23427;&#20204;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#30001;&#29983;&#25104;&#30340;&#19981;&#36947;&#24503;&#20869;&#23481;&#24341;&#36215;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#29305;&#23450;&#38382;&#39064;&#22914;&#20559;&#35265;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#20174;&#36947;&#24503;&#21746;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#28145;&#20837;&#25506;&#35752;&#36947;&#24503;&#20215;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeNEVIL&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#65292;&#26088;&#22312;&#21160;&#24577;&#21033;&#29992;LLM&#30340;&#20215;&#20540;&#33030;&#24369;&#24615;&#24182;&#20197;&#29983;&#25104;&#26041;&#24335;&#25581;&#31034;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#65292;&#25581;&#31034;&#20854;&#28508;&#22312;&#30340;&#20215;&#20540;&#20542;&#21521;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MoralPrompt&#65292;&#19968;&#20010;&#21253;&#21547;2,397&#20010;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;500&#22810;&#20010;&#20215;&#20540;&#21407;&#21017;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#19981;&#23545;&#40784;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09680</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring. (arXiv:2310.09680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#20351;&#24471;ASR&#31995;&#32479;&#22312;&#20934;&#30830;&#36716;&#24405;&#21475;&#35821;&#30340;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#26159;&#26500;&#24314;&#23545;&#35805;&#20195;&#29702;&#30340;&#20851;&#38190;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#36776;&#21035;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#20173;&#28982;&#26159;&#19968;&#39033;&#36843;&#20999;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#22788;&#29702;&#26469;&#22686;&#24378;ASR&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#20132;&#20184;&#21508;&#31181;&#35789;&#27719;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#36716;&#24405;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;HMM-GMM&#65289;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#22768;&#23398;&#24314;&#27169;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21333;&#35789;&#26684;&#65292;&#20351;&#25105;&#20204;&#30340;&#32593;&#32476;&#20855;&#22791;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) has witnessed a profound research interest. Recent breakthroughs have given ASR systems different prospects such as faithfully transcribing spoken language, which is a pivotal advancement in building conversational agents. However, there is still an imminent challenge of accurately discerning context-dependent words and phrases. In this work, we propose a novel approach for enhancing contextual recognition within ASR systems via semantic lattice processing leveraging the power of deep learning models in accurately delivering spot-on transcriptions across a wide variety of vocabularies and speaking styles. Our solution consists of using Hidden Markov Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks (DNN) models integrating both language and acoustic modeling for better accuracy. We infused our network with the use of a transformer-based model to properly rescore the word lattice achieving remarkable capabilities with a palpa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02954</link><description>&lt;p&gt;
DQ-LoRe: &#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20302;&#31209;&#36817;&#20284;&#21452;&#37325;&#26597;&#35810;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#30340;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24341;&#23548;LLMs&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#33539;&#24335;&#20013;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26368;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#36873;&#25321;&#31034;&#20363;&#26469;&#20419;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#65288;DQ-LoRe&#65289;&#26469;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;&#21452;&#37325;&#26597;&#35810;&#39318;&#20808;&#26597;&#35810;LLM&#20197;&#33719;&#21462;LLM&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;CoT&#65292;&#28982;&#21518;&#36890;&#36807;&#38382;&#39064;&#21644;&#30693;&#35782;&#26597;&#35810;&#26816;&#32034;&#22120;&#20197;&#33719;&#21462;&#26368;&#32456;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31532;&#20108;&#20010;&#26597;&#35810;&#65292;LoRe&#21033;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#25913;&#36827;&#31034;&#20363;&#36873;&#25321;&#65292;&#30830;&#20445;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#20999;&#23545;&#40784;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;DQ-LoRe&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;&#30693;&#35782;&#20928;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#34987;&#35810;&#38382;&#25935;&#24863;&#20449;&#24687;&#26102;&#29983;&#25104;&#26080;&#23475;&#22238;&#31572;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#24182;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sanitization of Large Language Models. (arXiv:2309.11852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;&#30693;&#35782;&#20928;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#34987;&#35810;&#38382;&#25935;&#24863;&#20449;&#24687;&#26102;&#29983;&#25104;&#26080;&#23475;&#22238;&#31572;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#24182;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#30693;&#35782;&#20928;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169;Web&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LLMs&#21487;&#20197;&#35760;&#20303;&#24182;&#28508;&#22312;&#22320;&#36879;&#38706;&#25935;&#24863;&#25110;&#26426;&#23494;&#20449;&#24687;&#65292;&#24341;&#21457;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36890;&#36807;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#65292;&#20419;&#20351;&#23427;&#20204;&#22312;&#34987;&#35810;&#38382;&#29305;&#23450;&#20449;&#24687;&#26102;&#29983;&#25104;&#26080;&#23475;&#30340;&#22238;&#31572;&#65292;&#20363;&#22914;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#23553;&#38381;&#24335;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#31616;&#21333;&#30340;&#26041;&#27861;&#19981;&#20165;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#29305;&#23450;&#30693;&#35782;&#27844;&#28431;&#65292;&#36824;&#20445;&#30041;&#20102;LLM&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#36825;&#20004;&#20010;&#20248;&#28857;&#21152;&#24378;&#20102;&#23545;&#25552;&#21462;&#25915;&#20987;&#30340;&#38450;&#24481;&#65292;&#24182;&#20943;&#23569;&#20102;&#20135;&#29983;&#24187;&#35273;&#31561;&#26377;&#23475;&#20869;&#23481;&#30340;&#21457;&#36865;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs). LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. Our technique fine-tunes these models, prompting them to generate harmless responses such as ``I don't know'' when queried about specific information. Experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of LLM. These two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;FaNS&#65292;&#36890;&#36807;&#25552;&#21462;&#32463;&#20856;&#30340;&#20116;W&#19968;H&#35201;&#32032;&#24182;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21465;&#20107;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FaNS&#19982;&#20256;&#32479;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;&#39640;37%&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.04823</link><description>&lt;p&gt;
FaNS&#65306;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
FaNS: a Facet-based Narrative Similarity Metric. (arXiv:2309.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;FaNS&#65292;&#36890;&#36807;&#25552;&#21462;&#32463;&#20856;&#30340;&#20116;W&#19968;H&#35201;&#32032;&#24182;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21465;&#20107;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FaNS&#19982;&#20256;&#32479;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;&#39640;37%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#30340;&#21465;&#20107;&#26816;&#32034;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#21465;&#20107;&#23545;&#20110;&#35299;&#37322;&#21644;&#29702;&#35299;&#20107;&#20214;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22810;&#20010;&#30456;&#20851;&#30340;&#21465;&#20107;&#36890;&#24120;&#26377;&#21161;&#20110;&#21019;&#24314;&#23545;&#25152;&#20851;&#27880;&#20107;&#20214;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;&#20026;&#20102;&#20934;&#30830;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21465;&#20107;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35201;&#32032;&#30340;&#21465;&#20107;&#30456;&#20284;&#24230;&#65288;FaNS&#65289;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#32463;&#20856;&#30340;&#20116;W&#19968;H&#35201;&#32032;&#65288;Who&#65292;What&#65292;When&#65292;Where&#65292;Why&#21644;How&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25552;&#21462;&#12290;&#19982;&#29616;&#26377;&#30340;&#20165;&#20851;&#27880;&#25972;&#20307;&#35789;&#27719;/&#35821;&#20041;&#21305;&#37197;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#19981;&#21516;&#65292;FaNS&#25552;&#20379;&#20102;&#26356;&#20026;&#32454;&#33268;&#30340;&#21305;&#37197;&#65292;&#21253;&#25324;&#20845;&#20010;&#19981;&#21516;&#30340;&#35201;&#32032;&#30340;&#29420;&#31435;&#21305;&#37197;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#12290;&#20026;&#20102;&#35780;&#20272;FaNS&#65292;&#25105;&#20204;&#20174;&#31532;&#19977;&#26041;&#26032;&#38395;&#38376;&#25143;AllSides&#25910;&#38598;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#21465;&#20107;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FaNS&#24230;&#37327;&#26041;&#27861;&#19982;&#30452;&#25509;&#24230;&#37327;&#30340;&#20256;&#32479;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;&#39640;37%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar Narrative Retrieval is a crucial task since narratives are essential for explaining and understanding events, and multiple related narratives often help to create a holistic view of the event of interest. To accurately identify semantically similar narratives, this paper proposes a novel narrative similarity metric called Facet-based Narrative Similarity (FaNS), based on the classic 5W1H facets (Who, What, When, Where, Why, and How), which are extracted by leveraging the state-of-the-art Large Language Models (LLMs). Unlike existing similarity metrics that only focus on overall lexical/semantic match, FaNS provides a more granular matching along six different facets independently and then combines them. To evaluate FaNS, we created a comprehensive dataset by collecting narratives from AllSides, a third-party news portal. Experimental results demonstrate that the FaNS metric exhibits a higher correlation (37\% higher) than traditional text similarity metrics that directly measur
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.04739</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#26597;&#35810;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#22914;&#20247;&#21253;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#65292;&#22240;&#27492;&#22312;&#27492;&#24773;&#26223;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#32531;&#35299;&#23545;&#35805;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25945;&#31243;&#20840;&#38754;&#19988;&#26368;&#26032;&#22320;&#27010;&#36848;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;DA&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#27169;&#22411;&#30340;&#33539;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#30456;&#20851;&#35777;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#35757;&#32451;LLM&#26469;&#25512;&#26029;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04550</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#26816;&#32034;&#35777;&#25454;&#65306;&#21487;&#33021;&#24615;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges. (arXiv:2309.04550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#30456;&#20851;&#35777;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#35757;&#32451;LLM&#26469;&#25512;&#26029;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#65292;&#24182;&#19988;&#27169;&#22411;&#21487;&#20197;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#19982;&#24433;&#20687;&#25968;&#25454;&#20114;&#34917;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#38480;&#21046;&#21644;&#19982;&#27599;&#20010;&#24739;&#32773;&#30456;&#20851;&#30340;&#22823;&#37327;&#31508;&#35760;&#20351;&#24471;&#25163;&#21160;&#27983;&#35272;&#27492;&#31867;&#25968;&#25454;&#20197;&#35782;&#21035;&#30456;&#20851;&#35777;&#25454;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#26410;&#32467;&#26500;&#21270;&#30340;EHR&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#26816;&#32034;&#21644;&#24635;&#32467;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#26410;&#32467;&#26500;&#21270;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;LLM&#65288;Flan-T5 XXL&#65289;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#25512;&#26029;&#19968;&#20010;&#24739;&#32773;&#26159;&#21542;&#26377;&#25110;&#22788;&#20110;&#26576;&#31181;&#29305;&#23450;&#30142;&#30149;&#30340;&#39118;&#38505;&#65292;&#24182;&#22312;&#26159;&#30340;&#24773;&#20917;&#19979;&#25552;&#31034;&#27169;&#22411;&#24635;&#32467;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#25918;&#23556;&#31185;&#21307;&#29983;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#25552;&#20379;&#30340;&#36755;&#20986;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unstructured Electronic Health Record (EHR) data often contains critical information complementary to imaging data that would inform radiologists' diagnoses. However, time constraints and the large volume of notes frequently associated with individual patients renders manual perusal of such data to identify relevant evidence infeasible in practice. Modern Large Language Models (LLMs) provide a flexible means of interacting with unstructured EHR data, and may provide a mechanism to efficiently retrieve and summarize unstructured evidence relevant to a given query. In this work, we propose and evaluate an LLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we task the LLM to infer whether a patient has or is at risk of a particular condition; if so, we prompt the model to summarize the supporting evidence. Enlisting radiologists for manual evaluation, we find that this LLM-based approach provides outputs consistently preferred to a standard information retrieval base
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;LMs&#21487;&#20197;&#25351;&#31216;&#30340;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2308.05576</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Refer?. (arXiv:2308.05576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05576
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;LMs&#21487;&#20197;&#25351;&#31216;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29992;&#35821;&#35328;&#20570;&#20160;&#20040;&#65311;&#22823;&#23478;&#37117;&#21516;&#24847;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#65288;&#22823;&#37096;&#20998;&#65289;&#36830;&#36143;&#30340;&#21477;&#23376;&#12290;&#20294;&#26159;&#23427;&#20204;&#29992;&#36825;&#20123;&#23383;&#31526;&#20018;&#34920;&#36798;&#20102;&#20160;&#20040;&#65292;&#36824;&#26159;&#21482;&#26159;&#20197;&#19968;&#31181;&#20196;&#20154;&#20449;&#26381;&#30340;&#35821;&#35328;&#36816;&#29992;&#30340;&#27169;&#25311;&#20013;&#32993;&#35328;&#20081;&#35821;&#65311;&#36825;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#26041;&#27861;&#21487;&#20197;&#20351;&#20854;&#26126;&#30830;&#21270;&#12290;&#36825;&#37324;&#25105;&#20204;&#23558;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21363;&#65292;LMs&#30340;&#35789;&#35821;&#26159;&#21542;&#25351;&#31216;&#65306;&#21363;&#65292;LMs&#30340;&#36755;&#20986;&#26159;&#21542;&#23454;&#29616;&#20102;&#8220;&#35789;&#35821;-&#19990;&#30028;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26377;&#21021;&#27493;&#30340;&#29702;&#30001;&#35748;&#20026;&#23427;&#20204;&#19981;&#20855;&#22791;&#25351;&#31216;&#33021;&#21147;&#65292;&#22240;&#20026;LMs&#27809;&#26377;&#20687;&#26222;&#36890;&#35821;&#35328;&#29992;&#25143;&#37027;&#26679;&#19982;&#19990;&#30028;&#20114;&#21160;&#12290;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#30340;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#34920;&#35937;&#26159;&#35823;&#23548;&#30340;&#65292;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#35748;&#20026;LMs&#21487;&#20197;&#25351;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
What do language models (LMs) do with language? Everyone agrees that they produce sequences of (mostly) coherent sentences. But are they saying anything with those strings or simply babbling in a convincing simulacrum of language use? This is a vague question, and there are many ways of making it precise. Here we will address one aspect of the question, namely, whether LMs' words refer: that is, whether the outputs of LMs achieve "word-to-world" connections. There is prima facie reason to think they do not since LMs do not interact with the world in the way that ordinary language users do. Drawing on insights from the externalist tradition in philosophy of language, we argue that appearances are misleading and that there is good reason to think that LMs can refer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15337</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#39592;&#26550;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#24310;&#36831;&#12290;&#39640;&#29983;&#25104;&#24310;&#36831;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;LLMs&#37117;&#37319;&#29992;&#20102;&#39034;&#24207;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#20889;&#20316;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#23427;&#25351;&#23548;LLMs&#39318;&#20808;&#29983;&#25104;&#31572;&#26696;&#30340;&#39592;&#26550;&#65292;&#28982;&#21518;&#36890;&#36807;&#24182;&#34892;API&#35843;&#29992;&#25110;&#25209;&#37327;&#35299;&#30721;&#26469;&#24182;&#34892;&#23436;&#25104;&#27599;&#20010;&#39592;&#26550;&#28857;&#30340;&#20869;&#23481;&#12290;SoT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65288;&#22312;11&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;2.39&#20493;&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#22312;&#22810;&#20010;&#38382;&#39064;&#31867;&#21035;&#19978;&#30340;&#31572;&#26696;&#36136;&#37327;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;SoT&#26159;&#19968;&#31181;&#38024;&#23545;&#25928;&#29575;&#30340;&#25968;&#25454;&#23548;&#21521;&#20248;&#21270;&#30340;&#21021;&#27493;&#23581;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#23558;LLMs&#25512;&#21160;&#26356;&#20687;&#20154;&#31867;&#24605;&#32771;&#20197;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.05499</link><description>&lt;p&gt;
LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20854;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#22312;&#23427;&#20204;&#21608;&#22260;&#21050;&#28608;&#20102;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24212;&#29992;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#26381;&#21153;&#20013;&#30340;&#24191;&#27867;&#34701;&#21512;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#23558;&#35299;&#26500;&#23454;&#38469;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#23545;&#21313;&#20010;&#21830;&#19994;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#25915;&#20987;&#31574;&#30053;&#22312;&#23454;&#36341;&#20013;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#21463;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#38543;&#21518;&#21046;&#23450;&#20102;HouYi&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;&#65292;&#23427;&#20511;&#37492;&#20102;&#20256;&#32479;&#30340;Web&#27880;&#20837;&#25915;&#20987;&#12290;HouYi&#20998;&#20026;&#19977;&#20010;&#20851;&#38190;&#20803;&#32032;: &#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#30340;&#39044;&#26500;&#24314;&#25552;&#31034;&#12289;&#19968;&#20010;&#27880;&#20837;&#25552;&#31034;&#35825;&#23548;&#19978;&#19979;&#25991;&#20998;&#21306;&#20197;&#21450;&#19968;&#20010;&#24694;&#24847;&#36733;&#33655;&#65292;&#26088;&#22312;&#23454;&#29616;&#25915;&#20987;&#30446;&#26631;&#12290;&#21033;&#29992;HouYi&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#65292;&#24182;&#28436;&#31034;&#20102;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and sev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07912</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;TKGC&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22312;&#24050;&#30693;&#30340;&#26102;&#38388;&#25139;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#32570;&#22833;&#37096;&#20998;&#30340;&#20107;&#23454;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#21516;&#26102;&#31895;&#30053;&#22320;&#25552;&#21462;&#26102;&#38388;&#25139;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#19981;&#20805;&#20998;&#21033;&#29992;&#20851;&#31995;&#20013;&#38544;&#21547;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;TKGC&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;&#37319;&#26679;&#30340;&#22235;&#20803;&#32452;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#23558;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#38388;&#38548;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#24418;&#25104;&#24102;&#26377;&#38544;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#36830;&#36143;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#30422;&#31574;&#30053;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#26041;&#26696;&#65292;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#36741;&#23548;&#21592;&#21644;&#32534;&#36753;&#22120;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#36755;&#20986;&#30340;&#33258;&#23450;&#20041;&#12290;&#22312;&#20004;&#20010;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.02483</link><description>&lt;p&gt;
ChatGPT&#24341;&#23548;&#30340;&#32534;&#36753;&#36741;&#21161;&#24037;&#20855;&#29992;&#20110;&#25688;&#35201;&#27719;&#24635;&#33258;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-steered Editing Instructor for Customization of Abstractive Summarization. (arXiv:2305.02483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#26041;&#26696;&#65292;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#36741;&#23548;&#21592;&#21644;&#32534;&#36753;&#22120;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#36755;&#20986;&#30340;&#33258;&#23450;&#20041;&#12290;&#22312;&#20004;&#20010;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#26681;&#25454;&#29305;&#23450;&#29992;&#25143;&#38656;&#27714;&#35843;&#25972;&#20854;&#36755;&#20986;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#26041;&#26696;&#8212;&#8212;&#29983;&#25104;&#22120;&#12289;&#36741;&#23548;&#21592;&#21644;&#32534;&#36753;&#22120;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#36755;&#20986;&#30340;&#33258;&#23450;&#20041;&#12290;&#29983;&#25104;&#22120;&#20135;&#29983;&#21021;&#22987;&#36755;&#20986;&#65292;&#38024;&#23545;&#29992;&#25143;&#38656;&#27714;&#30340;&#36741;&#23548;&#21592;&#20135;&#29983;&#32534;&#36753;&#25351;&#23548;&#65292;&#32780;&#32534;&#36753;&#22120;&#20135;&#29983;&#31526;&#21512;&#29992;&#25143;&#20559;&#22909;&#30340;&#20462;&#35746;&#36755;&#20986;&#12290;&#26080;&#27861;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGPT&#65289;&#26082;&#20805;&#24403;&#29983;&#25104;&#22120;&#21448;&#20805;&#24403;&#32534;&#36753;&#22120;&#65292;&#32780;&#36739;&#23567;&#30340;&#27169;&#22411;&#21017;&#20805;&#24403;&#29992;&#25143;&#29305;&#23450;&#30340;&#36741;&#23548;&#21592;&#65292;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29992;&#25143;&#38656;&#27714;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#36741;&#23548;&#21592;&#20351;&#29992;&#32534;&#36753;&#32773;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22521;&#35757;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#32534;&#36753;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#25351;&#23548;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tailoring outputs of large language models, such as ChatGPT, to specific user needs remains a challenge despite their impressive generation quality. In this paper, we propose a tri-agent generation pipeline consisting of a generator, an instructor, and an editor to enhance the customization of generated outputs. The generator produces an initial output, the user-specific instructor generates editing instructions, and the editor generates a revised output aligned with user preferences. The inference-only large language model (ChatGPT) serves as both the generator and the editor, while a smaller model acts as the user-specific instructor to guide the generation process toward user needs. The instructor is trained using editor-steered reinforcement learning, leveraging feedback from the large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#30005;&#35270;&#21095;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20010;&#20307;&#20010;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#35777;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.02313</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#65306;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Personality-aware Human-centric Multimodal Reasoning: A New Task. (arXiv:2304.02313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#30005;&#35270;&#21095;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20010;&#20307;&#20010;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#35777;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#29702;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#35832;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#22810;&#27169;&#24577;&#20449;&#21495;&#20013;&#36827;&#34892;&#25512;&#29702;&#21644;&#21028;&#26029;&#65292;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#19981;&#21516;&#20010;&#24615;&#30340;&#20154;&#21487;&#33021;&#23545;&#21516;&#19968;&#24773;&#22659;&#20570;&#20986;&#19981;&#21516;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#20010;&#24615;&#36825;&#19968;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65288;Personality-aware HMR&#65289;&#65292;&#24182;&#26681;&#25454;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#30005;&#35270;&#21095;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39044;&#27979;&#29305;&#23450;&#26102;&#21051;&#29305;&#23450;&#20154;&#29289;&#30340;&#34892;&#20026;&#65292;&#22522;&#20110;&#20854;&#36807;&#21435;&#21644;&#26410;&#26469;&#26102;&#21051;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;Myers-Briggs&#31867;&#22411;&#25351;&#26631;&#65288;MBTI&#65289;&#34987;&#27880;&#37322;&#24182;&#29992;&#20110;&#34920;&#31034;&#20010;&#20307;&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#31181;&#22522;&#32447;&#26041;&#27861;&#26469;&#22522;&#20934;&#27979;&#35797;&#35813;&#20219;&#21153;&#65292;&#20854;&#20013;&#20004;&#31181;&#26159;&#20174;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#19968;&#31181;&#26159;&#26032;&#25552;&#20986;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#24773;&#24863;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21069;&#26223;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#20010;&#24615;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal reasoning, an area of artificial intelligence that aims at make inferences from multimodal signals such as vision, language and speech, has drawn more and more attention in recent years. People with different personalities may respond differently to the same situation. However, such individual personalities were ignored in the previous studies. In this work, we introduce a new Personality-aware Human-centric Multimodal Reasoning (Personality-aware HMR) task, and accordingly construct a new dataset based on The Big Bang Theory television shows, to predict the behavior of a specific person at a specific moment, given the multimodal information of its past and future moments. The Myers-Briggs Type Indicator (MBTI) was annotated and utilized in the task to represent individuals' personalities. We benchmark the task by proposing three baseline methods, two were adapted from the related tasks and one was newly proposed for our task. The experimental results demonstrate that person
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;XLMRScore&#21644;&#19968;&#20123;&#25913;&#36827;&#25514;&#26045;&#26469;&#35299;&#20915;&#26410;&#32763;&#35793;&#26631;&#35760;&#21644;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.00463</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19981;&#21305;&#37197;&#24863;&#30693;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages. (arXiv:2208.00463v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;XLMRScore&#21644;&#19968;&#20123;&#25913;&#36827;&#25514;&#26045;&#26469;&#35299;&#20915;&#26410;&#32763;&#35793;&#26631;&#35760;&#21644;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#65288;QE&#65289;&#26159;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#36755;&#20986;&#36136;&#37327;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#22312;&#26426;&#22120;&#32763;&#35793;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;XLMRScore&#65292;&#23427;&#26159;&#36890;&#36807;XLM-RoBERTa&#65288;XLMR&#65289;&#27169;&#22411;&#35745;&#31639;&#30340;BERTScore&#30340;&#36328;&#35821;&#35328;&#23545;&#24212;&#29289;&#12290;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#29992;&#20316;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;QE&#26041;&#27861;&#65292;&#20294;&#20351;&#29992;&#23427;&#20250;&#23548;&#33268;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#23548;&#33268;&#24847;&#22806;&#39640;&#32763;&#35793;&#20998;&#25968;&#30340;&#26410;&#32763;&#35793;&#26631;&#35760;&#65292;&#20108;&#26159;&#22312;XLMRScore&#20013;&#24212;&#29992;&#36138;&#23146;&#21305;&#37197;&#26102;&#28304;&#35821;&#35328;&#21644;&#20551;&#35774;&#35821;&#35328;&#20043;&#38388;&#19981;&#21305;&#37197;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26410;&#32763;&#35793;&#30340;&#35789;&#26367;&#25442;&#20026;&#26410;&#30693;&#26631;&#35760;&#65292;&#24182;&#36328;&#35821;&#35328;&#23545;&#40784;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#26356;&#25509;&#36817;&#23545;&#40784;&#30340;&#35789;&#12290;&#25105;&#20204;&#22312;WMT21 QE&#20849;&#20139;&#20219;&#21153;&#30340;&#22235;&#20010;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (QE) is the task of predicting the quality of machine translation (MT) output without any reference. This task has gained increasing attention as an important component in the practical applications of MT. In this paper, we first propose XLMRScore, which is a cross-lingual counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric can be used as a simple unsupervised QE method, while employing it results in two issues: firstly, the untranslated tokens leading to unexpectedly high translation scores, and secondly, the issue of mismatching errors between source and hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate these issues, we suggest replacing untranslated words with the unknown token and the cross-lingual alignment of the pre-trained model to represent aligned words closer to each other, respectively. We evaluate the proposed method on four low-resource language pairs of WMT21 QE shared task, as well as
&lt;/p&gt;</description></item></channel></rss>