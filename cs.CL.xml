<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00929</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#23637;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24076;&#26395;&#23454;&#29616;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#27604;&#22914;&#35821;&#35328;&#19981;&#24179;&#34913;&#12289;&#22810;&#35821;&#35328;&#23545;&#40784;&#21644;&#22266;&#26377;&#20559;&#35265;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;MLLMs&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#22260;&#32469;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#30340;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24341;&#20837; NaijaHate &#25968;&#25454;&#38598;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#25512;&#29305;&#19978;&#35780;&#20272; HSD&#65292;&#21457;&#29616;&#22312;&#20195;&#34920;&#24615;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340; HSD &#24615;&#33021;&#39640;&#20272;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986; NaijaXLM-T &#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#26368;&#22823;&#21270; HSD &#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.19260</link><description>&lt;p&gt;
NaijaHate: &#20351;&#29992;&#20195;&#34920;&#24615;&#25968;&#25454;&#35780;&#20272;&#23612;&#26085;&#21033;&#20122; Twitter &#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19260
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24341;&#20837; NaijaHate &#25968;&#25454;&#38598;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#25512;&#29305;&#19978;&#35780;&#20272; HSD&#65292;&#21457;&#29616;&#22312;&#20195;&#34920;&#24615;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340; HSD &#24615;&#33021;&#39640;&#20272;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986; NaijaXLM-T &#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#26368;&#22823;&#21270; HSD &#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22312;&#32447;&#24179;&#21488;&#19978;&#24694;&#24847;&#20869;&#23481;&#34067;&#24310;&#30340;&#20840;&#29699;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#22312;&#32654;&#22269;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65288;HSD&#65289;&#27169;&#22411;&#65292;&#20174;&#32780;&#26080;&#27861;&#25512;&#24191;&#21040;&#26469;&#33258;&#22823;&#22810;&#25968;&#19990;&#30028;&#30340;&#33521;&#35821;&#26041;&#35328;&#12290;&#27492;&#22806;&#65292;HSD&#27169;&#22411;&#36890;&#24120;&#22312;&#31574;&#21010;&#26679;&#26412;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;HSD&#26631;&#27880;&#30340; NaijaHate &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23612;&#26085;&#21033;&#20122;&#25512;&#25991;&#30340;&#20195;&#34920;&#24615;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;HSD&#22312;&#20256;&#32479;&#25991;&#29486;&#20013;&#20256;&#32479;&#20351;&#29992;&#30340;&#26377;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#22312;&#20195;&#34920;&#24615;&#25968;&#25454;&#19978;&#24456;&#22823;&#31243;&#24230;&#19978;&#39640;&#20272;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; NaijaXLM-T&#65292;&#19968;&#20010;&#38024;&#23545;&#23612;&#26085;&#21033;&#20122; Twitter &#19978;&#19979;&#25991;&#37327;&#36523;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#26368;&#22823;&#21270;HSD&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;-&#26426;&#28151;&#21512;&#26041;&#27861;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19260v1 Announce Type: new  Abstract: To address the global issue of hateful content proliferating in online platforms, hate speech detection (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data. We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, we show that in this context, a human-in-the-l
&lt;/p&gt;</description></item><item><title>NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.18680</link><description>&lt;p&gt;
NL-ITI&#65306;&#20248;&#21270;&#25506;&#27979;&#21644;&#24178;&#39044;&#20197;&#25913;&#36827;ITI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18680
&lt;/p&gt;
&lt;p&gt;
NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#23481;&#26131;&#36820;&#22238;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#29702;&#26102;&#24178;&#39044;(Inference-Time-Intervention, ITI)&#26041;&#27861;&#24341;&#20837;&#30340;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;ITI&#26041;&#27861;&#35782;&#21035;&#21253;&#21547;&#26368;&#22810;&#25152;&#38656;&#30693;&#35782;&#31867;&#22411;(&#20363;&#22914;&#30495;&#23454;&#20449;&#24687;)&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#38543;&#21518;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#28608;&#27963;&#34987;&#31227;&#21160;&#21040;&#25152;&#36873;&#27880;&#24847;&#21147;&#22836;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;-&#38750;&#32447;&#24615;ITI(NL-ITI)&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;ITI&#26694;&#26550;&#12290;NL-ITI&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;TruthfulQA&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#22522;&#20934;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;ITI&#32467;&#26524;&#25253;&#21578;&#20102;&#32422;14%&#30340;MC1&#25351;&#26631;&#25913;&#36827;&#12290;NL-ITI&#36824;&#22312;&#20854;&#20182;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#32489;-&#22312;MMLU&#30340;&#21830;&#19994;&#20262;&#29702;&#23376;&#39046;&#22495;&#19978;&#65292;&#27604;&#22522;&#32447;LLaMA2-7B&#26377;&#32422;18%&#30340;MC1&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;NL-ITI&#22312;&#25928;&#26524;&#26356;&#22909;&#30340;&#21516;&#26102;&#20063;&#26356;&#23569;&#20405;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15097</link><description>&lt;p&gt;
&#35770;&#25454;&#24863;&#30693;&#20107;&#20214;&#38142;&#25509;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Argument-Aware Approach To Event Linking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15097
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#20107;&#20214;&#38142;&#25509;&#23558;&#25991;&#26412;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#20013;&#30456;&#20851;&#33410;&#28857;&#36830;&#25509;&#36215;&#26469;&#12290;&#20808;&#21069;&#22312;&#20107;&#20214;&#38142;&#25509;&#26041;&#38754;&#30340;&#30740;&#31350;&#20027;&#35201;&#20511;&#37492;&#20102;&#23454;&#20307;&#38142;&#25509;&#30340;&#26041;&#27861;&#65292;&#24573;&#30053;&#20102;&#20107;&#20214;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#19982;&#24191;&#27867;&#25506;&#35752;&#30340;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30456;&#27604;&#65292;&#20107;&#20214;&#20855;&#26377;&#26356;&#21152;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#20854;&#20851;&#32852;&#30340;&#35770;&#25454;&#26356;&#26377;&#25928;&#22320;&#21152;&#20197;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#20107;&#20214;&#30340;&#20449;&#24687;&#20016;&#23500;&#24615;&#23548;&#33268;&#20107;&#20214;&#30693;&#35782;&#24211;&#30340;&#31232;&#32570;&#24615;&#12290;&#36825;&#24378;&#35843;&#20102;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#38656;&#35201;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#20316;&#20026;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#36825;&#19968;&#39046;&#22495;&#21463;&#21040;&#20102;&#26377;&#38480;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26631;&#35760;&#20107;&#20214;&#35770;&#25454;&#20449;&#24687;&#26469;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26377;&#20851;&#20107;&#20214;&#25552;&#21450;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
&lt;/p&gt;</description></item><item><title>Wav2Gloss&#25552;&#20986;&#20102;&#20174;&#35821;&#38899;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#35328;&#27880;&#37322;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;Fieldwork&#65292;&#20998;&#26512;&#34920;&#26126;&#39044;&#20808;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#26377;&#21161;&#20110;&#32763;&#35793;&#21644;&#27880;&#37322;&#65292;&#24182;&#19988;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#25928;&#26524;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.13169</link><description>&lt;p&gt;
Wav2Gloss&#65306;&#20174;&#35821;&#38899;&#29983;&#25104;&#20998;&#35789;&#21518;&#30340;&#25991;&#23383;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Wav2Gloss: Generating Interlinear Glossed Text from Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13169
&lt;/p&gt;
&lt;p&gt;
Wav2Gloss&#25552;&#20986;&#20102;&#20174;&#35821;&#38899;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#35328;&#27880;&#37322;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;Fieldwork&#65292;&#20998;&#26512;&#34920;&#26126;&#39044;&#20808;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#26377;&#21161;&#20110;&#32763;&#35793;&#21644;&#27880;&#37322;&#65292;&#24182;&#19988;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#25104;&#21315;&#19978;&#19975;&#31181;&#35821;&#35328;&#38754;&#20020;&#28781;&#32477;&#30340;&#21361;&#38505;&#65292;&#36825;&#23545;&#25991;&#21270;&#36523;&#20221;&#21644;&#20154;&#31867;&#35821;&#35328;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#20998;&#35789;&#21518;&#30340;&#25991;&#23383;&#27880;&#37322;&#65288;IGT&#65289;&#26159;&#19968;&#31181;&#35821;&#35328;&#27880;&#37322;&#24418;&#24335;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#36825;&#20123;&#35821;&#35328;&#31038;&#21306;&#36827;&#34892;&#25991;&#26723;&#32534;&#21046;&#21644;&#36164;&#28304;&#21019;&#24314;&#12290;IGT&#36890;&#24120;&#21253;&#25324;&#65288;1&#65289;&#36716;&#24405;&#65292;&#65288;2&#65289;&#24418;&#24577;&#20998;&#21106;&#65292;&#65288;3&#65289;&#25991;&#26412;&#27880;&#37322; &#21644;&#65288;4&#65289;&#21040;&#20027;&#27969;&#35821;&#35328;&#30340;&#33258;&#30001;&#32763;&#35793;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Wav2Gloss&#65306;&#19968;&#20010;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#33258;&#21160;&#25552;&#21462;&#36825;&#22235;&#20010;&#27880;&#37322;&#32452;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;Fieldwork&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;37&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26377;&#26631;&#20934;&#26684;&#24335;&#21644;&#35757;&#32451;/&#35780;&#20272;&#38598;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13169v1 Announce Type: new  Abstract: Thousands of the world's languages are in danger of extinction--a tremendous threat to cultural identities and human language diversity. Interlinear Glossed Text (IGT) is a form of linguistic annotation that can support documentation and resource creation for these languages' communities. IGT typically consists of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4) free translations to a majority language. We propose Wav2Gloss: a task to extract these four annotation components automatically from speech, and introduce the first dataset to this end, Fieldwork: a corpus of speech with all these annotations covering 37 languages with standard formatting and train/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods, with analysis suggesting that pre-trained decoders assist with translation and glossing, that multi-task and multilingual approaches are underperformant, and that end-to-end systems perform 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10081</link><description>&lt;p&gt;
DRAGIN&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#26816;&#32034;&#12290;&#35813;&#33539;&#24335;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#30830;&#23450;&#28608;&#27963;&#26816;&#32034;&#27169;&#22359;&#30340;&#26368;&#20339;&#26102;&#26426;&#65288;&#20915;&#23450;&#20309;&#26102;&#26816;&#32034;&#65289;&#20197;&#21450;&#19968;&#26086;&#35302;&#21457;&#26816;&#32034;&#65292;&#21046;&#23450;&#36866;&#24403;&#30340;&#26597;&#35810;&#65288;&#30830;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21160;&#24577;RAG&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#26816;&#32034;&#30340;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#20915;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#30340;&#31574;&#30053;&#36890;&#24120;&#23616;&#38480;&#20110;LLM&#30340;&#26368;&#36817;&#19968;&#21477;&#25110;&#26368;&#21518;&#20960;&#20010;&#26631;&#35760;&#65292;&#32780;LLM&#30340;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#21487;&#33021;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DRAGIN&#65292; &#21363;&#22522;&#20110;LLMs&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
&lt;/p&gt;</description></item><item><title>LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07974</link><description>&lt;p&gt;
LiveCodeBench&#65306;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#21644;&#26080;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07974
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#39046;&#22495;&#65292;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;LLMs&#30340;&#24320;&#21457;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#65288;&#20363;&#22914;HumanEval&#65292;MBPP&#65289;&#19981;&#20877;&#36275;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;LiveCodeBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#23427;&#20250;&#20174;&#19977;&#20010;&#31454;&#36187;&#24179;&#21488;&#65288;LeetCode&#12289;AtCoder&#21644;CodeForces&#65289;&#19978;&#36830;&#32493;&#22320;&#25910;&#38598;&#26032;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#36824;&#30528;&#37325;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#33021;&#21147;&#65292;&#22914;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#21644;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20195;&#30721;&#29983;&#25104;&#12290;&#30446;&#21069;&#65292;LiveCodeBench&#25176;&#31649;&#20102;&#22312;2023&#24180;5&#26376;&#33267;2024&#24180;2&#26376;&#20043;&#38388;&#21457;&#24067;&#30340;400&#20010;&#39640;&#36136;&#37327;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#24050;&#32463;&#35780;&#20272;&#20102;9&#20010;&#22522;&#26412;LLMs&#21644;20&#20010;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07974v1 Announce Type: cross  Abstract: Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs o
&lt;/p&gt;</description></item><item><title>ERA-CoT &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25903;&#25345;&#24605;&#32500;&#38142;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29702;&#35299;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#20219;&#21153;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06932</link><description>&lt;p&gt;
ERA-CoT: &#36890;&#36807;&#23454;&#20307;&#20851;&#31995;&#20998;&#26512;&#25913;&#36827;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06932
&lt;/p&gt;
&lt;p&gt;
ERA-CoT &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25903;&#25345;&#24605;&#32500;&#38142;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29702;&#35299;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#20219;&#21153;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22797;&#26434;&#22330;&#26223;&#26102;&#65292;LLMs &#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#23384;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#38544;&#24335;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; ERA-CoT&#65292;&#36890;&#36807;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#24110;&#21161; LLMs &#29702;&#35299;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25903;&#25345;&#19981;&#21516;&#20219;&#21153;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#30340; CoT &#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;ERA-CoT &#34920;&#29616;&#20986;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312; GPT3.5 &#19978;&#24179;&#22343;&#27604;&#20197;&#21069;&#30340; SOTA &#22522;&#32447;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340; 5.1% &#25913;&#36827;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ERA-CoT &#25552;&#39640;&#20102;LLM&#23545;&#23454;&#20307;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06932v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06840</link><description>&lt;p&gt;
RA-ISF: &#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#23398;&#20064;&#26816;&#32034;&#22686;&#24378;&#20197;&#22238;&#31572;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06840
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#20197;&#21069;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#36845;&#20195;&#33258;&#21453;&#39304;(RA-ISF)&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#27169;&#22359;&#36845;&#20195;&#20998;&#35299;&#20219;&#21153;&#24182;&#22788;&#29702;&#23427;&#20204;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#22312;&#35832;&#22914;GPT3.5&#12289;Llama2&#20043;&#31867;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06840v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05535</link><description>&lt;p&gt;
&#35762;&#36848;&#65292;&#32780;&#19981;&#26159;&#23637;&#31034;&#65281;&#65306;&#35821;&#35328;&#25351;&#23548;&#26377;&#21161;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LaGTran&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21363;&#21487;&#33719;&#24471;&#25110;&#26131;&#20110;&#33719;&#21462;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24341;&#23548;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21040;&#20855;&#26377;&#22495;&#20559;&#31227;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#30693;&#35782;&#36716;&#31227;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#23500;&#35821;&#20041;&#30340;&#25991;&#26412;&#27169;&#24577;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#36716;&#31227;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#26426;&#21046;&#65292;&#20351;&#29992;&#28304;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#30446;&#26631;&#25991;&#26412;&#25551;&#36848;&#19978;&#29983;&#25104;&#39044;&#27979;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20316;&#20026;&#30456;&#24212;&#22270;&#20687;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35821;&#35328;&#25351;&#23548;&#20026;&#39537;&#21160;&#65292;&#20986;&#22855;&#22320;&#31616;&#21333;&#26131;&#34892;&#65292;&#21364;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22914;GeoNet&#21644;DomainNet&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#26497;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PARADISE&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.03167</link><description>&lt;p&gt;
PARADISE&#65306;&#36890;&#36807;&#36807;&#31243;&#35686;&#21578;&#21644;&#25552;&#31034;&#25968;&#25454;&#38598;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#24335;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03167
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PARADISE&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31038;&#21306;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#35268;&#21010;&#25110;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#36234;&#21457;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30740;&#31350;&#20351;&#29992;LLMs&#20026;&#31616;&#21270;&#22330;&#26223;&#29983;&#25104;&#39640;&#32423;&#35745;&#21010;&#65292;&#36866;&#24212;&#24230;&#37327;&#32570;&#20047;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#39046;&#22495;&#22810;&#26679;&#24615;&#65292;&#38480;&#21046;&#20854;&#35268;&#21010;&#33021;&#21147;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PARADISE&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;Q&#65286;A&#26684;&#24335;&#30340;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#26469;&#33258;wikiHow&#30340;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#12290;&#23427;&#28041;&#21450;&#19982;&#30446;&#26631;&#30452;&#25509;&#30456;&#20851;&#30340;&#35686;&#21578;&#21644;&#25552;&#31034;&#25512;&#26029;&#20219;&#21153;&#65292;&#25490;&#38500;&#20013;&#38388;&#27493;&#39588;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#38544;&#21547;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03167v1 Announce Type: new  Abstract: Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&amp;A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#29983;&#25104;&#26694;&#26550;CoGenesis&#65292;&#25972;&#21512;&#22823;&#22411;&#21644;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#36923;&#36753;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03129</link><description>&lt;p&gt;
CoGenesis&#65306;&#19968;&#20010;&#21327;&#20316;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03129
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#29983;&#25104;&#26694;&#26550;CoGenesis&#65292;&#25972;&#21512;&#22823;&#22411;&#21644;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#36923;&#36753;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#25509;&#35302;&#31169;&#20154;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#19981;&#21487;&#36991;&#20813;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#65288;&#23588;&#20854;&#26159;&#36739;&#23567;&#30340;&#27169;&#22411;&#65289;&#22312;&#20010;&#20154;&#35774;&#22791;&#19978;&#65292;&#22914;PC&#21644;&#26234;&#33021;&#25163;&#26426;&#19978;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#30427;&#34892;&#36235;&#21183;&#12290;&#22312;&#20805;&#28385;&#29992;&#25143;&#20449;&#24687;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#27169;&#22411;&#26082;&#33021;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21448;&#33021;&#39640;&#25928;&#25191;&#34892;&#21629;&#20196;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CoGenesis&#65292;&#19968;&#20010;&#21327;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22823;&#22411;&#27169;&#22411;&#65288;&#25176;&#31649;&#22312;&#20113;&#22522;&#30784;&#35774;&#26045;&#19978;&#65289;&#21644;&#23567;&#22411;&#27169;&#22411;&#65288;&#37096;&#32626;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#65289;&#65292;&#20197;&#36923;&#36753;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31649;&#36947;&#26469;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#20889;&#20316;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#65292;&#20316;&#20026;&#36825;&#19968;&#30740;&#31350;&#38382;&#39064;&#30340;&#27979;&#35797;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#33609;&#22270;&#21644;&#23545;&#25968;&#30340;&#20004;&#20010;CoGenesis&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#25105;&#20204;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#21478;&#22806;&#20004;&#20010;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03129v1 Announce Type: new  Abstract: With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional op
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#21475;&#35821;&#23545;&#35805;&#30340;ToM&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;LM&#22312;ToM&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#34920;&#26126;&#22312;Common-ToM&#19978;&#25972;&#21512;&#31616;&#21333;&#26126;&#30830;&#30340;&#20449;&#24565;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;LM&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02451</link><description>&lt;p&gt;
&#35266;&#28857;&#26159;&#25105;&#30340;&#65292;&#20063;&#26159;&#20320;&#30340;&#65306;&#20351;&#29992;&#20849;&#21516;&#22522;&#30784;&#35780;&#20272;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02451
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#21475;&#35821;&#23545;&#35805;&#30340;ToM&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;LM&#22312;ToM&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#34920;&#26126;&#22312;Common-ToM&#19978;&#25972;&#21512;&#31616;&#21333;&#26126;&#30830;&#30340;&#20449;&#24565;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;LM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#33021;&#21147;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23454;&#39564;&#32467;&#26524;&#19982;&#20154;&#31867;&#34892;&#20026;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#21457;&#29983;&#30340;&#21475;&#22836;&#23545;&#35805;&#30340;ToM&#25968;&#25454;&#38598;&#65292;Common-ToM&#65292;&#24182;&#23637;&#31034;LMs&#22312;&#23637;&#31034;ToM&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;Common-ToM&#19978;&#25972;&#21512;&#19968;&#20010;&#31616;&#21333;&#26126;&#30830;&#30340;&#20449;&#24565;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;LM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02451v1 Announce Type: new  Abstract: Evaluating the theory of mind (ToM) capabilities of language models (LMs) has recently received much attention. However, many existing benchmarks rely on synthetic data which risks misaligning the resulting experiments with human behavior. We introduce the first ToM dataset based on naturally occurring spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We then show that integrating a simple, explicit representation of beliefs improves LM performance on Common-ToM.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02271</link><description>&lt;p&gt;
RIFF: &#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#25913;&#20889;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#31934;&#30830;&#22320;&#20026;&#19979;&#28216;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20248;&#21270;&#36755;&#20837;&#25552;&#31034;&#25110;&#35843;&#25972;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914; LoRA&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21464;&#21407;&#22987;&#20219;&#21153;&#30340;&#36755;&#20837;&#25991;&#26412;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#37325;&#20889;&#36755;&#20837;&#25991;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#36229;&#20986;&#20102;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02271v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;VariErr&#65292;&#19987;&#27880;&#20110;NLI&#20219;&#21153;&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#21644;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#21306;&#20998;&#12290;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#22788;&#29702;&#20449;&#21495;&#38750;&#40657;&#30333;&#24773;&#20917;&#19979;&#30340;&#20808;&#21069;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.01931</link><description>&lt;p&gt;
VariErr NLI: &#23558;&#27880;&#37322;&#38169;&#35823;&#19982;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#21306;&#20998;&#24320;&#26469;
&lt;/p&gt;
&lt;p&gt;
VariErr NLI: Separating Annotation Error from Human Label Variation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01931
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;VariErr&#65292;&#19987;&#27880;&#20110;NLI&#20219;&#21153;&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#21644;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#21306;&#20998;&#12290;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#22788;&#29702;&#20449;&#21495;&#38750;&#40657;&#30333;&#24773;&#20917;&#19979;&#30340;&#20808;&#21069;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#26159;&#30001;&#20110;&#27880;&#37322;&#32773;&#20986;&#20110;&#26377;&#25928;&#21407;&#22240;&#23558;&#19981;&#21516;&#26631;&#31614;&#20998;&#37197;&#32473;&#21516;&#19968;&#39033;&#32780;&#20135;&#29983;&#30340;&#65292;&#32780;&#27880;&#37322;&#38169;&#35823;&#26159;&#25351;&#30001;&#20110;&#26080;&#25928;&#21407;&#22240;&#20998;&#37197;&#26631;&#31614;&#12290;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#26159;&#23396;&#31435;&#30740;&#31350;&#23427;&#20204;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#21069;&#27809;&#26377;&#19987;&#27880;&#20110;&#21306;&#20998;&#38169;&#35823;&#19982;&#20449;&#21495;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#21495;&#36229;&#36234;&#40657;&#30333;&#20043;&#22788;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#21644;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;VariErr&#65288;&#21464;&#24322;&#19982;&#38169;&#35823;&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#33521;&#35821;NLI&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#36718;&#27880;&#37322;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#27880;&#37322;&#32773;&#35299;&#37322;&#27599;&#20010;&#26631;&#31614;&#65292;&#28982;&#21518;&#21028;&#26029;&#26631;&#31614;&#35299;&#37322;&#23545;&#30340;&#26377;&#25928;&#24615;&#12290;VariErr&#21253;&#21547;&#23545;500&#20010;&#37325;&#26032;&#27880;&#37322;&#30340;NLI&#39033;&#30446;&#19978;&#30340;1,933&#20010;&#35299;&#37322;&#36827;&#34892;&#30340;7,574&#20010;&#26377;&#25928;&#24615;&#21028;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#33258;&#21160;&#38169;&#35823;&#26816;&#27979;&#65288;AED&#65289;&#26041;&#27861;&#21644;GPT&#22312;&#25581;&#31034;&#38169;&#35823;&#19982;&#20449;&#21495;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01931v1 Announce Type: new  Abstract: Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white. To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs. \name{} contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01166</link><description>&lt;p&gt;
DINER&#65306;&#20351;&#29992;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#26469;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#27169;&#22411;&#23481;&#26131;&#20174;&#27880;&#37322;&#20559;&#35265;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#23545;&#25239;&#24615;&#25968;&#25454;&#36716;&#25442;&#19978;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;&#22312;&#21435;&#20559;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#65292;&#20027;&#35201;&#21487;&#20998;&#20026;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#21435;&#20559;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21333;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#19978;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#21464;&#37327;&#65288;&#30446;&#26631;&#26041;&#38754;&#21644;&#35780;&#35770;&#65289;&#30340;ABSA&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;ABSA&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#35265;&#22522;&#20110;&#19981;&#21516;&#30340;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#24471;&#21040;&#22788;&#29702;&#12290;&#23545;&#20110;&#35780;&#35770;&#20998;&#25903;&#65292;&#20559;&#35265;&#34987;&#24314;&#27169;&#20026;&#26469;&#33258;&#19978;&#19979;&#25991;&#30340;&#38388;&#25509;&#28151;&#26434;&#65292;&#20854;&#20013;&#23454;&#26045;&#21453;&#21521;&#35843;&#25972;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01166v1 Announce Type: cross  Abstract: Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01165</link><description>&lt;p&gt;
STAR: &#20351;&#29992;&#21160;&#24577;&#20027;&#21160;&#23398;&#20064;&#32422;&#26463;LoRA&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#25552;&#31034;&#26041;&#27861;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20173;&#38656;&#30417;&#30563;&#35757;&#32451;&#12290;&#38024;&#23545;LLMs&#30340;&#21442;&#25968;&#20247;&#22810;&#21644;&#20869;&#23384;&#28040;&#32791;&#22823;&#38382;&#39064;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#26041;&#27861;&#21644;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26088;&#22312;&#35299;&#20915;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#28040;&#32791;&#30340;&#38382;&#39064;&#65292;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#19968;&#31181;&#26126;&#26174;&#30340;&#26041;&#24335;&#26159;&#23558;PEFT&#26041;&#27861;&#19982;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#32452;&#21512;&#24182;&#38750;&#31616;&#21333;&#65292;&#24182;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#38024;&#23454;&#39564;&#65292;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21487;&#33021;&#30001;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#35299;&#37322;&#65306;&#19981;&#30830;&#23450;&#24615;&#24046;&#36317;&#21644;&#27169;&#22411;&#26657;&#20934;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01165v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the u
&lt;/p&gt;</description></item><item><title>Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18334</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18334
&lt;/p&gt;
&lt;p&gt;
Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bonito&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#29983;&#25104;&#65306;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#30340;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#29992;&#25143;&#19987;&#38376;&#30340;&#31169;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;1.65M&#20010;&#31034;&#20363;&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;Bonito&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#37325;&#26032;&#28151;&#21512;&#25104;&#20803;&#27169;&#26495;&#32780;&#21019;&#24314;&#30340;&#12290;&#25968;&#25454;&#38598;&#30340;&#20803;&#27169;&#26495;&#20135;&#29983;&#35757;&#32451;&#31034;&#20363;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#36755;&#20986;&#21253;&#25324;&#25351;&#20196;&#21644;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Bonito&#20026;&#19971;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#20219;&#21153;&#65292;&#36328;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411; -- &#26159;&#38750;&#38382;&#31572;&#12289;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702; -- &#24182;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Bonito&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18158</link><description>&lt;p&gt;
&#35780;&#20272;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20855;&#20307;&#22320;&#65292;PTQ&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;LLMs&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#28385;&#36275;&#21508;&#31181;&#22330;&#26223;&#19979;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#35201;&#27714;&#65292;&#23545;&#37327;&#21270;LLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#25351;&#23548;&#37327;&#21270;&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;PTQ&#23545;11&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;&#21253;&#25324;OPT&#12289;LLaMA2&#12289;Falcon&#12289;Bloomz&#12289;Mistral&#12289;ChatGLM&#12289;Vicuna&#12289;LongChat&#12289;StableLM&#12289;Gemma&#21644;Mamba&#65289;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#33539;&#22260;&#20174;125M&#21040;180B&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65306;&#22522;&#30784;NLP&#12289;&#31361;&#28982;&#20986;&#29616;&#30340;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#12289;&#23545;&#35805;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18059</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#28508;&#22312;&#22320;&#23384;&#22312;&#35823;&#23548;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#26469;&#21152;&#20197;&#35268;&#33539;&#30340;&#24517;&#35201;&#24615;&#12290;&#27700;&#21360;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#22312;LLM&#25512;&#29702;&#38454;&#27573;&#21521;&#25991;&#26412;&#20013;&#23884;&#20837;&#38544;&#34255;&#26631;&#35760;&#65292;&#32780;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#31639;&#27861;&#38754;&#20020;&#30528;&#23454;&#29616;&#25554;&#20837;&#27700;&#21360;&#30340;&#21487;&#26816;&#27979;&#24615;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#24120;&#24120;&#20250;&#25439;&#23475;&#21478;&#19968;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27700;&#21360;&#25216;&#26415;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MOO&#26469;&#20248;&#21270;&#26816;&#27979;&#21644;&#35821;&#20041;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.17641</link><description>&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Variational Learning is Effective for Large Deep Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17641
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26080;&#25928;&#30340;&#26222;&#36941;&#30475;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Improved Variational Online Newton (IVON)&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65288;&#22914;GPT-2&#21644;ResNets&#65289;&#26102;&#22987;&#32456;&#33021;&#22815;&#19982;Adam&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#23427;&#12290;IVON&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#19982;Adam&#30456;&#21516;&#65292;&#20294;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IVON&#30340;&#20960;&#31181;&#26032;&#29992;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#27169;&#22411;&#21512;&#24182;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#21644;&#24544;&#23454;&#20272;&#35745;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#21464;&#20998;&#23398;&#20064;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17447</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Named Entity Recognition Models for Recipes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#36890;&#36807;&#21508;&#31181;&#21162;&#21147;&#26041;&#24335;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21253;&#25324;&#21475;&#21619;&#12289;&#33829;&#20859;&#12289;&#20581;&#24247;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#39135;&#35889;&#26159;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20195;&#20195;&#30456;&#20256;&#30340;&#25991;&#21270;&#33014;&#22218;&#12290;&#33258;&#21160;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#21327;&#35758;&#65292;&#21363;&#39135;&#35889;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#26469;&#35828;&#37117;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#65292;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#26032;&#39062;&#39135;&#35889;&#29983;&#25104;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#24050;&#30693;&#26631;&#31614;&#30340;&#38750;&#32467;&#26500;&#21270;&#25110;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#25163;&#21160;&#27880;&#37322;&#30340;6,611&#20010;&#25104;&#20998;&#30701;&#35821;&#30340;&#25968;&#25454;&#24320;&#22987;&#65292;&#32047;&#31215;&#21019;&#24314;&#20102;26,445&#20010;&#30701;&#35821;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#20998;&#26512;&#20102;&#26469;&#33258;RecipeDB&#30340;&#25104;&#20998;&#30701;&#35821;&#65292;&#36825;&#26159;&#40644;&#37329;&#26631;&#20934;&#30340;&#39135;&#35889;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;Stanford NER&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#23545;88,526&#20010;&#30701;&#35821;&#30340;&#23376;&#38598;&#36827;&#34892;&#20102;&#21462;&#26679;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#31574;&#30053;&#23637;&#24320;&#20840;&#38754;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.16775</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31574;&#30053;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Quantization Strategies for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#31574;&#30053;&#23637;&#24320;&#20840;&#38754;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#36890;&#24120;&#20250;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#37327;&#21270;&#25216;&#26415;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#26435;&#37325;&#25110;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#65292;&#24182;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#65292;&#24050;&#32463;&#22240;LLMs&#30340;&#20852;&#36215;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37327;&#21270;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#65292;&#37327;&#21270;&#23545;&#35843;&#25972;&#36807;&#25351;&#20196;&#30340;LLMs&#30340;&#24433;&#21709;&#20197;&#21450;&#37327;&#21270;LLMs&#30340;&#22256;&#24785;&#24230;&#19982;&#22522;&#20934;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#19981;&#26126;&#30830;&#12290;&#23545;&#37327;&#21270;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#23569;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#22312;&#20854;&#20182;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#65288;1&#65289;&#30693;&#35782;&#21644;&#23481;&#37327;&#65292;&#65288;2&#65289;&#23545;&#40784;&#24615;&#21644;&#65288;3&#65289;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \&amp; capacity, (2) alignment, and (3) efficiency, and conduct extensive experi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#21487;&#20197;&#35299;&#37322;&#20854;&#22810;&#35821;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#28608;&#27963;&#27010;&#29575;&#29109;&#65288;LAPE&#65289;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#30340;&#33021;&#21147;&#20027;&#35201;&#30001;&#23569;&#37327;&#31070;&#32463;&#20803;&#20915;&#23450;&#12290;</title><link>https://arxiv.org/abs/2402.16438</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#35821;&#33021;&#21147;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16438
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#21487;&#20197;&#35299;&#37322;&#20854;&#22810;&#35821;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#28608;&#27963;&#27010;&#29575;&#29109;&#65288;LAPE&#65289;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#30340;&#33021;&#21147;&#20027;&#35201;&#30001;&#23569;&#37327;&#31070;&#32463;&#20803;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#35821;&#33021;&#21147;&#65292;&#21363;&#20351;&#26410;&#32463;&#36807;&#19987;&#38376;&#31574;&#21010;&#30340;&#22810;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#35299;&#37322;LLMs&#22788;&#29702;&#22810;&#35821;&#25991;&#26412;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;Transformer&#26550;&#26500;&#30340;&#26500;&#25104;&#65292;&#20197;&#25214;&#20986;&#35821;&#35328;&#29305;&#23450;&#21306;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#35821;&#35328;&#28608;&#27963;&#27010;&#29575;&#29109;&#65288;LAPE&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;LLMs&#20869;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22522;&#20110;LAPE&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65292;&#21363;LLaMA-2&#21644;BLOOM&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#30340;&#33021;&#21147;&#20027;&#35201;&#26159;&#30001;&#19968;&#23567;&#37096;&#20998;&#31070;&#32463;&#20803;&#20915;&#23450;&#30340;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#20027;&#35201;&#20301;&#20110;&#27169;&#22411;&#30340;&#39030;&#37096;&#21644;&#24213;&#37096;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36873;&#25321;&#24615;&#28608;&#27963;&#25110;&#20572;&#29992;&#31070;&#32463;&#20803;&#26469;&#8220;&#24341;&#23548;&#8221;LLMs&#30340;&#36755;&#20986;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16438v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on two representative LLMs, namely LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to "steer" the output language of LLMs by selectively activating or deactivatin
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15637</link><description>&lt;p&gt;
&#22788;&#29702;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#39034;&#24207;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15637
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#39034;&#24207;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLMs&#65289;&#23545;&#27492;&#39034;&#24207;&#27604;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLMs&#65289;&#26356;&#25935;&#24863;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#29616;&#35937;&#24402;&#22240;&#20110;CausalLMs&#20013;&#30340;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#25513;&#27169;&#65292;&#36825;&#20123;&#25513;&#27169;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#19981;&#33021;&#35775;&#38382;&#38543;&#21518;&#30340;&#26631;&#35760;&#30340;&#20449;&#24687;&#12290;&#36825;&#23548;&#33268;&#19981;&#21516;&#20301;&#32622;&#30340;&#26679;&#26412;&#20855;&#26377;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#20301;&#32622;&#30340;&#34920;&#24449;&#24046;&#24322;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#20449;&#24687;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#19981;&#21516;&#20301;&#32622;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#34920;&#24449;&#65292;&#24182;&#24341;&#20837;&#19968;&#33268;&#24615;&#25439;&#22833;&#20197;&#30830;&#20445;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15637v1 Announce Type: new  Abstract: In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models (CausalLMs) are more sensitive to this order compared to prefix language models (PrefixLMs). We attribute this phenomenon to the auto-regressive attention masks within CausalLMs, which restrict each token from accessing information from subsequent tokens. This results in different receptive fields for samples at different positions, thereby leading to representation disparities across positions. To tackle this challenge, we introduce an unsupervised fine-tuning method, termed the Information-Augmented and Consistency-Enhanced approach. This approach utilizes contrastive learning to align representations of in-context examples across different positions and introduces a consistency loss to ensure simi
&lt;/p&gt;</description></item><item><title>PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15082</link><description>&lt;p&gt;
PEMT: &#22810;&#20219;&#21153;&#30456;&#20851;&#24615;&#24341;&#23548;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15082
&lt;/p&gt;
&lt;p&gt;
PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#20316;&#20026;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#24050;&#32463;&#23835;&#36215;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#21040;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20197;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#36866;&#37197;&#22120;&#65292;&#35201;&#20040;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#30693;&#35782;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEMT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21019;&#26032;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#12290;PEMT&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26694;&#26550;&#25193;&#23637;&#20026;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#30340;&#21152;&#26435;&#32452;&#21512;&#20197;&#25429;&#33719;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#36825;&#20123;&#26435;&#37325;&#30001;&#19968;&#20010;&#38376;&#25511;&#21333;&#20803;&#30830;&#23450;&#65292;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#27979;&#37327;&#30446;&#26631;&#20219;&#21153;&#21644;&#27599;&#20010;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.14979</link><description>&lt;p&gt;
&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#26159;&#19968;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Models for Human Preferences is a Causal Inference Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#21830;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#25991;&#26412;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#20174;&#30452;&#25509;&#32467;&#26524;&#25968;&#25454;&#38598;&#20013;&#38024;&#23545;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#30001;&#19968;&#27573;&#25991;&#26412;&#21644;&#19968;&#20010;&#34913;&#37327;&#35835;&#32773;&#21709;&#24212;&#30340;&#30456;&#20851;&#25968;&#20540;&#32467;&#26524;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#24212;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#27491;&#30830;&#23398;&#20064;&#25991;&#26412;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27491;&#24335;&#21270;&#20102;&#36825;&#20010;&#22240;&#26524;&#35821;&#35328;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;--&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;(CPO)--&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26080;&#20559;&#26367;&#20195;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#30340;CPO(DR-CPO)&#25193;&#23637;CPO&#65292;&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26126;&#26174;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807; Logit Lens &#21644;&#24178;&#39044;&#23454;&#39564;&#25581;&#31034;&#20102;LLMs&#20013;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20462;&#34917;&#32452;&#21512;&#25512;&#29702;&#38169;&#35823;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14328</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20462;&#34917;LLMs&#20013;&#30340;&#32452;&#25104;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Understanding and Patching Compositional Reasoning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807; Logit Lens &#21644;&#24178;&#39044;&#23454;&#39564;&#25581;&#31034;&#20102;LLMs&#20013;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20462;&#34917;&#32452;&#21512;&#25512;&#29702;&#38169;&#35823;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807; Logit Lens &#21644;&#24178;&#39044;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#37096;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#25968;&#25512;&#29702;&#22833;&#36133;&#28304;&#33258;&#20110;&#19981;&#27491;&#30830;&#29983;&#25104;&#25110;&#21033;&#29992;&#30340;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#22312;&#20013;&#38388;&#23618;&#20013;&#30340;&#20986;&#29616;&#65292;&#24182;&#23545;&#26368;&#32456;&#26174;&#24335;&#25512;&#29702;&#32467;&#26524;&#30340;&#24418;&#25104;&#36215;&#21040;&#22240;&#26524;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102; MHSA &#27169;&#22359;&#22312;&#36825;&#20123;&#23618;&#20013;&#30340;&#23384;&#22312;&#65292;&#25104;&#20026;&#20934;&#30830;&#29983;&#25104;&#21644;&#21033;&#29992;&#38544;&#24615;&#25512;&#29702;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#20197;&#19978;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; CREME&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#36753;&#20301;&#20110;&#30340; MHSA &#27169;&#22359;&#26469;&#20462;&#34917;&#32452;&#21512;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14328v1 Announce Type: new  Abstract: LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;TMPT&#26694;&#26550;&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14298</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Stance Detection: New Datasets and Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;TMPT&#26694;&#26550;&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#35782;&#21035;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#30340;&#20844;&#20247;&#24847;&#35265;&#12290;&#20197;&#24448;&#30340;&#31435;&#22330;&#26816;&#27979;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#32431;&#25991;&#26412;&#19978;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25512;&#25991;&#30340;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#36825;&#22312;&#24403;&#20170;&#24555;&#36895;&#22686;&#38271;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20154;&#20204;&#32463;&#24120;&#21457;&#24067;&#22810;&#27169;&#24335;&#28040;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;Twitter&#21019;&#24314;&#20102;&#20116;&#20010;&#26032;&#30340;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30446;&#26631;&#22810;&#27169;&#24335;&#25552;&#31034;&#35843;&#25972;&#65288;TMPT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#23398;&#20064;&#22810;&#27169;&#24335;&#31435;&#22330;&#29305;&#24449;&#12290;&#23545;&#25105;&#20204;&#30340;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TMPT&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14298v1 Announce Type: new  Abstract: Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.
&lt;/p&gt;</description></item><item><title>FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.14116</link><description>&lt;p&gt;
FanOutQA&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14116
&lt;/p&gt;
&lt;p&gt;
FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#20110;&#26085;&#24120;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#31867;&#22411;&#26159;&#8220;fan-out&#8221;&#38382;&#39064;&#65292;&#21363;&#22797;&#26434;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#25512;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#25214;&#21040;&#22823;&#37327;&#23454;&#20307;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#36164;&#28304;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#20013;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FanOutQA&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;fan-out&#38382;&#39064;-&#31572;&#26696;&#23545;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#20154;&#24037;&#27880;&#37322;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#21046;&#23450;&#20102;&#19977;&#31181;&#22522;&#20934;&#35774;&#32622;&#65292;&#24182;&#23545;7&#20010;LLMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;GPT-4&#12289;LLaMA 2&#12289;Claude-2.1&#21644;Mixtral-8x7B&#65292;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#25512;&#29702;&#36328;&#25991;&#26723;&#20381;&#36182;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#24320;&#28304;&#24037;&#20855;&#26469;&#36816;&#34892;&#27169;&#22411;&#65292;&#20197;&#40723;&#21169;&#22312;https://fanoutqa.com&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14116v1 Announce Type: cross  Abstract: One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;OlympiadBench&#65292;&#19968;&#20010;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#22522;&#20934;&#65292;&#21253;&#25324;8952&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14008</link><description>&lt;p&gt;
OlympiadBench&#65306;&#19968;&#20010;&#20855;&#26377;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;OlympiadBench&#65292;&#19968;&#20010;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#22522;&#20934;&#65292;&#21253;&#25324;8952&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#19968;&#33324;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25509;&#36817;&#20102;&#22810;&#20010;&#39046;&#22495;&#20154;&#31867;&#19987;&#23478;&#30340;&#29087;&#32451;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OlympiadBench&#65292;&#19968;&#20010;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#31185;&#23398;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#22885;&#26519;&#21305;&#20122;&#32423;&#21035;&#25968;&#23398;&#21644;&#29289;&#29702;&#31454;&#36187;&#20197;&#21450;&#20013;&#22269;&#39640;&#32771;&#30340;8952&#20010;&#38382;&#39064;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#37197;&#26377;&#19987;&#23478;&#32423;&#27880;&#37322;&#65292;&#20197;&#36827;&#34892;&#36880;&#27493;&#30340;&#25512;&#29702;&#12290;&#22312;OlympiadBench&#19978;&#35780;&#20272;&#39030;&#23574;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;GPT-4V&#22312;OlympiadBench&#19978;&#33719;&#24471;&#20102;17.23%&#30340;&#24179;&#22343;&#20998;&#65292;&#20854;&#20013;&#22312;&#29289;&#29702;&#23398;&#20013;&#20165;&#20026;11.28%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14008v1 Announce Type: new  Abstract: Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\texttt{Se}^2$&#65292;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24110;&#21161;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13874</link><description>&lt;p&gt;
$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning
&lt;/p&gt;
&lt;p&gt;
$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\texttt{Se}^2$&#65292;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24110;&#21161;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31034;&#20363;&#31034;&#33539;&#26469;&#28608;&#27963;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#29992;&#20110;ICL&#30340;&#31034;&#20363;&#36873;&#25321;&#65292;&#20027;&#35201;&#36981;&#24490;&#8220;&#20808;&#36873;&#25321;&#20877;&#32452;&#32455;&#8221;&#30340;&#33539;&#24335;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#31034;&#20363;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23384;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#24207;&#36143;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;$\texttt{Se}^2$&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#21453;&#39304;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;ICL&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#26463;&#25628;&#32034;&#26469;&#23547;&#25214;&#21644;&#26500;&#24314;&#31034;&#20363;&#24207;&#21015;&#65292;&#22686;&#24378;&#20102;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#31867;&#21035;&#20013;&#30340;23&#20010;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13874v1 Announce Type: new  Abstract: The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories i
&lt;/p&gt;</description></item><item><title>Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13212</link><description>&lt;p&gt;
&#36719;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency Improves Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13212
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#21644;&#35780;&#20998;&#26469;&#25913;&#36827;&#65292;&#20197;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#24403;&#21069;&#30340;&#8220;&#25277;&#26679;&#21644;&#36873;&#25321;&#8221;&#26041;&#27861;&#22914;&#33258;&#19968;&#33268;&#24615;&#65288;SC&#65289;&#20381;&#36182;&#20110;&#22810;&#25968;&#25237;&#31080;&#26469;&#35780;&#20998;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#26377;&#35768;&#22810;&#19981;&#21516;&#19988;&#26377;&#25928;&#30340;&#31572;&#26696;&#26102;&#65292;&#36890;&#36807;&#25237;&#31080;&#36827;&#34892;&#36873;&#25321;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#36825;&#20351;&#24471;SC&#22312;&#28041;&#21450;&#39034;&#24207;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#65288;&#31572;&#26696;&#65289;&#30340;&#20114;&#21160;&#20219;&#21153;&#26102;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#30830;&#23450;&#22823;&#22810;&#25968;&#25237;&#31080;&#26410;&#33021;&#20026;&#27492;&#31867;&#20219;&#21153;&#25552;&#20379;&#19968;&#33268;&#30340;&#25910;&#30410;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#33258;&#19968;&#33268;&#24615;&#65288;Soft-SC&#65289;&#65292;&#23427;&#29992;&#27169;&#22411;&#21487;&#33021;&#24615;&#35745;&#31639;&#36830;&#32493;&#20998;&#25968;&#26469;&#21462;&#20195;SC&#30340;&#19981;&#36830;&#32493;&#35780;&#20998;&#65292;&#21363;&#20351;&#21160;&#20316;&#20998;&#24067;&#31232;&#30095;&#65292;&#20063;&#20801;&#35768;&#36873;&#25321;&#12290;&#36719;&#33258;&#19968;&#33268;&#24615;&#22312;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
&lt;/p&gt;</description></item><item><title>TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.12991</link><description>&lt;p&gt;
TRAP: &#38754;&#21521;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#30340;&#26377;&#38024;&#23545;&#24615;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#35825;&#39285;
&lt;/p&gt;
&lt;p&gt;
TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12991
&lt;/p&gt;
&lt;p&gt;
TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21644;&#27169;&#22411;&#36890;&#24120;&#20276;&#38543;&#30528;&#20851;&#20110;&#35841;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#20197;&#21450;&#20182;&#20204;&#24517;&#39035;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#30340;&#27861;&#24459;&#35268;&#23450;&#12290;&#35780;&#20272;&#21457;&#24067;&#30340;LLMs&#30340;&#21512;&#35268;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#23450;&#20445;&#25252;&#20102;LLM&#36129;&#29486;&#32773;&#30340;&#21033;&#30410;&#24182;&#38450;&#27490;&#20102;&#28389;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#65288;BBIV&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#31532;&#19977;&#26041;&#24212;&#29992;&#26159;&#21542;&#36890;&#36807;&#20854;&#32842;&#22825;&#21151;&#33021;&#20351;&#29992;&#26576;&#20010;&#29305;&#23450;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#65288;TRAP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#27491;&#22312;&#20351;&#29992;&#30340;&#20855;&#20307;LLM&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#26368;&#21021;&#29992;&#20110;&#36234;&#29425;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#20197;&#20174;&#30446;&#26631;LLM&#33719;&#24471;&#39044;&#23450;&#20041;&#30340;&#31572;&#26696;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#32473;&#20986;&#38543;&#26426;&#31572;&#26696;&#12290;TRAP&#21487;&#20197;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;&#21363;&#20351;LLM&#26377;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#30340;&#32454;&#24494;&#21464;&#21270;&#65292;TRAP&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; Tree-Planted Transformers (TPT)&#65292;&#36890;&#36807;&#22312; Transformer LMs &#30340;&#27880;&#24847;&#26435;&#37325;&#20013;&#38544;&#24335;&#22320;&#8220;&#31181;&#26893;&#8221;&#26641;&#26408;&#26469;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#21477;&#27861;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLM&#65289;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.12691</link><description>&lt;p&gt;
&#26641;&#31181;&#26893;&#21464;&#21387;&#22120;&#65306;&#20855;&#26377;&#38544;&#24335;&#21477;&#27861;&#30417;&#30563;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12691
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; Tree-Planted Transformers (TPT)&#65292;&#36890;&#36807;&#22312; Transformer LMs &#30340;&#27880;&#24847;&#26435;&#37325;&#20013;&#38544;&#24335;&#22320;&#8220;&#31181;&#26893;&#8221;&#26641;&#26408;&#26469;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#21477;&#27861;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLM&#65289;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#22312;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#21477;&#27861;&#30417;&#30563;&#39640;&#25928;&#35757;&#32451;&#65292;&#36798;&#21040;&#30456;&#23545;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;LLMs&#21644;SLMs&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#31181;&#23558;LLMs&#30340;&#21487;&#25193;&#23637;&#24615;&#19982;SLMs&#30340;&#35757;&#32451;&#25928;&#29575;&#32467;&#21512;&#36215;&#26469;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#21477;&#27861;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLM&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#26641;&#31181;&#26893;&#8221;&#30340;&#26032;&#26041;&#27861;&#65306;&#22312;Transformer LMs&#30340;&#27880;&#24847;&#26435;&#37325;&#20013;&#26263;&#31034;&#22320;&#8220;&#31181;&#26893;&#8221;&#26641;&#26408;&#65292;&#20197;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#26641;&#31181;&#26893;&#35757;&#32451;&#30340;Transformer LMs&#23558;&#34987;&#31216;&#20026;&#26641;&#31181;&#26893;&#21464;&#21387;&#22120;&#65288;TPT&#65289;&#65292;&#23427;&#20204;&#36890;&#36807;&#26641;&#31181;&#26893;&#22312;&#23567;&#22411;&#26641;&#24211;&#19978;&#23398;&#20064;&#35821;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#32487;&#32493;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#19978;&#36827;&#34892;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12691v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly "plant" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via conti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12621</link><description>&lt;p&gt;
Reflect-RL&#65306;&#20004;&#20010;&#29609;&#23478;&#22312;&#32447;RL&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reflect-RL: Two-Player Online RL Fine-Tuning for LMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12621
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20854;&#33021;&#21147;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#38656;&#35201;&#22810;&#36718;&#20132;&#20114;&#30340;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#24615;&#65292;&#22240;&#27492;&#20165;&#22312;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#29615;&#22659;&#20869;&#30452;&#25509;&#23545;LM&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;LM&#36827;&#34892;&#24494;&#35843;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Reflect-RL&#65292;&#19968;&#20010;&#20004;&#20010;&#29609;&#23478;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22312;&#32447;RL&#23545;LM&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#20923;&#32467;&#30340;&#21453;&#23556;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#27169;&#22411;&#12290;&#20026;&#20102;&#20026;&#28909;&#36523;SFT&#38454;&#27573;&#29983;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36127;&#20363;&#29983;&#25104;&#26469;&#22686;&#24378;&#21453;&#23556;&#27169;&#22411;&#30340;&#32416;&#38169;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#65292;&#24182;&#24212;&#29992;&#20102;&#35838;&#31243;&#23398;&#20064;&#35753;&#31574;&#30053;&#27169;&#22411;&#23398;&#20064;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12451</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The (R)Evolution of Multimodal Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12451
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#22312;&#29983;&#25104;&#26234;&#33021;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30446;&#21069;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#26080;&#32541;&#22320;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#21516;&#26102;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#25552;&#20379;&#22522;&#20110;&#23545;&#35805;&#30340;&#25509;&#21475;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;MLLMs&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#36873;&#25321;&#12289;&#22810;&#27169;&#24577;&#23545;&#40784;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#35270;&#35273;&#23450;&#20301;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#12289;&#35270;&#35273;&#29702;&#35299;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32534;&#21046;&#24182;&#25551;&#36848;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#31181;&#31995;&#32479;&#39564;&#35777;&#36807;&#30340;&#31574;&#30053;&#29983;&#25104;&#30475;&#19981;&#35265;&#21644;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#24182;&#23454;&#29616;&#35780;&#20272;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11894</link><description>&lt;p&gt;
&#20320;&#35265;&#36807;&#25105;&#21527;&#65311;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#20197;&#23454;&#29616;&#21487;&#38752;&#21450;&#21450;&#26102;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#31181;&#31995;&#32479;&#39564;&#35777;&#36807;&#30340;&#31574;&#30053;&#29983;&#25104;&#30475;&#19981;&#35265;&#21644;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#24182;&#23454;&#29616;&#35780;&#20272;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#25193;&#22823;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;LLMs&#38754;&#20020;&#30528;&#26085;&#30410;&#20005;&#37325;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#23548;&#33268;&#23545;&#29616;&#26377;&#22522;&#20934;&#30340;&#36807;&#24230;&#20272;&#35745;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23450;&#26399;&#25163;&#21160;&#25972;&#29702;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#20197;&#23454;&#29616;&#21487;&#38752;&#21450;&#21450;&#26102;&#30340;&#35780;&#20272;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#22522;&#20110;&#29616;&#26377;&#26679;&#26412;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#39640;&#36136;&#37327;&#27979;&#35797;&#26679;&#26412;&#20197;&#20943;&#36731;&#27844;&#28431;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#39564;&#35777;&#12290;&#31532;&#19968;&#31181;&#26159;&#27169;&#20223;&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#21019;&#24314;&#31867;&#20284;&#29616;&#26377;&#26679;&#26412;&#30340;&#26032;&#26679;&#26412;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#23427;&#22312;&#22810;&#27425;&#23454;&#20363;&#20013;&#30340;&#35780;&#20272;&#31283;&#23450;&#24615;&#20197;&#21450;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#22788;&#29702;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20219;&#21153;&#25512;&#26029;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#25512;&#26029;&#24179;&#22343;&#25512;&#26029;&#26102;&#38388;&#20943;&#23569;1.46&#20493;&#65292;&#24182;&#19988;&#22312;MTI Bench&#19978;&#26174;&#31034;&#20986;&#26368;&#22810;&#39640;&#36798;12.4%&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.11597</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#25512;&#26029;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#36981;&#24490;&#22810;&#20010;&#25351;&#20196;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11597
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20219;&#21153;&#25512;&#26029;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#25512;&#26029;&#24179;&#22343;&#25512;&#26029;&#26102;&#38388;&#20943;&#23569;1.46&#20493;&#65292;&#24182;&#19988;&#22312;MTI Bench&#19978;&#26174;&#31034;&#20986;&#26368;&#22810;&#39640;&#36798;12.4%&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#34987;&#35201;&#27714;&#22312;&#27599;&#27425;&#25512;&#26029;&#35843;&#29992;&#20013;&#36981;&#24490;&#21333;&#20010;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#26159;&#21542;&#20063;&#20855;&#26377;&#22788;&#29702;&#22810;&#20010;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#31216;&#20026;&#22810;&#20219;&#21153;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MTI Bench&#65288;&#22810;&#20219;&#21153;&#25512;&#26029;&#22522;&#20934;&#65289;&#65292;&#19968;&#20010;&#21253;&#25324;25&#20010;&#20219;&#21153;&#30340;5000&#20010;&#23454;&#20363;&#30340;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#12290;MTI Bench&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#37117;&#28041;&#21450;2&#21040;3&#20010;&#23376;&#20219;&#21153;&#12290;&#27491;&#22914;&#39044;&#26399;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22810;&#20219;&#21153;&#25512;&#26029;&#24179;&#22343;&#38477;&#20302;&#20102;1.46&#20493;&#30340;&#24635;&#25512;&#26029;&#26102;&#38388;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#22810;&#27425;&#25512;&#26029;&#35843;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#19982;&#39044;&#26399;LLMs&#22312;&#20219;&#21153;&#34987;&#21010;&#20998;&#26102;&#34920;&#29616;&#26356;&#22909;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#20363;&#22914;Llama-2-Chat-70B&#21644;GPT-4&#65292;&#22312;MTI Bench&#19978;&#36890;&#36807;&#22810;&#20219;&#21153;&#25512;&#26029;&#19982;&#21333;&#20219;&#21153;&#25512;&#26029;&#30456;&#27604;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;7.3&#65285;&#21644;12.4&#65285;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;MTI Bench&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11597v1 Announce Type: new  Abstract: Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench
&lt;/p&gt;</description></item><item><title>KMMLU&#26159;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#22522;&#20934;&#65292;&#21253;&#21547;35,030&#36947;&#19987;&#23478;&#32423;&#22810;&#36873;&#39064;&#65292;&#20174;&#21407;&#22987;&#38889;&#35821;&#32771;&#35797;&#20013;&#25910;&#38598;&#32780;&#26469;&#65292;&#27979;&#35797;&#20102;26&#20010;LLM&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;KMMLU&#19978;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11548</link><description>&lt;p&gt;
KMMLU&#65306;&#22312;&#38889;&#35821;&#20013;&#27979;&#37327;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
KMMLU: Measuring Massive Multitask Language Understanding in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11548
&lt;/p&gt;
&lt;p&gt;
KMMLU&#26159;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#22522;&#20934;&#65292;&#21253;&#21547;35,030&#36947;&#19987;&#23478;&#32423;&#22810;&#36873;&#39064;&#65292;&#20174;&#21407;&#22987;&#38889;&#35821;&#32771;&#35797;&#20013;&#25910;&#38598;&#32780;&#26469;&#65292;&#27979;&#35797;&#20102;26&#20010;LLM&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;KMMLU&#19978;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KMMLU&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#20154;&#25991;&#31185;&#23398;&#21040;STEM&#30340;45&#20010;&#23398;&#31185;&#30340;35,030&#36947;&#19987;&#23478;&#32423;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#19982;&#20043;&#21069;&#20174;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#32763;&#35793;&#32780;&#26469;&#30340;&#38889;&#35821;&#22522;&#20934;&#19981;&#21516;&#65292;KMMLU&#26159;&#20174;&#21407;&#22987;&#38889;&#35821;&#32771;&#35797;&#20013;&#25910;&#38598;&#30340;&#65292;&#25429;&#25417;&#20102;&#38889;&#35821;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#19987;&#26377;&#30340;LLM&#65292;&#21457;&#29616;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26368;&#22909;&#30340;&#20844;&#24320;&#27169;&#22411;&#22312;KMMLU&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;50.54%&#65292;&#36828;&#20302;&#20110;&#24179;&#22343;&#20154;&#31867;&#34920;&#29616;&#30340;62.6%&#12290;&#36825;&#20010;&#27169;&#22411;&#20027;&#35201;&#26159;&#38024;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#19981;&#26159;&#38889;&#35821;&#12290;&#30446;&#21069;&#38024;&#23545;&#38889;&#35821;&#30340;LLM&#65292;&#22914;Polyglot-Ko&#65292;&#34920;&#29616;&#24471;&#26356;&#31967;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#26368;&#26377;&#33021;&#21147;&#30340;&#19987;&#26377;LLM&#65292;&#20363;&#22914;GPT-4&#21644;HyperCLOVA X&#65292;&#20063;&#21482;&#20998;&#21035;&#36798;&#21040;&#20102;59.95%&#21644;53.40%&#12290;&#36825;&#34920;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#26469;&#25913;&#36827;&#38889;&#35821;LLM&#65292;&#32780;KMMLU&#25552;&#20379;&#20102;&#36861;&#36394;&#36825;&#19968;&#36827;&#23637;&#30340;&#27491;&#30830;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11548v1 Announce Type: new  Abstract: We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We mak
&lt;/p&gt;</description></item><item><title>Knowledge-to-SQL&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#25552;&#20379;&#26377;&#29992;&#30693;&#35782;&#65292;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11517</link><description>&lt;p&gt;
&#30693;&#35782;&#21040;SQL&#65306;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#22686;&#24378;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11517
&lt;/p&gt;
&lt;p&gt;
Knowledge-to-SQL&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#25552;&#20379;&#26377;&#29992;&#30693;&#35782;&#65292;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#20934;&#30830;&#30340;SQL&#65288;&#25991;&#26412;&#21040;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#29983;&#25104;SQL&#38656;&#35201;&#29702;&#35299;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#25968;&#25454;&#24211;&#26816;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;&#29616;&#26377;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32508;&#21512;&#33021;&#21147;&#65292;&#26681;&#25454;&#25968;&#25454;&#24211;&#27169;&#24335;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#24517;&#35201;&#30340;&#30693;&#35782;&#27809;&#26377;&#26126;&#30830;&#21253;&#21547;&#22312;&#25968;&#25454;&#24211;&#27169;&#24335;&#20013;&#65292;&#25110;&#32773;&#34987;LLMs&#23398;&#20064;&#20102;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#19981;&#36275;&#30340;&#26597;&#35810;&#29983;&#25104;&#30340;SQL&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#36825;&#20250;&#23545;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Knowledge-to-SQL&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#23450;&#21046;&#30340;&#25968;&#25454;&#19987;&#23478;LLM&#65288;DELLM&#65289;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;DELLM&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#34920;&#26684;&#35835;&#21462;&#21644;&#22522;&#26412;&#24494;&#35843;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11517v1 Announce Type: new  Abstract: Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further prov
&lt;/p&gt;</description></item><item><title>LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11485</link><description>&lt;p&gt;
LEIA: &#21033;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20419;&#36827;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11485
&lt;/p&gt;
&lt;p&gt;
LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#33521;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25805;&#20316;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;LEIA&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#30340;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#33521;&#35821;&#23454;&#20307;&#21517;&#31216;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;LEIA&#65292;&#20351;&#29992;7B&#21442;&#25968;&#30340;LLMs&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#22686;&#30410;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/studio-ousia/leia&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11349</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26410;&#23398;&#20064;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tasks That Language Models Don't Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11349
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#25105;&#20204;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#26576;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20219;&#21153;&#65288;&#31216;&#20026;H-TEST&#65289;&#23545;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#31361;&#26174;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;LLMs&#30340;&#24863;&#23448;&#21463;&#38480;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;1. &#25925;&#24847;&#25512;&#29702;&#65288;&#24605;&#32500;&#38142;&#65289;&#65292;2. &#23569;&#37327;&#26696;&#20363;&#65292;&#25110;3. &#21516;&#19968;&#27169;&#22411;&#31995;&#21015;&#30340;&#26356;&#24378;&#22823;LLM&#65288;LLaMA 2 13B-&gt;LLaMA 2 70B&#65289;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#24102;&#26469;H-TEST&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#23558;&#20854;&#19982;&#29595;&#20029;&#30340;&#21746;&#23398;&#26696;&#20363;&#32852;&#31995;&#36215;&#26469;&#65292;&#22905;&#22312;&#24863;&#23448;&#21463;&#38480;&#29615;&#22659;&#20013;&#20102;&#35299;&#19990;&#30028;&#65288;Jackson&#65292;1986&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#19987;&#26377;LLMs&#30340;&#34920;&#29616;&#25509;&#36817;&#20110;&#38543;&#26426;&#22522;&#20934;&#20934;&#30830;&#29575;50&#65285;&#65292;&#31361;&#26174;&#20102;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -&gt; LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.11138</link><description>&lt;p&gt;
&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Contrastive Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#19968;&#30452;&#34987;&#29992;&#20316;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#38754;&#20020;&#26410;&#30693;&#25351;&#20196;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#30456;&#21516;&#30340;&#25351;&#20196;&#20197;&#31245;&#24494;&#21464;&#21270;&#30340;&#24418;&#24335;&#25110;&#35821;&#35328;&#39118;&#26684;&#25552;&#20986;&#26102;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#26126;LLMs&#23545;&#25991;&#26412;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#21644;&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#22823;&#21270;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#25351;&#20196;-&#23454;&#20363;&#23545;&#30340;&#38544;&#34255;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#37322;&#20041;&#20219;&#21153;&#25351;&#20196;&#65292;&#25193;&#20805;&#29616;&#26377;&#30340;FLAN&#38598;&#21512;&#12290;&#22312;PromptBench&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65288;CoIN&#65289;&#19968;&#30452;&#25552;&#39640;&#20102;LLMs&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10890</link><description>&lt;p&gt;
LLM&#35268;&#21010;&#20013;&#26641;&#25628;&#32034;&#20309;&#26102;&#26377;&#29992;&#65311;&#21462;&#20915;&#20110;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
When is Tree Search Useful for LLM Planning? It Depends on the Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10890
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22312;&#22810;&#27493;&#38382;&#39064;&#19979;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#35268;&#21010;&#26041;&#27861;&#19977;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20808;&#36827;&#35268;&#21010;&#26041;&#27861;&#65292;&#36845;&#20195;&#26657;&#27491;&#21644;&#26641;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#37492;&#21035;&#20934;&#30830;&#24615;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#25110;&#26356;&#31616;&#21333;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26102;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#20004;&#39033;&#20219;&#21153;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21644;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#30340;&#37492;&#21035;&#22120;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65307;&#65288;2&#65289;&#24403;&#21069;LLMs&#30340;&#37492;&#21035;&#33021;&#21147;&#23578;&#26410;&#28385;&#36275;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#38656;&#27714;&#65307;&#65288;3&#65289;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#37492;&#21035;&#22120;&#26102;&#65292;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#22312;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#28151;&#21512;&#36866;&#37197;&#22120;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20026;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#20248;&#21270;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#35265;</title><link>https://arxiv.org/abs/2402.10639</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21495;&#26435;&#37325;&#26041;&#21521;&#30340;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#30340;&#27867;&#21270;&#19982;&#20854;&#22312;&#26377;&#25928;&#27169;&#22411;&#21098;&#26525;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#22312;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#28151;&#21512;&#36866;&#37197;&#22120;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20026;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#20248;&#21270;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#23558;&#21333;&#19968;&#19987;&#19994;&#30693;&#35782;&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20013;&#65292;&#36824;&#33021;&#19968;&#27425;&#24615;&#25972;&#21512;&#22810;&#20010;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#22914;AdapterSoup&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21270;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#28151;&#21512;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#36825;&#31181;&#26032;&#20852;&#30340;&#26435;&#37325;&#31354;&#38388;&#36866;&#37197;&#22120;&#28151;&#21512;&#26426;&#21046;&#22312;&#26410;&#30693;&#30340;&#39046;&#22495;&#20869;&#20363;&#23376;&#19978;&#30340;&#22522;&#26412;&#27867;&#21270;&#24615;&#20173;&#26410;&#34987;&#25506;&#35752;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#22312;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#30340;&#26435;&#37325;&#31526;&#21495;&#26469;&#28145;&#20837;&#30740;&#31350;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#24471;&#20986;&#20102;&#20851;&#38190;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10639v1 Announce Type: new  Abstract: Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.10588</link><description>&lt;p&gt;
&#25289;&#39532;&#22312;&#33521;&#35821;&#20013;&#26377;&#25928;&#21527;&#65311;&#20851;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#28508;&#22312;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Do Llamas Work in English? On the Latent Language of Multilingual Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#22312;&#19981;&#24179;&#34913;&#12289;&#33521;&#35821;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#38382;&#39064;&#8212;&#8212;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290; &#25105;&#20204;&#20851;&#27880;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#33521;&#35821;&#25552;&#31034;&#21644;&#21807;&#19968;&#27491;&#30830;&#30340;&#21333;&#35789;&#24310;&#32493;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290; &#20174;&#19968;&#23618;&#21040;&#21478;&#19968;&#23618;&#65292;&#21464;&#21387;&#22120;&#36880;&#28176;&#23558;&#26368;&#32456;&#25552;&#31034;&#20196;&#29260;&#30340;&#36755;&#20837;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#23884;&#20837;&#65292;&#20174;&#20013;&#35745;&#31639;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#27010;&#29575;&#12290; &#36890;&#36807;&#36319;&#36394;&#20854;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20013;&#38388;&#23884;&#20837;&#65292;&#25581;&#31034;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#21363;&#20013;&#38388;&#23884;&#20837;&#65288;1&#65289;&#24320;&#22987;&#36828;&#31163;&#36755;&#20986;&#20196;&#29260;&#23884;&#20837;&#65307;&#65288;2&#65289;&#22312;&#20013;&#38388;&#23618;&#24050;&#32463;&#20801;&#35768;&#35299;&#30721;&#19968;&#20010;&#35821;&#20041;&#27491;&#30830;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#65292;&#20294;&#26356;&#20542;&#21521;&#20110;&#33521;&#35821;&#29256;&#26412;&#32780;&#19981;&#26159;&#36755;&#20837;&#35821;&#35328;&#30340;&#29256;&#26412;&#65307;&#65288;3&#65289;&#26368;&#32456;&#31227;&#21160;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10588v1 Announce Type: new  Abstract: We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;</title><link>https://arxiv.org/abs/2402.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#32622;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization with an Offset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DPO&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#23558;&#27599;&#20010;&#20559;&#22909;&#23545;&#35270;&#20026;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;ZeroSwot&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;-shot ST&#65292;&#36890;&#36807;CTC&#21387;&#32553;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20165;&#21033;&#29992;ASR&#25968;&#25454;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#24182;&#19982;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#30452;&#25509;&#20174;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.10422</link><description>&lt;p&gt;
&#25512;&#21160;&#38646;-shot&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of Zero-shot End-to-End Speech Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10422
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;ZeroSwot&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;-shot ST&#65292;&#36890;&#36807;CTC&#21387;&#32553;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20165;&#21033;&#29992;ASR&#25968;&#25454;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#24182;&#19982;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#30452;&#25509;&#20174;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#21644;&#35821;&#38899;&#19982;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#31995;&#32479;&#30340;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#24615;&#33021;&#12290; &#20197;&#24448;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;MT&#25968;&#25454;&#21644;&#20248;&#21270;&#36317;&#31163;&#24230;&#37327;&#26469;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#25509;&#36817;&#12290; &#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;ST&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290; &#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ZeroSwot&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;-shot ST&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;ST&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24357;&#21512;&#27169;&#24577;&#24046;&#36317;&#12290; &#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;CTC&#21387;&#32553;&#21644;&#26368;&#20248;&#20256;&#36755;&#25216;&#26415;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;ASR&#25968;&#25454;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#20197;&#19982;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290; &#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#26102;&#19982;MT&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#65292;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#22312;&#25152;&#26377;MT&#27169;&#22411;&#25903;&#25345;&#30340;&#35821;&#35328;&#20013;&#20174;&#35821;&#38899;&#32763;&#35793;&#20026;&#25991;&#26412;&#12290; &#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#24179;&#28369;&#22320;&#20851;&#38381;m&#27169;&#24577;&#38388;&#30340;&#31354;&#38388;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10422v1 Announce Type: new  Abstract: Data scarcity and the modality gap between the speech and text modalities are two major obstacles of end-to-end Speech Translation (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external MT data and optimizing distance metrics that bring closer the speech-text representations. However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for zero-shot ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model. The speech encoder seamlessly integrates with the MT model at inference, enabling direct translation from speech to text, across all languages supported by the MT model. Our experiments show that we can effectively close the m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;\textbf{MoEI}&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#32508;&#21512;&#25552;&#39640;&#20102;LLM&#30340;&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#32780;&#19981;&#25439;&#23475;&#20854;&#26222;&#36866;&#26234;&#33021;&#65288;GI&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10073</link><description>&lt;p&gt;
&#21516;&#26102;&#37325;&#35270;&#65306;&#22312;&#19981;&#25439;&#23475;&#26222;&#36866;&#26234;&#33021;&#30340;&#24773;&#24863;&#26234;&#33021;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#24378;&#24773;&#32490;&#26234;&#21147;
&lt;/p&gt;
&lt;p&gt;
Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;\textbf{MoEI}&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#32508;&#21512;&#25552;&#39640;&#20102;LLM&#30340;&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#32780;&#19981;&#25439;&#23475;&#20854;&#26222;&#36866;&#26234;&#33021;&#65288;GI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#21253;&#25324;&#24773;&#24863;&#24863;&#30693;&#12289;&#24773;&#24863;&#35748;&#30693;&#21644;&#24773;&#24863;&#34920;&#36798;&#65292;&#22312;&#25552;&#39640;&#24403;&#21069;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#24335;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#19982;&#29992;&#25143;&#20132;&#20114;&#20307;&#39564;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#23545;EI&#30456;&#20851;&#30340;&#20998;&#31867;&#25110;&#22238;&#24402;&#20219;&#21153;&#30340;&#22825;&#30495;&#24494;&#35843;&#25552;&#39640;&#20854;&#24773;&#24863;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;EI&#30340;&#19981;&#23436;&#20840;&#22686;&#24378;&#21644;&#23545;&#26222;&#36866;&#26234;&#33021;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#35206;&#30422;&#20102;EI&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#20219;&#21153;&#25351;&#31034;&#65292;&#20026;&#32508;&#21512;&#22686;&#24378;LLM&#30340;EI&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#24773;&#32490;&#26234;&#21147;&#22686;&#24378;&#26041;&#27861;&#65288;\textbf{MoEI}&#65289;&#65292;&#21253;&#21547;&#20102;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;EI&#21644;&#20445;&#25345;&#26222;&#36866;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10073v1 Announce Type: new  Abstract: Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;-&#29702;&#35770;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10013</link><description>&lt;p&gt;
&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#32553;&#23567;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;-&#29702;&#35770;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10013
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;-&#29702;&#35770;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#36817;&#20284;&#65292;&#20294;&#26159;&#21363;&#20351;&#29702;&#35770;&#24037;&#20316;&#34920;&#26126;&#36825;&#20123;&#23436;&#32654;&#30340;&#35299;&#21487;&#20197;&#30001;&#29305;&#23450;&#30340;&#26550;&#26500;&#26469;&#34920;&#31034;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#23436;&#32654;&#30340;&#27867;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#23637;&#31034;&#20102;&#29702;&#35770;&#19978;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#38469;&#19978;&#19981;&#26159;&#24120;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#27491;&#21017;&#21270;&#25216;&#26415;&#65288;&#22914;L1&#65292;L2&#65289;&#25110;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#26089;&#20572;&#65292;dropout&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;&#26631;&#20934;&#30446;&#26631;&#26367;&#25442;&#20026;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#21487;&#20197;&#20351;&#27491;&#30830;&#30340;&#35299;&#25104;&#20026;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10013v1 Announce Type: new  Abstract: Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives -- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout). However, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.
&lt;/p&gt;</description></item><item><title>DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07891</link><description>&lt;p&gt;
&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Model Selection for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07891
&lt;/p&gt;
&lt;p&gt;
DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32473;&#23450;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#38656;&#35201;&#23545;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#36136;&#37327;&#36827;&#34892;&#24191;&#27867;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DiffUse&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#20505;&#36873;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;DiffUse&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22312;&#35780;&#20272;&#20013;&#23453;&#36149;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;DiffUse&#36890;&#36807;&#32858;&#31867;&#34920;&#31034;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#26234;&#33021;&#36873;&#25321;&#23454;&#20363;&#12290;&#22240;&#27492;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#20123;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#20363;&#23376;&#26469;&#36827;&#34892;&#20559;&#22909;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35201;&#27880;&#37322;&#30340;&#23454;&#20363;&#25968;&#37327;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#20010;&#27169;&#22411;&#23545;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DiffUse&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#65292;&#26368;&#22810;&#21487;&#20943;&#23569;75%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35780;&#20272;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
&lt;/p&gt;</description></item><item><title>Mercury&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#25928;&#29575;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.07844</link><description>&lt;p&gt;
Mercury: &#19968;&#31181;&#29992;&#20110;LLM&#20195;&#30721;&#32508;&#21512;&#25928;&#29575;&#35780;&#20272;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mercury: An Efficiency Benchmark for LLM Code Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07844
&lt;/p&gt;
&lt;p&gt;
Mercury&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#25928;&#29575;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#20195;&#30721;&#32508;&#21512;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21151;&#33021;&#27491;&#30830;&#24615;&#19978;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#25928;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mercury&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#30721;&#32508;&#21512;&#20219;&#21153;&#30340;&#20195;&#30721;&#25928;&#29575;&#30340;&#22522;&#20934;&#12290;Mercury&#30001;1,889&#20010;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#20219;&#21153;&#32452;&#25104;&#65292;&#36824;&#21253;&#25324;&#29983;&#25104;&#26080;&#38480;&#26696;&#20363;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#22120;&#65292;&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;Mercury&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Beyond@K&#65292;&#20197;&#22522;&#20110;&#21382;&#21490;&#25552;&#20132;&#26469;&#34913;&#37327;&#24402;&#19968;&#21270;&#30340;&#20195;&#30721;&#25928;&#29575;&#65292;&#20174;&#32780;&#20026;&#20195;&#30721;&#32508;&#21512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#40723;&#21169;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20195;&#30721;&#65292;&#20307;&#29616;&#20102;&#29616;&#23454;&#19990;&#30028;&#36719;&#20214;&#24320;&#21457;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;LLM&#34920;&#29616;&#20986;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25928;&#29575;&#36755;&#20986;&#26041;&#38754;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde
&lt;/p&gt;</description></item><item><title>T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.07483</link><description>&lt;p&gt;
T-RAG: &#26469;&#33258;LLM&#25112;&#22330;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
T-RAG: Lessons from the LLM Trenches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07483
&lt;/p&gt;
&lt;p&gt;
T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#30340;&#23581;&#35797;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#23545;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#36827;&#34892;&#38382;&#31572;&#65292;&#20854;&#20013;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#25968;&#25454;&#23433;&#20840;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#23545;&#26597;&#35810;&#27491;&#30830;&#21709;&#24212;&#30340;&#20581;&#22766;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#25104;&#20026;&#26500;&#24314;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#37325;&#35201;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26500;&#24314;RAG&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#35201;&#20351;&#20854;&#20581;&#22766;&#21644;&#21487;&#38752;&#30340;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#24191;&#27867;&#30340;&#23450;&#21046;&#21270;&#21644;&#30456;&#23545;&#28145;&#20837;&#30340;&#24212;&#29992;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31169;&#20154;&#32452;&#32455;&#25991;&#20214;&#38382;&#31572;&#24212;&#29992;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#32467;&#21512;&#20102;RAG&#30340;&#20351;&#29992;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#20855;&#26377; ...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#20998;&#21106;&#25237;&#31080;&#65292;&#25506;&#32034;&#24459;&#24072;&#22312;&#22788;&#29702;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#26102;&#38754;&#20020;&#30340;&#24847;&#35265;&#20998;&#27495;&#21644;&#22256;&#38590;&#65292;&#24182;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#25910;&#38598;&#20102;&#27861;&#23448;&#30340;&#25237;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07214</link><description>&lt;p&gt;
&#36879;&#36807;&#20998;&#21106;&#25237;&#31080;&#30340;&#35270;&#35282;: &#25506;&#32034;&#22312;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12289;&#22256;&#38590;&#21644;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07214
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#20998;&#21106;&#25237;&#31080;&#65292;&#25506;&#32034;&#24459;&#24072;&#22312;&#22788;&#29702;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#26102;&#38754;&#20020;&#30340;&#24847;&#35265;&#20998;&#27495;&#21644;&#22256;&#38590;&#65292;&#24182;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#25910;&#38598;&#20102;&#27861;&#23448;&#30340;&#25237;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#20915;&#31574;&#20013;&#65292;&#24403;&#27861;&#23448;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#20915;&#23450;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#20998;&#21106;&#25237;&#31080;(SV)&#65292;&#32473;&#24517;&#39035;&#22788;&#29702;&#21508;&#31181;&#27861;&#24459;&#35770;&#28857;&#21644;&#24847;&#35265;&#30340;&#24459;&#24072;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#29702;&#35299;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26657;&#20934;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#22120;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#35748;&#30693;&#65292;&#36890;&#24120;&#26159;&#19982;&#20154;&#31867;&#30340;&#22810;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#22266;&#26377;&#24046;&#24322;&#65288;HLV&#65289;&#12290;&#26412;&#25991;&#23558;&#20998;&#21106;&#25237;&#31080;&#35270;&#20026;&#33258;&#28982;&#21487;&#35266;&#23519;&#30340;&#20154;&#31867;&#24847;&#35265;&#20998;&#27495;&#21644;&#20215;&#20540;&#22810;&#20803;&#20027;&#20041;&#65292;&#24182;&#20174;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECHR&#65289;&#25910;&#38598;&#27861;&#23448;&#30340;&#25237;&#31080;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#24102;&#26377;SV&#20449;&#24687;&#30340;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#25968;&#25454;&#38598;SV-ECHR&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21253;&#21547;SV&#29305;&#23450;&#23376;&#31867;&#21035;&#30340;&#19981;&#21516;&#24847;&#35265;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;COC&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#38480;&#21046;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe lim
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04788</link><description>&lt;p&gt;
MLLM&#20316;&#20026;&#27861;&#23448;&#65306;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#35780;&#20272;&#22810;&#27169;&#24577;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#23454;&#29992;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#31526;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#21463;&#21040;LLM&#27169;&#22411;&#20013;LLM&#20316;&#20026;&#27861;&#23448;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#34987;&#31216;&#20026;MLLM&#20316;&#20026;&#27861;&#23448;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#21327;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#35780;&#20998;&#35780;&#20272;&#12289;&#23545;&#27604;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31867;&#20154;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20687;GPT-4V&#36825;&#26679;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#30340;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;MLLM&#30340;&#25913;&#36827;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22686;&#37327;&#29983;&#25104;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#30340;&#39044;&#27979;&#19982;&#20154;&#20204;&#36827;&#34892;&#21151;&#33021;&#31070;&#32463;&#25104;&#20687;&#30340;&#26102;&#38388;&#25968;&#25454;&#30456;&#20851;&#65292;&#21457;&#29616;&#20102;&#20154;&#31867;&#22312;&#36880;&#35789;&#29702;&#35299;&#21477;&#23376;&#26102;&#23384;&#22312;&#22810;&#36335;&#24452;&#35299;&#26512;&#30340;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2401.18046</link><description>&lt;p&gt;
&#22810;&#36335;&#24452;&#35299;&#26512;&#22312;&#22823;&#33041;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multipath parsing in the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22686;&#37327;&#29983;&#25104;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#30340;&#39044;&#27979;&#19982;&#20154;&#20204;&#36827;&#34892;&#21151;&#33021;&#31070;&#32463;&#25104;&#20687;&#30340;&#26102;&#38388;&#25968;&#25454;&#30456;&#20851;&#65292;&#21457;&#29616;&#20102;&#20154;&#31867;&#22312;&#36880;&#35789;&#29702;&#35299;&#21477;&#23376;&#26102;&#23384;&#22312;&#22810;&#36335;&#24452;&#35299;&#26512;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36880;&#35789;&#29702;&#35299;&#21477;&#23376;&#26102;&#65292;&#20197;&#25152;&#21548;&#21040;&#30340;&#39034;&#24207;&#36827;&#34892;&#12290;&#36825;&#31181;&#22686;&#37327;&#26041;&#24335;&#38656;&#35201;&#35299;&#20915;&#20020;&#26102;&#30340;&#35821;&#27861;&#20851;&#31995;&#27495;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22686;&#37327;&#29983;&#25104;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#30340;&#39044;&#27979;&#19982;&#22312;&#21548;&#25773;&#38899;&#20070;&#26102;&#36827;&#34892;&#21151;&#33021;&#31070;&#32463;&#25104;&#20687;&#30340;&#20154;&#20204;&#30340;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#30456;&#20851;&#65292;&#26469;&#30740;&#31350;&#20154;&#31867;&#26159;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#35821;&#27861;&#27495;&#20041;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20851;&#20110;&#36880;&#35789;&#29702;&#35299;&#36807;&#31243;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#35821;&#27861;&#20998;&#26512;&#25968;&#37327;&#30340;&#31454;&#20105;&#24615;&#20551;&#35774;&#65306;&#19968;&#20010;&#19982;&#22810;&#20010;&#12290;&#36825;&#20010;&#27604;&#36739;&#28041;&#21450;&#23558;&#26368;&#20808;&#36827;&#30340;&#20381;&#23384;&#20851;&#31995;&#35299;&#26512;&#22120;&#20351;&#29992;&#32463;&#36807;LLM&#35843;&#25972;&#30340;&#32534;&#30721;&#26469;&#35780;&#20272;&#35821;&#27861;&#24778;&#35766;&#24230;&#65292;&#19982;&#29616;&#26377;&#30340;fMRI&#25968;&#25454;&#38598;&#30456;&#23545;&#29031;&#12290;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30340;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#36335;&#24452;&#35299;&#26512;&#30340;&#35777;&#25454;&#12290;&#19982;&#35813;&#22810;&#36335;&#24452;&#25928;&#24212;&#30456;&#20851;&#30340;&#33041;&#21306;&#21253;&#25324;&#21452;&#20391;&#39070;&#21494;&#19978;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;SAPT&#65289;&#65292;&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23398;&#20064;&#19982;&#36873;&#25321;&#27169;&#22359;&#23545;&#40784;PET&#23398;&#20064;&#21644;&#36873;&#25321;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08295</link><description>&lt;p&gt;
SAPT&#65306;&#19968;&#31181;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08295
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;SAPT&#65289;&#65292;&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23398;&#20064;&#19982;&#36873;&#25321;&#27169;&#22359;&#23545;&#40784;PET&#23398;&#20064;&#21644;&#36873;&#25321;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33021;&#21147;&#23545;&#20110;&#22312;&#21160;&#24577;&#19990;&#30028;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#35774;&#35745;&#23398;&#20064;&#27169;&#22359;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#22359;&#33719;&#21462;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#27169;&#22359;&#36873;&#25321;&#20986;&#30456;&#24212;&#30340;&#36755;&#20837;&#65292;&#26088;&#22312;&#24212;&#23545;CL&#20013;&#30340;&#28798;&#38590;&#24335;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#35299;&#20915;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#65292;&#24573;&#35270;&#20102;&#36890;&#36807;&#23558;&#20004;&#20010;&#27169;&#22359;&#23545;&#40784;&#26469;&#26377;&#25928;&#21516;&#26102;&#35299;&#20915;&#28798;&#38590;&#24335;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;SAPT&#65289;&#65292;&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23398;&#20064;&#19982;&#36873;&#25321;&#27169;&#22359;&#26469;&#23545;&#40784;PET&#23398;&#20064;&#21644;&#36873;&#25321;&#12290;&#22312;&#20004;&#20010;CL&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;SAPT&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#26102;&#65292;SAPT&#19968;&#30452;&#23637;&#29616;&#20986;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08295v2 Announce Type: replace  Abstract: The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \&amp; Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model si
&lt;/p&gt;</description></item><item><title>WatME&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#20887;&#20313;&#30340;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#65292;&#21160;&#24577;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#35789;&#27719;&#20351;&#29992;&#65292;&#36991;&#20813;&#36866;&#24403;&#35789;&#27719;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#32500;&#25345;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09832</link><description>&lt;p&gt;
WatME&#65306;&#36890;&#36807;&#35789;&#27719;&#20887;&#20313;&#23454;&#29616;&#26080;&#25439;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
WatME: Towards Lossless Watermarking Through Lexical Redundancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09832
&lt;/p&gt;
&lt;p&gt;
WatME&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#20887;&#20313;&#30340;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#65292;&#21160;&#24577;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#35789;&#27719;&#20351;&#29992;&#65292;&#36991;&#20813;&#36866;&#24403;&#35789;&#27719;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#32500;&#25345;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09832v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#12290;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#20219;&#24847;&#30340;&#35789;&#27719;&#20998;&#21106;&#65292;&#23548;&#33268;&#22312;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#35789;&#27719;&#65292;&#24182;&#30772;&#22351;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#25991;&#26412;&#21709;&#24212;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20114;&#26021;&#24335;&#27700;&#21360;&#65288;WatME&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#22266;&#26377;&#35789;&#27719;&#20887;&#20313;&#30340;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#65292;WatME &#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#20013;&#21160;&#24577;&#20248;&#21270;&#21487;&#29992;&#35789;&#27719;&#30340;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20114;&#26021;&#35268;&#21017;&#26469;&#31649;&#29702;&#36825;&#31181;&#20887;&#20313;&#65292;&#36991;&#20813;&#20102;&#36866;&#24403;&#30340;&#35789;&#27719;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;WatME&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09832v2 Announce Type: replace  Abstract: Text watermarking has emerged as an important technique for detecting machine-generated text. However, existing methods generally use arbitrary vocabulary partitioning during decoding, which results in the absence of appropriate words during the response generation and disrupts the language model's expressiveness, thus severely degrading the quality of text response. To address these issues, we introduce a novel approach, Watermarking with Mutual Exclusion (WatME). Specifically, by leveraging linguistic prior knowledge of inherent lexical redundancy, WatME can dynamically optimize the use of available vocabulary during the decoding process of language models. It employs a mutually exclusive rule to manage this redundancy, avoiding situations where appropriate words are unavailable and maintaining the expressive power of large language models (LLMs). We present theoretical analysis and empirical evidence demonstrating that WatME subst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TextEE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#26631;&#20934;&#21270;&#12289;&#20844;&#24179;&#21644;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09562</link><description>&lt;p&gt;
TextEE&#65306;&#20107;&#20214;&#25552;&#21462;&#20013;&#30340;&#22522;&#20934;&#12289;&#37325;&#26032;&#35780;&#20272;&#12289;&#21453;&#24605;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TextEE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#26631;&#20934;&#21270;&#12289;&#20844;&#24179;&#21644;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25552;&#21462;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#23545;&#35780;&#20272;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#34920;&#26126;&#25253;&#21578;&#30340;&#20998;&#25968;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#35780;&#20272;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#20551;&#35774;&#25110;&#39044;&#22788;&#29702;&#27493;&#39588;&#32780;&#24341;&#36215;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#30446;&#21069;&#35780;&#20272;&#26694;&#26550;&#30340;&#19981;&#36275;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#38598;&#25110;&#25968;&#25454;&#20998;&#21106;&#20559;&#35265;&#65292;&#20197;&#21450;&#19968;&#20123;&#20808;&#21069;&#26041;&#27861;&#30340;&#20302;&#21487;&#37325;&#22797;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TextEE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#26631;&#20934;&#21270;&#12289;&#20844;&#24179;&#21644;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#12290;TextEE&#21253;&#25324;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#33050;&#26412;&#21644;&#29992;&#20110;&#36328;&#19971;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;14&#20010;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#21253;&#25324;14&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#36827;&#34892;&#20840;&#38754;&#30340;&#22522;&#20934;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;TextEE&#22522;&#20934;&#19978;&#30340;&#20116;&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09562v2 Announce Type: replace  Abstract: Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how the
&lt;/p&gt;</description></item><item><title>GRASP&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#39057;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22522;&#30784;&#21644;&#29289;&#29702;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;Unity&#27169;&#25311;&#36827;&#34892;&#20004;&#23618;&#35780;&#20272;&#65292;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#22522;&#30784;&#21644;&#30452;&#35273;&#29289;&#29702;&#23398;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2311.09048</link><description>&lt;p&gt;
GRASP: &#19968;&#31181;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#35821;&#35328;&#22522;&#30784;&#21644;&#24773;&#22659;&#29289;&#29702;&#29702;&#35299;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09048
&lt;/p&gt;
&lt;p&gt;
GRASP&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#39057;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22522;&#30784;&#21644;&#29289;&#29702;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;Unity&#27169;&#25311;&#36827;&#34892;&#20004;&#23618;&#35780;&#20272;&#65292;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#22522;&#30784;&#21644;&#30452;&#35273;&#29289;&#29702;&#23398;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GRASP&#65292;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35821;&#35328;&#22522;&#30784;&#21644;&#29289;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;Unity&#27169;&#25311;&#30340;&#20004;&#23618;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#31532;&#19968;&#23618;&#27979;&#35797;&#35821;&#35328;&#22522;&#30784;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23558;&#31616;&#21333;&#30340;&#25991;&#26412;&#25551;&#36848;&#19982;&#35270;&#35273;&#20449;&#24687;&#30456;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#31532;&#20108;&#23618;&#35780;&#20272;&#27169;&#22411;&#23545;"&#30452;&#35273;&#29289;&#29702;&#23398;"&#21407;&#29702;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22914;&#29289;&#20307;&#27704;&#24658;&#24615;&#21644;&#36830;&#32493;&#24615;&#12290;&#38500;&#20102;&#21457;&#24067;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#23427;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#22522;&#30784;&#21644;&#30452;&#35273;&#29289;&#29702;&#23398;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#32570;&#38519;&#12290;&#23613;&#31649;&#23427;&#20204;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#22522;&#30784;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21551;&#21457;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25152;&#26377;&#27169;&#22411;&#22312;In&#37096;&#20998;&#30340;&#34920;&#29616;&#37117;&#20302;&#20110;&#25110;&#31561;&#20110;50%&#30340;&#38543;&#26426;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of "Intuitive Physics" principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the In
&lt;/p&gt;</description></item><item><title>MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;</title><link>https://arxiv.org/abs/2311.09033</link><description>&lt;p&gt;
MELA&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELA: Multilingual Evaluation of Linguistic Acceptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09033
&lt;/p&gt;
&lt;p&gt;
MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#24212;&#29992;&#39537;&#21160;&#30340;&#20219;&#21153;&#65292;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#65292;&#23548;&#33268;LLMs&#30340;&#32431;&#35821;&#35328;&#35780;&#20272;&#20005;&#37325;&#19981;&#36275;&#12290;&#38024;&#23545;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multilingual Evaluation of Linguistic Acceptability&#65288;MELA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#26469;&#33258;&#22810;&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;10&#31181;&#35821;&#35328;&#12289;&#20849;48K&#20010;&#26679;&#26412;&#30340;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22810;&#35821;&#35328;&#22522;&#20934;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24120;&#29992;LLMs&#21644;&#30417;&#30563;&#27169;&#22411;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;XLM-R&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#39564;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24494;&#35843;&#21518;&#30340;XLM-R&#30340;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#35782;&#21035;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36801;&#31227;&#22256;&#38590;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21463;&#30410;&#33391;&#22810;&#65292;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#65292;&#32780;GPT-4&#30340;&#24615;&#33021;&#19982;&#20043;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09033v2 Announce Type: replace-cross  Abstract: Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with f
&lt;/p&gt;</description></item><item><title>ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16467</link><description>&lt;p&gt;
ReGAL: &#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16467
&lt;/p&gt;
&lt;p&gt;
ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#24320;&#21457;&#26377;&#29992;&#25277;&#35937;&#25152;&#38656;&#30340;&#20840;&#23616;&#35270;&#35282;&#65307;&#23427;&#20204;&#36890;&#24120;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#31243;&#24207;&#65292;&#32463;&#24120;&#37325;&#22797;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#20887;&#20313;&#20195;&#30721;&#26082;&#20302;&#25928;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36890;&#29992;&#25277;&#35937;&#23398;&#20064;&#30340;&#37325;&#26500;&#26041;&#27861;&#65288;ReGAL&#65289;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#26500;&#26469;&#23398;&#20064;&#21487;&#37325;&#29992;&#20989;&#25968;&#24211;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#20195;&#30721;&#25191;&#34892;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#37325;&#32452;&#20195;&#30721;&#12290;ReGAL&#20174;&#19968;&#23567;&#32452;&#29616;&#26377;&#31243;&#24207;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#39564;&#35777;&#21644;&#32454;&#21270;&#25277;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ReGAL&#21457;&#29616;&#30340;&#20849;&#20139;&#20989;&#25968;&#24211;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#39044;&#27979;&#31243;&#24207;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;LOGO&#22270;&#24418;&#29983;&#25104;&#12289;&#26085;&#26399;&#25512;&#29702;&#21644;&#22522;&#20110;Minecraft&#30340;&#25991;&#23383;&#28216;&#25103;TextCraft&#65289;&#19978;&#65292;&#24320;&#28304;&#21644;&#19987;&#26377;&#30340;LLMs&#22312;&#20351;&#29992;ReGAL&#20989;&#25968;&#24211;&#39044;&#27979;&#31243;&#24207;&#26102;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;&#23427;&#20204;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.14556</link><description>&lt;p&gt;
&#19981;&#19968;&#23450;&#24635;&#26159;&#21521;&#21491;&#30475;&#65306;&#30740;&#31350;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;&#23427;&#20204;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#34429;&#28982;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;MLM-based&#32534;&#30721;&#22120;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#26368;&#36817;&#23558;&#35299;&#30721;&#22120;&#27169;&#22411;&#25193;&#23637;&#33267;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#36235;&#21183;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#19982;MLM-based&#32534;&#30721;&#22120;&#30456;&#25239;&#34913;&#12290;&#23613;&#31649;&#35268;&#27169;&#25193;&#22823;&#20102;&#23427;&#20204;&#22312;NLU&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#24207;&#21015;&#26631;&#27880;&#65288;SL&#65289;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#20110;SOTA&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;SL&#24615;&#33021;&#26159;&#30001;&#20854;&#22266;&#26377;&#30340;&#38480;&#21046;&#20915;&#23450;&#30340;&#36824;&#26159;&#21487;&#20197;&#25913;&#36827;&#30340;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25913;&#36827;"&#24320;&#25918;&#24335;"LLMs&#65288;Llama2&#21644;Mistral&#65289;&#22312;IE&#20219;&#21153;&#20013;&#30340;SL&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;&#22359;&#32452;&#20869;&#30340;&#21452;&#21521;&#20449;&#24687;&#27969;&#65292;&#24212;&#29992;&#20102;&#23618;&#27425;&#36880;&#23618;&#31227;&#38500;&#25110;&#21551;&#29992;&#22240;&#26524;&#25513;&#30721;&#65288;CM&#65289;&#36827;&#26469;LLM&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models based on masked language modeling (MLM) objective excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, a recent trend of scaling decoder models to multiple billion parameters resulted in large language models (LLMs), making them competitive with MLM-based encoders. Although scale amplifies their prowess in NLU tasks, LLMs fall short of SOTA results in information extraction (IE) tasks, many framed as sequence labeling (SL). However, whether this is an intrinsic limitation of LLMs or whether their SL performance can be improved remains unclear. To address this, we explore strategies to enhance the SL performance of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional information flow within groups of decoder blocks, applying layer-wise removal or enforcement of the causal mask (CM) during LLM fine-tuning. This approach yields
&lt;/p&gt;</description></item><item><title>VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.13649</link><description>&lt;p&gt;
VisualWebArena: &#22312;&#30495;&#23454;&#35270;&#35273;Web&#20219;&#21153;&#19978;&#35780;&#20272;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13649
&lt;/p&gt;
&lt;p&gt;
VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#35745;&#21010;&#12289;&#25512;&#29702;&#21644;&#25191;&#34892;&#21160;&#20316;&#30340;&#33258;&#20027;&#20195;&#29702;&#20026;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#65292;&#22312;&#25928;&#26524;&#19978;&#24573;&#35270;&#20102;&#35768;&#22810;&#38656;&#35201;&#35270;&#35273;&#20449;&#24687;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#30340;&#33258;&#28982;&#20219;&#21153;&#12290;&#37492;&#20110;&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#30028;&#38754;&#26159;&#20026;&#20154;&#31867;&#24863;&#30693;&#32780;&#35774;&#35745;&#30340;&#65292;&#35270;&#35273;&#20449;&#24687;&#24448;&#24448;&#20197;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#30340;&#26041;&#24335;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VisualWebArena&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;VisualWebArena&#21253;&#25324;&#19968;&#32452;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#22522;&#20110;Web&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#33258;&#20027;&#22810;&#27169;&#24577;&#20195;&#29702;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#25191;&#34892;&#65292;&#20195;&#29702;&#38656;&#35201;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#20197;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an ext
&lt;/p&gt;</description></item><item><title>Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2401.10774</link><description>&lt;p&gt;
Medusa: &#22810;&#35299;&#30721;&#22836;&#30340;&#31616;&#27905;LLM&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10774
&lt;/p&gt;
&lt;p&gt;
Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#21463;&#38480;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#32570;&#22833;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#25805;&#20316;&#21463;&#38480;&#20110;&#21152;&#36895;&#22120;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#33719;&#24471;&#21644;&#32500;&#25252;&#29420;&#31435;&#30340;&#33609;&#31295;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#65292;&#20197;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#12290;Medusa&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#21516;&#26102;&#26500;&#36896;&#22810;&#20010;&#20505;&#36873;&#24310;&#32493;&#24182;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#65292;Medusa&#22312;&#21333;&#27493;&#24310;&#36831;&#26041;&#38754;&#20165;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#35299;&#30721;&#27493;&#39588;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10186</link><description>&lt;p&gt;
&#36229;&#36234;&#22522;&#20110;&#21442;&#32771;&#25351;&#26631;&#65306;&#20998;&#26512;&#24320;&#25918;&#24335;LLM&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#19978;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10186
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#36830;&#36143;&#21644;&#30456;&#20851;&#25991;&#26412;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#38450;&#27490;&#22522;&#20934;&#27844;&#38706;&#21040;LLM&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;Quintd-1:&#19968;&#20010;&#20026;5&#20010;&#25968;&#25454;&#21040;&#25991;&#26412;(D2T)&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#19987;&#38376;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;&#20844;&#20849;API&#20013;&#25910;&#38598;&#30340;&#26631;&#20934;&#26684;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#35760;&#24405;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#21442;&#32771;&#35780;&#20272;&#25351;&#26631;&#21644;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#20889;&#20316;&#21442;&#32771;&#36164;&#26009;&#30340;&#24773;&#20917;&#19979;&#27979;&#35797;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;&#22312;token&#32423;&#21035;&#19978;&#23545;&#35821;&#20041;&#20934;&#30830;&#24615;&#38169;&#35823;&#36827;&#34892;&#27880;&#37322;&#65292;&#32467;&#21512;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#22522;&#20110;GPT-4&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;7B&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#24320;&#25918;&#24335;LLMs&#21487;&#20197;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#65292;80%&#30340;&#36755;&#20986;&#23384;&#22312;&#35821;&#20041;&#20934;&#30830;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06688</link><description>&lt;p&gt;
&#19981;&#35201;&#25490;&#21517;&#65292;&#35201;&#21512;&#24182;&#65281;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#26469;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36890;&#36807;&#32473;&#23450;&#28304;&#21477;&#23376;&#20272;&#35745;&#30446;&#26631;&#21477;&#23376;&#30340;&#27010;&#29575;&#65292;&#20294;&#36825;&#20123;&#20272;&#35745;&#21487;&#33021;&#19982;&#20154;&#31867;&#21916;&#22909;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QE-fusion&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#30340;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#65288;QE&#65289;&#26469;&#32508;&#21512;&#25913;&#36827;&#32763;&#35793;&#32467;&#26524;&#12290;QE-fusion&#21033;&#29992;&#20174;&#27169;&#22411;&#20013;&#25277;&#21462;&#30340;&#20505;&#36873;&#27744;&#65292;&#20351;&#29992;&#20687;CometKiwi&#36825;&#26679;&#30340;QE&#25351;&#26631;&#32452;&#21512;&#19981;&#21516;&#20505;&#36873;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;QE-fusion&#19982;&#27874;&#26463;&#25628;&#32034;&#21644;&#26368;&#36817;&#30340;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#65288;&#22914;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#25110;QE-&#37325;&#26032;&#25490;&#24207;&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#24403;&#24212;&#29992;&#20110;&#29992;&#20110;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PolyLM&#12289;XGLM&#12289;Llama2&#21644;Mistral&#65289;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#65288;NLLB&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COMET&#21644;BLEURT&#35780;&#20998;&#26041;&#38754;&#22987;&#32456;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#23545;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#26356;&#22823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06568</link><description>&lt;p&gt;
&#22312;&#28304;&#35821;&#35328;&#20013;&#36855;&#22833;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23545;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#20173;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#32763;&#35793;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#28085;&#30422;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#21644;&#27169;&#22411;&#31867;&#22411;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#24182;&#37319;&#29992;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26469;&#21306;&#20998;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#30340;&#23454;&#29992;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;LLMs&#36827;&#34892;&#20102;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;&#30340;&#20803;&#35780;&#20272;&#65292;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#36824;&#26263;&#31034;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#21442;&#32771;&#20449;&#24687;&#26469;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential
&lt;/p&gt;</description></item><item><title>&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05749</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#37117;&#26159;&#26426;&#22120;&#32763;&#35793;&#30340;&#65306;&#26469;&#33258;&#22810;&#21521;&#24182;&#34892;&#24615;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05749
&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20114;&#32852;&#32593;&#19978;&#30340;&#20869;&#23481;&#32463;&#24120;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#19988;&#36825;&#20123;&#22810;&#21521;&#32763;&#35793;&#30340;&#20302;&#36136;&#37327;&#34920;&#26126;&#23427;&#20204;&#24456;&#21487;&#33021;&#26159;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21019;&#24314;&#30340;&#12290;&#22810;&#21521;&#24182;&#34892;&#30340;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#19981;&#20165;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#19988;&#26500;&#25104;&#35813;&#35821;&#35328;&#20013;&#24635;&#20307;&#32593;&#39029;&#20869;&#23481;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#20869;&#23481;&#23384;&#22312;&#36873;&#25321;&#24615;&#20559;&#24046;&#65292;&#19982;&#23558;&#20302;&#36136;&#37327;&#33521;&#25991;&#20869;&#23481;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22823;&#35268;&#27169;&#32763;&#35793;&#25104;&#35768;&#22810;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#22312;&#32593;&#32476;&#19978;&#20174;&#21333;&#35821;&#21644;&#21452;&#35821;&#25968;&#25454;&#35757;&#32451;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31561;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04621</link><description>&lt;p&gt;
DebugBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32534;&#31243;&#33021;&#21147;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20043;&#21069;&#23545;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#35780;&#20272;&#21463;&#21040;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#27979;&#35797;&#28431;&#27934;&#31181;&#31867;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DebugBench&#8221;&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#21253;&#21547;4253&#20010;&#23454;&#20363;&#12290;&#23427;&#28085;&#30422;&#20102;C ++&#65292;Java&#21644;Python&#20013;&#22235;&#20010;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#21035;&#21644;18&#20010;&#27425;&#35201;&#31867;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;DebugBench&#65292;&#25105;&#20204;&#20174;LeetCode&#31038;&#21306;&#25910;&#38598;&#20102;&#20195;&#30721;&#29255;&#27573;&#65292;&#20351;&#29992;GPT-4&#21521;&#28304;&#25968;&#25454;&#20013;&#27880;&#20837;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#20004;&#20010;&#21830;&#19994;&#27169;&#22411;&#21644;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#22914;Code Llama&#26080;&#27861;&#36798;&#21040;&#20219;&#20309;&#21512;&#26684;&#29575;&#65307;&#65288;2&#65289;t
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#29983;&#25104;&#36830;&#36143;&#30340;&#24605;&#32500;&#38142;&#26465;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#23427;&#19982;&#30495;&#23454;&#35821;&#35328;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.13571</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20309;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#24605;&#32500;&#38142;&#26465;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Can Large Language Models Generate Correct Chain-of-Thoughts?. (arXiv:2310.13571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#29983;&#25104;&#36830;&#36143;&#30340;&#24605;&#32500;&#38142;&#26465;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#23427;&#19982;&#30495;&#23454;&#35821;&#35328;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#21160;&#23545;&#24605;&#32500;&#38142;&#26465;&#24341;&#21457;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#35825;&#23548;LLM&#29983;&#25104;&#36830;&#36143;&#30340;&#24605;&#32500;&#38142;&#26465;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#20004;&#32423;&#20998;&#23618;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#35828;&#26381;&#21147;&#30340;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#34913;&#37327;LLM&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#26465;&#19982;&#30495;&#23454;&#35821;&#35328;&#26469;&#28304;&#30340;&#24605;&#32500;&#38142;&#26465;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;LLM&#33021;&#22815;&#20135;&#29983;&#27491;&#30830;&#30340;&#24605;&#32500;&#24207;&#21015;&#65288;&#21487;&#33021;&#65289;&#35299;&#37322;&#20102;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#21319;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.
&lt;/p&gt;</description></item><item><title>AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10195</link><description>&lt;p&gt;
AdaLomo: &#20302;&#20869;&#23384;&#20248;&#21270;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10195
&lt;/p&gt;
&lt;p&gt;
AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#35268;&#27169;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#20869;&#23384;&#65292;&#20174;&#32780;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;LOMO&#65289;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#20854;&#20248;&#21270;&#25216;&#26415;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#24182;&#23637;&#29616;&#20986;&#27425;&#20248;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#27861;&#19982;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;AdamW&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#23545;Adam&#20248;&#21270;&#22120;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#21160;&#37327;&#26469;&#35828;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#23545;&#20110;&#24357;&#21512;&#24046;&#36317;&#26356;&#20026;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;AdaLomo&#65289;&#65292;&#20026;&#27599;&#20010;&#21442;&#25968;&#25552;&#20379;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#20026;&#20102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#29366;&#24577;&#20013;&#37319;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26469;&#20272;&#35745;&#20108;&#38454;&#30697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20998;&#32452;&#26356;&#26032;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update norma
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00160</link><description>&lt;p&gt;
&#33258;&#25105;&#29305;&#21270;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#20154;&#31867;&#32534;&#20889;&#30340;&#31181;&#23376;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#25945;&#23398;&#25968;&#25454;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23545;&#40784;&#20197;&#36981;&#24490;&#19968;&#33324;&#25351;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#20851;&#27880;&#19968;&#33324;&#23545;&#40784;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#19987;&#23478;&#39046;&#22495;&#29305;&#21270;&#30340;&#33258;&#25105;&#23545;&#40784;&#65288;&#20363;&#22914;&#65292;&#29983;&#29289;&#21307;&#23398;&#65289;&#65292;&#21457;&#29616;&#23427;&#23545;&#20110;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#38750;&#24120;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#23545;&#40784;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#20869;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#8220;&#36890;&#29992;&#8221;&#25351;&#31034;&#36319;&#38543;&#35757;&#32451;&#23545;&#19979;&#28216;&#19987;&#23478;&#39046;&#22495;&#24615;&#33021;&#30340;&#36793;&#38469;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#25105;&#29305;&#21270;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#36807;&#31243;&#12290;&#24403;&#36890;&#36807;&#26816;&#32034;&#26469;&#20943;&#23569;&#20135;&#29983;&#24187;&#35273;&#24182;&#25552;&#39640;&#23545;&#40784;&#30340;&#24182;&#21457;&#24615;&#21518;&#65292;&#33258;&#25105;&#29305;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15402</link><description>&lt;p&gt;
&#20851;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65306;&#36827;&#23637;&#12289;&#21069;&#27839;&#21644;&#26410;&#26469;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#20180;&#32454;&#24191;&#27867;&#22320;&#27010;&#36848;&#20102;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#29992;&#8220;X-of-Thought&#8221;&#26469;&#25351;&#20195;&#24191;&#20041;&#19978;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32452;&#32455;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#30340;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24605;&#32500;&#38142;&#22312;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#39046;&#22495;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22810;&#27169;&#24577;&#21644;&#29702;&#35770;&#31561;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#25104;&#20026;&#23547;&#27714;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
&lt;/p&gt;</description></item><item><title>CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.09552</link><description>&lt;p&gt;
CB-Whisper: &#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;Whisper
&lt;/p&gt;
&lt;p&gt;
CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09552
&lt;/p&gt;
&lt;p&gt;
CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21644;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#32463;&#24120;&#36935;&#21040;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;Contextual Biasing Whisper&#65288;CB-Whisper&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#25191;&#34892;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#26469;&#35782;&#21035;&#29992;&#25143;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#35782;&#21035;&#20986;&#30340;&#23454;&#20307;&#34987;&#29992;&#20316;Whisper&#35299;&#30721;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;OV-KWS&#21644;ASR&#20219;&#21153;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21407;&#22987;Whisper&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20013;&#22269;Aishell&#28909;&#35789;&#23376;&#38598;&#21644;&#20004;&#20010;&#20869;&#37096;&#20195;&#30721;&#20999;&#25442;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#21484;&#22238;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#30053;&#24494;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08047</link><description>&lt;p&gt;
&#35843;&#26597;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Gender Bias in News Summarization. (arXiv:2309.08047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#36848;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#12290;&#20197;&#24448;&#23545;&#27010;&#36848;&#27169;&#22411;&#30340;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#23427;&#20204;&#22312;&#20869;&#23481;&#36873;&#25321;&#12289;&#35821;&#27861;&#27491;&#30830;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;LLMs&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#19968;&#20010;&#30456;&#23545;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#65292;&#27604;&#22914;&#27010;&#36848;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#23545;&#27169;&#22411;&#30340;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#21527;&#65311;&#20026;&#20102;&#35299;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#27010;&#36848;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#30340;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#23454;&#38469;&#26041;&#27861;&#26469;&#37327;&#21270;&#23427;&#20204;&#12290;&#30001;&#20110;&#25105;&#20204;&#21457;&#29616;&#36755;&#20837;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#24178;&#25200;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#20180;&#32454;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#28982;&#20351;&#29992;&#19968;&#20123;&#29616;&#23454;&#30340;&#36755;&#20837;&#25991;&#26723;&#36827;&#34892;&#24037;&#20316;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#38376;&#26500;&#24314;&#30340;&#27010;&#36848;&#27169;&#22411;&#21644;&#36890;&#29992;&#29992;&#36884;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their performance in content selection, grammaticality and coherence. However, it is well known that LLMs reproduce and reinforce harmful social biases. This raises the question: Do these biases affect model outputs in a relatively constrained setting like summarization?  To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical measures to quantify them. Since we find biases inherent to the input document can confound our analysis, we additionally propose a method to generate input documents with carefully controlled demographic attributes. This allows us to sidestep this issue, while still working with somewhat realistic input documents.  Finally, we apply our measures to summaries generated by both purpose-built summarization models and general purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06054</link><description>&lt;p&gt;
&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23545;&#21512;&#25104;&#20219;&#21153;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#26159;Transformer&#30340;&#19968;&#39033;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#34920;&#31034;&#26356;&#21152;&#22797;&#26434;&#65292;&#34920;&#31034;&#21487;&#20197;&#21463;&#21040;&#27169;&#22411;&#26435;&#37325;&#21644;&#19978;&#19979;&#25991;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#20004;&#20010;&#27010;&#24565;&#26041;&#38754;&#30340;&#34920;&#31034;&#20998;&#21035;&#31216;&#20026;&#26435;&#37325;&#20869;&#37096;&#25104;&#20998;&#21644;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20004;&#20010;&#25104;&#20998;&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#35774;&#35745;&#20004;&#20010;&#25506;&#38024;&#65292;&#21363;&#26435;&#37325;&#20869;&#37096;&#25506;&#38024;&#21644;&#19978;&#19979;&#25991;&#25506;&#38024;&#65292;&#20998;&#21035;&#35780;&#20272;&#36825;&#20004;&#20010;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#30340;&#22909;&#22351;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32416;&#32544;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12568</link><description>&lt;p&gt;
&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#23567;&#22411;&#24555;&#36895;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21548;&#20889;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#26631;&#28857;&#31526;&#21495;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#23548;&#33268;&#20102;&#23545;&#21548;&#20889;&#25253;&#21578;&#30340;&#35823;&#35299;&#12290;&#20026;&#20102;&#20351;&#29992;ASR&#25552;&#20379;&#31934;&#30830;&#21644;&#26131;&#25026;&#30340;&#20020;&#24202;&#25253;&#21578;&#65292;&#38656;&#35201;&#36827;&#34892;&#33258;&#21160;&#26631;&#28857;&#20462;&#22797;&#12290;&#32771;&#34385;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65288;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#65289;&#26469;&#25552;&#28860;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26631;&#28857;&#20462;&#22797;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25552;&#28860;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;10%&#30340;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;95%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.15593</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#27700;&#21360;&#23545;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#35777;&#29983;&#25104;&#39044;&#31639;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;&#38543;&#26426;&#27700;&#21360;&#23494;&#38053;&#35745;&#31639;&#30340;&#38543;&#26426;&#25968;&#24207;&#21015;&#26144;&#23556;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#26469;&#29983;&#25104;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#35201;&#26816;&#27979;&#27700;&#21360;&#25991;&#26412;&#65292;&#21482;&#35201;&#30693;&#36947;&#23494;&#38053;&#30340;&#20219;&#20309;&#19968;&#26041;&#37117;&#21487;&#20197;&#23558;&#25991;&#26412;&#19982;&#38543;&#26426;&#25968;&#24207;&#21015;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#37319;&#26679;&#26041;&#26696;&#26469;&#23454;&#20363;&#21270;&#27700;&#21360;&#26041;&#27861;&#65306;&#21453;&#21464;&#25442;&#37319;&#26679;&#21644;&#25351;&#25968;&#26368;&#23567;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27700;&#21360;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;OPT-1.3B&#12289;LLaMA-7B&#21644;Alpaca-7B&#65292;&#20197;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#23545;&#21508;&#31181;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;OPT-1.3B&#21644;LLaMA-7B&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25200;&#21160;&#20102;40-50%&#30340;&#35789;&#20803;&#21518;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#65288;$p \leq 0.01$&#65289;&#65292;&#21482;&#38656;&#35201;35&#20010;&#35789;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20869;&#23384;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;LOMO&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#20174;&#32780;&#38477;&#20302;&#30740;&#31350;&#38376;&#27099;&#12290;</title><link>http://arxiv.org/abs/2306.09782</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20869;&#23384;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;LOMO&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#20174;&#32780;&#38477;&#20302;&#30740;&#31350;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;GPU&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#36896;&#25104;&#30740;&#31350;&#38376;&#27099;&#39640;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#21363;&#24494;&#35843;&#25110;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#65292;&#20294;&#24456;&#23569;&#26377;&#20851;&#27880;&#22312;&#26377;&#38480;&#36164;&#28304;&#24773;&#20917;&#19979;&#20840;&#21442;&#25968;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;LOw-Memory Optimization&#65288;LOMO&#65289;, &#36890;&#36807;&#23558;&#26799;&#24230;&#35745;&#31639;&#21644;&#21442;&#25968;&#26356;&#26032;&#19968;&#27493;&#34701;&#21512;&#20197;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;&#36890;&#36807;&#23558;LOMO&#19982;&#29616;&#26377;&#30340;&#20869;&#23384;&#33410;&#30465;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#20869;&#23384;&#20351;&#29992;&#37327;&#38477;&#20302;&#21040;DeepSpeed&#26041;&#26696;&#30340;10.8&#65285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;65B&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22312;&#21482;&#38656;&#21333;&#21488;&#26426;&#22120;&#19978;&#25191;&#34892;&#65292;&#35813;&#26426;&#22120;&#25645;&#36733;8&#20010;RTX 3090&#65292;&#27599;&#20010;&#26174;&#23384;&#20026;24GB&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.14592</link><description>&lt;p&gt;
&#24102;&#35789;&#20856;&#30340;&#25351;&#20196;&#20248;&#21270;&#29992;&#20110;&#38646;&#26679;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#29992;&#20110;&#20256;&#36798;&#20316;&#32773;&#30340;&#24847;&#22270;&#21644;&#24577;&#24230;&#12290;&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#39118;&#26684;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#12290;&#21551;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23545;&#39118;&#26684;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35821;&#35328;&#39118;&#26684;&#21487;&#33021;&#24456;&#38590;&#23450;&#20041;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#39118;&#26684;&#35789;&#20856;&#20316;&#20026;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35782;&#21035;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#35789;&#20856;&#30340;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style is used to convey authors' intentions and attitudes. Despite the success of large pre-trained language models on style classification, prior work relies on fine-tuning with labeled examples. Prompting large language models to classify style without fine-tuning is challenging because language styles can be difficult to define. In this study, we investigate the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training. Our experiments show that lexicon-based instructions improve transfer zero-shot performance significantly. We will release our code and data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.08146</link><description>&lt;p&gt;
&#20320;&#25152;&#22312;&#31038;&#21306;&#21457;&#29983;&#20102;&#20160;&#20040;&#65311;&#19968;&#31181;&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#21457;&#29616;&#26412;&#22320;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#26032;&#38395;&#26159;&#24433;&#21709;&#29305;&#23450;&#22320;&#29702;&#21306;&#22495;&#65288;&#22914;&#22478;&#24066;&#12289;&#21439;&#21644;&#24030;&#65289;&#29992;&#25143;&#30340;&#26032;&#38395;&#23376;&#38598;&#12290;&#26816;&#27979;&#26412;&#22320;&#26032;&#38395;&#26159;&#20934;&#30830;&#22320;&#25512;&#33616;&#26412;&#22320;&#26032;&#38395;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21270;&#30340;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#12290;&#26412;&#25991;&#30528;&#37325;&#20171;&#32461;&#20102;&#31649;&#36947;&#30340;&#31532;&#19968;&#27493;&#39588;&#65306;&#65288;1&#65289;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#65288;2&#65289;&#21487;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#35774;&#32622;&#12290;&#19982;&#26031;&#22374;&#31119;CoreNLP NER&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#22312;&#32463;&#36807;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#35780;&#20272;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled datas
&lt;/p&gt;</description></item></channel></rss>